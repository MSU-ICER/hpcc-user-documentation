{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>OS Upgrade</p> <p>ICER has upgraded the HPCC operating system to Ubuntu. Please see our documentation to help you transition to the new operating system. Note that some pages on the documentation are still being updated to reflect the new system.</p>"},{"location":"#what-is-icer","title":"What is ICER?","text":"<p>The Institute for Cyber-Enabled Research (ICER) provides the cyberinfrastructure for researchers from across academia and industry to perform their computational research. ICER supports multidisciplinary research in all facets of computational sciences. ICER continually works to enhance MSU\u2019s national and international presence and competitive edge in disciplines and research that rely on advanced computing. The High Performance Computing Center (HPCC) is ICER's computational facility.</p>"},{"location":"#getting-access-to-the-hpcc","title":"Getting Access to the HPCC","text":"<p>For potential users with an MSU NetID, accounts must be requested by a MSU tenure-track faculty member. Researchers at partner institutions should use the mechanism specified by their institution's  agreement with MSU. For more information, see: Obtain an HPCC Account.</p>"},{"location":"#the-data-machine","title":"The Data Machine","text":"<p>ICER is offering access to a new resource for data-intensive computing for use in research and in the classroom. We are seeking users willing to experiment with the machine and offer feedback. Read more here.</p>"},{"location":"#cpu-and-gpu-time-limits","title":"CPU and GPU Time Limits","text":"<p>Non-buyin users are limited to\u00a0500,000 CPU hours (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every year (from January 1st to December 31st). More information is available at Job Policies.</p>"},{"location":"#buy-in-options","title":"Buy-in Options","text":"<p>With each cluster purchase, ICER offers researchers the opportunity to purchase buy-in nodes for priority access to the cluster.</p>"},{"location":"#questions","title":"Questions?","text":"<p>If you have questions please check our Frequently Asked Questions or use the search bar above. If you still can't find answers in this documentation, please submit a ticket.</p>"},{"location":"#online-helpdesk-hours","title":"Online Helpdesk Hours","text":"<p>Monday and Thursday, 1-2pm at our ICER Public Help Desk Channel . More information about virtual support is available.</p>"},{"location":"#hpcc-workshops-and-training","title":"HPCC Workshops and Training","text":"<p>Our monthly workshops and our D2L training materials cover introductions to Linux, HPCC, and some popular software tools. Check out our training calendar for scheduled events and our course content available on Desire2Learn. </p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We encourage HPCC users to acknowledge ICER and MSU in publications based on work that was done with HPCC resources. A sample statement:\u00a0\"This work was supported in part through computational resources and services provided by the Institute for Cyber-Enabled Research at Michigan State University.\" </p> <p>Let us know that we have been referenced, and we will link to your publication on our\u00a0publication site,\u00a0which will further increase the visibility of your work.</p>"},{"location":"2022-06-27_LabNotebook_AntiSmash_installation/","title":"(2022-06-27) Lab Notebook: AntiSMASH installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","AntiSMASH"]},{"location":"2022-06-27_LabNotebook_AntiSmash_installation/#lab-notebook-installing-antismash-on-hpcc-using-conda-2022-06-27","title":"Lab Notebook --- Installing AntiSMASH on HPCC using Conda (2022-06-27)","text":"<p>AntiSMASH (The antibiotics &amp; Secondary Metabolite Analysis Shell) is bioinformatics program for identifying genes belonging to secondary metabolite pathways, particularly in plant, fungi and bacteria species. Documentation for AntiSMASH can be found at https://docs.antismash.secondarymetabolites.org, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>Small analyses can be done through their webportal (see https://docs.antismash.secondarymetabolites.org/website_submission/), however if your analysis requires running AntiSMASH on the HPCC, please see the instruction below for how to install this program localy using Conda</p> <p>If you have not installed Conda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing AnitSMASH</p> <pre><code># Clear modules and load Conda\nmodule purge\nmodule load Conda/3\n\n# Add biconda to your Conda install\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n\n# Create a cona environemnt for antimash and its depdencies\nconda create -n antismash\nconda activate antismash \n\n# As of version 6.1, you need to install each of these \n# programs first before install antismash or it never resolves\n# Seems to be an issue with the conda recipie, not AntiSMASH itself\nconda install hmmer2\nconda install hmmer\nconda install diamond\nconda install fasttree\nconda install prodigal\nconda install blast\nconda install muscle\nconda install glimmerhmm\n\n# Once the above installs have complete, this command will\n# install antismash and update the versions of the above programs # as needed. \n# \n# You may be asked to 'DOWNGRADE' diamond, hmmer, muslce, \n# and some perl libraries; this is normal \nconda install antismash\n\n# Test your install\nantismash --check-prereqs\nantismash --help\n\n# Download databases\ndownload-antismash-databases\n</code></pre> <p>Once you have completed the above steps, to run AntiSMASH in the future, do:</p> <pre><code>module purge\nmodule load Conda/3\n\nconda activate antismash\n\n# check\nantismash --help\n</code></pre>","tags":["lab notebook","Conda","AntiSMASH"]},{"location":"2022-09-20_LabNotebook_Pymesh_installation_with_conda/","title":"(2022-09-20) Lab Notebook: PyMesh installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","PyMesh"]},{"location":"2022-09-20_LabNotebook_Pymesh_installation_with_conda/#lab-notebook-installing-pymesh-on-hpcc-using-conda-2022-09-20","title":"Lab Notebook --- Installing PyMesh on HPCC using Conda (2022-09-20)","text":"<p>PyMesh is a code base developed by Qingnan Zhou for his PhD research at New York University. It is a rapid prototyping platform focused on geometry processing. PyMesh is written with both C++ and Python, where computational intensive functionalities are realized in C++, and Python is used for creating minimalistic and easy to use interfaces. Documentation for AntiSMASH can be found at https://github.com/PyMesh/PyMesh, but you will need to follow the instruction below to install AntiSMASH in your home directory on HPCC.</p> <p>If you have not installed Conda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing Pymesh</p> <pre><code># Clear modules and load Conda\nmodule purge\nmodule load Conda/3\n\n# You need to create a conda environment with an older version of python\nconda create -n pymesh python=3.6\n\n# Activate the environment\nconda activate pymesh\n\n# Use the the 0.2.1 version because the 0.3 version on conda forge required\n# GLIBC 2.18 which is incompatible with the version (2.17) on HPCC (Centos7 thing I think)\nconda install pymesh2=0.2.1\n\n# Once done, test by trying to do 'import pymesh' in python\n</code></pre> <p>Why not Pymesh 0.30</p> <p>The version of Pymesh 0.3 on conda forge produces the following error when I try to import pymesh</p> <pre><code>ImportError: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by /mnt/home/panchyni/miniforge3/envs/pymesh/lib/python3.6/site-packages/pymesh/lib/libstdc++.so.6)\n</code></pre> <p>As far as I understand it, the GLIBC version is tied to the OS, so its not a s simple as importing another library. If for some reason version 0.3 is required, there are a couple of options, though they may be time consuming:</p> <ul> <li>Build PyMesh from source using the files from the github. There are a number of dependencies, so you might want to see if you can use a conda environment to install the dependencies</li> <li>Create a conda environment within an Singularity container of Centos8</li> </ul>","tags":["lab notebook","Conda","PyMesh"]},{"location":"2022-09-21_LabNotebook_VisIt/","title":"(2022-09-21) Lab Notebook: VisIt on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#lab-notebook-using-visit-on-hpcc-2022-09-21","title":"Lab Notebook --- Using VisIt on HPCC (2022-09-21)","text":"<p>VisIt is an Open Source, interactive, scalable, visualization, animation and analysis tool (see the  VisIt Docs for more detail)</p> <p>Currently, there are two challenges using VisIt on HPCC (1) failed installation due to conflicts with GLIBC and (2) running the GUI without it crashing immediately after loading.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#easy-solution-using-an-older-visit","title":"Easy Solution, using an older VisIt","text":"<p>To address the install issues, the easiest solution is to use an older (before 3.x) version of VisIt which can be found here. Use the version titled:</p> <pre><code>Linux - x86_64 64 bit\nRedhat Enterprise Linux 7.5, 4.18.9-1.el7.elrepo.x86_64 #1 SMP, gcc 4.8.5\n</code></pre> <p>Or use the following command on HPCC:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v2.13.3/visit2_13_3.linux-x86_64-rhel7.tar.gz\n</code></pre> <p>Once this file is donwloaded, unpack using tar -xzf and then begin and then begin an Interactive Desktop session via OnDemand. This step is necessary because running VisIt from the command line will cause the GUI interface to crash immediately after loading (tested with XQuartz on Mac and MobXterm on Windows). Instead, after creating an Interactive Desktop,navigate to the unpacked VisIt folder, enter the bin folder within, and click on the fie named 'visit'. This should start VisIt within the Interactive Desktop.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#intermediate-solution-using-rhel7-visit-33-with-modules","title":"Intermediate Solution, using RHEL7 Visit 3.3 with modules","text":"<p>Following from the above solution, I realized that the new (as of writing) version of RHEL7 should work on HPCC without the GLIBC issue, but it doesn't out of the box. However, the fix wasn't difficult once I figured out that right module to load on HPCC, though the combinaton with the need to use OnDemand makes things a little clunky.</p> <p>First, get the RHEL7 3.3 version of VisIt with Mesa from their website, or use the following command:</p> <pre><code>wget https://github.com/visit-dav/visit/releases/download/v3.3.0/visit3_3_0.linux-x86_64-rhel7-wmesa.tar.gz\n</code></pre> <p>Once this file is downloaded, unpack using tar -xzf and then begin an Interactive Desktop session via OnDemand. This step is STILL necessary because running this version of VisIt from the command line ALSO will cause the GUI interface to crash though this time it waits until you try to draw something (tested with XQuartz on Mac).</p> <p>Because we need to use some module, once the Interactive Desktop is started, open a terminal and navigate to the folder where VisIt was unpacked and go to the 'bin' directory. There, run the following commands:</p> <pre><code>module load GCCcore/11.2.0\nmodule load PCRE2/10.37\n</code></pre> <p>Then, if you are in the bin folder, you can start Visit using:</p> <pre><code>./visit\n</code></pre> <p>I think this need to be done in the same terminal window for VisIt to start in the right environment (i.e. with the modules loaded)</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#future-getting-centos8-visit-to-run-on-hpcc","title":"Future: Getting CentOS8 Visit to run on HPCC","text":"<p>I've encountered several problem trying to getting any version VisIt 3.x to work on HPCC (tested 3.0 and the current as of now 3.3), though it turns out this was because I was using the CentOS8 version and RHEL7 is close enough to what we use that the workaround is not bad (see above). Using the prebuilt CentOS8 binary files will not work because they are looking for a version of GLIBC later than version 2.17 which is built into CentOS7. </p> <p>I am looking into this becaue I have encountered this issue before with other easily accessible installs of user requested software (see Pymesh on Conda) and might be a useful method to test software prior to a future OS update on HPCC. As such, I have been using Visit to explore ways to try to install CentOS8 built software on HPCC. Currently, the idea is to use a prebuilt binary on a Singualirty container of Centos8, though when I do this it cannot find the Qt5 library that comes with VisIt or, sometimes, an X11 library(might be able to fix this by messing aroubd with library paths within Singularity)</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-21_LabNotebook_VisIt/#why-no-just-build-visit-from-source-wouldnt-that-be-easier","title":"Why no just build Visit from source, wouldn't that be easier?","text":"<p>Short answer, no.</p> <p>Long answer, when I try to build VisIt 3.x from source, it fails to build its own version of Qt5 (cannot find the xkbcommon library on the system even though its there) and will not recognize Qt5 on the system. Might be related to the library issues trying to work in Singularity, not sure.</p>","tags":["lab notebook","VisIt"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/","title":"(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#lab-notebook-using-an-alternate-os-via-singularity-2022-09-23","title":"Lab Notebook --- Using an alternate OS via Singularity (2022-09-23)","text":"<p>Occasionally, I run into a peice of software which needs a newer version of GLIBC (&gt; 2.17) to be installed. Rather than try to upgrade GLIBC (which seems complicated and can go very poorly, see this thread for a general idea), its alot easier to run a different OS image in a container and run the software there. For this, we will use Singularity, which functions similarly to Docker, but is available on HPCC because unlike Docker it doesn't give you root privledges on the host system. Futhremore, its alot easier to work with than a virtual box (used them in the past, they can be fun but a lot of overhead + issues sharing files). Finally, you can run environment managers like Conda with Singularity to layer package management on top of the alternate OS, or do things like work with a downgraded python, all without affecting any system level install.</p> <p>Below, I will go through obtaining a CentOS8 image which has an updated GLIBC, building an overlay, and installing conda in that overlay, and building a conda environment for pymesh (version 0.3) as an example of this process</p> <p>Note</p> <p>This is very brief and direct explanation of installing something in a Singularity image. A more general explanation of Singularity images, overlay, etc will be added in the future (probably as  another Notebook)</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#getting-a-centos8-image","title":"Getting a CentOS8 image","text":"<p>You can pull a Docker built image directly using Singularity. The code below will grab a CentOS8 image with a new GLIBC which is needed from the pymesh 0.3 version on conda.</p> <pre><code># This works for now but\n# Probably want to update/find different OS since centos8 hit end of life at the end of 2021, probably centOS9\n# As of current, most similar OS with GLIBC &gt; 2.17\nsingularity pull centos8.sif docker://centos:8.3.2011\n</code></pre> <p>I keep my images in a folder called \"singularity_pull_images\" so, below replace \"../singularity_pull_images\" with whever your images are kept</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#pymesh-03-in-centos8-container","title":"Pymesh 0.3 in CentOS8 Container","text":"<p>Now what we have an image, we can make an 'overlay' which is basically a 'filesystem inside a singe file'. This both cuts down on the number of files used on your home directory AND help silo off environments/installs/etc that are associated with a particular Singularity image (although this last step isn't really necessary, but it helps). Also, you generally have more acess to the filesystem in the overlay, meaning you can install things outside of your home directory in the overlay.</p> <p>For now, we will make an overlay, install conda in that overlay and then use the overlay to contain all the files for pymesh and its dependencies</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#setup-singularity-container","title":"Setup Singularity Container","text":"<pre><code># The following commands are in powertools, which should be loaded by default; otherwise you will need to run 'module load powertools'\noverlay_build 5 overlay.img\noverlay_install_conda overlay.img https://github.com/conda-forge/miniforge/releases/download/24.3.0-0/Miniforge3-24.3.0-0-Linux-x86_64.sh ../singularity_pull_images/centos8.sif \n\n# Now we access our overlay within our CentOS8 image\noverlay_start overlay.img ../singularity_pull_images/centos8.sif \n\n# After this, your prompt should be 'Singularity&gt;'\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#install-pymesh-within-the-container","title":"Install Pymesh within the container","text":"<p>Now that we are inside of our container, we can install pymesh. Using the CentOS8 image, we shouldn't get a GLIBC error, but we still need an older version of python (CentOS8 uses 3.9, pymesh needs 3.6). So, we will make conda enviroment with an older python (which is alot easier that trying to downgrade the system python of the Singularity image)</p> <pre><code># Need an older version of python for pymesh 0.3\nconda create -n pymesh2 python=3.6 pymesh2=0.3    \n\n# Now, activate our new environmet\nconda init\nsource ~/.bashrc                                  # Need to manually update shell post conda init\nconda activate pymesh2\n\n# After this your prompt should be '(pymesh2) Singularity&gt;'\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#check-install-using-nose","title":"Check install using nose","text":"<p>Now, within our conda environment, within our Singualrity container, lets install the nose package (unit testing in python) and run the test from the pymesh github (https://github.com/PyMesh/PyMesh)</p> <pre><code># From within the pymesh2 environment\nconda install nose\npython -c \"import pymesh; pymesh.test()\"\n\n# You should get a couple warning and a couple S (skipped) tests, pretty sure this is fine\n</code></pre>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-23_LabNotebook_Singularity_Using_an_alternate_OS_/#endnote","title":"Endnote","text":"<p>This solution might seem (overly) complicated, because at the end we are running a conda environment inside of Singularity container. However, this allows us to use an older python version (via conda) and an alternative OS/GLIBC (via Singularity), either of which is an issue which is difficult to solve (downgrade python/upgrade GLIBC) and which it is not difficult to really screw things up by attempting</p>","tags":["lab notebook","Singularity","CentOS 8"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/","title":"(2022-09-24) Lab Notebook: Singularity Overlays","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#singularity-overlays-2022-09-24","title":"Singularity Overlays (2022-09-24)","text":"<p>Singularity is a versatile tool to give researchers more flexibility installing software and running their workflows on the HPC.   Most workflows don't need Singularity but it can be extremely helpful to solve certain weridly difficult problems.  Some common examples for researchers using singularity on the HPC include:</p> <ul> <li>Installing software that needs a special/different base operating system.</li> <li>Installing software that requires administrator privileges (aka root, su and/or sudo). </li> <li>Installing complex dependency trees (like python and R)</li> <li>Using existing software inside a pre-built virtual machine.</li> <li>Working with lots of tiny files on the HPC filesystems which are better designed for smaller numbers of big files. </li> <li>Building workflows that can easily move between different resources.</li> </ul> <p>NOTE This overview is specific to the High Performance Computing Center (HPCC) at Michigan State University (MSU).  For a complete tutorial see the Singularity documentation.  This overview assumes that you have an HPCC account and know how to navigate to and use a development node. </p> <p>The remainder of this overview will walk you through the first steps in using Singularity. However, if you want to skip the details just try running the following three powertools commands.  The first two will create a read/writable overlay and install miniforge. The third one will start this overlay inside a CentOS singularity image.  You will only need to run the first two commands once to build the overlay file (with conda), then you can just use the third command anytime you want to start the overlay:</p> <pre><code>overlay_build\noverlay_install_conda\noverlay_start\n</code></pre> <p>To exit singularity just type <code>exit</code>.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-1-get-a-singularity-image","title":"Step 1: Get a singularity image","text":"<p>As a starting point we need a singularity image, also known as a container or virtual machine.  You can think of a singularity image as a \"software hard drive\" that contains an entire operating system in a file. There are three main ways to get these images:</p> <ol> <li>Use one of the Singularity images already on the HPCC. </li> <li>Download an image form one of the many online libraries.</li> <li>Build your own image.</li> </ol> <p>If you don't know which one of the above to use, I recommend that you pick number 1 and just use the singularity image we already have on the system.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#1-use-one-of-the-singularity-images-already-on-the-hpcc","title":"1. Use one of the Singularity images already on the HPCC","text":"<p>For this introduction, we can keep things simple and just use one of the Singularity images already on the HPCC. This image runs CentOS 7 Linux and is a good starting point.  Use the following command to start singularity in a \"shell\" using the provided image:</p> <pre><code>singularity shell --env TERM=vt100 /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>Once you run this command you should see the \"Singularity\" prompt which will look something like the following:</p> <pre><code>Singularity&gt;\n</code></pre> <p>You did it!  You are running a different operating system (OS) than the base operating system.  All of the main HPCC folders are still accessible from this \"container\" (ex. /mnt/home, /mnt/research, /mnt/scratch/, etc) so it shouldn't look much different than before (except for the different prompt and you no longer have access to some of the base HPCC software). </p> <p>At this point, if you know what you need, you should be able use files in your home directory and it will compile/run using the singularity OS instead of the base OS.</p> <p>NOTE: You can just install software in your <code>/mnt/home/$USER</code> and/or <code>/mnt/research</code> folders. The software you install will probably only work from \"inside\" this singularity image. However, you will also be able to see and manipulate the files from within your standard HPC account.  This is fine for many researchers but I recommend you jump down to \"Step 3: Overlays\" to make Singularity even more flexible. </p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#2-download-an-image-form-one-of-the-many-online-libraries","title":"2. Download an image form one of the many online libraries","text":"<p>Many people publish singularity images and post them on public \"libraries\" for easy install.  Here is a list of online libraries you can browse (this section of the tutorial may need more work, not all of these may work on the HPCC):</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#sylabs-library","title":"Sylabs Library","text":"<p>Link to Browse Sylabs example:</p> <pre><code>singularity pull alpine.sif library://alpine:latest\n\nsingularity shell alpine.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#docker-hub","title":"Docker Hub","text":"<p>Link to Browse Docker Hub example:</p> <pre><code>singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest\n\nsingularity shell tensorflow.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#singularity-hub-aka-shub","title":"Singularity Hub (aka shub)","text":"<p>Link to Browse Singularity Hub example:</p> <pre><code>singularity pull shub_image.sif shub://vsoch/singularity-images\n\nsingularity shell shub_image.sif\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#3-build-your-own-image","title":"3. Build your own image","text":"<p>This one is more complex and outside the scope of the overview. However, if you are interested I recommend you try using the build command with a Docker image since it is fairly easy to install on your personal computer. Here is a link to how to use docker to make a singularity image:</p> <ul> <li>Link to singularity build command</li> </ul>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-2-running-commands-in-singularity","title":"Step 2: Running commands in Singularity","text":"<p>In Step 1 we showed you how to start a singularity \"shell\".  You can also just \"execute\" a command inside the singularity image and return the results.  For example, to run </p> <pre><code>singularity exec /opt/software/CentOS.container/7.4/bin/centos &lt;&lt;COMMAND&gt;&gt;\n</code></pre> <p>Where you replace <code>&lt;&lt;COMMAND&gt;&gt;</code> with whatever command you need to run.  This option will become very helpful when you want to run singularity inside a submission script \"See Step 4\" below.</p> <p>For example, the <code>df -hT</code> command will report file system disk space usage. So running the <code>df -hT</code> will give a different result running inside or outside a singularity image. You can test this using the following commands:</p> <pre><code>df -hT\n\nsingularity exec /opt/software/CentOS.container/7.4/bin/centos df -hT\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-3-overlays","title":"Step 3: Overlays","text":"<p>One problem we often encounter on the HPCC is \"lots-of-small-files\" (hundreds of files where each one is &lt; 50MB).  The filesystem is optimized for large files.  Lots of small files end up \"clogging\" things up which can slow things down for everyone.  One useful trick of singularity is you can make a single large file called an \"Overlay\" which can be attached to a singularity session. You can use an Overlay as a \"filesystem inside a single file\" where you can store lots of the small files inside the single overlay file. From the user point of view you can have as many small files as you want accessible from the singularity image (within reasonable limits). However, these small files act as a single file from the HPCC point of view and doesn't clog things up.</p> <p>This technique is really helpful if you are using complex software installs such as lots of Python, R or Conda installs.  It can also be helpful if your research data is lots of small files. </p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#make-your-overlay-file","title":"Make your overlay file","text":"<p>Making an overlay is not hard but takes multiple steps. For details on how to make an overlay we recommend viewing the singularity overlay documentation.</p> <p>Fortunately the HPCC has a \"powertool\" that can make a basic overlay for you.  All you need to do is run the following command:</p> <pre><code>overlay_build\n</code></pre> <p>This overlay can be applied to a singularity image using the <code>--overlay</code> option as follows:</p> <pre><code>singularity shell --overlay overlay.img --env TERM=vt100 /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>If you have an overlay called <code>overlay.img</code> in your current directory you can use the following powertool shortcut to run it inside the CentOS image:</p> <pre><code>overlay_start\n</code></pre> <p>You can also view the amount of filespace available on an overlay (using the <code>df -hT</code> command we used above) by using the following powertool:</p> <pre><code>overlay_size\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#writing-to-your-overlay","title":"Writing to your overlay","text":"<p>Once you are in the singularity shell you can now write to the overlay as if you were adding files to the \"root\" directory (/).  For example, running the following commands from inside of your singularity image should allow you to install miniforge3: </p> <pre><code>wget https://github.com/conda-forge/miniforge/releases/download/24.3.0-0/Miniforge3-24.3.0-0-Linux-x86_64.sh\n\n./Miniforge3-24.3.0-0-Linux-x86_64.sh -b -p /miniconda3/\n</code></pre> <p>Since we install miniforge3 a lot there is yet another powertool that will do this installation for you. Just run the following command:</p> <pre><code>overlay_install_conda\n</code></pre> <p>Once miniforge is installed in the <code>/miniforge3/</code> directory you need to add the folder <code>/miniforge3/</code> to the path with the following command:</p> <pre><code>export PATH=/miniforge3/bin:$PATH\n\nconda --init\n</code></pre> <p>Or, just use the powertool from before and it will automatically add <code>/miniforge3</code> to your path:</p> <pre><code>overlay_start\n</code></pre> <p>At this point you can use <code>pip</code> and <code>conda</code> installs as much as you like.  These generate hundreds of small files but it doesn't matter because everything will be stored in the overlay.img file as one big file.  </p> <p>To exit singularity just type <code>exit</code>. To start your overlay image just type <code>overlay_start</code></p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#step-4-submitting-jobs","title":"Step 4: Submitting Jobs","text":"<p>Once we have our image and our conda overlay working in a development node we can execute a script inside of the singularity image, \"in batch mode\" using the <code>exec</code> from above. For example, this script uses our miniforge installed overlay and runs the python3 script called \"mypython.py\" which is stored in my home directory on the HPCC.</p> <pre><code>singularity exec --overlay overlay.img /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> <p>Once the above is running on a development node we can just submit this as a job to the HPCC using the following submissions script:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\nsingularity exec --overlay overlay.img --env PATH=/miniforge3/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sysbin/ /opt/software/CentOS.container/7.4/bin/centos python3 /mnt/home/$USER/mypython.py\n</code></pre> <p>Again, we have a powertool to help clean this up for common workflows.  Using the <code>overlay_exec</code> command you can simply the above submission script using the following:</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCH -c 1\n#SBATCH -N 1\n\noverlay_exec python3 /mnt/home/$USER/mypython.py\n</code></pre>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#job-arrays","title":"Job Arrays","text":"<p>If you need to have multiple jobs running the same software (such as for a job array). You can't have them all writing to the same overlay file. To resolve this issue, there are two steps:</p> <ul> <li>embedding the overlay into the Singularity image</li> <li>and loading the image as temporarily writable.</li> </ul> <p>As a result, the changes you make to the system files (like <code>/miniforge3/</code>) will not persist after you are finished. You should make sure that any changes you want to keep (like important output files) are stored in a <code>/mnt/</code> directory that's shared with the HPCC, like <code>/mnt/home/$USER</code> or a <code>/mnt/research</code> space.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#embedding-the-overlay","title":"Embedding the overlay","text":"<p>First, we will embed the overlay into the image file. This links the image and with a copy of the overlay, so that any time the image is used the overlay copy will be brought along without needing to be specified.</p> <p>To do this, you first need a copy of the image file that you want to use with your overlay. In the case above where you are using the CentOS container that is already on the HPCC, you can create a copy called <code>centos7.sif</code> in your current working directory with</p> <pre><code>singularity build centos7.sif /opt/software/CentOS.container/7.4/bin/centos\n</code></pre> <p>Now, you should create an overlay or use the one created in the steps above:</p> <pre><code>overlay_build\n</code></pre> <p>Note</p> <p>After this step, many of the powertools like <code>overlay_exec</code> and <code>overlay_start</code> will not work correctly since they automatically use the <code>/opt/software/CentOS.container/7.4/bin/centos</code> image. In most cases, you can specify arguments to the powertools to use a desired overlay file or image, but this will not work properly with embedded overlays below.</p> <p>To embed the overlay into the container, you can use the (not very user-friendly) command:</p> <pre><code>singularity sif add --datatype 4 --partfs 2 --parttype 4 --partarch 2 --groupid 1 centos7.sif overlay.img\n</code></pre> <p>If you are using a different container or a different overlay, make sure to change the filenames at the end.</p> <p>Nothing looks any different, but your <code>centos7.sif</code> image will have a copy of the overlay embedded into it. Anything that was installed in the overlay (e.g., a Minicoda installation created using <code>overlay_install_conda</code>) is now available in the image.</p> <p>Note</p> <p>The original <code>overlay.img</code> overlay is now entirely disconnected from the one embedded into the <code>centos7.sif</code> image. Any changes made to the overlay will not be reflected in the image. And likewise, any changes to the overlay embedded in the image will not affect the original overlay file.</p> <p>However, you can always run your image with the original overlay file to ignore the embedded one: <pre><code>singularity shell --overlay overlay.img centos7.sif\n</code></pre></p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#running-the-embedded-overlay","title":"Running the embedded overlay","text":"<p>You can now run your image like normal:</p> <pre><code>singularity shell centos7.sif\n</code></pre> <p>You will see any changes that were in the original <code>overlay.img</code>. But if you try to make a change to a system file, you will get an error:</p> <pre><code>Singularity&gt; mkdir /hello\nmkdir: cannot create directory \u2018/hello\u2019: Read-only file system\n</code></pre> <p>To be able to make changes, you need to start the image with the <code>--writable</code> flag:</p> <pre><code>singularity shell --writable centos7.sif\nSingularity&gt; mkdir /hello\nSingularity&gt; ls -d /hello\n/hello\nSingularity&gt; exit\n</code></pre> <p>If you exit the container and restart it, you will still see the <code>/hello</code> directory: </p> <pre><code>singularity shell centos7.sif\nSingularity&gt; ls -d /hello\n/hello\n</code></pre> <p>However, this means that you still cannot use this container in multiple jobs at once since they will all try to get full access to the embedded overlay.</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-24_LabNotebook_Singularity_Overlays/#running-the-embedded-overlay-in-multiple-jobs","title":"Running the embedded overlay in multiple jobs","text":"<p>To fix the problem in the previous section, you need to load the overlay as temporarily read-writable. This loads the filesystem temporarily in a way that multiple jobs can use it at once:</p> <pre><code>singularity shell --writable-tmpfs centos7.sif\nSingularity&gt; mkdir /hello2\nSingularity&gt; ls -d /hello2\n/hello2\nSingularity&gt; exit\n</code></pre> <p>Any changes you make are discarded, so make sure your important files are somewhere accessible on the HPCC like your home or research space.</p> <pre><code>singularity shell --writable-tmpfs centos7.sif\nSingularity&gt; ls -d /hello2\nls: cannot access /hello2: No such file or directory\n</code></pre> <p>In a script with a job array, this might look something like</p> <pre><code>#!/bin/bash\n#SBATCH --walltime=04:00:00\n#SBATCH --mem=5gb\n#SBATCh --array 1-10\n#SBATCH -c 1\n#SBATCH -N 1\n\nsingularity exec --writable-tmpfs centos7.sif python3 /mnt/home/$USER/mypython.py $SLURM_ARRAY_ID\n</code></pre> <p>In this case <code>/mnt/home/$USER/mypython.py</code> should use the <code>$SLURM_ARRAY_ID</code> to do some analysis and write the output to somewhere like <code>/mnt/home/$USER/results</code> so it will be remain after the temporary filesystem is erased.</p> <p>This overview of singularity was initially written by Dirk Colbry.  Please contact the ICER User Support Team if you need any help getting your workflow up and running.</p> <p>link to ICER User Support Team online contact form</p>","tags":["lab notebook","Conda","Singularity"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/","title":"(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>The following is a tutorial by Wendy Leuenberger (with input from Alex Wright and Erin Zylstra) in the Zipkin Quantitative Ecology Lab.  Wendy wrote this tutorial for her lab group and agreed to share it here in case other groups find it useful.  If you or your team have similar tutorials we would be delighted to highlight them here as an ICER Lab Notebook.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#hpcc-via-rstudio-and-github-2022-09-28","title":"HPCC via RStudio and GitHub (2022-09-28)","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#wendy-leuenberger","title":"Wendy Leuenberger","text":"<p>This document is a guide to using MSU's HPCC. I access the HPCC via the Terminal in RStudio, and I move files from my computer to the HPCC using GitHub. I used the website https://happygitwithr.com/ to set up this system. I reference pages of that guide for instructions or additional reading. This method takes a decent amount of setting up but is convenient once in place.</p> <p>Alex Wright and Erin Zylstra contributed to the sections about using the HPCC.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#software-and-account-requirements","title":"Software and Account Requirements","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#rstudio","title":"RStudio","text":"<p>You need RStudio and R installed on your computer.</p> <p>RStudio: https://www.rstudio.com/products/rstudio/download/.</p> <p>R: https://cran.r-project.org/bin/windows/base/</p> <p>Additional reading on versions: https://happygitwithr.com/install-r-rstudio.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#git","title":"Git","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-account","title":"GitHub Account","text":"<p>You need a GitHub account for this method. You can make one here: https://github.com/. Here's some additional reading on making an account: https://happygitwithr.com/github-acct.html</p> <p>You will need to know your username, password, and the email associated with your account for later steps.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#gitbash","title":"GitBash","text":"<p>Make sure that RStudio is running GitBash when it opens the Terminal. To do so, click Tools -&gt; Global Options -&gt; Terminal -&gt; New terminals open with GitBash. If it doesn't say GitBash, change the drop down menu so that it does. It GitBash is not an option, you'll need to download Git. Here's reading on how to install Git for different operating systems: https://happygitwithr.com/install-git.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#check-installation","title":"Check installation","text":"<p>Go to the Terminal in RStudio. It's one of the tabs next to the Console. (Or use key command Shift+Alt+M) Type <code>which git</code> to make sure Git is installed.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#hpcc-set-up","title":"HPCC set up","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#account","title":"Account","text":"<p>You need an account for the HPCC. A PI can request one. Instructions are here: https://icer.msu.edu/users/getting-started</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#sign-in","title":"Sign-in","text":"<p>Make sure you can log in. Go to the Terminal window in RStudio. Type <code>ssh -XY MSUNetID@hpcc.msu.edu</code> and press enter. (Example: I type <code>ssh -XY leuenbe9@hpcc.msu.edu</code>)</p> <p>Type your password and press enter. Sometimes it makes you do it twice.</p> <p>If it doesn't work, contact ICER to see why.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#map-the-drive","title":"Map the drive","text":"<p>This method works best if the HPCC drive is mapped to your computer. Follow the instructions here to do so. If it doesn't work or you get weird results, contact ICER. https://docs.icer.msu.edu/Mapping_HPC_drives_with_SSHFS/</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-and-rstudio","title":"GitHub and RStudio","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#git-configuration","title":"Git configuration","text":"<p>Introduce yourself to Git using your GitHub user name and the email used for your GitHub account (may or may not be your MSU one). Use the Terminal in RStudio or the R package <code>usethis</code> to do so.</p> <p>Using the Terminal:</p> <p><code>git config --global user.name 'Jane Doe'</code></p> <p><code>git config --global user.email 'jane@example.com'</code></p> <p><code>git config --global --list</code></p> <p>Using <code>usethis</code>:</p> <p><code>## install if needed (do this exactly once):</code></p> <p><code>## install.packages(\"usethis\")</code></p> <p><code>library(usethis)</code></p> <p><code>use_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")</code></p> <p>For further reading: https://happygitwithr.com/hello-git.html. If you want GitHub to use 'main' instead of 'master' for the branch names, there is code for that on this page.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#personal-access-tokens","title":"Personal Access Tokens","text":"<p>Personal Access Tokens (PAT) are the way that GitHub authenticates your computer's identity. You need one for each computer that you work from. I use https tokens, though there are also ssh tokens. Read here and the following chapter if you want more information (There's a ton of information on this step): https://happygitwithr.com/https-pat.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-token","title":"Create token","text":"<p><code>usethis::create_github_token()</code> This line of code will open GitHub and begin the process of generating a token. You will need your password. The default settings that open up for the token have been fine by me so far. I usually name the token 'MSU laptop 9/22' or something similar to designate which computer it's for. They're easy enough to create that I leave the default 30 day expiration date, but you can change it if you wish. Click <code>Generate token</code>. Copy the string of numbers and characters that comes up. It'll be something like <code>ghp_LettersNumbersHere231</code>. This is the token.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#save-token","title":"Save token","text":"<p>Install the <code>gitcreds</code> package if necessary (<code>install.packages('gitcreds')</code>). Run <code>gitcreds::gitcreds_set()</code>. Follow the prompts. If this is your first token, I think it just asks you to enter the token. Paste the token when prompted. If you already have an active token, select 2 when prompted to replace the token, and enter the new token</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-repository","title":"GitHub Repository","text":"<p>There are some good chapters on testing out repositories on the web guide. I'm going to skip them so that this set-up is more streamlined. We may have to revisit them if we run into any trouble. Here's the chapters in question: https://happygitwithr.com/push-pull-github.html and https://happygitwithr.com/rstudio-git-github.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-a-repo","title":"Create a repo","text":"<p>We're going to create a new repository for a project. This is the easiest way to set it up. Here's the detailed reading: https://happygitwithr.com/new-github-first.html.</p> <p>There are also instructions for existing projects, either with GitHub first https://happygitwithr.com/existing-github-first.html or last https://happygitwithr.com/existing-github-last.html</p> <p>Go to your GitHub page. Click <code>New</code> next to your repositories, or on your profile, navigate to Repositories and then click <code>New</code>. Give your repo a name ('testrepo'). All other defaults are fine to start. You can add a description and a readme file if you want. The readme files are needed for the Zipkin lab and they can help keep things organized, but they're not required. If you work with proprietary data, you need to make sure the repo is set to private. I usually set my repos to private unless they're complete and ready to be shared. Once you're done with the settings, click <code>Create repository</code>.</p> <p>Click <code>Code</code> on the repo page. Copy the https URL (example: <code>https://github.com/wleuenberger/test-repo.git</code>)</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#link-github-and-rstudio","title":"Link GitHub and RStudio","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#create-rstudio-project-based-on-github","title":"Create RStudio project based on GitHub","text":"<p>There are many different ways to do this. I create RStudio projects for each of my Repos. I create one using the RStudio GUI. Click File -&gt; New Project -&gt; Version Control -&gt; Git. Enter the https URL for the repo. Click Browse to choose where to save the repository and R project file. I have a folder on OneDrive for GitHub projects. I also have some in other folders for research. As long as you can find it again, it's fine. Click Create Project.</p> <p>Additonal approaches and information is in the same chapter as creating repos: https://happygitwithr.com/new-github-first.html</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#github-via-rstudio-terminal","title":"GitHub via RStudio terminal","text":"<p>I find the command line really helpful for the RStudio workflow. There's only a couple of commands that are needed. There is also a Git GUI built into RStudio if you prefer that approach.</p> <p>In your R project that is linked to your GitHub repository, navigate to the Terminal (separate tab next to the Console, or Alt+Shift+M). Type <code>git status</code>. The output will show you tracked files (ones that have been uploaded to Git in a previous version but have changes) and untracked files (not on GitHub yet). Files uploaded to GitHub and haven't been changed don't show up. This output is a helpful summary to see what files might need attention. I usually do this first.</p> <p>Before you start making any changes, type <code>git pull</code>. I've found that it's helpful to do that first as a habit to prevent any merge problems if someone else on a shared repo uploaded something, or if you have files on the HPCC that got changed.</p> <p>Add a file to the folder on your computer that is linked to your repo. I created an R code file called <code>AddMe.R</code></p> <p>Go back to your Terminal in RStudio. Type <code>git status</code>. You can now see your new file in the Untracked files section. Type <code>git add AddMe.R</code> (or whatever you called your file). Type <code>git status</code>. Now <code>AddMe.R</code> is in the section of Changes to be committed. You've told GitHub that you want this file, but it's not ready yet. All changes to your repos have to be committed and then pushed to get them on the repo.</p> <p>Type <code>git commit -m \"Add my first file to GitHub\"</code>. Commits are kinda like a save point. These can be incremental points and updates. For example, if I get one new piece of code working, I'll commit it with a note of what part got updated. You have to commit before you can upload. Type <code>git status</code> and you can see that your local branch is ahead of your GitHub repo by one commit.</p> <p>To upload, type <code>git push</code>. This will upload the file to GitHub. You can see online that it's now present. Type <code>git status</code>. Your local computer is now up to date with the main GitHub.</p> <p>If you make a change from another source (the HPCC, or if a collaborator makes a change), then you'll want to pull those changes with <code>git pull</code>.</p> <p>These are pretty much all the functions I do with GitHub. There is more reading online https://happygitwithr.com/git-commands.html, as well as other chapters about how Git/GitHub work.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#three-terminals","title":"Three Terminals","text":"<p>All of these pieces result in being able to do everything from the Terminal in RStudio. You can open multiple terminals. Once you've navigated to the Terminal, there's a dropdown menu right under the word 'Terminal' that is probably labeled 'Terminal 1'. Use the 'New Terminal' option to open two additional Terminal windows.</p> <p>Terminal 1 saves and uploads files, Terminal 3 downloads files, and Terminal 2 is where you run the files on the HPCC.</p> <p>Terminal 1 is where you save and upload files to your repository. This Terminal's working directory should be your GitHub Repository. This on should already be in the correct place if you're working from an RStudio project that's linked to your GitHub. You can rename this Terminal if you want. Mine is labeled 'Laptop'.</p> <p>Terminal 2 is your HPCC account. This is where you run your code. I label this one 'HPCC'. On this terminal, log into your HPCC account with the <code>ssh -XY MSUNetID@hpcc.msu.edu</code> and your password. To do work on the HPCC, you need to navigate to a development node (ex. <code>dev-intel18</code>). There are multiple. It doesn't matter which one you use. I always use one with low current usage. Type <code>ssh dev-intel18</code> (or whichever <code>dev-intel</code> you want) to enter the node.</p> <p>Terminal 3 is your mapped HPCC drive. This is where you download your files from your repository. I have this one labeled 'Y Drive'. If you are off campus/not wired in, you need to be remotely connected to the BIG-IP Edge Client. You also need to be logged into your HPCC account. Find your mapped drive in your files and click on it. You may need to enter your password again. Then go to your third terminal Type <code>cd /y</code> (or whatever letter you picked) to set your mapped drive as the working directory for this terminal. This terminal window can be a bit slower than your personal computer.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#get-github-repo-on-the-mapped-drive","title":"Get GitHub Repo on the mapped drive","text":"<p>The repo needs to be on your mapped drive/HPCC account. You can copy and paste it into your mapped drive. Or you can add it using the <code>git clone</code> command. Type <code>git clone</code> and the https URL for that repo (ex <code>git clone https://github.com/wleuenberger/test-repo.git</code>). Check that it's working via the <code>git status</code> command.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#implementing-this-method-and-using-the-hpcc","title":"Implementing this method and using the HPCC","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#navigate-to-your-repository","title":"Navigate to your repository","text":"<p>Make sure that you are in your repository on all three terminals. Use the <code>cd</code> command to do. Just <code>cd</code> will put you at the base of your file structure (i.e.\u00a0the <code>c/</code> drive). <code>cd ..</code> will move you up one folder. <code>ls</code> shows you the contents of your current working directory. <code>cd test-repo/</code> would let me enter the test-repo repository if that were a folder in my current working directory. <code>cd GitHub/test-repo/</code> would enter the GitHub folder and then the test-repo Repository within the GitHub folder. <code>cd y/</code> would put me on the Y drive while <code>cd c/</code> would put me on the C drive. If <code>cd y/</code> (or whatever letter you used for your mapped drive) doesn't work, make sure you are logged into the HPCC, the BIG-IP Edge Client if necessary (if on WiFi), and can open the Y drive in your finder window.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#push-and-pull","title":"Push and pull","text":"<p>I do all of my changes from my computer itself. I regularly commit my changes and push whenever I want to move files to the HPCC. Navigate to the mapped drive terminal and pull those changes. Then switch to the HPCC and submit the jobs using the sbatch files (see Lab-Resources GitHub) or otherwise use the files on the HPCC. You then add, commit, and push any model output from the HPCC back to your computer.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#necessary-files","title":"Necessary files","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#r-code","title":"R code","text":"<p>If you are using R, do not set a working directory in your code. Your working directory can be set in the <code>.sb</code> file or can be the directory that you submit the job from.</p> <p>Make sure that you save any output from your code. You can save R objects as <code>.RData</code>. Here's an example of saving the <code>Out</code> object to the <code>Output</code> folder as <code>Out.RData</code></p> <p><code>save(list = Out, file = file.path('Output', 'Out.RData')))</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#data-and-models","title":"Data and models","text":"<p>Any data you use in your R code must be on the HPCC. It can be in your working directory or in a subfolder. The file path must be given when uploading it if it's not in the working directory. Any <code>.stan</code> text files or other files that are referenced must also be available</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#sb-file","title":"<code>.sb</code> file","text":"<p>The sbatch file contains information needed to submit a job. Here is an example for a model run with Stan. You will need to adapt, and some programs (JAGS) require different settings. If you are running code on the development node and not submitting a job, you don't need this file.</p> <p><code>#!/bin/bash --login</code></p> <p><code># how long? Can specify up to 7 days in hours</code>\\ <code>#SBATCH --time=168:00:00</code></p> <p><code># how much memory?</code>\\ <code>#SBATCH --mem=20G</code></p> <p><code># specify nodes needed.</code>\\ <code>#SBATCH --ntasks=1</code></p> <p><code># specify CPUs (or cores per task)</code>\\ <code># Match this to your number of chains</code>\\ <code>#SBATCH --cpus-per-task=3</code></p> <p><code># email me</code>\\ <code>#SBATCH --mail-type=FAIL,END</code>\\ <code>#SBATCH --mail-user=leuenbe9@msu.edu</code></p> <p><code># change to current directory or specify file path</code>\\ <code>cd $SLURM_SUBMIT_DIR</code></p> <p><code># export R_LIBS_USER=/mnt/home/leuenbe9/R_Lib/4.0.2-X11-20200622</code></p> <p><code># add necessary modules</code>\\ <code>module purge</code>\\ <code>module load GCC/11.2.0</code>\\ <code>module load OpenMPI/4.1.1</code>\\ <code>module load R/4.1.2</code>\\ <code>R --no-environ</code></p> <p><code># run R commandline with the Rscript command</code>\\ <code>Rscript Code/R/TryModel_ISM.R --vanilla</code></p> <p><code>squeue -l $SLURM_JOB_ID</code></p> <p><code>scontrol show job $SLURM_JOB_ID</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#line-endings","title":"Line endings","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#gitattributes","title":".gitattributes","text":"<p>The HPCC requires LF line breaks to run. It cannot run with the standard Windows line breaks (CRLF). You can change each file through command line, but it's often easier to tell git to change it for you. To do so, create a text file call <code>.gitattributes</code>. Set a default, and then state which file types need to be converted. A * indicates any text file, <code>*.R</code> means that any file with the extension <code>.R</code> will be converted to LF line endings when uploaded to GitHub. It won't change the original file on your computer. You will need this <code>.gitattributes</code> file in every repository.</p> <p>Here's example text for the <code>.gitattributes</code> file:</p> <p><code># Set the default behavior, in case people don't have core.autocrlf set.</code>\\ <code>* text=auto</code></p> <p><code># Declare files that should always have LF line endings once uploaded</code>\\ <code># Customize for which types of files you need</code>\\ <code>*.stan text eol=lf</code>\\ <code>*.R text eol=lf</code>\\ <code>*.sb text eol=lf</code></p> <p>Save this file in your repository. Then use <code>git add</code>, <code>git commit</code>, and <code>git push</code> to upload it to your GitHub.</p> <p>Here's additional information and ways to set it globally: https://docs.github.com/en/get-started/getting-started-with-git/configuring-git-to-handle-line-endings</p> <p>I think all you need to do to set it globally is type <code>git config -- global core.autolf</code>. I'm testing it now. We'll see if it works.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#manually-changing","title":"Manually changing","text":"<p>From your working directory, type <code>file *</code>. This command will provide details on each file. You may see <code>with CRLF line terminators</code> on some files. If you do, type <code>dos2unix filename.extension</code> for each file. You can do more than one at once <code>dos2unix filename1.extension filename2.extension</code>.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#running-code","title":"Running code","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#development-node","title":"Development node","text":"<p>If you want to test or troubleshoot, you can run an R on a development node. ( <code>dev-intel16</code>, <code>dev-intel18</code> <code>dev-amd20</code>, accessed by <code>ssh</code> when first logging onto the HPCC) Purge modules and then load the ones you need:</p> <p><code>module purge</code>\\ <code>module load GCC/11.2.0</code>\\ <code>module load OpenMPI/4.1.1</code>\\ <code>module load R/4.1.2</code></p> <p>Then you can either enter R by typing <code>R</code> (type <code>q()</code> to quit), or by running an entire file by typing <code>Rscript FileName.R</code></p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#submit-batch-job","title":"Submit batch job","text":"<p>From your working directory, type <code>sbatch FileName.sb</code> This will submit the job.</p> <p>To check on your job, type <code>sq</code>. It will show as PENDING or RUNNING. If it ran into errors or finished, it will no longer show up.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#out-file","title":".out file","text":"<p>Once your job is running, a <code>SLURM-#######.out</code> file will be created. Don't open it until the job is done. It will contain the output from R and your code, and will have some error messages if the job fails.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#common-errors","title":"Common errors","text":"","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#packages-not-loaded","title":"Packages not loaded","text":"<p>Not all R packages or R versions are available on the HPCC. If something you need isn't available, contact ICER via Teams or making a ticket. Include the R version you want to use and the package names. They're pretty fast about getting packages loaded.</p>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-28_LabNotebook_HPCC_via_Rstudio_and_Github/#contact-icer-if-needed","title":"Contact ICER if needed","text":"<ol> <li>Office Hours: Monday/Thursday 1-2 PM on Teams (maybe also in     person?)</li> <li>ICER User Manual: https://docs.icer.msu.edu (see     especially the sections labeled: Job Scheduling by SLURM and Job     Management by SLURM)</li> </ol>","tags":["lab notebook","RStudio","git"]},{"location":"2022-09-30_LabNotebook_ROS/","title":"(2022-09-30) Lab Notebook: ROS on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#the-problem-2022-09-30","title":"The problem (2022-09-30)","text":"<p>The latest version of the Robot Operating System, Humble Hawksbill https://www.openrobotics.org/blog/2022/5/24/ros-2-humble-hawksbill-release, uses a version of <code>libQtCore.so.5</code> that is not compatible with the HPCC Linux kernel version (3.10 as of writing). Specifically, it requires kernel version 3.15+. When the ROS is installed into a Docker container, and used inside a Singularity image with e.g. <code>singularity pull docker://morris2001/humble</code> and <code>singularity run humble_latest.sif</code>, the <code>rqt</code> command inside the Singularity image will result in the error:</p> <pre><code>ImportError: libQt5Core.so.5: cannot open shared object file: No such file or directory\n</code></pre>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#the-solution","title":"The solution","text":"<p>Inside your Dockerfile where you build the ROS container with Ubuntu 22.04, add the command</p> <pre><code>RUN /usr/bin/strip --remove-section=.note.ABI-tag /usr/lib/x86_64-linux-gnu/libQt5Core.so.5\n</code></pre> <p>This is based on discussion here: https://github.com/dnschneid/crouton/wiki/Fix-error-while-loading-shared-libraries:-libQt5Core.so.5</p> <p>This command removes the offending piece of code that is incompatible with the HPCC Linux kernel, and allows <code>rqt</code> to launch successfully.</p>","tags":["lab notebook","ROS"]},{"location":"2022-09-30_LabNotebook_ROS/#opencv","title":"OpenCV","text":"<p>OpenCV is an important Python package for various parts of ROS. Make sure you install it in your Docker container by adding the command</p> <pre><code>RUN apt install libopencv-dev python3-opencv\n</code></pre> <p>to your Dockerfile.</p>","tags":["lab notebook","ROS"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/","title":"(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/#installing-bactopia-on-the-hpcc-2022-10-03","title":"Installing Bactopia on the HPCC (2022-10-03)","text":"<p>Bactopia is a bacteria genome analysis tool. It can be found at https://bactopia.github.io/v2.1.1/ and has extensive documentation. However, if you follow the instructions for installation on the MSU HPCC you may run into issues with solving the Python environment with Conda.</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-03_LabNotebook_Bactopia_installation/#installation-steps","title":"Installation steps:","text":"<pre><code>module load Conda/3\n</code></pre> <p>Download miniconda https://docs.conda.io/en/latest/miniconda.html</p> <p>Activate the miniconda environment</p> <p>Install mamba</p> <pre><code>conda install -c conda-forge -c bioconda mamba\n</code></pre> <p>Update mamba: </p> <pre><code>conda install -c conda-forge 'mamba&gt;=0.24.*'\n</code></pre> <p>Downloaded and activate bactopia: </p> <pre><code>mamba create -n bactopia -c conda-forge -c bioconda bactopia\nconda activate bactopia\n</code></pre> <p>For further information, see this github issue: https://github.com/bactopia/bactopia/issues/355</p>","tags":["lab notebook","Bactopia","Conda"]},{"location":"2022-10-04_LabNotebook_OnDemand_CantClickInteractiveDesktop/","title":"(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","interactive desktop"]},{"location":"2022-10-04_LabNotebook_OnDemand_CantClickInteractiveDesktop/#lab-notebook-unable-to-click-icons-on-the-ondemand-interactive-desktop-2022-10-04","title":"Lab Notebook --- Unable to click icons on the OnDemand Interactive Desktop (2022-10-04)","text":"<p>If you start an Interactive Desktop session and find yourself unable to click on any of the icons or buttons on the deskop, you are probably have the 'View Only' option enabled. To fix this, look for icon that looks like a small arrow on the left side of your browser window (see below).</p> <p></p> <p>Click on this arrow to open a menu, which should have 'no VNC' on the top. In this menu, click on the gear symbol (it should be the second from the bottom) to open a setting menu (see below)</p> <p></p> <p>Uncheck the 'View Only' checkbox, and you should now be able to interact with you Desktop again.</p> <p></p> <p>Importantly, OnDemand will remember the whether 'View Only' is checked even if you close out of and reopen the same Interactive Desktop session AND if you close and delete a session and start a new one. If you leave 'View Only' check, close and delete session, and start a new Interactive Desktop session, sometimes your desktop will be confined to the center of browser window (see below)</p> <p></p> <p>To fix this, make sure 'View Only' is unchecked and then toggle 'Remote Resizing' to 'Local Scaling' then back to 'Remote Sizing'</p> <p></p>","tags":["lab notebook","OnDemand","interactive desktop"]},{"location":"2022-10-08_LabNotebook_Praat_Install/","title":"(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","Singularity","OnDemand"]},{"location":"2022-10-08_LabNotebook_Praat_Install/#running-praat-on-hpcc-2022-10-08","title":"Running Praat on HPCC (2022-10-08)","text":"<p>I got Praat working on the system.  It requires a newer version of CentOS and a few tricks but it is working.  To start we need to make a folder with a singularity overlay in it and add conda (for dependencies):</p> <pre><code>mkdir praat\ncd praat\nmodule load powertools\noverlay_build\noverlay_resize 10\noverlay_install_conda\n</code></pre> <p>Next we need to download a CentOS8 image which has the some upgraded libraries that praat needs:</p> <pre><code>singularity pull --arch amd64 library://godloved/secure/centos8:20200805.0.0\nmv centos8_20200805.0.0.sif centos8.sif\n</code></pre> <p>Now we can start singularity with the overlay and install some requirements (actually, mfa is not a requirement but this research team is using it for something else):</p> <p><pre><code>overlay_start overlay.img centos8.sif\nconda install -c conda-forge gtk3\n</code></pre> <pre><code>conda install -c conda-forge pulseaudio\n</code></pre> <pre><code>conda install -c conda-forge montreal-forced-aligner\n</code></pre></p> <p>Finally, we can download praat and copy it to our singularity image:</p> <pre><code>wget https://www.fon.hum.uva.nl/praat/praat6223_linux64.tar.gz\ntar -xzvf praat6223_linux64.tar.gz\nmv ./praat /praat\n</code></pre> <p>That should be it. We now connect to the HPCC using an X11 connection. The easiest way to do that is to start an interactive desktop form OnDemand. Once the desktop is running, open a terminal, change to the praat directory and run the following command:</p> <pre><code>cd praat\nsingularity exec -B /usr/bin/:/sysbin/ \\\n                 --env LD_LIBRARY_PATH=/miniconda3/lib:/.singularity.d/lib \\\n                 --env TERM=vt100 \\\n                 --env PATH=/miniconda3/bin/:/usr/local/sbin:/usr\n/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sysbin/ \\\n                 --overlay overlay.img centos8.sif /praat\n</code></pre> <p>I also have an OnDemand App working in a basic dev environment but need to do some testing to find an easy way to share overlay images.  </p> <p>This overview of singularity was initially written by Dirk Colbry and is just intended of an example of how to install and run things on the HPCC. Please contact the ICER User Support Team if you need any help getting your workflow up and running.</p> <p>link to ICER User Support Team online contact form</p>","tags":["lab notebook","Conda","Singularity","OnDemand"]},{"location":"2022-10-10_LabNotebook_OnDemand_Interactive_Desktop_error/","title":"(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand"]},{"location":"2022-10-10_LabNotebook_OnDemand_Interactive_Desktop_error/#ondemand-interactive-desktop-error-2022-10-10","title":"OnDemand Interactive Desktop Error (2022-10-10)","text":"<p>A user was having trouble connecting to the Interactive Desktop using OnDemand.  Once OnDemand started the would encounter a blank desktop similar to the following:</p> <p></p> <p>We finnally tracked down the problem to an error someplace in their user dconf file.  To fix the error the user just needs to remove their dconf file using the following command and restart their desktop:</p> <pre><code>rm ~/.config/dconf/user \n</code></pre> <p>We are still not exactly sure how this file got corrupted.  Please give the above a try and open a ticket if you have any trouble (or insight as to the original cause of the problem). </p>","tags":["lab notebook","OnDemand"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/","title":"(2022-10-19) Lab Notebook: OpenVDB installaion with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/#lab-notebook-building-openvdb-using-a-conda-environment-2022-10-19","title":"Lab Notebook --- Building OpenVDB using a Conda environment (2022-10-19)","text":"<p>Warning</p> <p>EXTRA WARNING: I'm not even sure I would recommmend this method as of writing. Now that HPCC recognizes the updated (1.21.0) Blosc module all the important modules (Boost, tbb, and blosc) should be available through module load. However, in case that breaks or a conda enviroment is needed, here are the instructions.</p> <p>OpenVDB is an open source C++ library comprising a novel hierarchical data structure and a large suite of tools for the efficient storage and manipulation of sparse volumetric data discretized on three-dimensional grids. For more information see their Github page</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_OpenVDB_installation/#building-the-library-via-a-conda-enviroment","title":"Building the library via a Conda enviroment","text":"<p>First, to setup you conda environment with the necessary dependencies, do:</p> <pre><code>conda create -n openvdb_dep cmake gcc gxx boost tbb tbb-devel blosc\nconda activate openvdb_dep\n</code></pre> <p>Then, in this environment, run the following commands which are a slight modified the install instructions from https://github.com/AcademySoftwareFoundation/openvdb:</p> <pre><code>git clone https://github.com/AcademySoftwareFoundation/openvdb.git\ncd openvdb\nmkdir build\ncd build\ncmake .. -DZLIB_ROOT=&lt;path_to_your_conda_enviroment&gt; -DCMAKE_INSTALL_PREFIX=&lt;path_to_where_you_want_to_install_openvdb&gt;\nmake -j4 &amp;&amp; make install\n</code></pre> <p>The file paths to the depedencies, <code>&lt;path_to_your_conda_enviroment&gt;</code>, should be something like <code>/mnt/home/your_netid_name/miniforge3/envs/openvdb_dep/</code> </p> <p>The install directory <code>&lt;path_to_where_you_want_to_install_openvdb&gt;</code> can be anywhere in your home directory (but its best to make it a folder called 'openvdb' because make will write the bin, include, and lib folders there), just make sure you know where it is so you can point to it/add it to your path as need.</p>","tags":["lab notebook","Conda","OpenVDB"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/","title":"(2022-10-19) Lab Notebook: TractSeg installation with Conda","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#lab-notebook-installing-tractseg-on-hpcc-using-conda-2022-10-19","title":"Lab Notebook --- Installing TractSeg on HPCC using Conda (2022-10-19)","text":"<p>TractSeg is a tool for fast and accurate white matter bundle segmentation from Diffusion MRI (see their GitHub page for further details). The program itself can be installed through pip, but requires a few dependencies, one of which, Mrtrix 3, may appear dauting at first because the link on the TractSeg page takes you to the source build instruction. However, Mrtrix 3 can be installed using Conda, which is recommened and enable by the developer (see here). Therefore, we are going to use Conda for the whole install. </p> <p>If you have not installed Conda in you home directory, see https://docs.icer.msu.edu/Using_conda/</p> <p>Installing TractSeg</p> <p>The code below will walk you through the process of creating a conda environment of TractSeg and install TactSeg in it:</p> <pre><code># Begin by loading the loading the Conda module:\n\nmodule purge\nmodule load Conda/3\n\n#After that is done, the following command should create a conda enviroment, activate it and install the dependencies:\nconda create -n tractseg\nconda activate tractseg\nconda install -c mrtrix3 mrtrix3\nconda install pytorch\n\n# Now we can install TractSeg with pip\npip install TractSeg\n\n#Finally, to test the install, try to get the help options for TractSeg:\n\nTractSeg --help\n</code></pre> <p>Once you have completed the above steps, to run TractSeg in the future, do:</p> <pre><code>module purge\nmodule load Conda/3\n\nconda activate tractseg\n\n# check\nTractSeg --help\n</code></pre>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#install-optional-dependencies","title":"Install Optional Dependencies","text":"<p>TractSeg has a couple extra dependencies which can be installed if needed, but are not necessary. I've tested installing them but not their functionality in TractSeg as I am not sure where pip installs the TractSeg example files nor am I an expert on the software itself.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#fsl","title":"FSL","text":"<p>An FSL python install script can be found here with accompanying install information for Linux here. This script WILL try to install to /usr/local, so use the -d option to install in our home directory (i.e., fslinstaller.py -d /mnt/home/your_netid/something/fsl).</p> <p>After finishing the install, you will need to take several setps that depdent on where you installed FSL. So first, let assume that the  pathe to the directory you installed FSL in is something like \"/mnt/home/your_netid/local/fsl\" (replace this with whatever the real path is). To get FSL running you will need to do the following in command line, but PLEASE be careful with the export PATH command:</p> <pre><code>export FSLDIR=/mnt/home/your_netid/local/fsl                  # Set an environment variable for the program\nsource /mnt/home/your_netid/local/fsl/etc/fslconf/fslfsl.sh   # Source the config file\nexport PATH=$PATH:/mnt/home/your_netid/local/fslbin           # Add FSL to your path\n</code></pre> <p>After this you should be able to run FSL from the command line by running 'fsl'</p> <p>If you are going to be using FSL alot, you can adde the above lines to your ~/.bashrc file, but agian, PLEASE be careful.</p>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-19_LabNotebook_TractSeg_installation/#xvfb","title":"xvfb","text":"<p>Xvfb is a virtual frame buffer for X11 servers. It is library which should be able to bed installed via conda using:</p> <pre><code>conda install xorg-x11-server-xvfb-cos7-x86_64\n</code></pre>","tags":["lab notebook","Conda","TractSeg"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/","title":"(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","sftp"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/#sftp-mapping-on-hpcc-file-systems-lasted-updated-2022-10-27","title":"SFTP Mapping on HPCC file systems (lasted updated 2022-10-27)","text":"<p>SFTP Net Drive is the software which can map remote HPCC file systems on your local Windows computers via SFTP. (Mac OS is not supported.) Once connected, you can browse and work with files on HPCC as if they were on a hard drive of your local machine. In order words, they do not need to be downloaded and uploaded when users read or modify them using their local computers.</p> <p>HPCC users can use the  download site to get a free version of SFTP Net Drive. Once it is downloaded and executed, users can do its setup. In the main menu (below), you can choose your own <code>Profile</code> name and <code>Drive Letter</code>. Make sure the <code>Server</code> is set to <code>http://rsync.hpcc.msu.edu</code> <code>Username</code> and <code>Password</code> are the same as your HPCC login.</p> <p></p> <p>For more advanced setting, please click on the <code>Advanced...</code> button. Three setting menus: <code>Connection</code>, <code>Protocol</code> and <code>Drive</code> can be modified. For <code>Connection</code>, the value on <code>Port</code> has to be 22. A longer initiation time can be adjusted on <code>Timeout</code>. You can also set up <code>reconnect times</code> in case the connection is dropped and <code>Send keep-alive</code> to prevent disconnection.</p> <p></p> <p>For <code>Protocol</code>, you may just use the default setting:</p> <p></p> <p>For <code>Drive</code>, if you would like, you may set up a different <code>Root folder</code> to start with other than your home folder. You might want to click on <code>Handle case-sensitive filenames</code> since the HPCC file system is case-sensitive. If you would like to show hidden files, you can click on <code>Show files started with dot</code>.</p> <p></p> <p>Once all of them are set, you can click on <code>Connect</code> button in the main menu. If it is successfully connected, the <code>Connect</code> button will become <code>Disconnect</code>:</p> <p></p> <p>Now, open the file explorer and click on <code>This PC</code>. You should find the HPCC file system shown in the <code>Network locations</code> area with the <code>Profile</code> name and the <code>Drive Letter</code> of your input:</p> <p></p>","tags":["lab notebook","sftp"]},{"location":"2022-10-27_LabNotebook_SFTP_Mapping_on_HPCC_file_systems/#mapping-hpcc-drive-with-more-spaces","title":"Mapping HPCC Drive with More Spaces","text":"<p>If you would like to do SFTP mapping with more than one space in HPCC, the better way is to set static links in the <code>Root folder</code> of your setting. For example, you want to map your home space and research space on your local Windows computer. You can set the <code>Root folder</code> to be your home folder as the setup above. Use\u00a0a ssh client to connect to the HPCC or Web Site Access to HPCC to run a command line on a dev node:</p> <pre><code>[username@dev-intel18 ~]$ ln -s /mnt/research/&lt;Research Name&gt; &lt;Research Name&gt;\n</code></pre> <p>where a static link to your research space is set in your home folder. Once it is done, run <code>ls</code> command and the static link <code>&lt;Research Name&gt;</code> should be shown with the light blue color. To access the research space through your local computer, just click on the HPCC drive of your setting in the <code>Network locations</code> area as mentioned in the upper section. Look for the space link <code>&lt;Research Name&gt;</code> and click on it. You can now see the files in the research space. If you would like to map more spaces, such as your scratch spaces or other research spaces, just create more static links in your home folder by the same way.</p>","tags":["lab notebook","sftp"]},{"location":"2022-12-07_LabNotebook_OnDemand_BadRequest/","title":"(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","Debugging"]},{"location":"2022-12-07_LabNotebook_OnDemand_BadRequest/#bad-request-in-ondemand-2022-12-07","title":"Bad Request in OnDemand (2022-12-07)","text":"<p>Some users are reporting the following error:</p> <p></p> <p>We are not exactly sure about all of the causes of this problem. It seems to be related to MSU authentication, the type of browser and if your computer switches networks.  </p> <p>In any case the problem is easy to fix. You can clear out your browsers cookies and/or just exit the browser and log back in.</p> <p>If you are getting this problem consistantly please feel free to open a ticket with ICER and help us track down and further debug the problem.</p>","tags":["lab notebook","OnDemand","Debugging"]},{"location":"2022-12-20_LabNotebook_Conda_Cleaning_out_cache/","title":"(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_Conda_Cleaning_out_cache/#lab-notebook-cleanning-out-your-conda-cache-on-hpcc-2022-12-20","title":"Lab Notebook --- Cleanning out your Conda Cache on HPCC (2022-12-20)","text":"<p>Warning</p> <p>EXTRA WARNING: This process is MOSTLY safe in that the deafult behavior of conda is to hardlink from enviroments to the packages, so if packages are removed from the conda cache, it will not affect the environment. However, if you have enable softlinking with conda (by setting  allow_softlinks or always_softlinks in your config) or manually soflinked to files/folders in the packages directory, this WILL break those links. For a discussion of why this process is MOSTLY safe, see the following links:</p> <ul> <li>\"Is it safe to manually delete all files in pkgs folder in anaconda python?\" (Stackoverflow)</li> <li>\"Can I delete files in the pkgs directory after installation.\" (Anaconda Google Group)</li> </ul> <p>Conda often creates many small files contributing to user's file quota on HPCC. A more permenant solution for this problem can be found using Singularity overlays, but as of the time of writing, Sigularity overlays remains something of a work in progress. Hence, this guide is meant to provide an interm solution by outling ways of cleaning out some of the extraneous files produced by Conda.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_Conda_Cleaning_out_cache/#conda-clean","title":"Conda clean","text":"<p>The command 'conda clean' can be used to clean out uneeded packages in the 'pkgs' cachce folder. Full options for this command can be found here, but the simplest options are to either remove all unused packages with:</p> <p><code>conda clean -p</code></p> <p>Or remove ALL packages with:</p> <p><code>conda clean -a</code></p> <p>Again, this should have no affect on hard-link enviroments installed under normal circumstances, however if there is any doubts consider making a backup of existing, important python environments as .yaml files which can be used to reinstalling enviroments or other options to reduce file count such as archiving old/unsued files and directories.</p>","tags":["lab notebook","Conda","many files","clean up"]},{"location":"2022-12-20_LabNotebook_SSHFS_Mapping_on_Windows/","title":"(2022-12-20) Lab Notebook: SSHFS mapping on Windows","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Windows","sshfs"]},{"location":"2022-12-20_LabNotebook_SSHFS_Mapping_on_Windows/#lab-notebook-mapping-hpcc-drives-with-sshfs-on-windows-2022-12-20","title":"Lab Notebook --- Mapping HPCC Drives with SSHFS on Windows (2022-12-20)","text":"<p>Warning</p> <p>EXTRA WARNING: This information was removed from the main SSHFS documentation after switching rsync.hpcc.msu.edu over to SSH key authentication. This  process is now much more difficult, less stable, and can no longer support convenient software like SSHFS-Win Manager. As such we no longer plan to  update or support these instructions and they mainly exist for archival purposes and for users who prefer this approach. Users are free to use this  approach and their own risk, but we recommend other drive mapping and file transfer options.</p> <ol> <li> <p>Install the latest stable release of <code>winfsp</code>. The .msi installer package is recommended. WinFsp release list</p> <ol> <li>Full upstream documentation: WinFsp Documentation</li> </ol> <p>Note</p> <p>Administrative access is required</p> </li> <li> <p>Install the latest stable release of SSHFS-Win.The .msi installer package is recommended. SSHFS-Win release list </p> <ol> <li>Full upstream documentation: SSHFS-Win Documentation</li> </ol> <p>Note</p> <p>Administrative access is required</p> </li> <li> <p>You will need to generate an authentication key pair by following the directions at SSH Key-Based Authentication. Follow the instructions for Windows as described for uploading your key to HPCC, but additionally you will need to created a folder on your Windows system at C:\\Users\\ called .ssh. Inside this folder use Notepad (NOT Office) to create a file named id_rsa.pub and copy your public key from the MobaKeyGen window into that file. Finally, go to the \"Conversions\" tab in the MobaKeyGene windows and click \"Export OpenSSH Key\" as save that as id_rsa in the same folder (C:\\Users\\.ssh). DO NOT use the \"Save public/private key\" buttons to save the keys in C:\\Users\\.ssh as these will NOT save the key in the proper format for SSHFS.  <li> <p>Mounting of Home and Scratch can be done through the command prompt or with the Windows File Explorer once WinFsp and SSHFS-Win are installed.</p> <ol> <li>Mounting Home directory with the command prompt. Note the '.k' after sshfs (<code>\\\\sshfs.k\\</code>) indicates key authentication should be used<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter:] \\\\sshfs.k\\&lt;username&gt;@rsync.hpcc.msu.edu</code></li> <li>example <code>C:\\&gt; net use F: \\\\sshfs.k\\ryanjos2@rsync.hpcc.msu.edu</code></li> </ul> </li> <li>Mounting Scratch directory with the command prompt requires a slightly different command.  Note the addition of '.r' to sshfs (<code>\\\\sshfs.kr\\</code>) that designates to start from the root directory.<ul> <li>DriveLetter (H: or Z: etc.) is optional, but username is required </li> <li>Command structure <code>C:\\&gt;net use [DriveLetter:] \\\\sshfs.kr\\&lt;username&gt;@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\&lt;username&gt;</code></li> <li>example <code>C:\\&gt;net use R: \\\\sshfs.kr\\ryanjos2@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\ryanjos2</code></li> </ul> </li> <li>Mounting Home or Scratch directory with Microsoft file browser:<ul> <li>Both Home and Scratch use the same steps, just with different network paths<ul> <li>Home: <code>\\\\sshfs.k\\username@rsync.hpcc.msu.edu</code></li> <li>Scratch:  <code>\\\\sshfs.kr\\username@rsync.hpcc.msu.edu\\mnt\\gs21\\scratch\\username</code></li> </ul> </li> <li>In file browser on the 'This PC' page, select 'Map Network Drive' </li> <li>Fill in the appropriate network path (<code>\\\\sshfs.k\\username@rsync.hpcc.msu.edu</code>), select a drive letter and any desired options, and click Finish. </li> </ul> </li> </ol> </li> <li> <p>Mounting a research space required an additional tool, the SSHFS-Win Manager GUI.  However, SSHFS-Win Manager GUI does not work properly with SSH key authentication so this is not longer supported with SSHFS.</p> </li>","tags":["lab notebook","Windows","sshfs"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/","title":"(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/#running-namd-across-multiple-nodes-2023-02-07","title":"Running NAMD across multiple nodes (2023-02-07)","text":"<p>NAMD is a parallel molecular dynamics code. At the time of writing we have 3  major versions installed on the HPCC. This guide is for running NAMD across multiple compute nodes.</p>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_NAMD_on_multiple_nodes/#recommended-script-setup","title":"Recommended script setup","text":"<p>Below is a sbatch script that should allow NAMD to be run across multiple nodes.  Note that items in &lt;&gt; should be replaced with appropriate values. </p> <p>Especially note that <code>&lt;NUMBER OF NODES&gt; * &lt;NUMBER OF CPUS&gt;</code> should be  entered as one number e.g. 5 nodes * 50 CPUs = 200. This script assumes one task per core.</p> <pre><code>#!/bin/bash --login\n#SBATCH --nodes=&lt;NUMBER OF NODES&gt;\n#SBATCH --ntasks-per-node=&lt;NUMBER OF CPUS&gt;\n#SBATCH --time=&lt;WALLTIME&gt;\n#SBATCH --mem=&lt;MEMORY REQUEST&gt;\n#SBATCH --output=namd.sb.o%j\n#SBATCH --error=namd.sb.e%j\n\nmodule purge\nmodule load GCC/9.3.0 OpenMPI/4.0.3\nmodule load NAMD/2.14-mpi\n\ncd $SLURM_SUBMIT_DIR\n\ncharmrun +p &lt;NUMBER OF NODES&gt; * &lt;NUMBER OF CPUS&gt; namd2 &lt;CONFIGURATION FILE&gt;\n</code></pre>","tags":["lab notebook","NAMD"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/","title":"(2023-02-07) Lab Notebook: WRF installation with EasyBuild","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#lab-notebook-installing-wrf-on-hpcc-using-easybuild-2023-02-07","title":"Lab Notebook --- Installing WRF on HPCC using EasyBuild (2023-02-07)","text":"","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#the-problem-as-of-easybuild-462","title":"The Problem (as of EasyBuild 4.6.2)","text":"<p>Currently, the most recent versions of WRF (4.1 and later) cannot be successfully built by the most recent version of EasyBuild instlled on HPCC (4.6.2). As best I can tell, although EasyBuild is loads more recent versions of GCC and OpenMPI and uses them to build dependencies such as netCDF, something in WRF itself (I think) is configure so that when attempting to compile tests cases (i.e. \"compile em_b_wave\"), the versions of GCC and OpenMPI the automatically load on HPCC (6.4.0 and 2.1.2 respectively) are used.</p> <p>In my experience, this problem tends to manifiest in one of two ways: either EasyBuild will fail while trying to build one of the test cases due to a mismatch in compiler version with netCDF (\"Fatal Error: Cannot read module file netcdf.mod opened at (1), because it was created by a different version of GNU Fortran\") or elements of the test case will fail to build due to the older GCC complier not recognizing  the flag \"-fallow-argument-mismatch\" (gfortran: error: unrecognized command line option -fallow-argument-mismatch; did you mean -Wlto-type-mismatch?) which is believe is a 10+ flag for GCC (see https://gcc.gnu.org/gcc-10/changes.html). You will also probably see something like:</p> <p>Serial Fortran compiler (mostly for tool generation): which SFC /opt/software/GCCcore/6.4.0/bin/gfortran</p> <p>In the output or log file when trying to compile a test case.</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#current-solution","title":"Current Solution","text":"<p>Currently, the best solution is to use an older version of EasyBuid (4.5.0) which can be loaded via \"module load EasyBuild/4.5.0\". This approach has been tested and works out of the box for WRF 4.1.3, 4.2.2, and 4.3.</p> <p>However, older (3.6.X) versions of WRF may require an even older verion of EasyBuild, but their remains problem with this approach realted to folder accesibility [TBD, consult with Andrew and Xaioge]</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-07_LabNotebook_WRF_installing_with_EasyBuild/#notes-and-future-considerations","title":"Notes and Future Considerations","text":"<p>I am neither well versed in Fortran nor WRF, so the above is a compilation of my observation of trying to get WRF intalled along with a little reserach along the way. As a Lab Notebook, this primarily serves as a reference point and installing WRF should probably be revisited if and when EasyBuild is updated (4.7.0 was released recently)</p> <p>Update (2-8-2023): Installed EasyBuild 4.7.0, same issues as 4.6.2. need to follow up with additional confgiuration mentioned by Xiaoge</p>","tags":["lab notebook","WRF","EasyBuild"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/","title":"(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#hpcc-to-onedrive-with-rclone-2023-02-09","title":"HPCC to OneDrive with Rclone (2023-02-09)","text":"<p>The documentation to use rclone to copy data from the HPC to their MSU OneDrive account is very complex.   Rclone is a powerful tool with lots of options so it can be frustrating to sift through all of the options to know what you need to do. Since I had never used rclone before I thought I would give it a try and type up what I learned.</p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#step-1-configure-rclone-to-connect-with-onedrive","title":"Step 1: Configure Rclone to connect with OneDrive","text":"<p>First, we need to log on to an interactive desktop using OnDemand. This is required because OneDrive needs to set up an authentication token and it uses a web browser running on the server to do this step.  Just log onto \"http://ondemand.hpcc.msu.edu/\" using your MSU NetID and password.  Then click on \"Interactive apps\" and then click on \"Interactive desktop\"  use the default settings which should be something like the following:</p> <pre><code>Number of hours 4\nNumber of cores per task 1\nAmount of memory 4gb\n</code></pre> <p>Then you can just hit launch and wait for the job to start. You will be taken to a screen with a job ID number and the word \"Queued\" in the upper right corner.  It often doesn't take long the word \"Queued\" will change to \"Starting\" and then \"Running\".  A button will appear that says \"Launch Interactive Desktop\". Click that button and a new window will appear in your browser with the desktop running. </p> <p>Then next step is to run the \"rclone config\" command from a terminal. To get to a terminal go up to the start menu at the stop of the desktop. Click on \"Applications--&gt;System Tools--&gt;Terminal\" and a terminal window will appear.  Type the following command to load rclone in the terminal:</p> <pre><code>module load rclone\n</code></pre> <p>Run the <code>rclone config</code> command (you only need to do this once).  I typed the following:</p> <pre><code>n\nremote\nonedrive\n&lt;&lt;enter&gt;&gt;\n&lt;&lt;enter&gt;&gt;\nn\ny\nonedrive\n0\ny\ny\nq\n</code></pre> <p>During this process a window browser window will pop up which may include a button. Just hit accept. You can close the browser when it says \"Success\". Here is a slightly more detailed breakdown of the command I used when configuring:</p> <pre><code>n (create new remote)\nremote (name of the new remote. We are just calling it remote but you can use whatever name)\nonedrive (storage type of remote. In this case onedrive)\n&lt;enter&gt; (Just using default ClientID of an empty string)\n&lt;enter&gt; (Just using default client_secret of an empty string)\nn (No we don't want to use advanced settings)\ny (Yes we do want to use automatic configuration, this is when the browser window will pop up)\nonedrive (Pick your Onedrive Personal or Bisness account)\n0 (Pick the first onedrive account. This assumes you only have one and it is the first in the list)\ny (Yes we want to use our MSU onedrive which will have a URL similar to https://michiganstate-my.sharepoint.com/personal/userid_msu_edu/Documents)\nq (quit the rclone config)\n</code></pre>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-09_LabNotebook_Rclone_onedrive/#step-2-run-rclone-commands-to-create-and-copy-files-to-onedrive","title":"Step 2: Run Rclone commands to create and copy files to OneDrive","text":"<p>Now that the rclone authentication is configured we can move on to actually using the rclone command. As an example I want to create a directory on my OneDrive (called testdir) and then copy the contents of a directory on the HPC to that directory.  Although I should be able to just run the rclone commands from the terminal I am going to run the command inside a job using the following job script.</p> <pre><code>#!/bin/bash\n#SBATCH --mem=2gb\n#SBATCH --time=4:00:00\nmodule load Rclone\nrclone mkdir remote:testdir\nrclone copy ~/UserCode/colbrydi/see-benchmark/see-segment/Image_data/ remote:testdir\n</code></pre> <p>I named this file \"rclone.sb\" and submitted the job to the cluster using the <code>sbatch rclone.sb</code> command.  Seemed to work for me. Please let us know if you found this helpful.</p> <p>Dirk Colbry Director of User Support Institute for Cyber-Enabled Research  </p>","tags":["lab notebook","rclone","files"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/","title":"(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#downloading-and-installing-cryosparc-2023-11-22","title":"Downloading and Installing cryoSPARC (2023-11-22)","text":"","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#prepare-for-installation","title":"Prepare for Installation","text":"<ul> <li>Register and obtain the license.  To obtain a License ID for cryoSPARC, go to https://cryosparc.com/download, fill out the form and submit it. Then on approval, you will receive an email with a license ID number. (Store license ID in some safe place in your home space)</li> <li> <p>Log into a development node with GPU on HPCC. (NOTE: Use only dev-amd20-v100 due to the GPU driver version requirement.)</p> </li> <li> <p>Determine where you'd like to install cryoSPARC and create the installation directory.  User should install this software in $HOME or $RESEARCH space. We use CryoSPARC in home directory as the installation directory in this document as an example. <pre><code>mkdir ~/CryoSPARC          # create the installation directory      \ncd ~/CryoSPARC             # go to the install directory\n</code></pre></p> </li> </ul> <p>Note</p> <p>`The installation directory could be any directory under the user's home or research space where use has full access permission. This will be the root directory where all cryoSPARC code and dependencies will be installed.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#download-software","title":"Download software","text":"<ul> <li>Set environment variable: run <pre><code>export LICENSE_ID=\"&lt;license_id&gt;\"\n</code></pre>     where  is the license ID you received from the registration. <li>Download package to the install directory <pre><code>cd ~/CryoSPARC\ncurl -L https://get.cryosparc.com/download/master-latest/$LICENSE_ID -o cryosparc_master.tar.gz \ncurl -L https://get.cryosparc.com/download/worker-latest/$LICENSE_ID -o cryosparc_worker.tar.gz\n</code></pre></li> <li>Extract the downloaded files:  <pre><code>tar -xf cryosparc_master.tar.gz cryosparc_master             \ntar -xf cryosparc_worker.tar.gz cryosparc_worker\n</code></pre></li> <p>Note</p> <p>After extracting the worker package, you may see a second folder called cryosparc2_worker (note the 2) containing a single version file. This is here for backward compatibility when upgrading from older versions of cryoSPARC and is not applicable for new installations. You may safely delete the cryosparc2_worker</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#installation-of-master","title":"Installation of Master","text":"<ul> <li>Load environment <pre><code>module purge                           # unload previous loaded modules.\nmodule load foss/2022b                 # load the latest compile and dependency.\n</code></pre></li> <li> <p>Master node Installationi <pre><code>cd &lt;dir_master_package&gt;        # go to master package directory.\n\n./install.sh --license $LICENSE_ID \\\n             --hostname &lt;master_hostname&gt; \\\n             --dbpath &lt;db_path&gt; \\\n             --port &lt;port_number&gt; \\\n             [--insecure] \\\n             [--allowroot] \\\n             [--yes] \\\n</code></pre> Example: <pre><code>cd ~/CryoSPARC/cryosparc_master           # go to master package directory.\n./install.sh --license $LICENSE_ID \\\n             --hostname localhost \\\n             --dbpath ~/CryoSPARC/cryoSPARC_database \\\n             --port 45000 \\\n</code></pre></p> </li> <li> <p>Start cryoSPARC: run <pre><code>export CRYOSPARC_FORCE_HOSTNAME=true\nexport CRYOSPARC_MASTER_HOSTNAME=$HOSTNAME\n./bin/cryosparcm start\n</code></pre></p> </li> <li>Create your user account of cryoSPARC: run <pre><code>./bin/cryosparcm createuser --email \"&lt;user email&gt;\" \\\n                      --password \"&lt;user password&gt;\" \\\n                      --username \"&lt;login username&gt;\" \\\n                      --firstname \"&lt;given name&gt;\" \\\n                      --lastname \"&lt;surname&gt;\"\n</code></pre></li> </ul> <p>Note</p> <p>For details of the meaning of the options of above master node installalation steps, See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/downloading-and-installing-cryosparc#glossary-reference-1.</p> <p>After completing the above, you are ready to access the user interface.</p> <ul> <li> <p>Access the user interface: navigate your browser to <code>http://&lt;master_hostname&gt;:&lt;port_number&gt;</code></p> <p>If you are physically using the same machine as the master node to interact with the cryoSPARC interface, you can connect to it as: <code>http://localhost:&lt;port_number&gt;</code> See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/accessing-cryosparc for more information.</p> </li> </ul>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#installation-of-worker","title":"Installation of worker:","text":"<ul> <li>Load environment if not yet done it <pre><code>module purge                           # unload previous loaded modules.\nmodule load foss/2022b                 # load the latest compile and dependency.\nmodule load CUDA/12.3.0                # cuda is needed for worker\n</code></pre></li> <li>Worker node Installation</li> </ul> <p><pre><code>cd &lt;install_path&gt; /cryosparc_worker\n\n./install.sh --license $LICENSE_ID \\\n             [--yes]\n</code></pre> Example: using CUDA/12.3.0 module. <pre><code>./install.sh --license $LICENSE_ID\n</code></pre></p> <p>Note</p> <p>For the meaning of the options of worker node installation script, See https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/downloading-and-installing-cryosparc#worker-installation-glossary-reference.</p> <p>Note</p> <p>Once the master and worker are successfully installed at the dev-node, stop the current cryosparc session using command \" ~/CryoSPARC/cryosparc_master/bin/cryosparcm stop\"</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#start-an-interactive-session-of-cryosparc-using-ondemand","title":"Start an interactive session of CryoSPARC using ondemand","text":"<ul> <li> <p>Request an interactive desktop with the number of GPUS equal to the number of workers. You need to request withthe sufficient resources (CPU, memory, wall time, etc.). Open a terminal on the desktop. </p> </li> <li> <p>Launch cryosparc master:  For the users convenience, create a file named \"cryosparc.sh\" containing commands for setting up environment (shown below). Before launch CryoSPARC, run command \"source cryosparc.sh\" first. <pre><code>#!/bin/bash\n# set up CryoSPARC environment\n#\n\n# Load modules\nmodule purge\nmodule load foss/2022b\n\n# set PATH\nexport PATH=~/cryosparc/cryosparc_master/bin:~/cryosparc/cryosparc_worker/bin:$PATH\nexport CRYOSPARC_FORCE_HOSTNAME=true\nexport CRYOSPARC_MASTER_HOSTNAME=$HOSTNAME\n</code></pre></p> </li> </ul>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#connect-a-cluster-to-cryosparc","title":"Connect a Cluster to CryoSPARC","text":"<p>Once the cryosparc_worker package is installed, the cluster must be registered with the master process. This requires a template for job submission commands and scripts that the master process will use to submit jobs to the cluster scheduler. To register the cluster, provide cryoSPARC with the following two files and call the cryosparcm cluster connect command: - cluster_info.json - cluster_script.sh</p> <p>The first file (cluster_info.json) contains template strings used to construct cluster commands (e.g., qsub, qstat, qdel etc., or their equivalents for your system). The second file (cluster_script.sh) contains a template string to construct appropriate cluster submission scripts for your system. The jinja2 template engine is used to generate cluster submission/monitoring commands as well as submission scripts for each job.</p> <ul> <li>Create the files  The following fields are required to be defined as template strings in the configuration of a cluster. Examples for SLURM are given; use any command required for your particular cluster scheduler. Note that parameters listed as \"optional\" can be omitted or included with their value as null.</li> </ul> <p>cluster_info.json: <pre><code>name               :  \"cluster1\"\n# string, required\n# Unique name for the cluster to be connected (multiple clusters can be \n# connected)\n\nworker_bin_path    :   \"/path/to/cryosparc_worker/bin/cryosparcw\"\n# string, required\n# Path on cluster nodes to the cryosparcw script\n\ncache_path         :   \"/path/to/local/SSD/on/cluster/node\"\n# string, optional\n# Path on cluster nodes that is a writable location on local SSD on each \n# cluster node. This might be /scratch or similar. This path MUST be the \n# same on all cluster nodes. Note that the installer does not check that \n# this path exists, so make sure it does and is writable. If you plan to \n# use the cluster nodes without SSD, you can omit this field.\n\ncache_reserve_mb   :   10000\n# integer, optional\n# The size (in MB) to initially reserve for the cache on the SSD. This \n# value is 10GB by default, which means cryoSPARC will always leave at\n# least 10GB of space on the SSD free.\n\ncache_quota_mb     :   1000000\n# integer, optional\n# The maximum size (in MB) to use for the cache on the SSD.\n\nsend_cmd_tpl       :   \"{{ command }}\"\n# string, required\n# Used to send a command to be executed by a cluster node (in case the \n# cryosparc master is not able to directly use cluster commands). If your \n# cryosparc master node is able to directly use cluster commands \n# (like qsub etc) then this string can be just \"{{ command }}\"\n\nqsub_cmd_tpl       :   \"sbatch {{ script_path_abs }}\"\n# string, required\n# The command used to submit a job to the cluster, where the job \n# is defined in the cluster script located at {{ script_path_abs }}. This \n# string can also use any of the variables defined in cluster_script.sh \n# that are available when the job is scheduled (e.g., num_gpus, num_cpus, etc.,)\n\nqstat_cmd_tpl      :   \"squeue -j {{ cluster_job_id }}\"\n# string, required\n# Cluster command that will report back the status of cluster job with its id \n# {{ cluster_job_id }}.\n\nqdel_cmd_tpl       :   \"scancel {{ cluster_job_id }}\"\n# string, required\n# Cluter command that will kill and remove {{ cluster_job_id }} from the \n# queue.\n\nqinfo_cmd_tpl      :   \"sinfo --format='%.8N %.6D %.10P %.6T %.14C %.5c %.6z %.7m %.7G %.9d %20E'\"\n# string, required\n# General cluster information command\n\ntransfer_cmd_tpl   :   \"scp {{ src_path }} loginnode:{{ dest_path }}\"\n# string, optional\n# Command that can be used to transfer a file {{ src_path }} on the cryosparc \n# master node to {{ dest_path }} on the cluster nodes. This is used when the \n# master node is remotely updating a cluster worker installation. This is \n# optional; if it is incorrect or omitted, you can manually update the \n# cluster worker installation.\n</code></pre> Along with the above commands, a complete cluster configuration requires a template cluster submission script. The script must send jobs into your cluster scheduler queue and mark them with the appropriate hardware requirements. The cryoSPARC internal scheduler submits jobs with this script as their inputs become ready. The following variables are available for use used within a cluster submission script template. When starting out, example templates may be generated with the commands explained below. <pre><code>{{ script_path_abs }}    # absolute path to the generated submission script\n{{ run_cmd }}            # complete command-line string to run the job\n{{ num_cpu }}            # number of CPUs needed\n{{ num_gpu }}            # number of GPUs needed.\n{{ ram_gb }}             # amount of RAM needed in GB\n{{ job_dir_abs }}        # absolute path to the job directory\n{{ project_dir_abs }}    # absolute path to the project dir\n{{ job_log_path_abs }}   # absolute path to the log file for the job\n{{ worker_bin_path }}    # absolute path to the cryosparc worker command\n{{ run_args }}           # arguments to be passed to cryosparcw run\n{{ project_uid }}        # uid of the project\n{{ job_uid }}            # uid of the job\n{{ job_creator }}        # name of the user that created the job (may contain spaces)\n{{ cryosparc_username }} # cryosparc username of the user that created the job (usually an email)\n</code></pre></p> <p>Note</p> <p>The cryoSPARC scheduler does not assume control over GPU allocation when spawning jobs on a cluster. The number of GPUs required is provided as a template variable. Either your submission script or your cluster scheduler is responsible for assigning GPU device indices to each job spawned based on the provided variable. The cryoSPARC worker processes that use one or more GPUs on a cluster simply use device 0, then 1, then 2, etc. Therefore, the simplest way to correctly allocate GPUs is to set the CUDA_VISIBLE_DEVICES environment variable in your cluster scheduler or submission script. Then device 0 is always the first GPU that a running job must use. </p> <ul> <li> <p>Load script and register the integration.</p> <p>To create or set a configuration for a cluster in cryoSPARC, use the following commands.</p> </li> </ul> <pre><code>cryosparcm cluster example &lt;cluster_type&gt;\n# dumps out config and script template files to current working directory\n# examples are available for pbs and slurm schedulers but others should \n# be very similar\n\ncryosparcm cluster dump &lt;name&gt;\n# dumps out existing config and script to current working directory\n\ncryosparcm cluster connect\n# connects new or updates existing cluster configuration, \n# reading cluster_info.json and cluster_script.sh from the current directory, \n# using the name from cluster_info.json\n\ncryosparcm cluster remove &lt;name&gt;\n# removes a cluster configuration from the scheduler\n</code></pre> <p>Note</p> <p>The command <code>cryosparcm cluster connect</code> attempts reading cluster_info.json and cluster_script.sh from the current working directory.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-16_LabNotebook_Cryosparc_installation/#examples-of-cluster_infojson-and-cluster_scriptsh-scripts-for-slurm-on-hpcc","title":"Examples of cluster_info.json and cluster_script.sh scripts for SLURM on HPCC:","text":"<p><code>cluster_info.json</code> <pre><code>{\n\"qdel_cmd_tpl\": \"scancel {{ cluster_job_id }}\",\n\"worker_bin_path\": \"/mnt/home/wangx147/cryoSPARC/cryosparc_worker/bin/cryosparcw\",\n\"title\": \"test_cluster\",\n\"cache_path\": \"/tmp\",\n\"qinfo_cmd_tpl\": \"sinfo --format='%.8N %.6D %.10P %.6T %.14C %.5c %.6z %.7m %.7G %.9d %20E'\",\n\"qsub_cmd_tpl\": \"sbatch {{ script_path_abs }}\",\n\"qstat_cmd_tpl\": \"squeue -j {{ cluster_job_id }}\",\n\"cache_quota_mb\": null,\n\"send_cmd_tpl\": \"{{ command }}\",\n\"cache_reserve_mb\": 10000,\n\"name\": \"test_cluster\"\n}\n</code></pre></p> <p><code>cluster_script.sh</code> <pre><code>#!/bin/bash\n#SBATCH --job-name=cryosparc_{{ project_uid }}_{{ job_uid }}\n#SBATCH --partition=general\n#SBATCH --output={{ job_log_path_abs }}\n#SBATCH --error={{ job_log_path_abs }}\n#SBATCH --nodes=1\n#SBATCH --mem={{ (ram_gb*1000)|int }}M\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task={{ num_cpu }}\n#SBATCH --gres=gpu:{{ num_gpu }}\n#SBATCH --gres-flags=enforce-binding\n\nsrun {{ run_cmd }}\n</code></pre> For more examples, see https://guide.cryosparc.com/setup-configuration-and-management/how-to-download-install-and-configure/cryosparc-cluster-integration-script-examples.</p> <p>Q: Where should these two files be stored?</p> <p>A: Working directory.</p>","tags":["lab notebook","CryoSPARC"]},{"location":"2023-02-27_LabNotebook_HRLDAS/","title":"(2023-02-27) Lab Notebook: HRLDAS","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","HRLDAS"]},{"location":"2023-02-27_LabNotebook_HRLDAS/#hrldas-installation-2023-02-27","title":"HRLDAS installation (2023-02-27)","text":"<p>HRLDAS is the High Resolution Land Data Assimilation System. Its user guide can be found here and its code is located here.</p> <p>To install on ICER, the command sequence is:</p> <p>Clone the repository and submodules <code>git clone --recurse-submodules https://github.com/NCAR/hrldas</code></p> <p>Change to the repository directory  <code>cd ~/hrldas/hrldas</code></p> <p>Configure the setup. For parallel computing, choose option 4. <code>perl configure</code></p> <p>Run these commands in sequence: <pre><code>module purge\nml iccifort/2020.1.217 impi/2019.7.217\nml JasPer/2.0.14\nml netCDF-Fortran/4.5.2\n</code></pre></p> <p>Open the file user_build_options in your favorite editor.</p> <p>In this file, change the following lines:</p> <pre><code>COMPILERF90 = mpiifort\n\nNETCDFMOD = -I/opt/software/netCDF-Fortran/4.5.2-iimpi-2020a/include\n\nNETCDFLIB = -L/opt/software/netCDF-Fortran/4.5.2-iimpi-2020a/lib -lnetcdff -L/opt/software/netCDF/4.7.4-iimpi-2020a/lib64 -L/opt/software/iccifort/2020.1.217/lib/intel64 -lnetcdf -lnetcdf -lm -liomp5 -lpthread\n</code></pre> <p>Run <code>make</code>. The compilation should succeed.</p> <p>Warning</p> <p>This is for Intel MPI compilation. It is likely that other architectures will have performance/instruction issues, especially intel14.</p>","tags":["lab notebook","HRLDAS"]},{"location":"2023-03-14_LabNotebook_Crystal_parallel_submission/","title":"(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>This Lab Notebook was adapted from the documentation kindly provided by the Mendoza Group Wiki available at https://sites.google.com/view/mendozagroup/home</p>","tags":["lab notebook","Crystal"]},{"location":"2023-03-14_LabNotebook_Crystal_parallel_submission/#parallel-submission-script-for-crystal23-on-the-msu-hpcc-2023-03-14","title":"Parallel submission script for CRYSTAL23 on the MSU HPCC (2023-03-14)","text":"<p>Usage: Your input file should be called .d12 and be in the same directory as the submission script. The script is submitted by running the command sbatch  <p>Tips: </p> <pre><code>The main parameters you will be changing are the job name (note that it is specified twice in the script), run time (currently set to 30 minutes), nodes/cores (currently 2 nodes, 8 cores per node), memory per cpu, and account (general or mendoza_q). These will depend on the type of job you are running and the amount of resources you would like to allocate.\n\nThere is one #SBATCH line commented out, this is to submit to scavenger queue instead of general or mendoza_q.\n\nThere are two cp lines commented out, these might come in handy to restart calculations.\n</code></pre> <pre><code>#!/bin/bash --login\n#SBATCH --job-name=test\n#SBATCH --account=general\n#SBATCH --time=0-00:30:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=8\n#SBATCH --mem-per-cpu=3G\n#SBATCH -C [intel16|intel18]\n\n\nmodule purge\nmodule load CRYSTAL/23\nUCX_TLS=ud,sm,self\n\n\nexport JOB=test\nexport DIR=$SLURM_SUBMIT_DIR\nexport scratch=/mnt/gs21/scratch/$USER/crys23\n\n\nrm -r $scratch/$JOB\nmkdir $scratch/$JOB/\ncp $DIR/$JOB.d12 $scratch/$JOB/INPUT\ncd $scratch/$JOB\n\n\nunset FORT_BUFFERED\nexport I_MPI_ADJUST_BCAST=3\nexport I_MPI_DEBUG=5\nulimit -s unlimited\nexport OMP_NUM_THREADS=1\n\nexport LD_LIBRARY_PATH=/opt/software/UCX/1.12.1-GCCcore-11.3.0/lib64:$LD_LIBRARY_PATH\n\nmpiexec -n $SLURM_NTASKS -genv UCX_TLS ud,sm,self /opt/software/CRYSTAL/23/bin/Pcrystal &gt; $DIR/${JOB}.out 2&gt;&amp;1 $DIR/${JOB}.out\ncp fort.9 ${DIR}/${JOB}.f9\n</code></pre>","tags":["lab notebook","Crystal"]},{"location":"2023-03-14_LabNotebook_VASP/","title":"(2023-03-14) Lab Notebook: VASP","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>This Lab Notebook was adapted from the documentation kindly provided by the Mendoza Group Wiki available at https://sites.google.com/view/mendozagroup/home</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#vasp621-in-hpcc-2023-03-14","title":"VASP/6.2.1 in HPCC (2023-03-14)","text":"<p>If you want an example of all the files needed to run a test calculation, you can download them from here. </p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#submission-script-parallel-version-hpcc","title":"Submission Script, Parallel version (hpcc)","text":"<p>For a job script example running VASP in parallel, you can use the following script:</p> <pre><code>#!/bin/bash --login\n#SBATCH --ntasks=32\n#SBATCH -N 1-2\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n#SBATCH --time=7-00:00:00\n#SBATCH -p general-long\n#SBATCH --job-name VASP_parallel\n\nexport JOB=VASP_parallel\nexport DIR=$SLURM_SUBMIT_DIR\nmodule load -* intel/2020b VASP/6.2.1\n</code></pre> <p><code>srun vasp &gt; vasp_new.out</code></p> <p>Besides the memory and the walltime request, you can also change the number of tasks and number of nodes in the first and second #SBATCH lines, respectively.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#submission-script-parallel-version-hpcc-saving-some-files-to-the-scratch","title":"Submission Script, Parallel version (hpcc) - Saving some files to the SCRATCH","text":"<pre><code>#!/bin/bash --login\n#SBATCH --ntasks=32\n#SBATCH -N 1-2\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n#SBATCH --time=7-00:00:00\n#SBATCH -p general-long\n#SBATCH --job-name IrMn3_test\n#SBATCH --output=job.out  \n\n\nexport JOB=IrMn3_test\nexport DIR=$SLURM_SUBMIT_DIR\nexport scratch=$SCRATCH/vasp\n\n\nml -* intel/2020b VASP/6.2.1\necho \"scratch job directory: \" \necho $scratch/$JOB             \n\n\nmkdir  -p $scratch/$JOB    \ncp $DIR/*  $scratch/$JOB/  \ncd $scratch/$JOB           \n\n\necho -e \"slurm submission directory:\\n$SLURM_SUBMIT_DIR\" &gt; SUBMITDIR  \nsrun vasp &gt; vasp.out\nsucceed=$?\ncp OUTCAR ${DIR}/OUTCAR        \ncp CONTCAR ${DIR}/CONTCAR      \ncp vasp.out ${DIR}/vasp.out    \n\nif [ $succeed -eq 0 ]; then    \n        cd ${DIR}              \n        rm -rf $scratch/$JOB/WAVECAR          \n        echo \"$JOB finished successfully\"     \nfi\n</code></pre> <p>To use it, please use the following command to load the module:</p> <p><code>ml -* intel/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP.</p> <p>The GPU version of VASP/6.2.1 is also installed</p> <p>[From one of our technicians]</p> <p>I also compiled VASP/6.2.1 with intel and CUDA compilers. You can also load the version with the command:</p> <p><code>ml -* intelcuda/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gpu, vasp_gpu_ncl, vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP.</p> <p>All of the compilations have the interface to Wannier90/3.1.0.</p> <p><code>ml -* intel/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP. The compilation used the intel compilers to compile VASP. </p> <p><code>ml -* intelcuda/2020b VASP/6.2.1</code></p> <p>After that you can use vasp_gpu, vasp_gpu_ncl, vasp_gam, vasp_ncl, vasp_std and vasp (same as vasp_std) to run VASP. All of the compilations have the interface to Wannier90/3.1.0. </p> <p><code>ml -* intel/2020b VASP/6.2.1-VASPsol</code></p> <p>on a dev node to load the version.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-14_LabNotebook_VASP/#some-general-points-to-remember-when-running-vasp-calculations","title":"Some General Points to Remember when Running VASP Calculations","text":"<p>To visualize and modify your structures you can use VESTA: https://jp-minerals.org/vesta/en/download.html</p> <p>For more instructions about VESTA you can use: https://www.youtube.com/channel/UCmOHJtv6B2IFqzGpJakANeg/videos</p> <p>As explained below, VASP calculations require 4 input files: INCAR, KPOINTS,  POSCAR, and POTCAR. </p> <p>One needs to create a separate directory for each calculation and copy the input files to the directory. A brief introduction to the files: </p> <p>(1) INCAR: All calculations set ups such as energy and force convergence, single point  energy calculation, optimization, or MD; 2D vs 3D calculations etc. are defined here. Most keywords are explained (commented). If you want to know about a particular keyword you could search for it in VASP WIKI documentation. Just type the word and VASP in google and it will take you to many online sources explaining it. VASP has been popular among researchers for many years. So, it is easy to find the explanations from online groups. </p> <p>(2) POSCAR: We define atomic positions and lattice vectors here. Note that the atomic positions are in crystal units. To convert it, I usually open the file in VESTA and export again as a POSCAR file, but now in angstrom unit. I don't have a script to automate it. But, will be useful when planning to run several calculations in one go.  </p> <p>(3) KPOINTS; Typically I use gamma-point centered high symmetry points in Brillouine zone. Monkhorst-Pack grids. You may change it according to the problem. </p> <p>(4) POTCAR: Potential files are distributed along with the VASP software and license. I use GGA-PBE PAW pseudo potentials. This is saved as default POTCAR file in the VASP pseudopotential directory in hpc. Once you open POTCAR file, you could see the explanation in the first few lines. </p> <p>If the system has more than one unique element, you need to copy the POTCAR file for each element and concatenate all of them in the same order as you have it in POSCAR file. To highlight the difference for a system with more than one unique element, I have attached a file named POSCAR_2. </p> <p>type module show to get the path for vasp installation and pseudopotential (POTCAR) files.</p> <p>I feel PBE PAW pseudopotentials should be good for most purposes. Test different convergence criteria for your system before starting the production run. </p> <p>To concatenate POTCAR files, use the command, cat ~/pot/Al/POTCAR ~/pot/C/POTCAR ~/pot/H/POTCAR &gt;POTCAR (this also shows that you dont need to coy all POTCAR files into the working directory. Instesd, you could directly create the final POTCAR file in one go). </p> <p>(5) I haven't exported the wave functions files from VASP for any specific purpose. So, I cannot help you with that for now. But, you may check online for guidance. </p> <p>VASP creates a lot of scratch files. So, it can use all the memory very quickly. If you need to run many calculations in one go, plan ahead to delete unnecessary files.</p>","tags":["lab notebook","VASP"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/","title":"(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#port-forwarding-for-jupyter-notebooks-updated-2023-06-30","title":"Port-Forwarding for Jupyter Notebooks (updated 2023-06-30)","text":"<p>The OnDemand system provides a wonderful way to run Jupyter notebooks on the HPCC, but sometimes it may not perfectly meet our needs. Perhaps you are having difficulty setting up the environment in which Jupyter should run, or you want to use the newer JupyterLab instead of the old-style Jupyter notebook server. </p> <p>If you run <code>jupyter notebook</code> from the command line on the HPCC, you may find that a browser window opens just as if you had run the command in a terminal on your own machine. This browser will be very slow to respond, however, as it has launched via the X Windows System.</p> <p>The solution is to run our Jupyter Notebook (or JupyterLab) server with port-forwarding, such that the server is running on the HPCC but we use the browser on our local machine to connect to it.</p> <p>The following information is modified from instructions Philipp Grete wrote for Brian O'Shea's research group.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#pre-requisites","title":"Pre-Requisites","text":"<p>These instructions assume you have access to a terminal on your local machine. Users interested in attempting this with MobaXterm should follow these instruction instead.</p> <p>As a prerequisite, set up an SSH Key and SSH Tunneling.</p> <p>Then, add the following to the bottom of your local <code>.ssh/config</code> file (this is the same file edited when setting up SSH tunneling):</p> <pre><code>    Host lac* vim* skl* nvl* amr* nvf* nif* nal* acm*\n    User here_you_put_your_net_id\n    ProxyJump intel18\n</code></pre> <p>The three-letter names correspond to the kinds of nodes available from each cluster. This will make it possible to SSH to compute nodes from your local machine! This is useful if you would like to run a Jupyter server from an interactive job.</p> <p>Note</p> <p>You can only SSH to a compute node if you have job running on that node.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#setting-up-a-server-on-the-hpcc","title":"Setting up a Server on the HPCC","text":"<p>After preparing your local machine, open a terminal and connect to a development node or compute node with an interactive job.</p> <p>Either from the command line or within a batch script, you should launch your Jupyter server on the HPCC with the following command: </p> <p><code>jupyter notebook --port=12345 --ip=\"*\" --no-browser</code> </p> <p>where <code>12345</code> can be a 5-digit number between 10000 and 65535. If you and someone else happen to be using the same port at the exact same time, issues may arise. Simply cancel the server and restart with a new port number.</p> <p>If using JupyterLab, simply swap <code>notebook</code> for <code>lab</code>.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Jupyter_port-forward_servers/#connecting-to-the-remote-server-from-your-local-machine","title":"Connecting to the Remote Server from your Local Machine","text":"<p>Having set up the server on the HPCC, we now need to connect to it from our local machine.</p> <p>In the same terminal that is connected to the HPCC, type <code>~C</code> (do not press Enter). You should see an <code>ssh&gt;</code> prompt appear.</p> <p>Warning</p> <p>These instructions will not work if you use the OnDemand development node terminals to access the HPCC.</p> <p>You'll type the following information without spaces:</p> <p><code>-L&lt;localport&gt;:&lt;remotehost&gt;:&lt;remoteport&gt;</code></p> <p>For example, <code>-L12345:lac-250:12345</code>. Then press Enter.</p> <p>Each piece of information, from right to left, is as follows:</p> <ul> <li><code>&lt;remoteport&gt;</code> is the port you entered when launching your Jupyter server. For this example, it was <code>12345</code>.</li> <li><code>&lt;remotehost&gt;</code> is the name of the host your server is running on. This could be a development node (e.g. <code>dev-amd20</code>) or it could be a compute note (e.g. <code>lac-250</code>). For this example, we'll assume a compute node. This is why setting up SSH Tunneling is required: it's the only way to connect directly to development and compute nodes!</li> <li><code>&lt;localport&gt;</code> is another 5-digit number like <code>&lt;remoteport&gt;</code>. For simplicity, we'll make it the exact same as <code>&lt;remoteport&gt;</code> (but it doesn't have to be).</li> </ul> <p>When you launched the Jupyter server, you should have been given a URL that starts with <code>http://127.0.0.1</code>. This is the URL from which you should access your server; copy and paste it into your browser. It should already have the notebook token as part of the URL, but if you are asked for it, copy the string of numbers and letters that follows <code>?token=</code>.</p>","tags":["lab notebook","Jupyter","ssh"]},{"location":"2023-03-27_LabNotebook_Mathematica_FixingStartupIssues/","title":"(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Mathematica"]},{"location":"2023-03-27_LabNotebook_Mathematica_FixingStartupIssues/#lab-notebook-fixing-startup-errors-in-mathematica-2023-03-27","title":"Lab Notebook --- Fixing Startup Errors in Mathematica (2023-03-27)","text":"<p>If you have errors upong starting the Mathematica GUI or notebook, particular if the error involves a value called \"Persistance\", you may find that some features of the program (such as starting parallel kernels) do not work.</p> <p></p> <p>This issue appears to be related/can be fixed by resetting Mathematica's preferences, caches, and history, as mentioned here. While this process should not cause any data loss, we DON NOT recommend immediately deleting the listed files. Instead rename the .Mathematica and .Wolfram folders in your home directory to .Mathematica.save and .Wolframe.save, respectively (this way, the folders can be reverted if something is lost). Please do not change any of the other folders listed in the link without conctacting us first (as we have not tested changing them).</p> <p>Renaming these folders with the 'mv' command from the command line:</p> <pre><code>mv .Mathematica .Mathematica.save\nmv .Wolfram .Wolfram.save\n</code></pre> <p>Once this is done, you can restart Mathematica, which should create new .Mathematica and .Wolfram folders and this should resolve the issue. If it does not, please open a ticket with us.</p>","tags":["lab notebook","Mathematica"]},{"location":"2023-04-04_LabNotebook_OnDemand_RunningMPIJob/","title":"(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","OnDemand","MPI"]},{"location":"2023-04-04_LabNotebook_OnDemand_RunningMPIJob/#running-mpi-job-throughs-the-ondemand-interactive-desktop-terminal-2023-04-04","title":"Running MPI Job throughs the OnDemand Interactive Desktop Terminal (2023-04-04)","text":"<p>To run an MPI job through the OnDemand Interactive Desktop, you need to make sure that your resource request through OnDemand asks for at least two (2) tasks under \"Advanced Options\" and that the number of cores per task (N) is set to the number that you want to the MPI job with. After starting the Interactive Desktop, go to the Terminal and you should be able to run your job with:</p> <p>srun --ntasks=N --cpus-per-task=1  <p>For example, if your want to run a job with 4 tasks inside of the Interactive Terminal, your resrouce request would look like:</p> <p></p> <p>And then your would be able to run your job with</p> <p>srun --ntasks=4 --cpus-per-task=1 some_job.sh</p>","tags":["lab notebook","OnDemand","MPI"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/","title":"SLURM --- Changes to <code>srun</code> for jobs with multiple CPUs per task","text":"<p>ICER has recently updated the HPCC's SLURM scheduling system to version 23.02. With this update comes a change to the behavior of <code>srun</code> that affects jobs using multiple cores per task.</p> <p>Take the following SLURM job script as an example. Here we request 5 tasks (or processes) where each tasks is being executed by 2 CPUs, for a total of 10 CPUs: <pre><code>#!/bin/bash/ --login\n\n#SBATCH --time=4:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=5\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=2G \n\nsrun my_job.exe --arg1 foo --arg2 bar\n</code></pre></p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#how-srun-used-to-work","title":"How <code>srun</code> used to work","text":"<p>Historically, <code>srun</code> would run <code>my_job.exe</code> with 5 tasks and 2 CPUs per task by default. No additional command line arguments need to be specified for <code>srun</code> to execute in this way.</p> <p>This is because SLURM sets a number of environment variables for a job which describe the resources requested. Specifically,</p> <ul> <li>The <code>--ntasks</code> (or <code>-n</code>) request is saved to the <code>SLURM_NTASKS</code> variable</li> <li>The <code>--cpus-per-task</code> (or <code>-c</code>) request is saved to the <code>SLURM_CPUS_PER_TASK</code></li> </ul> <p>Then, <code>srun</code> inherits values from these environment variables.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#how-srun-works-now","title":"How <code>srun</code> works now","text":"<p>As of version 22.05, <code>srun</code> no longer reads the variable <code>SLURM_CPUS_PER_TASK</code>.</p> <p>Instead, you must now re-specify the CPUs per task when calling <code>srun</code>. The final line of our example batch script should be replaced with: <pre><code>srun --cpus-per-task=2 my_job.exe --arg foo\n</code></pre> or <pre><code>srun -c 2 my_job.exe --arg foo\n</code></pre></p> <p>The environment variable <code>SLURM_CPUS_PER_TASK</code> is still available to the user, so it is also possible to execute <code>srun</code> by passing the value of this variable to the <code>--cpus-per-task</code>/<code>-c</code> option: <pre><code>srun -c $SLURM_CPUS_PER_TASK my_job.exe --arg foo\n</code></pre> The advantage of this approach is that users only need to adjust the number of CPUs per task at the top of their batch scripts.</p> <p>Alternatively, you can set the new <code>SRUN_CPUS_PER_TASK</code> environment variable: <pre><code>export SRUN_CPUS_PER_TASK=2\nsrun my_job.exe --arg foo\n</code></pre> or <pre><code>export SRUN_CPUS_PER_TASK=$SLURM_CPUS_PER_TASK\nsrun my_job.exe --arg foo\n</code></pre></p> <p>Warning</p> <p>If you are using OpenMP, setting OMP_NUM_THREADS will override both the <code>-c</code> option and <code>SRUN_CPUS_PER_TASK</code>.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-04_LabNotebook_srun_threading_changes/#try-it-for-yourself","title":"Try it for yourself","text":"<p>You can test the behavior of <code>srun</code> with a hybrid MPI/OpenMP example available through <code>getexample</code>:</p> <pre><code>getexample MPI_OpenMP_GPU\n\ncd MPI_OpenMP_GPU\nmake\n</code></pre> <p>Then, request a 30 minute interactive job with 5 tasks and 2 CPUs per task: <pre><code>salloc -n 5 -c 2 -t 00:30:00\n</code></pre></p> <p>Once your interactive job starts, try running: <pre><code>srun hybrid\nsrun -c 2 hybrid\n</code></pre></p> <p>You should see that <code>srun hybrid</code>, without any additional arguments, allocated 10 CPUs to each of the 5 processes. On the other hand, <code>srun -c hybrid</code> has the expected behavior of 2 CPUs for each of the 5 processes.</p> <p>Now, set the <code>SRUN_CPUS_PER_TASK</code> variable: <pre><code>export SRUN_CPUS_PER_TASK=2\nsrun hybrid\n</code></pre></p> <p>You'll see the same behavior as <code>srun -c hybrid</code>.</p>","tags":["lab notebook","slurm","srun"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/","title":"(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#lab-notebook-using-easybuild-to-install-a-postgresql-compatible-with-r-2023-05-18","title":"Lab Notebook --- Using EasyBuild to install a PostgreSQL compatible with R (2023-05-18)","text":"","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#problem-setup","title":"Problem setup","text":"<p>A user wanted to use PostgreSQL with the R/4.2.2 module. However, the versions of PostgreSQL installed</p> <pre><code>$ module spider PostgreSQL\n...\nVersions:\n   PostgreSQL/9.6.2-Python-2.7.12\n   PostgreSQL/11.3-Python-3.7.2\n...\n</code></pre> <p>require specific versions of the GCC module to be loaded:</p> <pre><code>$ module spider PostgreSQL/11.3-Python-3.7.2\n...\nYou will need to load all module(s) on any one of the lines below before the \"PostgreSQL/11.3-Python-3.7.2\" module is available to load.\n\n      Core/GCCcore/8.2.0\n      GCCcore/8.2.0\n...\n</code></pre> <p>This conflicts with the version GCC/11.3.0 necessary to load R/4.2.2:</p> <pre><code>$ module spider R/4.2.2\n...\n    You will need to load all module(s) on any one of the lines below before the \"R/4.2.2\" module is available to load.\n\n      Compiler/GCC/11.2.0/OpenMPI/4.1.1\n      Core/GCC/11.2.0  OpenMPI/4.1.1\n      GCC/11.2.0  OpenMPI/4.1.1\n...\n</code></pre> <p>So we need a version of PostgreSQL which is compatible with GCC/11.3.0.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#solution","title":"Solution","text":"<p>We often use EasyBuild to install software on the HPCC. One of the nice things about EasyBuild is that other users can contribute EasyConfigs which are recipes to build and install different types of software.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#loading-easybuild","title":"Loading EasyBuild","text":"<p>To get started, we load the EasyBuild module:</p> <pre><code>$ module purge\n$ module load EasyBuild\n</code></pre> <p>We now have access to the <code>eb</code> command and two aliases defined by MSU HPCC staff: <code>ebF</code> to find EasyConfigs and <code>ebS</code> to install software. We can first check our global EasyBuild configuration using</p> <pre><code>$ eb --show-config\n#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath            (E) = /tmp/grosscra/EASYBUILD\ncontainerpath        (D) = /mnt/home/grosscra/.local/easybuild/containers\ninstallpath          (E) = /opt\ninstallpath-modules  (E) = /opt/modules\ninstallpath-software (E) = /opt/software\nmodule-naming-scheme (E) = MigrateFromEBToHMNS\noptarch              (E) = GENERIC\nrepositorypath       (E) = /mnt/research/helpdesk/EB_Files_4\nrobot-paths          (E) = /mnt/research/helpdesk/EB_Files_4, /opt/software/EasyBuild/4.7.1/easybuild/easyconfigs, /mnt/research/helpdesk/ebfiles\nsourcepath           (E) = /mnt/research/helpdesk/src\n</code></pre> <p>Since I (<code>grosscra</code>) am part of the <code>helpdesk</code> group used by HPCC staff, my <code>--show-config</code> will look different from other user's configuration. In particular, I am set up to install the software into the root directory <code>/opt</code>, with modules going into <code>/opt/modules</code> and the actual software going into <code>/opt/software</code>.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#a-quick-digression-on-local-modules","title":"A quick digression on local modules","text":"<p>For a user not in <code>helpdesk</code> you will have directories in your <code>$HOME</code> directory (e.g., software in <code>$HOME/software</code> and modules in <code>$HOME/modules</code>). Thus, using EasyBuild, you can build your own software. If you add your module directory to your module path using</p> <pre><code>$ module use $HOME/modules\n$ echo $MODULEPATH\n/mnt/home/grosscra/modules:/opt/software/hpcc/modules:/opt/modules/Core\n</code></pre> <p>you can then load modules that you install using the exact same commands you use on the HPCC (e.g., <code>module load PostgreSQL</code>).</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#finding-our-easyconfig","title":"Finding our EasyConfig","text":"<p>So now that we're happy with and (mostly) understand our <code>eb --show-config</code> results, we can try finding the EasyConfig for PostgreSQL we'd like to use. Our first step is to use the <code>ebF</code> alias:</p> <pre><code>$ ebF PostgreSQL\n\nebF_PATH=/opt/software/EasyBuild/4.7.1/easybuild/easyconfigs\n\n====== $ebF_PATH/__archive__/p/PostgreSQL/\nPostgreSQL-9.3.5-intel-2014b.eb\n\n====== $ebF_PATH/p/PostgreSQL/\nPostgreSQL-10.2-intel-2018a-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2017b-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2018a-Python-2.7.14.eb\nPostgreSQL-10.3-foss-2018b.eb\nPostgreSQL-10.3-intel-2017b-Python-2.7.14.eb\nPostgreSQL-10.3-intel-2018a-Python-2.7.14.eb\nPostgreSQL-11.3-GCCcore-8.2.0-Python-2.7.15.eb\nPostgreSQL-11.3-GCCcore-8.2.0-Python-3.7.2.eb\nPostgreSQL-12.4-GCCcore-9.3.0.eb\nPostgreSQL-13.2-GCCcore-10.2.0.eb\nPostgreSQL-13.3-GCCcore-10.3.0.eb\nPostgreSQL-13.4-GCCcore-11.2.0.eb\nPostgreSQL-14.4-GCCcore-11.3.0.eb\nPostgreSQL-9.4.7-intel-2016a-Python-2.7.11.eb\nPostgreSQL-9.5.2-intel-2016a-Python-2.7.11.eb\nPostgreSQL-9.6.0-intel-2016b-Python-2.7.12.eb\nPostgreSQL-9.6.2-foss-2016b-Python-2.7.12.eb\nPostgreSQL-9.6.2-intel-2016b-Python-2.7.12.eb\n</code></pre> <p>This tells us that there many EasyConfigs available to help us install different versions PostgreSQL under different toolchains.</p> What is a toolchain? <p>A toolchain is a set of software dependencies used to install new software. Most often, this is a compiler like GCC or a compiler/MPI pair like GCC and OpenMPI. The most basic toolchains are just single compilers and are labeled using their software version (like <code>GCCcore-11.2.0</code>).</p> <p>EasyBuild organizes installed modules by toolchain. For example, if you look for the R/4.2.2 module file, it's under <code>/opt/modules/MPI/GCC/11.2.0/OpenMPI/4.1.1/R/4.2.2.lua</code> because it was built using a GCC/OpenMPI toolchain.</p> <p>Some of these are so commonly used that EasyBuild groups dependency software into larger toolchains like \"foss\" and \"intel\" that contain a compiler/MPI pair and a number of other common dependencies. These are labeled by their year and an <code>a</code> or <code>b</code> for the first or second half of the year. You can check what's in them by searching for their EasyConfig and showing it with <code>eb --show-ec</code>:</p> <pre><code>$ ebF foss\n...\n====== $ebF_PATH/f/foss/\nfoss-2016.04.eb  foss-2016b.eb    foss-2018b.eb  foss-2021a.eb    foss-2022b.eb\nfoss-2016.06.eb  foss-2017a.eb    foss-2019a.eb  foss-2021b.eb\nfoss-2016.07.eb  foss-2017b.eb    foss-2019b.eb  foss-2022.05.eb\nfoss-2016.09.eb  foss-2018.08.eb  foss-2020a.eb  foss-2022.10.eb\nfoss-2016a.eb    foss-2018a.eb    foss-2020b.eb  foss-2022a.eb\n...\n$ eb --show-ec foss-2022a.eb\neasyblock = 'Toolchain'\n\nname = 'foss'\nversion = '2022a'\n\nhomepage = 'https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain'\ndescription = \"\"\"GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\"\"\"\n\ntoolchain = SYSTEM\n\nlocal_gccver = '11.3.0'\n\n# toolchain used to build foss dependencies\nlocal_comp_mpi_tc = ('gompi', version)\n\n# we need GCC and OpenMPI as explicit dependencies instead of gompi toolchain\n# because of toolchain preparation functions\ndependencies = [\n    ('GCC', local_gccver),\n    ('OpenMPI', '4.1.4', '', ('GCC', local_gccver)),\n    ('FlexiBLAS', '3.2.0', '', ('GCC', local_gccver)),\n    ('FFTW', '3.3.10', '', ('GCC', local_gccver)),\n    ('FFTW.MPI', '3.3.10', '', local_comp_mpi_tc),\n    ('ScaLAPACK', '2.2.0', '-fb', local_comp_mpi_tc),\n]\n\nmoduleclass = 'toolchain'\n</code></pre> <p>We can see that <code>foss</code> includes <code>GCC</code>, <code>OpenMPI</code>, <code>FlexiBLAS</code>, <code>FFTW</code>, <code>FFTW.MPI</code>, and <code>ScaLAPACK</code>.</p> <p>Since we know that R/4.2.2 requires GCC/11.2.0 to load, we look for a PostgreSQL EasyConfig with a compatible toolchain. In this case, we see <code>PostgreSQL-13.4-GCCcore-11.2.0.eb</code>. If a version of PostgreSQL other than 13.4 were required, we would probably need to generate a new EasyConfig, but this version was suitable for the user.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#fixing-dependency-resolution","title":"Fixing dependency resolution","text":"<p>Now let's try to see how the installation will go. Since I'm writing this after having installed PostgreSQL/13.4, the output will look different than before it was installed. Instead, I'll use <code>PostgreSQL-14.4-GCCcore-11.3.0.eb</code> which as of now is still not installed.</p> <p>We can check to see if we're missing any of this install's dependencies on the system using <code>eb -M</code>:</p> <pre><code>$ eb -M PostgreSQL-14.4-GCCcore-11.3.0.eb\n...\n47 out of 47 required modules missing:\n\n* Core | M4/1.4.19 (M4-1.4.19.eb)\n* Core | Bison/3.8.2 (Bison-3.8.2.eb)\n* Core | OpenSSL/1.1 (OpenSSL-1.1.eb)\n* Core | zlib/1.2.12 (zlib-1.2.12.eb)\n* Core | help2man/1.47.4 (help2man-1.47.4.eb)\n* Core | M4/1.4.17 (M4-1.4.17.eb)\n* Core | Bison/3.0.4 (Bison-3.0.4.eb)\n* Core | M4/1.4.18 (M4-1.4.18.eb)\n* Core | flex/2.6.4 (flex-2.6.4.eb)\n* Core | binutils/2.38 (binutils-2.38.eb)\n* Core | GCCcore/11.3.0 (GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | M4/1.4.19 (M4-1.4.19-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | help2man/1.49.2 (help2man-1.49.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | zlib/1.2.12 (zlib-1.2.12-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | Bison/3.8.2 (Bison-3.8.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | flex/2.6.4 (flex-2.6.4-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | binutils/2.38 (binutils-2.38-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | groff/1.22.4 (groff-1.22.4-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | expat/2.4.8 (expat-2.4.8-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | ncurses/6.3 (ncurses-6.3-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | bzip2/1.0.8 (bzip2-1.0.8-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | DB/18.1.40 (DB-18.1.40-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | pkgconf/1.8.0 (pkgconf-1.8.0-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | libreadline/8.1.2 (libreadline-8.1.2-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | UnZip/6.0 (UnZip-6.0-GCCcore-11.3.0.eb)\n* Compiler/GCCcore/11.3.0 | Perl/5.34.1 (Perl-5.34.1-GCC\n...\n</code></pre> <p>So we're missing everything we need... But this doesn't seem right! It even says we're missing GCC/11.3.0, which is definitely installed:</p> <pre><code>$ module spider GCC/11.3.0\n\n----------------------------------------------------------------------------\n  GCC: GCC/11.3.0\n----------------------------------------------------------------------------\n    Description:\n      The GNU Compiler Collection includes front ends for C, C++,\n      Objective-C, Fortran, Java, and Ada, as well as libraries for these\n      languages (libstdc++, libgcj,...).\n\n\n    This module can be loaded directly: module load GCC/11.3.0\n...\n</code></pre> <p>What's happening is that the way modules are searched for on the HPCC are different than the way EasyBuild searches for them. EasyBuild wants to include \"Core/\" in front of the core module names (i.e., those that aren't installed under a toolchain). But if we look at where modules are searched for,</p> <pre><code>$ echo $MODULEPATH\n/opt/software/hpcc/modules:/opt/modules/Core\n</code></pre> <p>the \"Core\" part is already included in the path. This makes it so you don't need to use <code>module load Core/GCC/11.3.0</code> and can get right to the software name you need.</p> <p>To make things work correctly with EasyBuild's expectations , we can add <code>/opt/modules</code> to our module path and try again:</p> <pre><code>$ module use /opt/modules\n$ echo $MODULEPATH\n/opt/modules:/opt/software/hpcc/modules:/opt/modules/Core\n$ eb -M PostgreSQL-14.4-GCCcore-11.3.0.eb\n...\n1 out of 47 required modules missing:\n\n* Compiler/GCCcore/11.3.0 | PostgreSQL/14.4 (PostgreSQL-14.4-GCCcore-11.3.0.eb)\n...\n</code></pre> <p>Much better! Now we're only missing the module we want to install.</p> <p>In the case where we would actually be missing dependencies, EasyBuild would install those for us, so long as we use the <code>--robot</code> option when installing. This is included in the <code>ebS</code> alias by default.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#installing","title":"Installing","text":"<p>Now we're ready to install. We just use the <code>ebS</code> alias with our EasyConfig, and hope things go well!</p> <pre><code>$ ebS PostgreSQL-14.4-GCCcore-11.3.0.eb\n\n... Good luck! ...\n</code></pre>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-05-18_LabNotebook_EasyBuild_PostgreSQL_Installation/#checking-the-installation","title":"Checking the installation","text":"<p>Now that we have installed a version of PostgreSQL compatible with the same GCC that R needs, we can try to load them all</p> <pre><code>$ module purge\n$ module load GCC/11.2.0 OpenMPI/4.1.1 R PostgreSQL\n</code></pre> <p>We get a small warning about OpenMPI/4.1.1 being incompatible with intel14 nodes, but other than that, everything loads correctly.</p>","tags":["lab notebook","EasyBuild","PostgreSQL","R"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/","title":"(2023-07-13) Lab Notebook: Ignoring Local Python Libraries","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/#lab-notebook-ignoring-local-python-libraries-2023-07-13","title":"Lab Notebook --- Ignoring Local Python Libraries (2023-07-13)","text":"<p>Python will automatically populate the internal \"sys.path\" variable with the location of package libraries. One such location is the .local/lib folder of your home directory on HPCC, provided that the python folder there corresponds to python version being used (i.e. python3.6 for running any Python 3.6.x version). This can create conflicts with containerized software (such as Singularity containters) and (potentially) other python environments like conda by causing python to try and load a module from .local/lib instead of the intended source.</p> <p>In principle, this should not cause a problem if .local/lib is low enough on the load order in sys.path, but in practice this is not always the case. Since there is not benefit to having these additional libaries in sys.path from running something Singularity, the issue can be avoided by excluding the .local/lib path from sys.path.</p> <p>To have python ignore the .local/lib folder when populating sys.path, you can set the following environment variable:</p> <pre><code>export PYTHONNOUSERSITE=1\n</code></pre>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-13_LabNotebook_IgnoringLocalPythonLibraries/#example-useage-running-alphafold-from-a-singularity-container","title":"Example Useage: Running AlphaFold from a Singularity container","text":"<p>HPCC has versions of AlphaFold available as Singularity containers (such as /opt/software/alphafold/2.0.0/alphafold.sif). These containers run Python 3.8.x and if you have a .local/lib/python38 folder, you may face an error like the following:</p> <p></p> <p>Setting PYTHONNOUSERSITE as above should resolve this issue</p>","tags":["lab notebook","Python","Singularity"]},{"location":"2023-07-18_LabNotebook_AlphaFold2.3.1_WorkInProgress/","title":"(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Extra</p> <p>The 2.3.1 version of the AlphaFold singularity image is currently non-functional on HPCC. As of Ocotober, 2023, the image is no longer able to find the current libcusolver share library file.  Please use the 2.3.2 version of the image from this point forward. Please see the new documentation</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-07-18_LabNotebook_AlphaFold2.3.1_WorkInProgress/#lab-notebook-instructions-for-alphafold-version-231-singulairty-2023-07-18depcreated-see-warnings","title":"Lab Notebook --- Instructions for AlphaFold version 2.3.1, Singulairty (2023-07-18)(DEPCREATED, see warnings)","text":"<p>These instructions are a work in progression for running AlphaFold version 2.3.1 using the singularity container found at:</p> <pre><code>/opt/software/alphafold/2.3.1/alphafold_2.3.1_latest.sif\n</code></pre> <p>As with other containers in the <code>/opt/software/alphafold/</code> directory, AlphaFold 2.3.1 can be run via Singularity.</p> <p>Howevever, AlphaFold version after 2.3.0 use a database which is formatted differently than pevious versions. This database is located in <code>/mnt/research/common-data/alphafold/database_230</code>.</p> <pre><code>database_230\n\u2500\u2500 bfd -&gt; ../database/bfd/\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mgy_clusters_2022_05.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters.fa -&gt; mgy_clusters_2022_05.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 100d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101m.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102l.cif\n\u2502\u00a0\u00a0 |   ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 pdb_seqres\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_seqres.txt\n\u251c\u2500\u2500 small_bfd -&gt; ../database/small_bfd/\n\u251c\u2500\u2500 uniprot\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniprot.fasta\n\u251c\u2500\u2500 uniref30\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03.md5sums\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 UniRef30_2021_03.tar.1.gz\n\u2514\u2500\u2500 uniref90\n    \u2514\u2500\u2500 uniref90.fasta\n</code></pre> <p>Before running AlphaFold 2.3.1, you need to set</p> <pre><code>export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"  \nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n</code></pre> <p>We also recommend disbaling local Python libraries with the following argument to avoid conflicts with the Python installation within the Singularity container</p> <pre><code>export PYTHONNOUSERSITE=1\n</code></pre> <p>Both the database and Python library parameters will be set automatically if you load the module \"alphafold/2.3.1\"</p> <p>To run Alphafold 2.3.1 via SLURM, please use the following template (for more information about options/flags, please refer to the README on Github).</p> <p>In the script, <code>input.fasta</code> is your input data, and\u00a0you need to set up output_dir. Since the command <code>/usr/bin/hhsearch</code> inside the container does not work on intel14 nodes (<code>Illegal instruction</code>), please use the <code>SBATCH</code> option\u00a0<code>--constraint</code> in the job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name alphafold-run\n#SBATCH --time=08:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=20G\n#SBATCH --constraint=\"[intel16|intel18|amd20]\"\n\n\necho \"Export AlphaFold variables\"\n\nexport ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database_230\"\nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database_230/params\"\nexport PYTHONNOUSERSITE=1\nexport CUDA_VISIBLE_DEVICES=0\nexport NVIDIA_VISIBLE_DEVICES=\"${CUDA_VISIBLE_DEVICES}\"\n\necho \"Start Singularity run\"\nsingularity run --nv \\\n-B $ALPHAFOLD_DATA_OLD_PATH \\\n-B $ALPHAFOLD_DATA_PATH:/data \\\n-B $ALPHAFOLD_MODELS \\\n-B .:/etc \\\n--pwd  /app/alphafold /opt/software/alphafold/2.3.1/alphafold_2.3.1_latest.sif \\ \n--data_dir=/data \\\n--output_dir=output_dir \\\n--fasta_paths=input.fasta  \\\n--uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n--uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\\n--mgnify_database_path=/data/mgnify/mgy_clusters.fa \\\n--bfd_database_path=/mnt/research/common-data/alphafold/database/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n--obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n--pdb_seqres_database_path=/data/pdb_seqres/pdb_seqres.txt \\\n--uniprot_database_path=/data/uniprot/uniprot.fasta \\\n--max_template_date=2020-05-14   \\ # Update the template date if need be\n--model_preset=monomer \\\n--use_gpu_relax=true\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/","title":"(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#lab-notebook-matlab-offload-from-localpc-to-hpcc-2023-07-18","title":"Lab Notebook --- MATLAB offload from localPC to HPCC (2023-07-18)","text":"","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#cluster-configuration","title":"Cluster Configuration","text":"<p>MATLAB allows its users to run MATLAB on their local machine, such as laptop computer and offload the heavy computation to HPCC cluster, and retrieve the results back from the HPCC cluster to local machine. In this document, we</p> <p>provide the instructions for setting up the cluster profile for use in the MATLAB parallel computing toolbox running on a local computer.</p> <p>Following instructions is made from modifying the sample setup from  Configure Using the Generic Scheduler Interface - MATLAB &amp; Simulink specifically for SLURM on HPCC. The diagram illustrating the cluster for the sample system in that document is very similar to our HPCC cluster. </p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#requirements","title":"Requirements","text":"<p>The setup must meet the following conditions:</p> <ul> <li>The client node and cluster login node must support <code>ssh</code> and <code>sftp</code>.</li> <li>The cluster login node must be able to call the <code>sbatch</code> command to submit a job to an SLURM scheduler. You can find more about this in the <code>README</code> file in the <code>nonshared</code> subfolder within the installation folder.</li> </ul> <p>Note: HPCC cluster satisfies the requirements. </p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#configure-a-cluster-profile","title":"Configure a Cluster Profile","text":"<p>Follow these steps to configure the cluster profile. You can modify any of these options depending on your setup.</p> <ol> <li> <p>Down load provided plug-in script from GitHub repository at (https://github.com/mathworks/matlab-parallel-slurm-plugin.git)  and store it to a location that MATLAB clients (MATLAB on your local machine) can access. This location will be used in the cluster profile setting in step 6(10).</p> <p>Ex.  I saved it on my Mac at the directory: /Users/wangxg/matlab-parallel-slurm-plugin.</p> <pre><code>NOTE: the name of the directory will be used in setting up the profile later in step 6( 10 ).\n</code></pre> </li> <li> <p>Start a MATLAB session on the client host.</p> <p>Ex. I use  MATLAB/ R2022b on my Mac.</p> <pre><code>NOTE: the version of matlab used by client and cluster should be the match.\n\nNOTE: step 3 to 7 are for creating a cluster profile for matlab clients to offload jobs to HPCC. A sample profile is created for users as a reference. Users can import this sample profile ( The name of the sample profile file is \u201cRemote-HPCC-R2022b.mlsettings\u201d) and follow the instructions to create one and/or to customize the sample profile for their own use.\n</code></pre> </li> <li> <p>Start the Cluster Profile Manager from the MATLAB desktop. On the Home tab, in the Environment section, select Parallel &gt; Create and Manage Clusters.</p> </li> <li>Create a new profile in the Cluster Profile Manager by selecting Add Cluster Profile &gt; Generic.</li> <li>With the new profile selected in the list, in the Manage Profile section, select Rename and change the profile name to <code>InstallTest</code>. Press Enter. (Note, you can name it anything you like)</li> <li>In the Properties tab, select Edit and provide settings for the following fields:<ol> <li>Set the Description field to For testing installation.</li> <li>Set the JobStorageLocation to the location where you want job and task data to be stored on the client machine (not the cluster location), for example, <code>C:\\Temp\\joblocation</code>. \\ You must not share JobStorageLocation among parallel computing products running different versions. Each version on your cluster must have its own JobStorageLocation.</li> <li>Set NumWorkers to the number of workers for which you want to test your installation.</li> <li>Set NumThreads to the number of threads to use on each worker (By default, MATLAB worker runs sequentially, that is using 1 thread).</li> <li>Set ClusterMatlabRoot to the installation location of MATLAB to run on the worker machines, which is the MATLAB PATH on the cluster.</li> <li>If the cluster uses online licensing, set RequiresOnlineLicensing to true. For HPCC cluster, we use license server instead. If you are using MSU campus license and offloading to MSU HPCC cluster, you do not need to set this.     </li> <li>If you set RequiresOnlineLicensing to <code>true</code>, enter your LicenseNumber. For MSU campus license users, you do not need to set this.</li> <li>Set OperatingSystem to the operating system of your cluster worker machines. The operating system of HPCC cluster is linux.</li> <li>Set HasSharedFilesystem to <code>false</code>. This setting indicates that the client node (your local machine) and worker nodes (HPCC cluster nodes) cannot share the same data location.</li> <li>Set the PluginScriptsLocation to the location of your plugin scripts ( the location is set at step 1) .</li> <li>To set the connection to a remote cluster, under the AdditionalProperties table, select Add or directly edit the property** <code>ClusterHost</code> . For connect to HPCC cluster, the property with name <code>ClusterHost</code>, value <code>hpcc.msu.edu</code>, and type <code>String. </code></li> <li>To run jobs on a remote cluster without a shared file system, under the AdditionalProperties table, select Add or directly edit the property <code>RemoteJobStorageLocation </code>to specify a directory on the cluster for jobs to store related files. For HPCC cluster users, specify a new property with name <code>RemoteJobStorageLocation</code>, value as the path in your home or research space, for example, <code>/mnt/home/&lt;yourNetid&gt;/remoteJobStorage/</code>, and type <code>String</code>.</li> </ol> </li> <li>Click Done to save your cluster profile settings and changes. </li> </ol>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#test-examples","title":"Test examples","text":"<ol> <li> <p>To test if the profile \u201cRemote-HPCC-R2022b\u201d is set correctly for HPCC cluster, users can run the validation from local machine. </p> <p>Start MATLAB version R2022b on your local machine. Note that the version should match the version you will use on HPCC (define by the installation path).</p> <p>From HOME  -&gt; Parallel computing options -&gt; Create and Manage Clusters. Select the profile \u201cRemote-HPCC-R2022b\u201d, run \u201cvalidation\u201d. If you can pass the first 4 stages, the profile should work for offloading. You may ignore the last stage of the validation.</p> </li> <li> <p>To further test the profile setting, users can also run some function or script locally and offload to the cluster, and compare the results. For example, a function get_min is saved in file get_min.m show as </p> </li> </ol> <pre><code>function [s, t] =get_min(niter, n, m)\n%function [s,t] = many_parfor_cluster(niter, n, m)\n% This function runs myfunc.m may time in a parfor loop\n% Workers are created and running on compute nodes\n%\n% INPUT\n% niter: number of times to run.\n% n, m: parameters for implicit.m function call.\n%\n% OUTPUT\n% s: maximum value of niter output of myfunc.m\n% t: elapsed time of the function.\n%\ntic;\nS = zeros(niter,1);\nparfor i =1:niter\n    S(i) = max(svd(rand(n, m)));    % each worker will use 1 threads.\nend\ns = max(S);\nt = toc;\n</code></pre> <p>The function get_min can be run on a local machine as</p> <pre><code>&gt;&gt; parpool('local', 4);    % use a local worker pool\nStarting parallel pool (parpool) using the 'Local' profile ...\nConnected to the parallel pool (number of workers: 4).\n\n&gt;&gt; [s, t] = get_min(100,1000,1000)\n\ns =\n     5.010473515126743e+02\nt =\n     7.260608000000000e+00\n</code></pre> <p>The function get_min can also to offload as a job to run on the cluster. User need to create a cluster using the profile \u201cRemote-HPCC-R2022b\". Then create a job using \u201cbatch\u201d to offload the function \u201cget_min\u201d to the cluster \u201cHPCC\u201d with 4 workers. To obtain the jobs output, user can use function \"fetchOutputs\" after the job is completed. </p> <pre><code>&gt;&gt; HPCC = parcluster('Remote-HPCC-R2022b');    % Create cluster HPCC\n&gt;&gt; Job = batch(HPCC, @get_min, 2, {100,1000,1000},'pool', 4);  %create a job for cluster HPCC with 4 workers\n&gt;&gt; wait(Job);      % wait Job to complete before fetching the results.\n&gt;&gt; X = fetchOutputs(Job)\nX =\n\n  1\u00d72 cell array\n\n    {[501.0474]}    {[8.2506]}\n</code></pre> <p>To monitor the jobs, in the local MATLAB, from HOME -&gt; Parallel computing options -&gt; Monitor Jobs to open the job monitor window. Meanwhile, users can also monitor the jobs from a terminal window of the cluster and use SLURM commands to monitor the jobs.</p>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-07-18_LabNotebook_MATLAB_offload_from_localPC_to_HPCC/#references","title":"References:","text":"<ol> <li>Configure for Third-Party Scheduler Cluster Discovery - MATLAB &amp; Simulink</li> <li>Configure Using the Generic Scheduler Interface - MATLAB &amp; Simulink</li> <li>Customize Behavior of Sample Plugin Scripts - MATLAB &amp; Simulink</li> <li>Parallel Computing Toolbox Plugin for Slurm - File Exchange - MATLAB Central</li> </ol>","tags":["lab notebook","MATLAB","Parallel Comuting Toolbox"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/","title":"(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Extra</p> <p>Changing your shell on HPCC, particularly your login shell, is not recommended. We only  support bash and much of our system, particularly the module system and job scheduler, is set to work with bash. Additionaly, external software (such as VScode) may break with an alternative shell</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#using-an-alternative-linux-shell-on-hpcc","title":"Using an Alternative Linux Shell on HPCC","text":"<p>HPCC only supports bash, but if you would like to use an alternative shell, you may do so at your own risk. Please keep in mind the above warning before your proceed, and if you require help to restore the regular bash shell, please contact HPCC support</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#before-you-change-your-shell","title":"Before you change your shell...","text":"<p>While your default login shell on HPCC is bash, this does not mean you are limited to only running bash shell scripts.</p> <p>Shell scripts are simply text files with series of commands that are a mix of programs that are on the system (grep, date, time, a python program, your program, and 'built-ins' specific to the shell) and syntax for control statements like loops and if specific to the shell in which you are running. </p> <p>Shell scripts typically have a program that runs then, known as the interpreter, which could include a shell (bash, zsh, etc) or a language interpreter (python, Rscript, ruby, julia etc).</p> <p>You can run a shell script in (at least) two ways:</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#1-run-with-an-interpreter","title":"1. Run with an interpreter","text":"<p>Using an interpreter to run you shell script typically follows this syntax <code>&lt;interpereter&gt; &lt;options&gt; &lt;path/to/script&gt;</code></p> <p>A familiar example using python would be:</p> <pre><code>module load python\npython my_python_script.py\n</code></pre> <p>However, the same can be done with different shells:</p> <pre><code>bash my_bash_script.sh\nzsh my_zsh_script.sh\n</code></pre> <p>While HPCC only supports the bash shell for operations as the module system is written for bash,  if you have shell scripts written for the zsh shell, you can still run those using this method on the command line or even inside another bash script (such as a script to launch a job with SLURM).</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#2-using-the-shebang","title":"2. Using the 'shebang'","text":"<p>At the top of any script you run on a Linux system like the hPC, you may include what's known as the \"shebang\" line (which must be the first line, for more information, see the Wikipedia page).</p> <p>The general syntax for this line is <code>&lt;interpreter&gt; [optional-arg]</code></p> <p>Examples of the 'shebang' in use include:</p> <pre><code>#! /usr/bin/bash  : run the commands using bash\n\n#! /usr/bin/bash -l  : run the commands using bash, but using a login shell, typically used for job/slurm submission scripts to mimic behavior when run on the dev nodes\n\n#! /usr/bin/env python : run this script using the active python interpreter\n\n#! /usr/bin/env Rscript --vanilla : run this script using the activate python\n\n#! /usr/bin/zsh  : run this using zsh, even if launched through bash.\n</code></pre> <p>A further explanation of why you might use <code>/usr/bin/env</code> for your python or R scripts can be found here.</p> <p>To launch a script with a shebang, you must also change the permissions to allow execution ( <code>+x</code> ) if that is not already set with <code>chmod u+x my_shell_script.sh</code> which gives you permission to run the script.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#alternative-shells","title":"Alternative shells","text":"<p>If you still find it necessary to change your shell, see below for instructions on how to do so as well as important notes about setting environment variables and making startup scripts for different shells.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#changing-your-shell","title":"Changing your shell","text":"<p>Several common shell choices are available on HPCC:</p> <ul> <li><code>bash</code>: a Bourne-shell (sh) compatible shell with many newer advanced features as well </li> <li><code>tcsh</code>: an advanced variant on <code>csh</code> with all the features of modern shells</li> <li><code>zsh</code>: an advanced shell which incorprates all the functionality of <code>bash</code>, <code>tcsh</code>, and <code>ksh</code> combined</li> <li><code>csh</code>: the original C-style shell</li> </ul> <p>You can change you login shell with the <code>chsh</code> command.</p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#assigning-environment-variables","title":"Assigning Environment Variables","text":"<p>To assign a new value to an environment variable in <code>zsh</code>, you use the same format as <code>bash</code>: <code>export &lt;name&gt;=&lt;value&gt;</code></p> <p>To assign a new value to an environment variable in either <code>tcsh</code> or <code>csh</code>: <code>setenv &lt;name&gt; &lt;value&gt;</code></p>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-02_LabNotebook_AlternativeLinuxShells/#startup-scripts","title":"Startup scripts","text":"<p>A startup script is a shell script which the login process executes. It provides an opportunity to alter your environment. You are free to setup your own startup scripts but be careful to make sure they are set up correctly for both interactive and batch access or it may negatively affect your ability to log in or run batch jobs on the system. Below are startup scripts for each shell:</p> <ul> <li>bash:  <code>~/.bashrc</code></li> <li>tcsh:  <code>~/.chsrc</code> </li> <li>zsh:   <code>~/.zshrc</code> </li> <li>csh:   <code>~/.cshrc</code> </li> </ul>","tags":["lab notebook","Linux","shell","zsh","csh","tcsh"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/","title":"(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#port-forwarding-for-jupyter-notebooks-with-mobaxterm-updated-2023-08-11","title":"Port-Forwarding for Jupyter Notebooks with MobaXterm (updated 2023-08-11)","text":"<p>The OnDemand system provides a wonderful way to run Jupyter notebooks on the HPCC, but sometimes it may not perfectly meet our needs. Perhaps you are having difficulty setting up the environment in which Jupyter should run, or you want to use the newer JupyterLab instead of the old-style Jupyter notebook server. </p> <p>If you run <code>jupyter notebook</code> from the command line on the HPCC, you may find that a browser window opens just as if you had run the command in a terminal on your own machine. This browser will be very slow to respond, however, as it has launched via the X Windows System.</p> <p>The solution is to run our Jupyter Notebook (or JupyterLab) server with port-forwarding, such that the server is running on the HPCC but we use the browser on our local machine to connect to it.</p> <p>The following information is modified from instructions Philipp Grete wrote for Brian O'Shea's research group.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#pre-requisites","title":"Pre-Requisites","text":"<p>These instructions assume you are using Windows and have access to MobaXterm on your local machine. They also assume that you have setup an SSH connection to the HPCC in MobaXterm using these instructions.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#setting-up-a-server-on-the-hpcc","title":"Setting up a Server on the HPCC","text":"<p>After preparing your local machine, open a terminal and connect to a development node or compute node with an interactive job.</p> <p>Either from the command line or within a batch script, you should launch your Jupyter server on the HPCC with the following command: </p> <p><code>jupyter notebook --port=12345 --ip=\"*\" --no-browser</code> </p> <p>where <code>12345</code> can be a 5-digit number between 10000 and 65535. If you and someone else happen to be using the same port at the exact same time, issues may arise. Simply cancel the server and restart with a new port number.</p> <p>If using JupyterLab, simply swap <code>notebook</code> for <code>lab</code>.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#connecting-to-the-remote-server-from-your-local-machine","title":"Connecting to the Remote Server from your Local Machine","text":"<p>Having set up the server on the HPCC, we now need to connect to it from our local machine. There are two steps to this process.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#1-forwarding-the-gateway-node-to-the-development-or-compute-node","title":"1. Forwarding the gateway node to the development or compute node","text":"<p>Open MobaXterm and start a new SSH session to the HPCC gateway. Take note of which gateway node is next to your username when you login. For example</p> <pre><code>[&lt;username&gt;@gateway-02 ~]$\n</code></pre> <p>means that I will use <code>gateway-02</code> in the next step.</p> <p>Instead of logging into a development node directly, you will use an extra command flag to forward the port from where Jupyter is running to the gateway. The template for this is:</p> <pre><code>ssh -L&lt;port&gt;:&lt;jupyter-host&gt;:&lt;port&gt; &lt;dev-node&gt;\n</code></pre> <p>In our case, let's say that we started Jupyter in an interactive session on <code>lac-250</code> with port <code>12345</code>. We can use</p> <pre><code>ssh -L12345:lac-250:12345 dev-intel18\n</code></pre> <p>to forward the port. You can use any development node. If you are running the Jupyter server on a development node, you should use it in between the ports.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#2-forwarding-the-local-port-to-the-gateway-node","title":"2. Forwarding the local port to the gateway node","text":"<p>From the MobaXterm's \"Tools\" menu, choose \"MobaSSHTunnel (port forwarding)\". Select \"New SSH tunnel\" from the window that opens.</p> <p>In the next Window, you will fill out three sections:</p> <ol> <li>Between \"Local clients\" and \"My computer with MobaXterm\"<ul> <li>Forwarded port: This can be any port number but to reduce confusion, you   can use the same port number that you started Jupyter with, in our   example, <code>12345</code>.</li> </ul> </li> <li>Between \"My computer with MobaXterm\" and \"SSH server\"<ul> <li>\"SSH server\": <code>gateway-##.hpcc.msu.edu</code>  where <code>gateway-##</code> is the   gateway node you logged into in the previous step. In the example above,   we used <code>gateway-02</code>.</li> <li>\"SSH login\": Your HPCC username</li> <li>\"SSH port\": <code>22</code></li> </ul> </li> <li>Between \"SSH server\" and \"Remote server\"<ul> <li>\"Remote server\": <code>localhost</code></li> <li>\"Remote port\": The port you used to launch the Jupyter server with. In   our case, <code>12345</code>.</li> </ul> </li> </ol>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#opening-the-jupyter-notebook-on-your-local-computer","title":"Opening the Jupyter notebook on your local computer","text":"<p>When you launched the Jupyter server, you should have been given a URL that starts with <code>http://127.0.0.1</code>. This is the URL from which you should access your server; copy and paste it into the browser on your local computer. It should already have the notebook token as part of the URL, but if you are asked for it, copy the string of numbers and letters that follows <code>?token=</code>.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-11_LabNotebook_Jupyter_and_mobaxterm_port-forward_servers/#reusing-the-port-forwarding-in-the-future","title":"Reusing the port forwarding in the future","text":"<p>MobaXterm will save the local to gateway port forwarding configuration that we set up in step 2 above. You can restart it from the \"MobaSSHTunnel\" menu at any time, however, be aware that you need to match the <code>gateway-##</code> used in this port forwarding with the <code>gateway-##</code> used to forward the port from the compute node to the gateway in step 1. If this gateway node is different than the one saved in MobaXterm, you will need to setup another SSH tunnel in MobaXterm to correspond to that one.</p> <p>You can also setup MobaXterm tunnels for all four gateway nodes (<code>gateway-00</code>, <code>gateway-01</code>, <code>gateway-02</code>, and <code>gateway-03</code>) using the configuration steps above, and choose the corresponding tunnel depending on where your gateway to compute tunnel starts.</p>","tags":["lab notebook","Jupyter","ssh","MobaXterm"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/","title":"(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#incident-report-on-system-slowdown-caused-by-excessive-file-io","title":"Incident report on system slowdown caused by excessive file I/O","text":"<p>On the afternoon of August 28th, 2023, ICER staff received reports about slowdown across multiple development nodes on HPCC. After investigating the issue, we identified a series of jobs that were saturating the file systems transaction capacity. Cancelling these jobs resolved the slowdown and further investigation found previous runs of these jobs are correlated with reported slowdown incidents since late July.</p> <p>Based on our own investigation and working with the user to correct the behavior of the causative jobs, we have identified a number of factors which we believe contributed to the slowdown. We will discuss these below both as an explanation of the incident and as reference for HPCC users to review in order to avoid creating jobs with similar issues in the future.</p>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#factors-contributing-to-slowdown-on-hpcc","title":"Factors contributing to slowdown on HPCC","text":"","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#1-multiple-io-operations-on-small-files-in-the-same-folder-from-many-nodes","title":"1. Multiple I/O operations on small files in the same folder from many nodes","text":"<p>The cause of the slowdown was an excessive number of file input/output (I/O) operations that saturated the file systems transaction capacity, due to jobs frequently recording their state during the run. However, the impact on the file system was magnified by a number of other issues:</p> <ul> <li>Reading and writing to many small files simultaneously in general requires more transactions than a single, large file</li> <li>Reading and writing to many files in the same directory, especially the same file, creates additional overhead on the file system to lock and check files</li> <li>Reading and writing to the same folder from multiple nodes further increases the overhead </li> <li>Reading and writing to a folder where disaster recovery is enabled (home directories), further increases the overhead again</li> </ul> <p>We emphasize the above points to illustrate how different facets of the HPCC can magnify the effect of I/O operations and would make the following recommendations to users developing and using software on our system:</p> <ul> <li>Organizing input and output files for jobs in multiple folders can help reduce impact on the file system</li> <li>Non-essential/intermediate files can be stored in scratch space which is not backed up in disaster recovery</li> <li>Writing output in larger chunks to fewer files less frequently reduces the overall number of file transactions</li> <li>When checkpointing the state of a job, please consider how frequently the job state must be sampled to track significant changes. For example, for systems that converge to a solution, consider dynamically adjusting when the state is saved to be longer as the system converges</li> </ul>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-08-30_LabNotebook_SlowdownIncidentReport/#2-running-the-maximum-of-jobs-for-long-periods-of-time","title":"2. Running the maximum of jobs for long periods of time","text":"<p>Users can queue up to 1000 and run up to 520 jobs at one time (except the scavenger queue). However, running 500+ jobs constantly for a long period of time increases the risk of compounding problems on HPCC. In this case, running multiple simultaneous jobs contributed both to the overall number of I/O operations and the overhead of operating out of the same folder for all jobs. Additionally, past cases where we have had to put holds on a user's account for overuse or unwanted behavior have often involved running a larger number of jobs with a looping process in each job.</p> <p>We do not want to discourage users from making use of HPCC resources to their fullest, but would have the following recommendations if you plan to queue a larger number of jobs such that you will have the maximum number of jobs running for a long period:</p> <ul> <li>Test and benchmark a single run of your jobs before queuing hundreds of instances. Proper testing and benchmarking will help you accurately estimate the resources you need per instance and avoid submitting hundreds of jobs to the queue only to have them fail.</li> <li>Do some arithmetic for all your jobs: How many files will 1000 instances of this job take? How much disk space will the output occupy? How much of the yearly allotment of your CPU/GPU hours will these jobs use?</li> <li>Consider setting the working directory independently for each job to reduce the overhead on the filesystem. Alternatively, you can copy your input data to /mnt/local within a job, which will store it in a temporary folder on the compute node (NOTE: since this a temporary folder, be careful to COPY not MOVE as files here will not be preserved after the job completes)</li> </ul>","tags":["lab notebook","incident report","I/O"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/","title":"(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#installing-mfem-on-the-hpcc","title":"Installing MFEM on the HPCC","text":"<p>Our research group is developing a new MFEM module as part of an NSF CISS award.  Since we need to mess around with the installation it doesn't make sense to install globally.   The following are instructions I used to install MFEM in a folder in my home directory. The videos at the end of this post walk through my process to figure out what needed to be done.  </p> <p>I'm not sure many people will need these specific instructions but I also recorded some over-the-shoulder videos of the process. My hope is that some people may learn something watching these (very long) videos. </p> <p>See https://mfem.org/building/ for the official detailed installation instructions.</p> <ul> <li>Dirk</li> </ul> <p>The first thing you need to do is create a n empty working directory and cd into that empty directory</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#load-the-needed-modules","title":"Load the needed modules","text":"<pre><code>module swap GCCcore GCCcore/11.2.0\nmodule load mesa\nmodule load SDL2\nmodule load glew\nmodule load freetype\nmodule load fontconfig\nmodule load CMake\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#or-better-yet-create-a-script-to-load-modules","title":"or, better yet, create a script to load modules","text":"<pre><code>cat &lt;&lt; EOF &gt; setup.sh\nmodule swap GCCcore GCCcore/11.2.0\nmodule load mesa\nmodule load SDL2\nmodule load glew\nmodule load freetype\nmodule load fontconfig\nmodule load CMake\nEOF\n</code></pre> <p>Run the script using the following command: <pre><code>source ./setup.sh\n</code></pre></p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#download-the-needed-software","title":"Download the needed software","text":"<pre><code>wget https://github.com/mfem/tpls/raw/gh-pages/metis-4.0.3.tar.gz\n\nwget https://bit.ly/mfem-4-5-2 \nmv mfem-4-5.2 mfem-4-5-2.tgz\n\nwget https://bit.ly/glvis-4-2\nmv glvis-4-2 glvis-4-2.tgz\n\nwget https://github.com/hypre-space/hypre/archive/refs/tags/v2.29.0.tar.gz\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#untar-folders","title":"Untar folders","text":"<pre><code>tar -xzvf glvis-4-2.tgz\ntar -xzvf hypre-v2.29.0.tar.gz\ntar -xzvf metis-4.0.3.tar.gz\ntar -xavf mfem-4-5-2.tgz\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-hypre","title":"Build hypre","text":"<pre><code>cd hypre-2.29.0/src/\n./configure --disable-fortran\nmake -j\ncd ../..\nln -s hypre-2.29.0 hypre\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-metis","title":"Build metis","text":"<pre><code>cd metis-4.0.3\nmake OPTFLAGS=-Wno-error=implicit-function-declaration\ncd ..\nln -s metis-4.0.3 metis-4.0\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-mfem","title":"build mfem","text":"<pre><code>cd mfem-4.5.2\nmake parallel -j\ncd examples\nmake -j\ncd ../..\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#download-glm","title":"download glm","text":"<pre><code>git clone https://github.com/g-truc/glm.git\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#build-glvis","title":"build glvis","text":"<pre><code>cd glvis-4.2\nmake MFEM_DIR=../mfem-4.5.2 -j\n</code></pre>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#test-everything","title":"Test everything","text":"<p>Step 1: Start ondemand desktop and open a terminal. Step 2: Run the above module commands or type <code>source ./setup.sh</code>  Step 3: cd to glvis-4.2 directory and run <code>./glivs</code> Step 4: Open a second terminal Step 5: Run the above module commands or type <code>source ./setup.sh</code> Step 6: cd to <code>mfem-4.5.2/examples</code> and run one of the examples</p>","tags":["lab notebook","software"]},{"location":"2023-09-21_LabNotebook_MFEM_Install/#videos","title":"Videos","text":"<p>The following videos were recorded when figuring out how to install FEM.  These are not edited and demonstrate the debugging process. Consider watching at double speed as not to be board.</p> <ul> <li>MFEM Install Part 1</li> <li>MFEM Install Part 2</li> <li>MFEM Install Part 3</li> <li>MFEM Install Part 4</li> <li>MFEM Install Instruction Write-up</li> <li>MFEM Install Test/Summary</li> </ul>","tags":["lab notebook","software"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/","title":"(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#connecting-to-a-singularity-container-with-vs-code-updated-2023-11-03","title":"Connecting to a Singularity container with VS Code (updated 2023-11-03)","text":"","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#problem-setup","title":"Problem setup","text":"<p>A user has a Singularity image containing an installation of a Python installation they use to run their Python code which includes specific scientific libraries. This user also writes code in Visual Studio Code on Windows, and uses things like the integrated debugger, the integrated terminal, and the Python extension for autocomplete.</p> <p>The user would like to connect VS Code to their Singularity container so that VS Code can integrate the environment and Python installation within the container into its IDE features.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#solution","title":"Solution","text":"<p>This solution is based on the following Github comment and issue.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#set-up-the-ssh-configuration","title":"Set up the SSH configuration","text":"<p>If you haven't already, follow the instructions to connect to the HPCC through SSH.</p> <p>Press F1 to open the command pallette and begin typing \"Remote-SSH: Open SSH Configuration File...\", press Enter. Select the file <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\config</code>.</p> <p>Add a new section to the SSH configuration file with the following lines, making sure to replace the pieces in brackets (<code>&lt;&gt;</code>) with your information:</p> <pre><code>Host &lt;container&gt;~intel18\n    HostName dev-intel18\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n    RemoteCommand singularity shell &lt;path_to_singularity_image&gt;.sif\n    RequestTTY yes\n</code></pre> <p>Above, change <code>&lt;container&gt;</code> to a shortname for your container, <code>&lt;netid&gt;</code> to your NetID that you use to access the HPCC, and <code>&lt;path_to_singularity_image&gt;.sif</code> to be the location of your Singularity image on the HPCC. For example, I (<code>grosscra</code>) tested this using a Singularity image on the HPCC at <code>~/tensorflow_latest-gpu-jupyter.sif</code> with Tensorflow installed. So the configuration looks like</p> <pre><code>Host tf~intel18\n    HostName dev-intel18\n    User grosscra\n    ProxyJump grosscra@hpcc.msu.edu\n    RemoteCommand singularity shell /mnt/home/grosscra/tensorflow_latest-gpu-jupyter.sif\n    RequestTTY yes\n</code></pre> <p>You are also free to change the development node to whatever node you like in the <code>Host</code> and <code>HostName</code> lines.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#set-up-vs-code-settings","title":"Set up VS Code settings","text":"<p>We will now set up VS Code so that when it connects to the <code>&lt;container&gt;~intel18</code> host through the \"Remote - SSH\" extension, it will run the <code>RemoteCommand</code> set above to start the container.</p> <p>Open the VS Code Settings JSON files by pressing F1, and type Preferences: Open User Settings (JSON).</p> <p>Important: Make sure you edit the JSON files directly, do not manipulate the settings using VS Code's menu. There is a bug that does not allow you to set the correct settings from the menu.</p> <p>Add the following lines, or if they are already there, ensure that they have these values, replacing <code>&lt;container&gt;</code> with the short name you used in the SSH configuration above and <code>&lt;Windows_Account_Name&gt;</code> with your Windows account:</p> <pre><code>{\n    ...\n    \"remote.SSH.enableRemoteCommand\": true,\n    \"remote.SSH.useLocalServer\": true,\n    \"remote.SSH.serverInstallPath\": {\n        \"&lt;container&gt;~intel18\": \"~/.vs-code-container/&lt;container&gt;\"\n    },\n    \"remote.SSH.configFile\": \"C:/Users/&lt;Windows_Account_Name&gt;/.ssh/config\",\n    ...\n}\n</code></pre> <p>Note that the SSH configuration file path above uses forward-slashes instead of back-slashes.</p> <p>Additionally, if you have a <code>remote.SSH.remotePlatform</code> section in your <code>settings.json</code> with a line corresponding to your <code>&lt;container&gt;~intel18</code> section, remove it. For example, if my <code>settings.json</code> has the lines</p> <pre><code>{\n    ...\n    \"remote.SSH.remotePlatform\": {\n            \"amd20\": \"linux\",\n            \"tf~intel18\": \"linux\"\n        },\n    ...\n}\n</code></pre> <p>I should remove the <code>\"tf~intel18\": \"linux\"</code> line.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#kill-vs-code-servers","title":"Kill VS Code Servers","text":"<p>To ensure that your new connection does not reuse any old connections, you will need to kill those old connections if they exist.</p> <p>Press F1 and type the command \"Remote-SSH: Kill VS Code Server on Host...\". If the option appears, choose <code>intel18</code> (or whatever development node you're using). Repeat and choose the <code>&lt;container&gt;~intel18</code> option.</p> <p>Press F1 and type the command \"Remote-SSH: Kill Local Connection Server For Host...\". If the option appears, choose <code>intel18</code> (or whatever development node you're using). Repeat and choose the <code>&lt;container&gt;~intel18</code> option.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-03_LabNotebook_VSCode_Singularity/#start-the-connection","title":"Start the connection","text":"<p>You are now ready to start the connection. In the future, this is the only step you need to do.</p> <p>Press F1 and type the command \"Remote-SSH: Connect to Host...\". Choose the <code>&lt;container&gt;~intel18</code> option and you should be connected to a VS Code instance running inside your container.</p>","tags":["lab notebook","VS Code","VScode","Singularity"]},{"location":"2023-11-09_LabNotebook_Molpro_Parallel/","title":"(2023-11-09) Lab Notebook: Molpro Parallel","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","Molpro"]},{"location":"2023-11-09_LabNotebook_Molpro_Parallel/#using-molpro-in-parallel-mode","title":"Using Molpro in parallel mode","text":"<p>Molpro 2022.3 can be run using multiple node and CPUs on both the development  and compute nodes. More general information can be found here:  https://www.molpro.net/manual/doku.php?id=running_molpro_on_parallel_computers</p> <p>However, on compute nodes you need to use specific commands so that Molpro will recognise your requested CPUs. For example, to run on 4 CPUs per node:</p> <pre><code>#Your other SBATCH options go above here\n#SBATCH --ntasks-per-node=4\n\n#Load Molpro 2022.3\nmodule purge\nmodule load GCC/11.2.0  OpenMPI/4.1.1 Molpro/molpro_2022.3\n\n#Gather the list of CPUs so Molpro can use them. \n#Add a job number to the file name if you need to run multiple jobs at once.\nsrun -s hostname &gt; nodefile\ncat ./nodefile\n\n#Set up your Molpro directories here as desired\nmkdir -p your_directory\n\n#Run Molpro, note the --nodefile option.\nmolpro -d your_directory -n 4 --nodefile nodefile your_molpro_file\n</code></pre>","tags":["lab notebook","Molpro"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/","title":"(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#lab-notebook-instructions-for-alphafold-version-232-singularity-2023-11-10work-in-progress","title":"Lab Notebook --- Instructions for AlphaFold version 2.3.2, Singularity (2023-11-10)(WORK IN PROGRESS)","text":"<p>Currently, due to certain system limitations, AlphaFold version 2.3.0 and later cannot be installed on HPCC or run as Docker images.</p> <p>Therefore, we are working to make Singularity images created by third-party user available on HPCC (see https://github.com/prehensilecode/alphafold_singularity). As of the writing of this Lab Notebook, the most current image is 2.3.2-1</p> <p>These instructions are a work in progress for running AlphaFold version 2.3.2 using the singularity container found at:</p> <pre><code>/opt/software/alphafold/2.3.2/alphafold_2.3.2-1.sif\n</code></pre> <p>As with other containers in the <code>/opt/software/alphafold/</code> directory, AlphaFold 2.3.2 can be run via Singularity.</p> <p>Howevever, AlphaFold version after 2.3.0 use a database which is formatted differently than pevious versions. This database is located in <code>/mnt/research/common-data/alphafold/database_230</code>.</p> <pre><code>database_230\n\u2500\u2500 bfd -&gt; ../database/bfd/\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mgy_clusters_2022_05.fa\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters.fa -&gt; mgy_clusters_2022_05.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 LICENSE\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5_multimer_v3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 100d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 101m.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102d.cif\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 102l.cif\n\u2502\u00a0\u00a0 |   ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 pdb_seqres\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_seqres.txt\n\u251c\u2500\u2500 small_bfd -&gt; ../database/small_bfd/\n\u251c\u2500\u2500 uniprot\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniprot.fasta\n\u251c\u2500\u2500 uniref30\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03_hhm.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 UniRef30_2021_03.md5sums\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 UniRef30_2021_03.tar.1.gz\n\u2514\u2500\u2500 uniref90\n    \u2514\u2500\u2500 uniref90.fasta\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#runninging-alphafold-232-using-run_singluaritypy","title":"Runninging AlphaFold 2.3.2 using run_singluarity.py","text":"<p>To run AlphaFold 2.3.2, first load the following modules:</p> <pre><code>module load GCC/6.4.0-2.28  OpenMPI/2.1.2 Python/3.6.4 \nmodule load alphafold/2.3.2\n</code></pre> <p>The alphafold/2.3.2 module will set the ALPHAFOLD_DIR, ALPHAFOLD_DATADIR, and ALPHAFOLD_MODELS environment variables for you. </p> <p>We  recommend you use the python script \"run_singularity.py\" (which is also in /opt/software/alphafold/2.3.2/) to work with the Singularity image. This script helps automate many of the more challenging parts of using the image, such as correctly binding paths to your data directories and enabling GPU support. Below is an example of how to run this script:</p> <pre><code>export output_dir=&lt;some_output_folder&gt; # Set the output directory as a enviroment variable as this is what the script expects\n\npython3 ${ALPHAFOLD_DIR}/run_singularity.py \\ \n    --use_gpu \\ #Use the GPU, which makes the neural network calculations faster\n    --output_dir=$output_dir \\ #Here is where I want to put the result\n    --data_dir=${ALPHAFOLD_DATADIR} \\ #Here is where the AlphaFold data like pdb sequences live\n    --fasta_paths=input.fasta \\ #Here is our input fasta sequence\n    --max_template_date=2020-05-14 \\ #When looking for PDB templates, this is the maximum date we will consider\n    --model_preset=monomer \\ #We are predicting a monomeric protein, change to \"multimer\" for multimer \n    --db_preset=reduced_dbs #Use the reduced database\n</code></pre> <p>Note that any of the flags normally passed to AlphaFold should be able to be passed through this script.</p> <p>Additionally, the --output_dir argument must be passed EXACTLY as above using an enviroment variable because the script expects an enviroment variable (i.e. the uses os.environ to fill in the outdir)</p> <p>If you would like to submit an AlphaFold job to SLURM, we have included an example script below. Note that you will need to adust the resource requests (mainly time) depending on the complexity of your protein.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name 2023alphafold\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:1\n#SBATCH -C [nvf|nal|nif] ## We want the good GPUs\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=12G\n#SBATCH -o 2023.log\n\nmodule load GCC/6.4.0-2.28  OpenMPI/2.1.2 Python/3.6.4\nmodule load alphafold/2.3.2\n\necho \"Export AlphaFold variables\"\n# These variables are now set by the module\necho INFO: ALPHAFOLD_DIR=$ALPHAFOLD_DIR\necho INFO: ALPHAFOLD_DATADIR=$ALPHAFOLD_DATADIR\n\nexport output_dir=$SLURM_SUBMIT_DIR/2023 # you chnage this to whatever path you like\n\ncd $SLURM_SUBMIT_DIR\nmkdir -p $output_dir\ntimestamp=$(date)\necho \"Starting AlphaFold at $timestamp\"\n\npython3 ${ALPHAFOLD_DIR}/run_singularity.py \\\n    --use_gpu \\\n    --output_dir=$output_dir \\\n    --data_dir=${ALPHAFOLD_DATADIR} \\\n    --fasta_paths=8IBQ.fasta \\\n    --max_template_date=2023-08-01 \\\n    --model_preset=monomer \\\n    --db_preset=reduced_dbs\n\necho INFO: AlphaFold returned $?\n\ntimestamp=$(date)\necho \"Finishing AlphaFold at $timestamp\"\n</code></pre>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#running-the-singularity-image-manually","title":"Running the singularity image manually","text":"<p>If for whatever reason you need to manually run AlphaFold from the singularity image, we recommend you still run \"run_singularity.py\" first as this script will print the \"singularity run ...\" command it generates. It will be much easier to work from this command, which should have all of the bind paths properly set for the image, than to try to write your own command from scratch.</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-10_LabNotebook_AlphaFold2.3.2/#additional-resources-and-acknowledgement","title":"Additional Resources and Acknowledgement","text":"<p>I would like to thank Dr. Josh Vermaas for helping me troubleshoot this new image and providing the example SLURM script. </p> <p>For additional details about the Singularity image and run_singularity.py script, please see the Github of the original author:</p> <p>https://github.com/prehensilecode/alphafold_singularity</p>","tags":["lab notebook","AlphaFold","Singularity"]},{"location":"2023-11-21_LabNotebook_Git_Research_Spaces_/","title":"(2023-11-21) Lab Notebook: Git in Research Spaces","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Git is a powerful version control tool. However, it does  not interact nicely with multiple users accessing and modifying the same  repository in a shared space.</p> <p>Your research group may have software that you all contribute to and use. If you want to version control it with git, we recommend the following practice:</p> <ul> <li>The git repository in a research space should be pull ONLY. This prevents group permissions issues. Periodic pulls should be run to keep the repository up-to-date with the remote.</li> <li>Development of the software should be handled by group members in their home directories using branches  that can be merged and then pulled into the research space repository.</li> <li>If the PI of the research group would prefer control of the research space repository, it can be placed in a directory with explicit read-only group permissions.</li> </ul> <p>In general, be aware of permissions management  on the HPCC to maintain a working repository.</p>","tags":["lab notebook","git"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/","title":"(2024-01-13) Lab Notebook: Sharing directories with other HPCC users","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#overview","title":"Overview","text":"<p>MSU HPCC's GPFS filesystem supports NFS V4 access control lists (ACLs). By modifying ACLs of your directories and files, you will be able to fine control access by other users. This tutorial provides a step-by-step instruction for setting up file sharing through modifying ACLs. We assume that you've put all your files in a single directory and are ready to share that directory with another HPCC user. </p> <p>To deal with ACLs, you will use three  commands: 1) <code>mmgetacl</code> for obtaining existing ACLs; 2) <code>mmputacl</code> for changing ACLs; and 3) <code>mmdelacl</code> for deleting ACLs. Compared with the basic <code>chmod</code> utility, being able to modify ACLs gives users more fine-grained control over file access.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#viewing-existing-acls","title":"Viewing existing ACLs","text":"<p>To view ACL of a directory or a file, we use the command <code>mmgetacl</code>:</p> <pre><code>$ mmgetacl &lt;my dir&gt;\n$ mmgetacl &lt;my file&gt;\n</code></pre> <p>The output should be mostly self-explanatory; for a complete description of the format of ACL entries, refer to IBM NFS V4 ACL Syntax or IU's supercomputer user's guide.</p>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#example-of-sharing-a-directory","title":"Example of sharing a directory","text":"<p>Let's say under <code>myuser</code>'s home directory, there is a subdirectory named <code>acl_demo</code>. It contains two files and two directories (each dir has one file inside), as shown below. </p> <pre><code>$ ll acl_demo/\ntotal 17K\ndrwxrwx--- 2 myuser mygroup 8.0K Jan  9 12:38 dir1/\ndrwxrwx--- 2 myuser mygroup 8.0K Jan  9 12:38 dir2/\n-rwxrwx--- 1 myuser mygroup  151 Jan  9 22:33 file1\n-rwxrwx--- 1 myuser mygroup  194 Jan  9 12:37 file2\n\n$ tree acl_demo/\nacl_demo/\n\u251c\u2500\u2500 dir1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ffile1\n\u251c\u2500\u2500 dir2\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ffile2\n\u251c\u2500\u2500 file1\n\u2514\u2500\u2500 file2\n\n2 directories, 4 files\n</code></pre> <p>Our goal is to share the entire <code>acl_demo</code> directory with our collaborator  <code>collab</code> on the HPCC. Specifically, we will grant read-only access to <code>collab</code>.</p> <p>First, we need to get the original ACLs for the <code>acl_demo</code> directory and one of the files (say, <code>file1</code>) inside it, using <code>mmgetacl</code> that we've used earlier. The original ACL of a directory/file when created is determined by umask set by <code>myuser</code>. We will save their ACLs to two files in <code>~/tmp</code>, since we want to differentiate directories from files when manipulating their ACLs. Make sure you have created a subdirectory <code>~/tmp</code> in your home directory beforehand.</p> <pre><code>cd # go to home dir (~/) \nmmgetacl acl_demo &gt; ~/tmp/acl-dir.txt # ACL of dirs\nmmgetacl acl_demo/file1 &gt; ~/tmp/acl-file.txt # ACL of files\n\nmore ~/tmp/acl-*.txt | cat # take a look at the two ACL files generated\n</code></pre> <p>Then, we will change the ACLs (for directories and files) using the  <code>mmputacl</code>  command. Its usage is simple:</p> <pre><code>mmputacl -i &lt;ACL entries&gt; &lt;file/dir name&gt;\n</code></pre> <p>The above shows that <code>mmputacl</code> takes an input file (<code>-i</code>) containing user-defined ACL entries. It's important to note that, in this example, we will need to use <code>mmputacl</code> to change ACLs for both directories and files.  This also means that we need to prepare two different input files for <code>mmputacl</code>.</p> <p>To give <code>collab</code> read and execute (execute permission is needed to navigate to the directory) permission of a directory, we only need to add an entry to that directory's existing ACL. The same applies to changing a file.  Below we display two files that each contain the additional entry for granting <code>rx</code> (read and execute) access to user <code>collab</code>:</p> <pre><code>$ cat tmp/dir.acl\nuser:collab:r-xc:allow:FileInherit:DirInherit\n (X)READ/LIST (-)WRITE/CREATE (-)APPEND/MKDIR (X)SYNCHRONIZE (X)READ_ACL  (X)READ_ATTR  (X)READ_NAMED\n (-)DELETE    (-)DELETE_CHILD (-)CHOWN        (X)EXEC/SEARCH (-)WRITE_ACL (-)WRITE_ATTR (-)WRITE_NAMED\n\n$ cat tmp/file.acl\nuser:collab:r-xc:allow\n (X)READ/LIST (-)WRITE/CREATE (-)APPEND/MKDIR (X)SYNCHRONIZE (X)READ_ACL  (X)READ_ATTR  (X)READ_NAMED\n (-)DELETE    (-)DELETE_CHILD (-)CHOWN        (X)EXEC/SEARCH (-)WRITE_ACL (-)WRITE_ATTR (-)WRITE_NAMED\n</code></pre> <p>Above, selected permissions are marked with an <code>X</code>; permissions that are not selected are marked with a <code>-</code>. ACLs for directories have two additional flags (<code>FileInherit</code> and <code>DirInherit</code>) as compared to those for files.  These two files (i.e., <code>dir.acl</code> and <code>file.acl</code>) are saved in <code>~/tmp</code> as before. In practice, you can copy the above content down to your own files and make changes as needed, such as user name and/or permission choices.</p> <p>Our next step is to combine the new entry (which corresponds to the modified permission) with existing ACL. To do so, in the code below we append our newly added entry to the current ACL, for both directories and files. In other words, the two previously generated files, <code>acl-dir.txt</code> and <code>acl-file.txt</code> have been updated now (take a look at the two files to make sure everything is in order).</p> <pre><code>cat ~/tmp/dir.acl &gt;&gt; ~/tmp/acl-dir.txt\ncat ~/tmp/file.acl &gt;&gt; ~/tmp/acl-file.txt\n</code></pre> <p>With all the preparation completed, we are now ready to call <code>mmputacl</code>. In order to make sure all the files and directories inside will be accessible to <code>collab</code>, we need to apply ACL changes at all levels, or, recursively. The following commands accomplish this.</p> <pre><code>cd # go to home dir (~/) \nmmputacl acl_demo -i ~/tmp/acl-dir.txt\ncd acl_demo\nfind . -type d -exec mmputacl \\{} -i ~/tmp/acl-dir.txt \\;\nfind . -type f -exec mmputacl \\{} -i ~/tmp/acl-file.txt \\; \n</code></pre> <p>Here is a breakdown of the above code:</p> <ol> <li>The first line applies the change to the very top level directory <code>acl_demo</code>.  This allows <code>collab</code> to traverse into this directory.</li> <li>The second line brings us inside the <code>acl_demo</code> directory.</li> <li>The third line recursively changes ACLs for subdirectories (<code>dir1</code> and <code>dir2</code>), based on permissions defined in  <code>acl-dir.txt</code>. Furthermore, if there are subdirectories within <code>dir1</code> and/or <code>dir2</code>, their ACLs will be changed in the same manner.</li> <li>The fourth line recursively changes ACLs for files (<code>file1</code>, <code>file2</code>, <code>dir1/ffile1</code>, and <code>dir2/ffile2</code>), based on permissions defined in  <code>acl-file.txt</code>.</li> </ol>","tags":["lab notebook","files"]},{"location":"2024-01-13_LabNotebook_NFSv4ACLs/#optional-deleting-acls","title":"Optional: deleting ACLs","text":"<p>If we want to disable access from the collaborator after file sharing is complete,  we can use <code>mmdelacl</code> to delete extended ACLs we've added to <code>acl_demo</code>. Then the ACLs will be reset to their original entries. To perform deletion, we can use the following commands:</p> <pre><code>cd # go to home dir (~/) \nmmdelacl acl_demo\ncd acl_demo\nfind . -type d -exec mmdelacl \\{} \\;\nfind . -type f -exec mmdelacl \\{} \\; \n</code></pre> <p>After deletion, we can run <code>mmgetacl</code> on those files and directories again, to confirm.</p>","tags":["lab notebook","files"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/","title":"(2024-06-13) Lab Notebook: CTSM Installation","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#ctsm-and-lilac","title":"CTSM and LILAC","text":"<p>CTSM (Community Terrestrial Systems Model) is a land model for geographic data. The latest version is built with LILAC (Lightweight Infrastructure for Land Atmosphere Coupling) to connect land models with atmosphere models.</p> <p>Extensive documentation for CTSM, CESM et al. is available at https://escomp.github.io/ctsm-docs/versions/master/html/lilac/index.html (CTSM-LILAC) and https://escomp.github.io/CESM/versions/master/html/index.html (CESM).</p>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#installation-on-icer","title":"Installation on ICER","text":"<p>The installation of CTSM requires CESM (Community Earth System Model) and its dependencies to be installed first.</p> <ol> <li> <p>Log in to a gateway node. Do not connect to a development node yet.</p> </li> <li> <p>Load <code>git</code> and <code>Subversion</code>.</p> </li> </ol> <pre><code>module purge\nmodule load Subversion\nmodule load git\n</code></pre> <ol> <li>Clone the latest CESM release (2.2.2 as of this notebook) into your working directory.</li> </ol> <pre><code>git clone -b release-cesm2.2.2 https://github.com/ESCOMP/cesm.git cesm-2.2.2\n</code></pre> <ol> <li>Download the external packages for CESM (this will take time).</li> </ol> <pre><code>cd cesm-2.2.2\n./manage_externals/checkout_externals\n</code></pre> <ol> <li> <p>Connect to a development node (this is because CTSM uses Python to handle its dependencies, and the Subversion module does not have a new enough matching Python module.)</p> </li> <li> <p>Clone CTSM.</p> </li> </ol> <pre><code>cd ..\ngit clone https://github.com/ESCOMP/CTSM.git\n</code></pre> <ol> <li>Download the external packages for CTSM.</li> </ol> <pre><code>cd CTSM\n./bin/git-fleximod update\n</code></pre> <ol> <li>Load the required modules for building CTSM.</li> </ol> <pre><code>module purge\nmodule load ESMF/8.6.0-foss-2023a\nmodule load CMake Python lxml Perl XML-LibXML PnetCDF git git-lfs\n</code></pre> <ol> <li>Try building CTSM.</li> </ol> <pre><code>./lilac/build_ctsm ../ctsm_build --os linux --compiler gnu --netcdf-path $EBROOTNETCDF --esmf-mkfile-path $EBROOTESMF/lib/esmf.mk --max-mpitasks-per-node 20 --pnetcdf-path $EBROOTPNETCDF --pio-filesystem-hints gpfs\n</code></pre>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#configuration","title":"Configuration","text":"<p>Warning</p> <p>ESMF is compiled with the FOSS (GNU/GCC) toolchain. Make sure you specify the GNU gcc compiler when building your models.</p> <p>CESM and CTSM use \"machine files\" to understand the environment they are running in. You will need to set up a <code>config_machines.xml</code> file containing the appropriate settings for ICER.</p> <p>These files should be in this directory for a user $USER <code>/mnt/home/$USER/.cime</code></p>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#config_machinesxml","title":"config_machines.xml","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n&lt;!-- This is a partial cime machine port which should be able to get\n          through the case.build step. Trying to get further than that will\n     cause problems, because various directory paths won't be set up\n     properly.\n\n     If you are looking at the template file: Variable names prefixed\n     with a dollar sign will be replaced with machine-specific values. A\n     double dollar sign gets replaced with a single dollar sign, so\n     something like $MYVAR refers to the MYVAR cime variable.\n     --&gt;\n\n&lt;config_machines version=\"2.0\"&gt;\n  &lt;machine MACH=\"dev-amd20\"&gt;\n\n    &lt;!-- Text description of the machine --&gt;\n    &lt;DESC&gt;Temporary build information for a CTSM build&lt;/DESC&gt;\n\n    &lt;!-- OS: the operating system of this machine. Passed to cppflags for\n                  compiled programs as -DVALUE  recognized are LINUX, AIX, Darwin, CNL --&gt;\n    &lt;OS&gt;UBUNTU&lt;/OS&gt;\n\n    &lt;!-- COMPILERS: compilers supported on this machine, comma seperated list, first is default --&gt;\n    &lt;COMPILERS&gt;gnu&lt;/COMPILERS&gt;\n\n    &lt;!-- MPILIBS: mpilibs supported on this machine, comma seperated list,\n                  first is default, mpi-serial is assumed and not required in\n         this list.\n\n         It appears that the particular value is not important here: all\n         that matters is that we're self-consistent. (See\n         https://github.com/ESMCI/cime/issues/3537)\n    --&gt;\n    &lt;MPILIBS&gt;mpich&lt;/MPILIBS&gt;\n\n    &lt;!-- CIME_OUTPUT_ROOT: Base directory for case output,\n                  the case/bld and case/run directories are written below here --&gt;\n    &lt;CIME_OUTPUT_ROOT&gt;/mnt/scratch/$USER/&lt;/CIME_OUTPUT_ROOT&gt;\n\n    &lt;!-- DIN_LOC_ROOT: location of the inputdata data directory\n                  inputdata is downloaded automatically on a case by case basis as\n         long as the user has write access to this directory. --&gt;\n    &lt;DIN_LOC_ROOT&gt;/mnt/scratch/$USER/&lt;/DIN_LOC_ROOT&gt;\n\n    &lt;!-- DIN_LOC_ROOT_CLMFORC: override of DIN_LOC_ROOT specific to CLM\n                  forcing data --&gt;\n    &lt;DIN_LOC_ROOT_CLMFORC&gt;$DIN_LOC_ROOT/&lt;/DIN_LOC_ROOT_CLMFORC&gt;\n\n    &lt;!-- DOUT_S_ROOT: root directory of short term archive files, short term\n               archiving moves model output data out of the run directory, but\n      keeps it on disk--&gt;\n    &lt;DOUT_S_ROOT&gt;$CIME_OUTPUT_ROOT/archive/$CASE&lt;/DOUT_S_ROOT&gt;\n\n    &lt;!-- GMAKE: gnu compatible make tool, default is 'gmake' --&gt;\n    &lt;GMAKE&gt;gmake&lt;/GMAKE&gt;\n\n    &lt;!-- GMAKE_J: optional number of threads to pass to the gmake flag --&gt;\n    &lt;GMAKE_J&gt;12&lt;/GMAKE_J&gt;\n\n    &lt;!-- BATCH_SYSTEM: batch system used on this machine,\n                  supported values are: none, cobalt, lsf, pbs, slurm.\n\n         This is irrelevant for this build-only port.\n    --&gt;\n    &lt;BATCH_SYSTEM&gt;slurm&lt;/BATCH_SYSTEM&gt;\n\n    &lt;!-- SUPPORTED_BY: contact information for support for this system\n               this field is not used in code --&gt;\n    &lt;SUPPORTED_BY&gt;CTSM&lt;/SUPPORTED_BY&gt;\n\n    &lt;!-- MAX_TASKS_PER_NODE: maximum number of threads*tasks per shared\n                  memory node on this machine, should always be &gt;=\n         MAX_MPITASKS_PER_NODE.\n\n         This is irrelevant for this build-only port.\n    --&gt;\n    &lt;MAX_TASKS_PER_NODE&gt;28&lt;/MAX_TASKS_PER_NODE&gt;\n\n    &lt;!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node\n                  on this machine, in practice the MPI tasks per node will not\n         exceed this value.\n    --&gt;\n    &lt;MAX_MPITASKS_PER_NODE&gt;28&lt;/MAX_MPITASKS_PER_NODE&gt;\n\n    &lt;!-- mpirun: The mpi exec to start a job on this machine, supported values\n                  are values listed in MPILIBS above, default and mpi-serial.\n\n         This is irrelevant for this build-only port.\n    --&gt;\n    &lt;mpirun mpilib=\"default\"&gt;\n      &lt;!-- name of the exectuable used to launch mpi jobs --&gt;\n      &lt;executable&gt;srun&lt;/executable&gt;\n      &lt;!-- arguments to the mpiexec command, the name attribute here is ignored--&gt;\n      &lt;arguments&gt;\n              &lt;!-- &lt;arg name=\"anum_tasks\"&gt; -ntasks $TOTALPES&lt;/arg&gt;\n                        &lt;arg name=\"labelstdout\"&gt;-prepend-rank&lt;/arg&gt;--&gt;\n        &lt;arg name=\"num_tasks\"&gt;--ntasks={{ total_tasks }}&lt;/arg&gt;\n      &lt;/arguments&gt;\n    &lt;/mpirun&gt;\n\n    &lt;!-- module system: allowed module_system type values are:\n                  module  http://www.tacc.utexas.edu/tacc-projects/mclay/lmod\n         soft http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html\n         none\n\n         For this build-only port, we don't specify a module system:\n         instead, we assume that the user has loaded all relevant\n         modules prior to starting the build. (Similarly, we assume that\n         the user has set all relevant environment variables, so don't\n         specify any environment variables here.)\n      --&gt;\n    &lt;module_system type=\"none\"/&gt;\n\n    &lt;environment_variables comp_interface=\"nuopc\"&gt;\n            &lt;env name=\"ESMFMKFILE\"&gt;/opt/software-current/2023.06/x86_64/generic/software/ESMF/8.6.0-foss-2023a/lib/esmf.mk&lt;/env&gt;\n            &lt;env name=\"NETCDF_PATH\"&gt;/opt/software-current/2023.06/x86_64/generic/software/netCDF/4.9.2-gompi-2023a&lt;/env&gt;\n            &lt;env name=\"PNETCDF_PATH\"&gt;/opt/software-current/2023.06/x86_64/generic/software/PnetCDF/1.12.3-gompi-2023a&lt;/env&gt;\n    &lt;/environment_variables&gt;\n\n  &lt;/machine&gt;\n&lt;/config_machines&gt;\n</code></pre>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#config_compilerxml","title":"config_compiler.xml","text":"<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n\n&lt;compiler COMPILER=\"gnu\" MACH=\"dev-amd20\"&gt;\n\n&lt;/compiler&gt;\n</code></pre>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-13_LabNotebook_CTSM_Install/#config_batchxml","title":"config_batch.xml","text":"<pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n &lt;batch_system type=\"slurm\" MACH=\"dev-amd20\"&gt;\n    &lt;batch_query per_job_arg=\"-j\"&gt;squeue&lt;/batch_query&gt;\n    &lt;batch_submit&gt;sbatch&lt;/batch_submit&gt;\n    &lt;batch_cancel&gt;scancel&lt;/batch_cancel&gt;\n    &lt;batch_directive&gt;#SBATCH&lt;/batch_directive&gt;\n    &lt;jobid_pattern&gt;(\\d+)$&lt;/jobid_pattern&gt;\n    &lt;depend_string&gt; --dependency=afterok:jobid&lt;/depend_string&gt;\n    &lt;depend_allow_string&gt; --dependency=afterany:jobid&lt;/depend_allow_string&gt;\n    &lt;depend_separator&gt;,&lt;/depend_separator&gt;\n    &lt;walltime_format&gt;%H:%M:%S&lt;/walltime_format&gt;\n    &lt;!-- replace $USER with your username --&gt;\n    &lt;batch_mail_flag&gt;--mail-user=$USER@msu.edu&lt;/batch_mail_flag&gt; \n    &lt;batch_mail_type_flag&gt;--mail-type=$USER@msu.edu&lt;/batch_mail_type_flag&gt;\n    &lt;batch_mail_type&gt;end&lt;/batch_mail_type&gt;\n    &lt;directives&gt;\n      &lt;directive&gt; --job-name={{ job_id }}&lt;/directive&gt;\n      &lt;directive&gt; --nodes={{ num_nodes }}&lt;/directive&gt;\n      &lt;directive&gt; --ntasks-per-node={{ tasks_per_node }}&lt;/directive&gt;\n      &lt;directive&gt; --output={{ job_id }}   &lt;/directive&gt;\n      &lt;directive&gt; --time=02:00:00\u00a0\u00a0 &lt;/directive&gt;\n      &lt;directive&gt; --mem=205G                 &lt;/directive&gt;\n    &lt;/directives&gt;\n    &lt;queues&gt;\n      &lt;queue default=\"true\"&gt;&lt;/queue&gt;\n    &lt;/queues&gt;\n  &lt;/batch_system&gt;\n</code></pre>","tags":["lab notebook","CTSM","ESMF","CIME"]},{"location":"2024-06-19_LabNotebook_LM_Studio_Install/","title":"(2024-06-19) Lab Notebook: LM Studio Installation","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","LLM","AI","AppImage"]},{"location":"2024-06-19_LabNotebook_LM_Studio_Install/#lm-studio","title":"LM Studio","text":"<p>LM Studio is a graphical user interface tool designed for developers and researchers to explore pre-trained LLMs. This tool typically requires quite a bit a ICER resources including multple CPUs, a GPU and a lot of memory.  The following is instrucions for getting LM Studio Downloaded and running on the MSU HPCC. </p>","tags":["lab notebook","LLM","AI","AppImage"]},{"location":"2024-06-19_LabNotebook_LM_Studio_Install/#installation-and-running-llm-studio-on-the-hpcc","title":"Installation and Running LLM Studio on the HPCC.","text":"<p>This tutorial will show you how to download/install and run LLM studio ont he HPCC.  </p> <ol> <li>Connect to the HPCC via an OnDemand using your MSU Login/password.</li> <li>Start an \"Interactive Desktop\" via the \"Interactive Apps\" Tab at the top of the page:<ul> <li>Ask for 4 hours</li> <li>Ask for 10 cores</li> <li>Ask for 300GB of Memory</li> <li>Select checkbox \"I would like to receive an email when the session starts\" (this is helpful because it can take a while to start)</li> <li>Select checkbox \"Advanced Options\" then ask for 1 GPU.</li> </ul> </li> <li>When ready, Click the button to \"Launch Interactive Desktop\"</li> <li>From the \"Menu\" in the upper left select \"Internet\" and then open the \"Firefox\" app inside the online desktop.</li> <li>Navigate to http://lmstudio.ai and select \"Download LM Studio for Linux\" option.</li> <li>From the \"Menu\" in the upper left select \"System Tools\" then select \"MATE Terminal\" </li> <li>Load the CUDA module by typing <code>module load CUDA</code> in the terminal (this may not be required but it dosn't hurt anything).</li> <li>Inside the Terminal command line change to the \"Downloads\" directory by typing <code>cd Downloads</code></li> <li>View that the downloaded file is present by typing <code>ls</code></li> <li>Change permissions on the download file so it can be run by typing <code>chmod 755 ./LM_Studio-0.2.25.AppImage</code> (Note version number may be different from when these instructions were made).</li> <li>Run the downloaded file by typing <code>./LM_Studio-0.2.25.AppImage</code></li> </ol>","tags":["lab notebook","LLM","AI","AppImage"]},{"location":"2024-06-19_LabNotebook_LM_Studio_Install/#optimizing-lm-studio-to-take-advantage-of-the-cpus-and-gpus","title":"Optimizing LM Studio to take advantage of the CPUs and GPUs","text":"<p>Once LM Studio is up and running inside the OnDemand Desktop you will be able to download lM models and try them out.  However, the default settings do not take full advantage of the resources that we requested in step 2 above. Click on the small \"gear\" icon in the upper right of the LM Studio interface to open up settings and make the following changes (Notice that there is a virtical scroll bar and some of the following sections my be outside your current view).</p> <ol> <li>Find the \"Inference Parameters\" and set \"CPU Threads\" n_threads to 10 to match the number of requested cores. </li> <li>Find the \"GPU Acceleration\" section. Make sure \"GPU Offload\" checkbox is set and slide the bar to \"Max\".</li> </ol> <p>NOTE: We are planning to wrap LM Studio in it's own dedicated OnDemand App but this should get most people started. </p>","tags":["lab notebook","LLM","AI","AppImage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/","title":"(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>Extra</p> <p>The information here specifically pertains to the New OS transition and OnDemand outtage in summer 2024</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#introduction","title":"Introduction","text":"<p>The HPCC is experiencing issues with our service related to upgrading the system OS and an unrelated, unplanned issue with the filesystem which is making the system unresponsive and impacting the OnDemand service. Below we will document workarounds to some common issues users have been encountering due to these events.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#new-os","title":"New OS","text":"<p>The transition to the new OS has dramatically change how the module system works on HPCC and some software may be available in different (often newer) versions or yet to be installed on the new OS. Our ongoing plan is support users by contiuning to install software on the new OS and address issues with existing one, but this process may be slower than usual due to the number of request and issues with the file system.</p> <p>The easiest workaround for any issues related to the new OS is to make use of the backwards compatibility containers.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#ondemand","title":"OnDemand","text":"<p>OnDemand is periodically experiencing a communication error between it and the rest of the gateway nodes. When this happens, home or the scratch system becomes temporarily unavailable, making the service inaccessible as it stores certain files in the users home directory. Because this issue is intermittment, users may notice that OnDemand is available at time as we work on it, but be aware that service may drop again without warning. Below we have some alternatives for commonly used servies on OnDemand.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#file-uploaddownload","title":"File Upload/Download","text":"<p>File transfer can be done via the Globus service, which runs from a web browser independent of OnDemand. Documentation for this service can be found here</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Jupyter notebooks running from the command line on HPCC may be accessed on your local computer using port-forwarding. Detailed instruction for the process are available here.</p> <p>Please me aware that this process depends on maintaing two open sessions with HPCC, so it may be unstable if you are frequently being disconnected.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#rstudio-server","title":"RStudio Server","text":"<p>This is only possible on nodes running the new operating system. First, follow the Pre-Requisites on the Jupyter Port-Forwarding page. Then load the <code>powertools</code>, <code>RStudio-Server</code> and <code>R</code> module you wish to use.</p> <pre><code>module purge\nmodule load powertools\nmodule load RStudio-Server\nmodule load R/4.3.2-gfbf-2023a  # For example\n</code></pre> <p>Then run</p> <pre><code>rstudio_server\n</code></pre> <p>and follow the onscreen instructions to connect.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#rstudio","title":"Rstudio","text":"<p>Outdated</p> <p>Since August 6, 2024 legacy nodes are no longer available, and RStudio (the non-Server version) cannot be launched using the new operating system. Please use the RStudio Server option above or contact ICER support if you are experiencing issues.</p> <p>The legacy nodes (see the New OS section) can launch Rstudio session which you can access on your local computer. To run Rstudio interactively you will need to:</p> <ul> <li>Have a X server running on your local machine (MobaXterm on windows, XQuartz for mac)</li> <li>Access HPCC using ssh with the -X option (i.e \"ssh -X username@hpcc.msu.edu)</li> <li>Load R and rstudion (for example: \"module load  GCC/11.2.0 OpenMPI/4.1.1 R/4.2.2 rstudio/2022.07.2-576\")</li> <li>Run \"rstudio\" on the command line, which should open a Rstudio window on you local machine</li> </ul> <p>As a warning, please be aware that multiple session of Rstduio can have the same script, creating a danager of unintentionally overwriting change in your files. Considering manually backing up file your are actively working, particularly if they were open in an  OnDemand Rstduio session before starting a command line session.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-07-01_LabNotebook_OnDemandWorkaround/#vs-code","title":"VS Code","text":"<p>Visual Studio Code may fail to correctly set up its terminal, resulting in <code>module</code> and <code>quota</code> commands failing, among others. As we investigate this issue, please run <code>source /etc/profile</code> in the VS Code terminal after you connect to the HPCC.</p>","tags":["lab notebook","New OS","OnDemand","filesystem","outage"]},{"location":"2024-08-07_LabNotebook_R_ggplot_without_X11/","title":"(2024-08-07) Lab Notebook: Running ggplot in R without X11","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","R","ggplot","X11"]},{"location":"2024-08-07_LabNotebook_R_ggplot_without_X11/#introduction","title":"Introduction","text":"<p>Even when run on the command line, the ggplot function in R requires X11 to function. While HPCC in general provides X11 support, specific environments and/or module loads might intefere with this. If you run into this issue using are, the following link will show you how to use the Caro package to resolve the issue:</p> <p>https://beniamino.org/post/2023-11-16-r-plots-without-x11/</p>","tags":["lab notebook","R","ggplot","X11"]},{"location":"2024-08-12_RStudio_Server_Gray_Screen/","title":"(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","R","RStudio","RStudio Server","OnDemand"]},{"location":"2024-08-12_RStudio_Server_Gray_Screen/#rstudio-server-shows-blank-or-gray-screen","title":"RStudio Server Shows Blank or Gray Screen","text":"<p>When using RStudio Server through OnDemand, some users have found that the job will start normally, but when connecting from OnDemand, a tab with a blank or gray screen shows up instead of RStudio Server. Sometimes, RStudio Server will load successfully from this screen. Please wait at least a full minute on this screen before proceeding to the solution below.</p>","tags":["lab notebook","R","RStudio","RStudio Server","OnDemand"]},{"location":"2024-08-12_RStudio_Server_Gray_Screen/#resolving","title":"Resolving","text":"<p>However, if you notice that the gray screen persists after one minute of waiting, it is possible that RStudio Server is stuck trying to load. As a workaround, sometimes reloading the tab and waiting another minute can work.</p> <p>Otherwise, clearing RStudio's temporary files stored at <code>~/.local/share/rstudio</code> can fix the issue. Note that this will likely reset your RStudio session, so you may need to reopen previous projects and files and could lose any unsaved work.</p> <p>To do this, we recommend moving the files to a backup using a command like</p> <pre><code>mv ~/.local/share/rstudio ~/.local/share/rstudio.backup\n</code></pre> <p>Trying to load RStudio Server should work again (though you may need to wait a minute or two for the gray screen to go away as described above).</p>","tags":["lab notebook","R","RStudio","RStudio Server","OnDemand"]},{"location":"2024-08-12_RStudio_Server_Gray_Screen/#preventing","title":"Preventing","text":"<p>While we are not sure what exactly causes this, our guess is that either 1) RStudio Server's temporary files have been corrupted after a job shut down incorrectly or 2) RStudio Server has saved large files from the last time it was open and tries to reload them. We highly recommend that you:</p> <ol> <li>Shut down your RStudio session instead of letting your job time out in OnDemand. This is done by clicking the power button in the top right corner of RStudio Server.</li> <li>Ensure that RStudio Server does not save your workspace data on exit. This can be accomplished by clicking the \"Tools\" menu and selecting \"Global Options\". In the default \"General\" pane under the \"Workspace\" header ensure that \"Restore .RData into workspace at startup:\" is unchecked and that \"Save workspace to .RData on exit:\" is set to Never.</li> </ol>","tags":["lab notebook","R","RStudio","RStudio Server","OnDemand"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/","title":"(2024-08-28) Lab Notebook: Change to modules in SLURM jobs","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#change-to-modules-in-slurm-jobs","title":"Change to modules in SLURM jobs","text":"<p>Summary: The way that modules are loaded in SLURM jobs is changing slightly. SLURM jobs will now require you to load modules before you use them in your SLURM script. Previously, this was only a best practice recommended by ICER.</p> <p>What do I need to do?: </p> <pre><code>graph TD\n  A[Start] --&gt; B[Do you only use the default modules?];\n  B --&gt;|Yes| C([&lt;b&gt;You have to do nothing!&lt;/b&gt;]);\n  B --&gt;|No| D[Where do you load modules for your job?];\n  D --&gt;|In the batch script| E([&lt;b&gt;You have to do nothing!&lt;/b&gt;]);\n  D --&gt;|On the development node| F([&lt;b&gt;Add &lt;code&gt;module load&lt;/code&gt; lines to your SLURM script.&lt;/b&gt;]);</code></pre> <p>If you use a workflow manager like Nextflow or Snakemake and are using non-default modules, please see the recommendations below.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#why-is-this-happening","title":"Why is this happening?","text":"<p>With the new module system, ICER is able to build software adapted to the specific types of nodes in the HPCC. For example, our <code>intel18</code> nodes have capabilities like AVX-512 that are not available in the <code>amd20</code> nodes.</p> <p>Previously, ICER would build one version of the software that works on every type of node. Now we have the capability to build multiple versions of software each adapted to the unique capabilities of our hardware generations. However, this means that when you load one of these \"node-adapted\" modules, that same module gets used in the SLURM job no matter where in the HPCC it runs. This leads to \"illegal instruction\" errors when a software built for newer capabilities runs on a node without those capabilities.</p> <p>By making this change, SLURM will load modules from the collection adapted to the node the job is running on, no matter what development node was used to submit that job. This means that your code will run as quickly and efficiently as possible on the nodes that SLURM assigns it.</p> <p>While there are other solutions (like constraining your job to the same type of node that you are submitting from) this solution is most flexible, is in line with our previous recommendations, and allows you access to the largest collection of nodes at once, reducing queue times.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#what-exactly-is-being-changed","title":"What exactly is being changed?","text":"<p>ICER is changing the way that the module system and SLURM interact. The way it is now, SLURM inherits the entire environment of the development node you submit your job from including all loaded modules and all changes to the module path (the location where modules are found).</p> <p>In the new configuration, SLURM jobs will start by resetting all loaded modules inherited from the development node back to the appropriate defaults for your assigned compute node and changing the module path accordingly (see above)</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#what-do-i-need-to-do","title":"What do I need to do?","text":"<p>If you already load all modules in your SLURM scripts before you use them (as is recommended by ICER), you don't need to make any changes!</p> <p>However, if you load non-default modules on the development nodes and then use those modules in your SLURM scripts, please add those module load commands to your script before you use the programs in those modules. Additionally, if you make any changes to your module path using the <code>module use</code> command (e.g., you are loading modules that are not provided by ICER), make sure you do this before you load modules in your SLURM scripts as well.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#example","title":"Example","text":"<p>Suppose that in a typical session, I have a SLURM script that looks like</p> script.sb<pre><code>#!/bin/bash --login\n\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=32GB\n\nstata do myjob.do\n</code></pre> <p>The <code>stata</code> command comes from the <code>Stata</code> module. Before I submit this script, I login and load modules like:</p> <pre><code>ssh user@hpcc.msu.edu\nssh dev-amd20\nmodule purge\nmodule load Stata/18-MP\nsbatch script.sb\n</code></pre> <p>This will no longer work because when the SLURM job starts, the <code>Stata</code> module will be unloaded and replaced by all default modules (which do not include <code>Stata</code>). The fix is to add the <code>module purge</code> and <code>module load Stata/18-MP</code> lines to the beginning of the SLURM script like:</p> script_fixed.sb<pre><code>#!/bin/bash --login\n\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=32GB\n\nmodule purge\nmodule load Stata/18-MP\n\nstata do myjob.do\n</code></pre> <p>You no longer have to load the modules before submitting the job.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#special-considerations-for-nextflow-and-snakemake","title":"Special considerations for Nextflow and Snakemake","text":"<p>Nextflow and Snakemake are two workflow managers that can submit jobs to SLURM for you. Since they build the SLURM scripts, you will need to take extra measures to ensure that they load the required modules in the steps where they are used.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#nextflow","title":"Nextflow","text":"<p>In Nextflow, add the modules you need to the process definition using the <code>module</code> directive. For examples and more information, please see Nextflow's documentation.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-08-28_Change_to_Modules_in_SLURM_Jobs/#snakemake","title":"Snakemake","text":"<p>In Snakemake, add the modules you need to the rule using the <code>envmodules</code> key and run Snakemake with the <code>--use-envmodules</code> flag. For examples and more information, please see Snakemake's documentation.</p>","tags":["lab notebook","modules","slurm"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/","title":"(2024-08-28) Lab Notebook: cuQuantum Installation and Usage","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#cuquantum-installation-and-usage","title":"cuQuantum Installation and Usage","text":"<p>This lab notebook discusses the installation and usage of the cuQuantum software development kit (SDK), and in particular, the cuTensorNet library. It is mostly oriented towards students in the fall 2024 section of CMSE 890-001 using the Data Machine, but will hopefully be useful to a general audience.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#usage-instructions","title":"Usage instructions","text":"<p>We recommend using a Conda environment with <code>cuTensorNet</code> installed through a Jupyter notebook. To do so, visit ICER's OnDemand portal and click the Jupyter app.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#enter-resource-request","title":"Enter resource request","text":"<p>In the settings, enter the time, cores, and memory you would like.</p> <p>When using a GPU slice on the Data Machine as recommended below, it is best to use <code>4</code> as the \"Number of cores per task\" and <code>18GB</code> as the \"Amount of memory\". This ensures that a single node can be split equally 28 ways. However, if you need more resources, please feel free to ask for them with the understanding that your job may take longer to queue.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#setup-conda-environment","title":"Setup Conda environment","text":"<p>Under \"Jupyter Location\", choose \"Conda Environment using Miniforge3 module\". You now have two options:</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#use-a-preinstalled-conda-environment","title":"Use a preinstalled Conda environment","text":"<p>Note</p> <p>This section applies only to students in the fall 2024 section of CMSE 890-001.</p> <p>If you are in the fall 2024 section of CMSE 890-001, in the \"Conda Environment name or path\" field, use <code>/mnt/research/CMSE890_FS24_S001/envs/cuquantum</code>. Otherwise, follow the Use your own Conda environment instructions.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#use-your-own-conda-environment","title":"Use your own Conda environment","text":"<p>Follow the setup instructions below. In the \"Conda Environment name or path\" field, use <code>cuquantum</code> (or whatever you named your Conda environment).</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#run-using-a-gpu-on-the-data-machine","title":"Run using a GPU on the Data Machine","text":"<p>Note</p> <p>This section only applies to users who have Data Machine access.</p> <p>Click the \"Advanced Options\" checkbox. In the \"Number of GPUs\" field, enter <code>a100_1g.10gb</code>. This reserves a slice of the Data Machine A100 GPUs with 10GB of GPU memory. In the \"SLURM Account\" field, enter <code>data-machine</code>.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#launch-jupyter","title":"Launch Jupyter","text":"<p>Press the \"Launch\" button at the bottom and wait for the job to queue and then for Jupyter to start up. This can take a couple of minutes. When Jupyter is ready, click the \"Connect to Jupyter\" button.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#running-a-sample-program","title":"Running a sample program","text":"<p>NVIDIA provides an example of using the Python API for cuTensorNet on their technical blog. You can create a new Jupyter notebook, copy and paste this code into a cell, and run the cell. You should get a similar FLOP count of the optimized contraction path.</p> <p>For more examples, see NVIDIA's cuQuantum Python documentation.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#compiling-a-sample-cuda-code","title":"Compiling a sample CUDA code","text":"<p>The Conda environment also includes libraries to compile C++ based CUDA code for similar types of calculation with much more flexibility (and complexity). In order to use these libraries, you need to use the <code>nvcc</code> compiler which comes with the <code>CUDA</code> module on the HPCC. For more details about compiling CUDA programs, please see our documentation on Compiling for GPUs.</p> <p>These commands will be run from the command line. You can open one in your Jupter Lab instance by opening a new tab and clicking terminal. Or you can submit an interactive job from an SSH session if you prefer.</p> <p>In this example, we'll use the cuTensorNet contraction example provided by NVIDIA that performs similar calculations to the above referenced Python code. This can be downloaded to the HPCC with a command like</p> <pre><code>wget https://github.com/NVIDIA/cuQuantum/raw/main/samples/cutensornet/tensornet_example.cu\n</code></pre> <p>To compile this code, you first need to make sure CUDA is loaded and your conda environment is activated.</p> <pre><code>module purge\nmodule load CUDA/12.3.0\nmodule load Miniforge3\n# Substitute the name of your environment if you created it yourself\nconda activate /mnt/research/CMSE890_FS24_S001/envs/cuquantum\n</code></pre> <p>To compile the code, you need to point the <code>nvcc</code> compiler to the headers and libraries using the <code>-I</code> and <code>-L</code> flags. These are stored in <code>${CUQUANTUM_ROOT}/include</code> and <code>${CUQUANTUM_ROOT}/lib</code> respectively. This example uses the <code>cutensornet</code> and <code>cutensor</code> libraries, brought in using the <code>-l</code> flag. Thus, you can compile the example with the command</p> <pre><code>nvcc tensornet_example.cu -I${CUQUANTUM_ROOT}/include -L${CUQUANTUM_ROOT}/lib -lcutensornet -lcutensor -o tensornet_example\n</code></pre> <p>You can then run the executable with</p> <pre><code>./tensornet_example\n</code></pre> <p>Make sure CUDA is loaded and libraries are on your <code>LD_LIBRARY_PATH</code></p> <p>To run your code, it is important that the exectuable can find the libraries it needs at runtime. Make sure to always run your code after loading the CUDA module and activating your conda environment with</p> <pre><code>module purge\nmodule load CUDA/12.3.0\nmodule load Miniforge3\n# Substitute the name of your environment if you created it yourself\nconda activate /mnt/research/CMSE890_FS24_S001/envs/cuquantum\n</code></pre> <p>In particular, the libraries you used to compile need to be added to the <code>LD_LIBRARY_PATH</code> environment variable which is done by setting the extra environment variables in your Conda environment setup.</p> <p>Optionally, if you skipped this step, you can also set your <code>LD_LIBRARY_PATH</code> manually with</p> <pre><code>export LD_LIBRARY_PATH=\"${CUQUANTUM_ROOT}/lib:${LD_LIBRARY_PATH}\"\n</code></pre> <p>You will need to do this everytime you start a new shell before running your code.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#appendix-further-reading","title":"Appendix: Further reading","text":"<p>After experimenting with the samples above, you may be interested in the following:</p> <ul> <li>Requesting multiple GPUs and chaining them together with MPI. Note that you can request multiple Data Machine GPU slices using <code>a100_1g.10gb:n</code> where <code>n</code> is the number of slices you would like to use.</li> <li>Using whole GPUs in the Data Machine instead of slices (subject to availability).</li> <li>Exploring the different types of Python bindings provided by the cuQuantum Python package.</li> <li>Compile and run more cuTensorNet examples</li> <li>Run more cuTensorNet Python examples</li> </ul>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#appendix-setup-instructions","title":"Appendix: Setup instructions","text":"<p>Use these instructions to setup your own Conda environment. You can make any customizations you like to better fit your workflow.</p>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#login","title":"Login","text":"<p>The first step is to login and make sure that you are on a development node with a GPU. This specific tutorial uses <code>dev-amd20-v100</code> because it has a newer GPU more in line with the GPUs found on the Data Machine. Most importantly, they are both compatible with CUDA 12 whereas the k20 and k80 GPUs are not.</p> <pre><code># From a gateway node\nssh dev-amd20-v100\n</code></pre>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#install-packages","title":"Install packages","text":"<p>The next step is to get access to Conda. Using the <code>Miniforge3</code> module, create a new environment and install the required packages from NVIDIA.</p> <pre><code>module purge\nmodule load Miniforge3\nconda create -n cuquantum\nconda activate cuquantum\nconda install cuquantum cuquantum-python openmpi cuda-version=12 cutensor\nconda install jupyter  # For some reason jupyter needs to be installed separately\n</code></pre>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-04_cuQuantum_Installation_and_Usage/#set-some-extra-environment-variables","title":"Set some extra environment variables","text":"<p>To make the most use of cuQuantum for CUDA compilation and native MPI support in cuTensorNet, you can set a few variables when you activate your Conda environment.</p> <pre><code>conda env config vars set CUQUANTUM_ROOT=${CONDA_PREFIX}\n\ncat &lt;&lt; 'EOF' &gt; \"${CONDA_PREFIX}/etc/conda/activate.d/env.sh\"\nexport OMPI_MCA_opal_cuda_support=\"true\"\nexport OLD_LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}\"\nexport LD_LIBRARY_PATH=\"${CUQUANTUM_ROOT}/lib:${LD_LIBRARY_PATH}\"\nEOF\n\ncat &lt;&lt; 'EOF' &gt; \"${CONDA_PREFIX}/etc/conda/deactivate.d/env.sh\"\nunset OMPI_MCA_opal_cuda_support\nexport LD_LIBRARY_PATH=\"${OLD_LD_LIBRARY_PATH}\"\nunset OLD_LD_LIBRARY_PATH\nEOF\n\nchmod +x \"${CONDA_PREFIX}/etc/conda/activate.d/env.sh\" \"${CONDA_PREFIX}/etc/conda/deactivate.d/env.sh\"\n</code></pre>","tags":["lab notebook","conda","python","CUDA"]},{"location":"2024-09-05_LabNotebook_VSCode_Terminal_login/","title":"(2024-09-05) Lab Notebook: Setting up a terminal with VS Code to connect to the HPCC","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","VS Code","VScode"]},{"location":"2024-09-05_LabNotebook_VSCode_Terminal_login/#setting-up-a-terminal-with-vs-code-to-connect-to-the-hpcc","title":"Setting up a terminal with VS Code to connect to the HPCC","text":"","tags":["lab notebook","VS Code","VScode"]},{"location":"2024-09-05_LabNotebook_VSCode_Terminal_login/#problem","title":"Problem","text":"<p>Your local VS Code terminal shows \"command not found\" when you try to load modules after connecting to the HPCC.</p>","tags":["lab notebook","VS Code","VScode"]},{"location":"2024-09-05_LabNotebook_VSCode_Terminal_login/#cause","title":"Cause","text":"<p>VS Code is not initializing a login shell to load <code>/etc/profile</code>, which sets up command links to the module system.</p>","tags":["lab notebook","VS Code","VScode"]},{"location":"2024-09-05_LabNotebook_VSCode_Terminal_login/#solutions","title":"Solutions","text":"<p>1) Temporary: run <code>source /etc/profile</code> in the VS Code terminal every time you log in.</p> <p>2) Permanent:</p> <ul> <li>Access your VS Code Settings</li> <li>Search for <code>terminal linux</code></li> <li>Under <code>Terminal \u203a Integrated \u203a Profiles: Linux</code> click on <code>Edit in settings.json</code> </li> <li>Add the line <code>\"args\": [\"--login\"]</code> to the <code>bash</code> terminal entry. The result  should look like</li> </ul> <pre><code>\"bash\": {\n            \"path\": \"bash\",\n            \"icon\": \"terminal-bash\",\n            \"args\": [\"--login\"]\n        },\n</code></pre> <ul> <li>Save the file. Back in the Settings tab, under <code>Terminal \u203a Integrated \u203a Default Profile: Linux</code> click on the drop down menu and select <code>bash</code>. </li> <li>Now when you connect to the HPCC, your terminal should have access to all commands as normal.</li> </ul>","tags":["lab notebook","VS Code","VScode"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/","title":"(2024-09-16) Lab Notebook: PyTorch OnDemand Usage","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#using-pytorch-and-friends-on-ondemand","title":"Using PyTorch (and friends) on OnDemand","text":"<p>This documentation shows two techniques for accessing PyTorch in a Jupyter notebook on the HPCC and is oriented towards shared, class usage. The first assumes that there is no shared \"research\" space that all participants can access and is ready to go as is. The second requires a shared research space (using <code>$RESEARCH_SPACE</code> as a placeholder) that participants are added to, and requires a one-time setup (see the Appendix).</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#option-1-using-the-hpcc-module-system","title":"Option 1: Using the HPCC module system","text":"","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#start-your-ondemand-session","title":"Start your OnDemand session","text":"<ol> <li>Log into OnDemand.</li> <li>Click the \"Interactive Apps\" dropdown and choose \"Jupyter\".</li> <li> <p>Select the desired \"Number of hours\", \"Number of cores per task\", and \"Amount of memory\". The following are suggested values to mimic a laptop (or to use a small slice of the Data Machine if that is desired):</p> <ul> <li>Number of hours: 1</li> <li>Number of cores per task: 4</li> <li>Amount of memory: 18GB</li> </ul> </li> <li> <p>Leave the JupyterLab box checked.</p> </li> <li>Choose the \"Default\" location for \"Jupyter Location\".</li> </ol> <p>(Optional) Using the Data Machine</p> <p>If you would like to use the Data Machine (optionally, with a GPU), follow these steps</p> <ol> <li>Click the \"Advanced Options\" checkbox.</li> <li>(Optional) Under \"Number of GPUs\", enter <code>a100_1g.10gb</code> to get a Data Machine GPU slice.</li> <li>Under \"SLURM Account\" enter <code>data-machine</code>.</li> </ol> <p>Click \"Launch\" at the bottom. Your request will queue, and when ready, the \"job card\" will change to show a button that says \"Connect to Jupyter\". Click this button to access your Jupyter session.</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#load-additional-software-modules","title":"Load additional software modules","text":"<p>Do this before opening a Jupyter notebook</p> <p>Make sure to follow these steps before you open a Jupyter notebook. Otherwise, they will not take affect until you restart the notebook's kernel.</p> <p>When a session starts, you will have access to our \"Default\" set of software modules that provide Python, a few helpful packages (like SciPy and numpy) as well as all dependencies. However, to access additional Python packages, you will need to load them yourself.</p> <p>On the left-hand side, you will five tabs. Click the lowest one that looks like a cube with the center removed. This will show you all software modules loaded on the HPCC.</p> <p>Note that because the default version of CUDA, <code>CUDA/12.3.0</code> conflicts with the version used later, please first unload it by finding it in the \"LOADED MODULES\" section and clicking \"Unload\"</p> <p>The non-default modules that need to be loaded are:</p> <ul> <li><code>matplotlib/3.7.2-gfbf-2023a</code></li> <li><code>scikit-learn/1.3.1-gfbf-2023a</code></li> <li><code>torchvision/0.16.0-foss-2023a-CUDA-12.1.1</code></li> </ul> <p>Enter the first few characters of each in the \"Filter available modules...\" box at the top, select the correct version, and choose \"Load\".</p> <p>Torchvision warning</p> <p>When loading the <code>torchvision</code> module, you will see the following warning</p> <pre><code>WARNING: This installation of PyTorch fails a very small percentage of the official test suite. While ICER investigates please use this module at your own risk. Or consider installing your own copy: https://docs.icer.msu.edu/Installing_pytorch_using_anaconda/\n</code></pre> <p>Please be aware of this warning, but note that PyTorch works normally in nearly all circumstances seen by ICER.</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#open-your-jupyter-notebook","title":"Open your Jupyter notebook","text":"<p>Going back to the left-hand side, click the top tab that looks like a file. Navigate to where your Jupyter notebook is stored. Double click to open, and begin running your code!</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#option-2-using-a-conda-environment","title":"Option 2: Using a Conda environment","text":"<p>Recall that <code>$RESEARCH_SPACE</code> is a placeholder here for the location of the research space containing the conda environment. Please replace with the desired location.</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#start-your-ondemand-session_1","title":"Start your OnDemand session","text":"<ol> <li>Log into OnDemand.</li> <li>Click the \"Interactive Apps\" dropdown and choose \"Jupyter\".</li> <li> <p>Select the desired \"Number of hours\", \"Number of cores per task\", and \"Amount of memory\". The following are suggested values to mimic a laptop (or to use a small slice of the Data Machine if that is desired):</p> <ul> <li>Number of hours: 1</li> <li>Number of cores per task: 4</li> <li>Amount of memory: 18GB</li> </ul> </li> <li> <p>Leave the JupyterLab box checked.</p> </li> <li>Choose the \"Conda Environment using Miniforge3 module\" location for \"Jupyter Location\".</li> <li>In the \"Conda Environment name or path\" box, use <code>$RESEARCH_SPACE/envs/pytorch</code>.</li> </ol> <p>(Optional) Using the Data Machine</p> <p>If you would like to use the Data Machine (optionally, with a GPU), follow these steps</p> <ol> <li>Click the \"Advanced Options\" checkbox.</li> <li>(Optional) Under \"Number of GPUs\", enter <code>a100_1g.10gb</code> to get a Data Machine GPU slice.</li> <li>Under \"SLURM Account\" enter <code>data-machine</code>.</li> </ol> <p>Click \"Launch\" at the bottom. Your request will queue, and when ready, the \"job card\" will change to show a button that says \"Connect to Jupyter\". Click this button to access your Jupyter session.</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#open-your-jupyter-notebook_1","title":"Open your Jupyter notebook","text":"<p>Navigate to where your Jupyter notebook is stored. Double click to open, and begin running your code!</p>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-16_PyTorch_OnDemand_Usage/#appendix-conda-environment-setup-instructions","title":"Appendix: Conda environment setup instructions","text":"<p>Make sure you are on the <code>v100</code> development node:</p> <pre><code>ssh dev-amd20-v100\n</code></pre> <p>Then, get access to the <code>conda</code> command using the <code>Miniforge3</code> module:</p> <pre><code>module purge\nmodule load Miniforge3\n</code></pre> <p>Then create the Conda environment in a convenient location (in this case, a research space):</p> <pre><code>conda create -p $RESEARCH_SPACE/envs/pytorch\n</code></pre> <p>Then activate and install packages</p> <pre><code>conda activate $RESEARCH_SPACE/envs/pytorch\nconda install matplotlib pandas scikit-learn pytorch torchvision torchaudio pytorch-cuda=12.1 -c conda-forge -c pytorch -c nvidia\nconda install jupyter  # jupyter prefers to be installed separately\n</code></pre> <p>It is highly recommended to also install the jupyter-lmod plugin to interface with the module system from inside Jupyter (e.g., to change the version of CUDA):</p> <pre><code>python -m pip install jupyterlmod\n</code></pre>","tags":["lab notebook","conda","python","PyTorch"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/","title":"(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#common-machine-learning-tools-tensorflow-keras-scikit-learn-on-ondemand","title":"Common Machine Learning Tools (TensorFlow, Keras, scikit-learn) on OnDemand","text":"<p>This lab notebook discusses the installation and usage of common machine learning tools (TensorFlow, Keras, and scikit-learn) in Jupyter Notebooks through the HPCC's OnDemand interface. It is mostly oriented towards students in the fall 2024 section of CMSE 492-001 or CMSE 802-001 using the Data Machine, but will hopefully be useful to a general audience.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#usage-instructions","title":"Usage instructions","text":"<p>We recommend using a Conda environment including the above tools through a Jupyter notebook. To do so, visit ICER's OnDemand portal and click the Jupyter app.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#enter-resource-request","title":"Enter resource request","text":"<p>In the settings, enter the time, cores, and memory you would like.</p> <p>When using a GPU slice on the Data Machine as recommended below, it is best to use <code>4</code> as the \"Number of cores per task\" and <code>18GB</code> as the \"Amount of memory\". This ensures that a single node can be split equally 28 ways. However, if you need more resources, please feel free to ask for them with the understanding that your job may take longer to queue.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#setup-conda-environment","title":"Setup Conda environment","text":"<p>Under \"Jupyter Location\", choose \"Conda Environment using Miniforge3 module\". You now have two options:</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#use-a-preinstalled-conda-environment","title":"Use a preinstalled Conda environment","text":"<p>Note</p> <p>This section applies only to students in the fall 2024 section of CMSE 492-001 or CMSE 802-001.</p> <p>If you are in the fall 2024 section of CMSE 492-001, in the \"Conda Environment name or path\" field, use <code>/mnt/research/CMSE_492_FS24_S001/envs/ml</code>. If you are in the fall 2024 section of CMSE 802-001, in the \"Conda Environment name or path\" field, use <code>/mnt/research/CMSE_802_FS24_S001/envs/ml</code>. Otherwise, follow the Use your own Conda environment instructions.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#use-your-own-conda-environment","title":"Use your own Conda environment","text":"<p>Follow the setup instructions below. In the \"Conda Environment name or path\" field, use <code>ml</code> (or whatever you named your Conda environment).</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#run-using-a-gpu-on-the-data-machine","title":"Run using a GPU on the Data Machine","text":"<p>Note</p> <p>This section only applies to users who have Data Machine access.</p> <p>Click the \"Advanced Options\" checkbox. In the \"Number of GPUs\" field, enter <code>a100_1g.10gb</code>. This reserves a slice of the Data Machine A100 GPUs with 10GB of GPU memory. In the \"SLURM Account\" field, enter <code>data-machine</code>.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#launch-jupyter","title":"Launch Jupyter","text":"<p>Press the \"Launch\" button at the bottom and wait for the job to queue and then for Jupyter to start up. This can take a couple of minutes. When Jupyter is ready, click the \"Connect to Jupyter\" button.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#appendix-setup-instructions","title":"Appendix: Setup instructions","text":"<p>Use these instructions to setup your own Conda environment. You can make any customizations you like to better fit your workflow.</p>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#login","title":"Login","text":"<p>The first step is to login and make sure that you are on a development node with a GPU. This specific tutorial uses <code>dev-amd20-v100</code> because it has a newer GPU more in line with the GPUs found on the Data Machine. Most importantly, they are both compatible with CUDA 12 whereas the k20 and k80 GPUs are not.</p> <pre><code># From a gateway node\nssh dev-amd20-v100\n</code></pre>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-17_Common_Machine_Learning_Tools_OnDemand/#install-packages","title":"Install packages","text":"<p>The next step is to get access to Conda. Using the <code>Miniforge3</code> module, create a new environment and install the required packages from NVIDIA. Note that even though all of the packages are installed using <code>pip</code>, we still create a Conda environment as this works best with OnDemand.</p> <pre><code>module purge\nmodule load Miniforge3 CUDA\nconda create -n ml\nconda activate ml\nconda install pip\npython -m pip install tensorflow[and-cuda]\npython -m pip install cupy-cuda12x\npython -m pip install scikit-learn\npython -m pip install jupyter\n</code></pre>","tags":["lab notebook","conda","python","tensorflow","scikit-learn","CUDA"]},{"location":"2024-09-24_LabNotebook_Ollama_module/","title":"(2024-09-24) Lab Notebook: Ollama Module","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","modules"]},{"location":"2024-09-24_LabNotebook_Ollama_module/#how-to-use-ollama","title":"How to Use Ollama","text":"<p>Ollama is a command-line tool for managing and running machine learning models. Below is a guide on how to use the various commands available in Ollama.</p> <ul> <li>Ollama github</li> <li>Ollama documentation</li> </ul>","tags":["lab notebook","modules"]},{"location":"2024-09-24_LabNotebook_Ollama_module/#quick-start","title":"Quick Start","text":"<p><pre><code>ollama\n</code></pre> Will show you the available commands.</p> <p>Note: If this is the first time you are running Ollama, you will most likely want to run this powertool: <pre><code>ollama_link_models\n</code></pre> This will symbolically link the models into your home directory. They will not take up any of your space but you can have access to them, learn more about our generative AI common folder.</p> <p><pre><code>ollama serve\n</code></pre> Will start the Ollama server. It is recommended to only run Ollama when you have access to a GPU.</p> <p>(If you want to run Ollama in a single command, you can use <code>ollama serve &amp;</code> to run it in the background)</p> <p>Next you can use <pre><code>ollama list\n</code></pre></p> <p>To see the available models. More info about models on the hpcc here</p> <p>Find the model you want to use and run it with</p> <pre><code>ollama run [model]\n</code></pre>","tags":["lab notebook","modules"]},{"location":"2024-09-24_LabNotebook_Ollama_module/#available-commands","title":"Available Commands","text":"<ul> <li>serve: Start Ollama.</li> <li>create: Create a model from a Modelfile.</li> <li>show: Show information for a model.</li> <li>run: Run a model.</li> <li>pull: Pull a model from a registry.</li> <li>push: Push a model to a registry.</li> <li>list: List models.</li> <li>ps: List running models.</li> <li>cp: Copy a model.</li> <li>rm: Remove a model.</li> <li>help: Help about any command.</li> </ul>","tags":["lab notebook","modules"]},{"location":"2024-09-25_Transitioning_from_Anaconda_to_Miniforge/","title":"(2024-09-25) Lab Notebook: Transitioning from Anaconda to Miniforge","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p>","tags":["lab notebook","conda","python"]},{"location":"2024-09-25_Transitioning_from_Anaconda_to_Miniforge/#transitioning-from-anaconda-to-miniforge","title":"Transitioning from Anaconda to Miniforge","text":"<p>Due to recent changes in Terms of Service, ICER does not recommend or support the use of Anaconda or its package channels. However, ICER has provided streamlined access to the <code>conda</code> command and the conda-forge channels with Miniforge.</p> <p>Follow the steps below to transition an existing Anaconda installation to Miniforge.</p>","tags":["lab notebook","conda","python"]},{"location":"2024-09-25_Transitioning_from_Anaconda_to_Miniforge/#replace-module-loads","title":"Replace module loads","text":"<p>Miniforge is pre-installed on the HPCC. You can access it with</p> <pre><code>module load Miniforge3\n</code></pre> <p>Anywhere you were previously using <code>module load Conda/3</code>, you can replace it with <code>module load Miniforge3</code>. For more details see our page on Using Conda.</p>","tags":["lab notebook","conda","python"]},{"location":"2024-09-25_Transitioning_from_Anaconda_to_Miniforge/#get-access-to-current-environments-optional","title":"Get access to current environments (optional)","text":"<p>Optional</p> <p>Note that if you complete this step, future Conda environments will be installed into your old installation's directory. You may wish to skip this step if you are planning to reinstall all current environments.</p> <p>Assuming you have previously set the <code>CONDA3PATH</code> variable in your <code>.bashrc</code> file, you can run</p> <pre><code>conda config --append envs_dirs ${CONDA3PATH}/envs\n</code></pre> <p>to access all of your old Conda environments.</p> <p>Treat these environments as read-only!</p> <p>You are free to use these environments with Miniforge, but do not install new packages into them! Environments remember they were installed with Anaconda and will use it to install new packages.</p> <p>If you need to make changes or install new packages, you should reinstall your entire environment. Read on...</p>","tags":["lab notebook","conda","python"]},{"location":"2024-09-25_Transitioning_from_Anaconda_to_Miniforge/#reinstall-current-environments","title":"Reinstall current environments","text":"<p>If you want to make changes to an existing environment, reinstall it using Miniforge. First, export its installed packages (we will assume the environment is called <code>test_env</code>):</p> <pre><code>conda activate test_env\nconda env export --override-channels --from-history --channel conda-forge | head -n -1 &gt; env.yaml\n</code></pre> <p>The above command may change versions of packages!</p> <p>To ensure that you can reinstall your environments using the <code>conda-forge</code> channel and Miniforge, you need to strip out all version information (which is what <code>--from-history</code> does). However, this will likely install newer versions of the packages that you previously installed which may have unexpected outcomes!</p> <p>If you need to use the same versions, try using </p> <pre><code>conda env export --override-channels --no-builds --channel conda-forge | head -n -1 &gt; env.yaml\n</code></pre> <p>instead. Depending on whether the requested package versions are available in the <code>conda-forge</code> channels, this may not work. You may need to manually change versions to ones that are suitable for your work and can be found in <code>conda-forge</code>.</p> <p>Then reinstall the package using Miniforge:</p> <pre><code>conda create --name test_env --file env.yaml\n</code></pre> <p>Reusing environment names</p> <p>If you use the same name in the previous line as the existing environment name, you will be asked to delete the existing version first. If you are comfortable with the new environment (that may have different versions of packages, see above), you can agree. Otherwise, use a different name.</p> <p>You can now access this environment using</p> <pre><code>conda activate test_env\n</code></pre> <p>and install new packages from <code>conda-forge</code> using</p> <pre><code>conda install &lt;package_name&gt;\n</code></pre>","tags":["lab notebook","conda","python"]},{"location":"2024-10-01-ColdFront_Testing/","title":"(2024-10-01) Lab Notebook: New ICER User Database Testing Requested (ColdFront)","text":"<p>Warning</p> <p>This is as a Lab Notebook which describes how to solve a specific problem at a specific time. Please keep this in mind as you read and use the content. Please pay close attention to the date, version information and other details.</p> <p>If you or your lab uses ICER, I would like to ask you to help us out and update your research information (publications and grants) in the Beta version of the new ICER User Database called \"ColdFront\" (See the link and detailed instructions below).</p> <p>ColdFront is an open-source resource and allocation management system developed at the University of Buffalo, designed to provide a central portal for administration, reporting, and measuring scientific impact of cyberinfrastructure resources. </p> <p>The short-term goal is to use ColdFront to gather publication and grant information to provide an estimation of ICER\u2019s return on investment for the university. Our long-term goal is to integrate ColdFront\u2019s capabilities into our systems to give PIs better control to manage and keep track of ICER resources.</p> <p>Please provide feedback on the process by sending emails to Dirk Colbry (colbrydi@msu.edu). We would like all Principal Investigators to go through this exercise, so we want it to be as painless as possible while also giving us the information that is needed. </p>","tags":["lab notebook","ColdFront"]},{"location":"2024-10-01-ColdFront_Testing/#coldfront-beta-testing-instructions","title":"ColdFront Beta Testing Instructions","text":"<p>Make sure you log-in via \"OpenID Connect\" button (using your MSU login/password) using the following website:  </p> <p>http://coldfront.hpcc.msu.edu </p> <p>We would like you to try out the following: 1. Update your profile to include your ORCID ID. 2. Either create a new project or update one of the existing projects. 3. If desired, authorize a team member to become a \"project manager\" to help with the next steps. 4. Import from ORCID or manually add all publications you and your team have that used ICER resources. 5. Import from ORCID or manually add all grants you and your team have received that have used ICER resources.  </p> <p>Email us with feedback on the process.  </p> <p>The following video is intended to help walk you through the steps:  </p> <p>Direct Link to the ColdFront Video</p>","tags":["lab notebook","ColdFront"]},{"location":"ABySS/","title":"ABySS","text":"<p>ABySS is a de novo, parallel, paired-end sequence assembler. It can run as an MPI job in the HPCC cluster. The latest version currently installed on the HPCC is 2.1.5, which can be loaded by</p> <pre><code>module load ABySS/2.1.5\n</code></pre> <p>You can optionally load other tools as needed, provided that they have been installed under the same toolchain environment as ABySS/2.1.5. For example,</p> <pre><code>module load BEDTools/2.27.1 SAMtools/1.9 BWA/0.7.17\n</code></pre> <p>is valid after you've loaded ABySS.</p> <p>A sample SLURM script is below.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=abyss_test  \n#SBATCH --nodes=4  \n#SBATCH --ntasks-per-node=2  \n#SBATCH --mem-per-cpu=5G  \n#SBATCH --time=1:00:00  \n#SBATCH --output=%x-%j.SLURMout\n\necho \"$SLURM_JOB_NODELIST\"\n\nmodule load ABySS/2.1.5\n\nexport OMPI_MCA_mpi_warn_on_fork=0  \nexport OMPI_MCA_mpi_cuda_support=0\n\nabyss-pe k=25 name=test in='/mnt/research/common-data/Bio/ABySS/test-data/reads1.fastq /mnt/research/common-data/Bio/ABySS/test-data/reads2.fastq' v=-v np=8 j=2\n</code></pre> <p>This script launches an MPI job by requesting 8 processes; they are distributed on 4 nodes (<code>--nodes=4</code>) with two processes each (<code>--ntasks-per-node=2</code>). Accordingly, in the <code>abyss-pe</code> command line, we specify <code>np=8</code>. Regarding parameter j, the manual states</p> <p>The paired-end assembly stage is multithreaded, but must run on a single machine. The number of threads to use may be specified with the parameter j. The default value for j is the value of np.</p> <p>So, rather than using np as the default value for j, we set j = 2 which is the number of CPUs per node as requested (in this case \"task\" is equivalent to CPU). To submit the job,</p> <p><code>sbatch --constraint=\"[intel16|intel18]\"</code></p> <p>While the job is running, you may look at the SLURM output file, in this example,\u00a0<code>abyss_test-&lt;job ID&gt;.SLURMout</code>, which has a lot of running log, including the following:</p> <p><code>Running on 8 processors</code> <code>6: Running on host lac-391</code> <code>0: Running on host lac-194</code> <code>2: Running on host lac-225</code> <code>4: Running on host lac-287</code> <code>7: Running on host lac-391</code> <code>3: Running on host lac-225</code> <code>1: Running on host lac-194</code> <code>5: Running on host lac-287</code></p>"},{"location":"AMD_Optimizing_CPU_Libraries_and_Compilers/","title":"AMD Optimizing CPU Libraries and Compilers","text":"<p>With the purchase of amd20 cluster, we've installed AOCC (AMD Optimizing C/C++ Compiler) compiler system and AOCL (AMD Optimizing CPU Libraries) on the HPCC system. Here we introduce how to use the installed compilers and libraries.</p> <p>Based on LLVM 10.0 release, AMD compilers use the commands clang, clang++ and flang to compile c, c++ and fortran codes respectively. Users can simply load an AOCC module and use the command directly. To find out the version of AOCC installed in HPCC, please run the following command\u00a0on a dev node. All modules should be able to load directly.</p> <pre><code> module spider AOCC\n</code></pre> <p>AOCL are a set of numerical libraries specifically tuned for the AMD EPYC processor family. The Libraries can work with either AOCC or GCC compilers. Users need to load a version of GCC or AOCC module before AOCL can be loaded. All available versions of AOCL can be found by running the following command\u00a0on a dev node. Using AOCL on the new amd20 nodes is considered to provide better performance than using GCC compiled libraries (such as OpenBLAS, ScaLAPACK ...). User can find out how to use the libraries and the linkers from the  documentation.</p> <pre><code> module spider AOCL\n</code></pre> <p>Besides AOCC and AOCL, a version of OpenMPI compiled with AOCC (version 2.2.0) is also installed in case users would like to test MPI programs with AMD compilers. Users can simply load the <code>aompi</code> toolchain by running the following command\u00a0on a dev node to load both AOCC and OpenMPI (version 4.0.3).\u00a0</p> <pre><code>ml -* aompi\n</code></pre> <p>An\u00a0<code>aoacl</code> toolchain is also available and able to load the triple modules: AOCC, OpenMPI and AOCL at a time.</p> <p>Note</p> <p>We do not currently have the latest AOCL module installed. You must instead install and link from your home directory if you would like to use the latest AOCL with the latest AOCC.</p> <p>For more information, please see AMD's pages on AOCC and AOCL.</p>"},{"location":"ANSYS/","title":"ANSYS","text":"<p>These instructions are for using Ansys on the current HPCC environment that uses the SLURM scheduler.</p>"},{"location":"ANSYS/#license-issues-version-192-and-older","title":"License Issues (version 19.2 and older)","text":"<p>There is an issue with Ansys licensing due to changes made over the many versions HPCC has installed.\u00a0\u00a0 If you have license issues when Using Ansys, here is the work-around:</p> <p>1. Start an interactive desktop session in OnDemand  or start an X11  terminal (with MobaXterm/Windows or XQuartz/Mac) (see Connect to HPCC System ).\u00a0</p> <p>2. SSH Connect to any development node, remembering to add -X options. \u00a0\u00a0</p> <p>3.\u00a0 Start this program on any dev node.\u00a0\u00a0 it launches a GUI </p> <p>/opt/software/ANSYS/19.2/ansys_inc/shared_files/licensing/lic_admin/anslic_admin  </p> <ol> <li>On the left side of the window are three buttons. Click the button \"set License Preferences for User \\&lt;username&gt;\". A new window will open</li> </ol> <p>5. select Release 19.2 in that new window and click OK</p> <p>6. another window will open with tabs across the top and two options in the bottom that are the same for each tab. On the bottom, click the option for \"Use a seperate license for each application\". It doesn't matter which Tab you've slected (Solver/PrePost/etc). That setting should be the same for all tabs.</p> <p>7. click OK, which closes that window.</p> <p>8. In the original Ansys license utility, click File, and then \"exit\" to close it. This modifies the config file in your home directory.</p> <p>9. Close any current sessions in which you running Ansys and start it again on any method (dev node, in 'salloc' interactive job etc). You should now be able to use the features you needed before.</p>"},{"location":"ANSYS/#guidelines-for-scheduling-parallel-mpi-jobs","title":"Guidelines for scheduling parallel (MPI) jobs","text":"<p>Here are some guidelines for requesting resources.\u00a0\u00a0\u00a0</p>"},{"location":"ANSYS/#use-ntasks-instead-of-nodes","title":"Use --ntasks instead of nodes","text":"<p>Note that <code>-N</code>\u00a0 or <code>-\u2013nodes=</code> will request that number of unique computers, but what most users want is the number of tasks across nodes.\u00a0 Use the number of tasks you requested instead of number of nodes for the `-t`` parameter:</p> <p><code>-t $SLURM_NTASKS</code></p>"},{"location":"ANSYS/#dont-forget-to-request-memory","title":"Don't forget to request memory","text":"<p>Request memory per task, and since the default is to have 1 cpu per task, you can request memory using e.g. <code>--mem-per-cpu=1gb</code></p>"},{"location":"ANSYS/#create-a-temporary-file-for-node-list","title":"Create a temporary file for node list","text":"<p>Inside the job, Fluent requires a file of a particular format ,and the SLURM node file doesn't work.\u00a0\u00a0 This seems to work</p> <pre><code># create and save a unique temporary file \nFLUENTNODEFILE=`mktemp`\n# fill that tmpfile with node list Fluent can use\nscontrol show hostnames &gt; $FLUENTNODEFILE\n# in your fluent command, use this parameter\n-cnf=$FLUENTNODEFILE\n</code></pre> <p>Example fluent Job script (using Intel compiler). \u00a0 Increase tasks and memory as needed</p> <p>Ansys/Fluent job</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\n\n# create host list\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $NODEFILE\n\n# Load the ansys/cfx v19.2 module\nmodule load ANSYS\n\n\n# The Input file\nDEF_FILE=baseline.def  # this file is something you have to provide!\n\n\ncfx5solve -def $DEF_FILE -parallel -par-dist $NODEFILE -start-method \"Platform MPI Distributed Parallel\"&gt; cfx5.log\n</code></pre> <p>After you have logged into a development nodes with an X11 terminal (or use the OnDemand desktop as described above),\u00a0 You may run ANSYS tools in parallel and interactively as follows.</p> <pre><code># start a approximately 4 hour interactive job with 10 tasks.  Adjust tasks and memory as needed\n# you'll have 4 hours to work. You must be in a X11 terminal for this to work\n\n salloc --ntasks=10 --cpus-per-task=1 --mem-per-cpu=1gb --time=3:59:00 --x11 \n\n# wait for log-in and then...\n\n# load module\nml intel ansys\n\n# this creates a temporary file and fills it with node list Fluent can use\nNODEFILE=`mktemp`\nscontrol show hostnames &gt; $FLUENTNODEFILE\n\n#for example, run the workbench \nrunwb2\n\u00a0\n# after running workbench you can start fluent directly\n# note we are using Intel mpi\nfluent 3ddp -t $SLURM_NTASKS -mpi=intel -cnf=$FLUENTNODEFILE -ssh\n</code></pre>"},{"location":"ANSYS/#cfx5-solver","title":"CFX5 Solver","text":"<p>This solver uses a different hosts file format for the par-dist parameter.\u00a0 The following uses an example Definition file provided by Ansys 19.2.\u00a0</p> <p>The batch script will adapted the par-dist file depending on how you specify tasks and tasks-per-node (the example below does not specify tasks per node).\u00a0\u00a0\u00a0 Code is taken from https://secure.cci.rpi.edu/wiki/index.php?title=CFX.</p> <p>CFX5 Solver Example sbatch</p> <pre><code>#!/usr/bin/bash --login\n# example 1 hour job with ntasks across any number of nodes\n# adjust the ram and tasks as needed\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1 \n#SBATCH --ntasks=10\n#SBATCH --mem-per-cpu=1gb \n\nmodule load ansys\n\n# codde adapts the hosts file depending on if you use multiple nodes and the tasks-per-node option.   \nsrun hostname -s &gt; /tmp//hosts.$SLURM_JOB_ID\nif [ \"x$SLURM_NPROCS\" = \"x\" ]; then\n    if [ \"x$SLURM_NTASKS_PER_NODE\" = \"x\" ];then\n     SLURM_NTASKS_PER_NODE=1\n     fi\n     SLURM_NPROCS=`expr $SLURM_JOB_NUM_NODES \\* $SLURM_NTASKS_PER_NODE`\nfi\n# use ssh instead of rsh\nexport CFX5RSH=ssh\n# format the host list for cfx\ncfxHosts=`tr '\\n' ',' &lt; /tmp//hosts.$SLURM_JOB_ID`\n\n# example file\nDEF=/opt/software/ANSYS/19.2/ansys_inc/v192/CFX/examples/StaticMixer.def\n# run the partitioner and solver\ncfx5solve -par -par-dist \"$cfxHosts\" -def $DEF -part $SLURM_NPROCS -start-method \"Platform MPI Distributed Parallel\"\n# cleanup\nrm /tmp/hosts.$SLURM_JOB_ID\n\n\n# output will be in a file named like StaticMixer_001.out and StaticMixer_001.res\n</code></pre>"},{"location":"Accessing_Nodes_Running_Ubuntu/","title":"Accessing nodes Running Ubuntu","text":"","tags":["reference","OS upgrade"]},{"location":"Accessing_Nodes_Running_Ubuntu/#development-nodes","title":"Development nodes","text":"<p>Most development nodes have been transitioned to Ubuntu including</p> <ul> <li><code>dev-intel16</code></li> <li><code>dev-intel16-k80</code></li> <li><code>dev-intel18</code></li> <li><code>dev-amd20</code></li> <li><code>dev-amd20-v100</code> (NVIDIA v100 GPU)</li> </ul> <p>You can access these via SSH like normal:</p> <pre><code>ssh user@hpcc.msu.edu  # First SSH into the gateway\nssh dev-intel16  # Then SSH into the desired dev node\n</code></pre> <p>These nodes were previously appended with an <code>-ubuntu</code> suffix, e.g., <code>dev-intel16-ubuntu</code>, which is no longer necessary.</p>","tags":["reference","OS upgrade"]},{"location":"Accessing_Nodes_Running_Ubuntu/#slurm-jobs","title":"SLURM jobs","text":"<p>All SLURM jobs are now submitted to nodes running Ubuntu by default. Previously, the line <code>#SBATCH --reservation=ubuntu_compute</code> was used to run jobs on these nodes, but is no longer necessary</p>","tags":["reference","OS upgrade"]},{"location":"AlphaFold_installed_in_HPCC/","title":"AlphaFold installed in HPCC","text":"<p>The following command can be used to find all versions of AlphaFold installed on HPCC:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold\n</code></pre> <p>To find how to load a specific AlphaFold version, where <code>&lt;version&gt;</code> is the version of AlphaFold to be load, use:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ ml spider AlphaFold/&lt;version&gt;\n</code></pre> <p>All AlphaFold versions use the same data structure and location\u00a0<code>/mnt/research/common-data/alphafold/database</code> as mentioned in\u00a0Alphafold via Singularity.</p>","tags":["tutorial","AlphaFold"]},{"location":"AlphaFold_installed_in_HPCC/#illegal-memory-address","title":"Illegal memory address","text":"<p>If CUDA_ERROR_ILLEGAL_ADDRESS or an illegal memory access was encountered while running AlphaFold, this is usually due to not enough memory in GPU cards (from the python package \"jax\"). Please try to request the high memory GPU card A100 (79GB) to run your AlphaFold jobs.</p> <p>You can also set the environment variable to see if the allocated memory is enough or not:</p> <pre><code>export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n</code></pre> <p>Please see GPU memory allocation for more information.</p>","tags":["tutorial","AlphaFold"]},{"location":"AlphaFold_installed_in_HPCC/#alphafold-example","title":"AlphaFold example","text":"<p>Users can get an example of AlphaFold to run on HPCC nodes. Log into HPCC and ssh to a dev node with GPU cards, then run the following command to copy the example directory <code>AlphaFold</code> to the current directory:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ getexample AlphaFold\n</code></pre> <p>(If the above command fails, you may need to manually load powertools with <code>module load powertools</code>)</p> <p>After you cd to the directory, you should be able to see the files inside:</p> <pre><code>[UserName@dev-amd20-v100 ~]$ cd AlphaFold\n[UserName@dev-amd20-v100 AlphaFold]$ ls\ndata.fasta  README  slurm_script.sb\n[UserName@dev-amd20-v100 AlphaFold]$ cat slurm_script.sb\n#!/bin/bash\n#SBATCH --job-name AlphaFold\n#SBATCH --time=12:00:00\n#SBATCH --gpus=4\n#SBATCH --cpus-per-task=24\n#SBATCH --mem=90GB\n#SBATCH --constraint=[intel18|amr|nvf|nal|nif]\n\nexport NVIDIA_VISIBLE_DEVICES=\"${CUDA_VISIBLE_DEVICES}\"\n\nml -* fosscuda/2020a AlphaFold/2.0.0\n\nalphafold --fasta_paths=$PWD/data.fasta --output_dir=$PWD --preset=casp14 --max_template_date=2020-05-14 --model_names=model_1\n\nscontrol show job ${SLURM_JOBID}\njs -j ${SLURM_JOBID}\n</code></pre> <p>The job script file <code>slurm_script.sb</code> shows how to load the <code>AlphaFold</code> module and run the command <code>alphafold</code>. Since most of the specifications and variables have been set in the <code>alphafold</code> command script and the module file, five options for the job are  specified in the command line:</p> <pre><code>--fasta_paths\n--output_dir\n--preset\n--max_template_date\n--model_names\n</code></pre> <p>Users can look into the AlphaFold documentation or use the commands:</p> <pre><code>[UserName@dev-amd20-v100 AlphaFold]$ alphafold --help\n</code></pre> <p>to find out how to use all options after you load the AlphaFold module.</p>","tags":["tutorial","AlphaFold"]},{"location":"Alphafold_via_Singularity/","title":"AlphaFold via Singularity","text":"<p>Note</p> <p>These instructions are for AlphaFold Singularitiy container for version 2.2.2 and earlier. If you are using an AlphFold 2.3.0 or later container, please see the update instructions the new documentation</p> <p>AlphaFold can be run via Singularity.</p> <p>AlphaFold database is located in <code>/mnt/research/common-data/alphafold/database</code>.</p> <pre><code>database\n\u251c\u2500\u2500 bfd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt_a3m.ffdata\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz\n\u251c\u2500\u2500 mgnify\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mgy_clusters_2018_12.fa\n\u251c\u2500\u2500 params\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_1_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_2_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_3_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_4_ptm.npz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 params_model_5.npz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 params_model_5_ptm.npz\n\u251c\u2500\u2500 pdb70\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 md5sum\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_a3m.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_clu.tsv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_cs219.ffindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffdata\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pdb70_hhm.ffindex\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pdb_filter.dat\n\u251c\u2500\u2500 pdb_mmcif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mmcif_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 obsolete.dat\n\u251c\u2500\u2500 small_bfd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bfd-first_non_consensus_sequences.fasta\n\u251c\u2500\u2500 uniclust30\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 uniclust30_2018_08\n\u2514\u2500\u2500 uniref90\n    \u251c\u2500\u2500 uniref90.fasta\n    \u2514\u2500\u2500 uniref90.fasta.1.gz\n</code></pre> <p>Before running AlphaFold, you need to set</p> <pre><code>export ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"  \nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n</code></pre> <p>To run alphafold, please use the following template (for more information about options/flags, please refer to the README on Github).</p> <p>In the script, <code>input.fasta</code> is your input data, and\u00a0you need to set up output_dir. Since the command <code>/usr/bin/hhsearch</code> inside the container does not work on intel14 nodes (<code>Illegal instruction</code>), please use the <code>SBATCH</code> option\u00a0<code>--constraint</code> in the job script.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name alphafold-run\n#SBATCH --time=08:00:00\n#SBATCH --gpus=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=20G\n#SBATCH --constraint=\"[intel16|intel18|amd20]\"\n\nexport ALPHAFOLD_DATA_PATH=\"/mnt/research/common-data/alphafold/database\"\nexport ALPHAFOLD_MODELS=\"/mnt/research/common-data/alphafold/database/params\"\n\nsingularity run --nv \\\n-B $ALPHAFOLD_DATA_PATH:/data \\\n-B $ALPHAFOLD_MODELS \\\n-B .:/etc \\\n--pwd  /app/alphafold /opt/software/alphafold/2.0.0/alphafold.sif \\\n--data_dir=/data \\\n--output_dir=/mnt/gs18/scratch/users/my_id/alphafold/output \\\n--fasta_paths=/mnt/gs18/scratch/users/my_id/alphafold/input.fasta  \\\n--uniref90_database_path=/data/uniref90/uniref90.fasta  \\\n--mgnify_database_path=/data/mgnify/mgy_clusters_2018_12.fa   \\\n--bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\\n--pdb70_database_path=/data/pdb70/pdb70  \\\n--template_mmcif_dir=/data/pdb_mmcif/mmcif_files  \\\n--obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\\n--max_template_date=2020-05-14   \\\n--model_names=model_1 \\\n--preset=casp14\n</code></pre>","tags":["tutorial","AlphaFold","Singularity"]},{"location":"An_SSH_tunneling_via_multiple_hops/","title":"SSH tunneling to directly access development nodes","text":"<p>In some cases, you want to access dev nodes directly from your local machine instead of manually connecting to the gateway first. There are two ways to accomplish this.</p>","tags":["how-to guide","ssh"]},{"location":"An_SSH_tunneling_via_multiple_hops/#using-proxyjump","title":"Using ProxyJump","text":"<p>We can use a configuration file to set custom names for SSH hosts and simplify the process of connecting. On your local machine, open the <code>~/.ssh</code> directory with the command <code>cd ~/.ssh</code> and create a file called <code>config</code> by running <code>touch config</code> (or edit it, if it already exists).</p> <p>For Windows users using OpenSSH for Windows</p> <p>For most Windows users using OpenSSH for Windows (e.g., to connect to the HPCC through VS Code), the SSH config file should be located at <code>C:\\Users\\&lt;username&gt;\\.ssh\\config</code> where <code>&lt;username&gt;</code> is your username on your Windows computer (or more generally, <code>%userprofile%\\.ssh\\config</code>). Note that the <code>config</code> file does not have a file extension and should not be considered a text (or any other) type of file by Windows. You can open your config file in VS Code by pressing the F1 key, then typing <code>Open SSH Configuration file</code>.</p> <p>In the following example <code>.ssh/config</code> file, we have defined hosts for all seven development nodes. Each entries each contain the line <code>ProxyJump &lt;netid&gt;@hpcc.msu.edu</code> to be able to connect to the development nodes through the gateway nodes.</p> <p>To use the following template <code>.ssh/config</code>,  change all instances of <code>&lt;netid&gt;</code> to your NetID that you use to login to the HPCC.</p> <pre><code>Host intel16\n    HostName dev-intel16\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost intel18\n    HostName dev-intel18\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost amd20\n    HostName dev-amd20\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost k80\n    HostName dev-intel16-k80\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n\nHost v100\n    HostName dev-amd20-v100\n    User &lt;netid&gt;\n    ProxyJump &lt;netid&gt;@hpcc.msu.edu\n</code></pre> <p>With this file, you can connect to any a development node from your local machine with, e.g., <code>ssh intel18</code> or <code>ssh k80</code>. Your connection will automatically be routed through the HPCC gateway.</p> <p></p> <p>Note</p> <p>With SSH Key-Based Authentication you don't have to type your password when you login.</p>","tags":["how-to guide","ssh"]},{"location":"An_SSH_tunneling_via_multiple_hops/#using-port-forwarding","title":"Using port forwarding","text":"<p>Instead of using ProxyJump, you can use port forwarding. For this method, you need to open two terminals on your local machine.</p> <p>1st terminal (left in the picture): type </p> <pre><code>ssh -L 1234:dev-intel18:22\u00a0&lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>You can change 1234 to any number larger than 1024 (1234 here is a port number you are using). You can change dev-intel18 to any dev-node name, but 22 (port number of dev node) should be remained. For example,\u00a0</p> <pre><code>ssh -L 4321:dev-intel16:22 &lt;your_net_id&gt;@gateway.hpcc.msu.edu\n</code></pre> <p>is also working.</p> <p>2nd terminal (right in the picture): type </p> <pre><code>ssh -p 1234 &lt;your_net_id&gt;@localhost\n</code></pre> <p>If it is the first time, it would request connection confirmation. type yes. Then you will arrive at the dev-node on the 2nd terminal.</p> <p></p>","tags":["how-to guide","ssh"]},{"location":"Application_Icons_on_Desktop/","title":"Application Icons on Desktop","text":"<p>It is much easier to execute your favorite apps by clicking icons on the desktop just like using your Windows or Mac PC. We can certainly do this through Open OnDemand. There are some app icons already created in the directory <code>/opt/software/OnDemand/Desktop-Icons</code>, where you can see them by listing the folder:</p> <pre><code>$ ls /opt/software/OnDemand/Desktop-Icons\nANSYS.desktop             Dolphin.desktop  GaussView.desktop  Maestro.desktop   rstudio.desktop  tecplot.desktop           VMD.desktop\nchromium-browser.desktop  firefox.desktop  GSEA.desktop       MATLAB.desktop    sas.desktop      Terminal.desktop\nCOMSOL.desktop            Fluent.desktop   Jupyter.desktop    Nautilus.desktop  Stata.desktop    User's Anaconda3.deskto\n</code></pre> <p>User can easily add the icons to their OnDemand interactive desktop by following the sections below.</p> <p>[ Use Command Lines ] [ Use Interactive Desktop ] [ Create App Icons ]</p> <p>A video instruction is also provided (click to start).</p>"},{"location":"Application_Icons_on_Desktop/#use-command-lines","title":"Use Command Lines","text":"<p>You can simply copy them to your desktop directory <code>~/Desktop</code> . For example, if you would like to have MATLAB icon on your OnDemand desktop, you can run</p> <pre><code>$ mkdir -p ~/Desktop\n$ cp /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop ~/Desktop/\n</code></pre> <p>Once the apps' desktop files are copied, request an  Interactive Desktop session as mentioned in the Open OnDemand page. You should see the apps' icons on your desktop once you launch it:</p> <p></p>"},{"location":"Application_Icons_on_Desktop/#use-interactive-desktop","title":"Use Interactive Desktop","text":"<p>You can also request and start an  Interactive Desktop session to copy the app icons from\u00a0<code>/opt/software/OnDemand/Desktop-Icons</code>. Simply double-click the \"<code>Trash</code>\" or \"<code>Computer</code>\" icon on your desktop. It will pop out the file manager window. In the \"<code>Location:</code>\" place, please enter the directory <code>/opt/software/OnDemand/Desktop-Icons</code>. (If the \"<code>Location:</code>\" place does not allow any input, please click on\u00a0 ). It should show all app icons in the window. Right click on an app icon you would like to copy to your desktop. Choose \"Copy to\" and click \"Desktop\":</p> <p></p> <p>The icon you choose will be copied to your desktop.</p>"},{"location":"Application_Icons_on_Desktop/#create-app-icons","title":"Create App Icons","text":"<p>If your favorite app icons are not in the directory, you can try to use one them as an example</p> <pre><code>$ cat /opt/software/OnDemand/Desktop-Icons/MATLAB.desktop\n\n[Desktop Entry]\nType=Application\nName=MATLAB\nIcon=/opt/software/OnDemand/images/matlab.png\nExec=bash -c \"module load MATLAB/2018a; matlab -desktop\"\nTerminal=false\nGenericName=\n</code></pre> <p>and modify it. Change the following contents</p> <pre><code>Name=&lt;Software Name&gt;\nIcon=&lt;Location and File name of the Software Icon&gt;\nExec=&lt;Commands to Run the Software&gt;\n</code></pre> <p>to your app's. Save the file with your app's file name in <code>~/Desktop</code> directory. Every time you launch an Interactive Desktop session, the icon shows on your desktop. If you have any question, please let us know. We can help you to create one.</p>"},{"location":"Aspera_bulk_file_transfer/","title":"Aspera bulk file transfer","text":"<p>The Aspera Connect application (ascp) is a useful file transfer tool for downloading or uploading large files in bulk between the HPCC and data repository sites such as those operated by NCBI. In order to interact  with a server via aspera, the remote host must be running the Aspera server. </p> <p>This short tutorial will demonstrate how to load and use the command line version of Aspera to download files from the NCBI ftp site.</p> <p>Step 1: Log onto HPCC rsync gateway node:</p> <pre><code>ssh -XY netid@rsync.hpcc.msu.edu\n</code></pre> <p>Step 2: Load Aspera 3.9.8 module:</p> <pre><code>module load Aspera-Connect/3.9.8\n</code></pre> <p>You can only execute Aspera file transfers from a gateway node. Transfers on the dev-nodes will not work.</p> <p>Tip</p> <p>If you need a higer version than 3.9.8, you can try installing it with conda (<code>conda install -c rpetit3 aspera-connect</code>). Conda will handle Glibc issues.</p> <p>Example command for downloading data from NCBI:</p> <pre><code>ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/refseq/uniprotkb ~/NCBI_data\n</code></pre> <p>For uploading files from the HPCC, please refer to the NCBI instructions for uploading SRA files.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/","title":"Assembly of PacBio long reads with Canu","text":""},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#introduction","title":"Introduction","text":"<p>Canu is used for de novo assembly using long reads, as generated from PacBio or Oxford Nanopore technologies. It consists of three steps: read correction, read trimming and contig assembly.</p> <p>As of Sept 2021, we have the latest: version 2.2 installed on the HPCC. You can load it by</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n</code></pre> <p>Then, simply running <code>canu</code> will give you a good amount of help information. For example, at the bottom of the help document, we learn that canu supports three types of raw input data:</p> <pre><code>[technology]\n-pacbio      &lt;files&gt;\n-nanopore    &lt;files&gt;\n-pacbio-hifi &lt;files&gt;\n</code></pre> <p>While canu can automate job submission using SLURM, we don't recommend this method. Therefore, please specify <code>useGrid=false</code> in the canu command to disable grid support. Users will write a job script manually, treating canu as an ordinary program.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#an-example-using-pacbio-reads","title":"An example using PacBio reads","text":"<p>The PacBio reads we will be assembling are the same as the ones used in the canu tutorial, which can be downloaded using the following command:</p> <pre><code>curl -L -o pacbio.fastq http://gembox.cbcb.umd.edu/mhap/raw/ecoli_p6_25x.filtered.fastq\n</code></pre> <p>By default, the canu pipeline will correct the reads, trim the reads, and then assemble the reads to contigs. Minimally, you can run canu on a dev-node in the following way (we need to first load all necessary modules):</p> <pre><code>module purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1 &amp;\n</code></pre> <p>Above,</p> <ul> <li><code>pacbio.fastq</code> is the input file, considered as raw and unprocessed     reads. Coupled with <code>-pacbio</code>, canu knows which technology has     generated these reads.</li> <li><code>-p</code>: set the file name prefix of intermediate and output files;     it's mandatory.</li> <li><code>-d</code>: set assembly directory name for canu to run in. If not     supplied, it'll run in the current directory. It is not possible to     run two different assemblies in the same directory.</li> <li><code>genomeSize</code>: in bases, with common prefixes allowed, such as 4.7m     or 2.8g. canu uses it to determine coverage in the input reads.</li> <li><code>useGrid=false</code>: make canu run on the local machine.</li> <li><code>maxThreads</code>: the maximum number of threads that each task can use.</li> <li>Finally, we put <code>time -v</code> in front of the canu command in order to     get resource usage, which will be shown at the end of the log file     <code>runCanu_2021-09-14.log</code>. For example,<ul> <li><code>Maximum resident set size (kbytes): 4113216</code> tells us that the     maximum memory used during the process is about 4G.</li> <li><code>Percent of CPU this job got: 476%</code> tells us we've used on     average 5 CPUs.</li> </ul> </li> </ul> <p>If we want to have canu run in the HPCC cluster, we can write a job script accordingly:</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=canu_ecoli\n#SBATCH --cpus-per-task=10\n#SBATCH --mem=10G\n#SBATCH --time=2:00:00\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCCcore/8.3.0 Java/11 Perl/5.30.0 gnuplot/5.2.8\nexport PATH=/opt/software/canu/canu-2.2/bin:$PATH\n\n/bin/time -v canu -p ecoli -d ecoli-pacbio genomeSize=4.8m useGrid=false maxThreads=10 -pacbio pacbio.fastq &gt; runCanu_2021-09-14.log 2&gt;&amp;1\n</code></pre> <p>The canu command is exactly the same as the one we run on the dev-node, except that the trailing <code>&amp;</code> sign should be removed when it is within a job script.</p> <p>The primary output file for most users is the assembled contigs. In this example, it is <code>ecoli-pacbio/ecoli.contigs.fasta</code> under your current working directory. Refer to this page when you want to learn more about the output, such as the various statistics of the reads analyzed, as reported in the <code>ecoli.report</code> file.</p>"},{"location":"Assembly_of_PacBio_long_reads_with_Canu/#notes","title":"Notes","text":"<ul> <li>To adjust default parameters, you need to consult the     canu parameter reference.</li> <li>The three steps (error correction, trimming and assembly) can be     individually run. See this example.</li> <li>If your data is     PacBio HiFi reads (i.e.     CCS reads with predicted accuracy &gt;= Q20 or 99%), you may want to     use the option <code>-pacbio-hifi</code> rather than <code>-pacbio</code>. Canu will skip     read correction and trimming in this case.</li> </ul>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/","title":"BLAST/BLAST+ with Multiple Processors","text":""},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#overview","title":"Overview","text":"<p>It is possible to run BLAST or BLAST+ on the HPCC in multi-threaded mode. \u00a0This is advantageous in that is allows users to leverage multiple processors to complete their BLAST searches, thereby decreasing compute time.</p> <p>To load BLAST or BLAST+ on the HPCC:</p> <pre><code># Loading BLAST\nmodule purge\nmodule load BLAST/2.2.26-Linux_x86_64\n\n# Loading BLAST+\nmodule purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28  impi/2017.3.196 BLAST+/2.8.1-Python-2.7.14\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#multi-threading-vs-mpi","title":"Multi-Threading vs. MPI","text":"<p>Multi-threaded BLAST runs enable the user to launch multiple worker threads on a single node. \u00a0However, because standard BLAST and BLAST+ do not use distributed memory, you cannot accomplish multi-threaded runs across multiple nodes. \u00a0Therefore, users executing multi-threaded BLAST or BLAST+ runs should not reserve more than one node, as this will reserve hardware resources that cannot be used.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#job-submission-guidelines","title":"Job Submission Guidelines","text":"<p>First, we need to differentiate between traditional NCBI BLAST and BLAST+. \u00a0Traditional NCBI BLAST utilizes the \"-a #\" flag to specify the number of processors to use for the job (default is 1). \u00a0BLAST+ uses the \"-num_threads #\" flag to specify the number of worker threads to use. \u00a0Depending upon which type of BLAST you use, you will need to adjust your job submission script parameters accordingly.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#traditional-blast","title":"Traditional BLAST","text":"<p>Using the \"-a\" flag in BLAST will specify the number of\u00a0processors\u00a0to use. \u00a0To reserve the appropriate quantity of resources in your job submission script, you will need to reserve a number of cores equal to the value specified by the \"-a\" flag \u00a0For example, if you used a command like:</p> <pre><code>blastall -p blastp -d swissprot -i\u00a0prot.fasta\u00a0-o test1.blast -e 0.001 -a 4\n</code></pre> <p>You should specify something like the following in your SLURM job submission script:</p> <pre><code>#SBATCH --cpus-per-task=4\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast","title":"BLAST+","text":"<p>In contrast, BLAST+ uses the \"-num_threads\" flag to specify the number of worker\u00a0threads\u00a0to create. \u00a0In order to specify the correct number of cores for the job, you will need to\u00a0ADD ONE\u00a0to the number of threads specified. \u00a0This is to account for the number of worker threads,\u00a0PLUS\u00a0the main process thread. \u00a0So if you used an equivalent BLAST+ command like:</p> <pre><code>blastn -task blastn -db swissprot -query prot.fasta -out test1.blast -evalue 0.001 -num_threads 4\n</code></pre> <p>You should use the following in your SLURM\u00a0script:</p> <pre><code>#SBATCH --cpus-per-task=5\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blastdb","title":"BLASTDB","text":"<p>The BLASTDB environmental variable tells BLAST or BLAST+ where to find your databases that can be searched. \u00a0On the HPCC, we offer select BLAST-ready data sets for this purpose in a common read-only area. \u00a0BLAST data sets can be accessed at:</p> <pre><code>/mnt/research/common-data/Bio/blastdb\n</code></pre> <p>If you are using the FASTA sequences instead of nucleotide data sets, you need to augment the path above as follows:</p> <pre><code>/mnt/research/common-data/Bio/blastdb/FASTA\n</code></pre> <p>For cluster jobs, you will need to set the value of BLASTDB in your job submission script, for example:</p> <pre><code>export BLASTDB=/mnt/research/common-data/Bio/blastdb:/mnt/research/common-data/Bio/blastdb/FASTA:$BLASTDB\n</code></pre>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#a-word-about-memory","title":"A Word About Memory","text":"<p>In either case (BLAST or BLAST+) your requested memory (in the examples above, 4gb) will be divided amongst all of your task threads. \u00a0Plan accordingly.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#blast-data-preparation","title":"BLAST data preparation","text":"<p>Data downloaded from the NCBI website, or prepared by users can, in most cases, be easily converted for use with BLAST. \u00a0This brief tutorial is designed to illustrate a fairly basic scenario where the user wants to download a set of FASTA sequences from the NCBI website and prepare them for BLAST-ing.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#download","title":"Download","text":"<p>The simplest way to do this is to note the link of the FASTA file, and use either the \"wget\" or \"curl\" command. \u00a0For example:</p> <pre><code>wget ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>or</p> <pre><code>curl -O ftp://ftp.ncbi.nih.gov/repository/UniGene/Triticum_aestivum/Ta.seq.all.gz\n</code></pre> <p>This will download the file \"Ta.seq.all.gz\" into the current directory. Now unzip the file:</p> <pre><code>gunzip Ta.seq.all.gz\n</code></pre> <p>This will leave a file called \"Ta.seq.all\" in your directory.</p>"},{"location":"BLAST_BLAST%2B_with_Multiple_Processors/#preparing-the-indices","title":"Preparing the Indices","text":"<p>To prepare the BLAST indices for nucleotides:</p> <pre><code>formatdb -i Ta.seq.all -p F\n</code></pre> <p>The command above will produce several files, such as:</p> <pre><code>Ta.seq.all.fa.nhr\nTa.seq.all.fa.nin\nTa.seq.all.fa.nsq\n</code></pre> <p>If you want to produce protein indices instead of, or in addition to nucleotides, run:</p> <pre><code>formatdb -i Ta.seq.all -p T\n</code></pre> <p>In this case, this will produce the files:</p> <pre><code>Ta.seq.all.fa.phr\nTa.seq.all.fa.pin\nTa.seq.all.fa.psq\n</code></pre> <p>You can verify whether your BLAST formatting was successful by looking at the \"formatdb.log\" file which should now be present in your directory.</p>"},{"location":"Backwards_Compatibility_with_CentOS/","title":"Backwards Compatibility with CentOS","text":"<p>Early testing!</p> <p>Please note that the advice and commands on this page are still being tested. Expect the possibility for bugs and errors, and always test for accuracy by comparing against commands you know already work.</p> <p>If you discover a problem with anything on this page, please contact us so we can work to resolve it.</p> <p>As ICER upgrades the operating system on the HPCC to Ubuntu, we are providing limited support for running code based on the current CentOS system. This page outlines a few scripts and tips to help you run your code through a backwards compatibility \"container\".</p> <p>For an overview of containers see our Containers Overview page. Knowledge of containers is helpful, but not required to use the helper scripts provided.</p> <p>MPI and multi-node jobs are not supported</p> <p>Running multiple tasks with MPI is difficult to replicate using containers. As such, the scripts and instructions given here have not been tested for MPI jobs and likely will not work.</p> <p>That being said, running MPI with containers is possible with some extra configuration. For more information, see the Singularity documentation or this tutorial from Pawsey Supercompting.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#automatically-submitting-batch-scripts-with-sbatch_old","title":"Automatically submitting batch scripts with <code>sbatch_old</code>","text":"<p>We have provided a <code>powertool</code> called <code>sbatch_old</code>. This tool will submit all of the commands in your current SLURM batch scripts through the compatibility container. In particular, it is possible to access all modules available on CentOS using their original name. To use it, replace <code>sbatch</code> with <code>sbatch_old</code> when submitting your script to any Ubuntu node.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#example","title":"Example","text":"<p>Consider the following SLURM batch script:</p> my_script.sb<pre><code>#!/bin/bash --login\n#SBATCH --job-name=Rscript\n#SBATCH --ntasks=1\n#SBATCH --mem=20M\n#SBATCH --time=01:00:00\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=yournetid@msu.edu\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 powertools\n\n# Run our job\ncd /mnt/home/user123\nsrun Rscript myscript.R\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre> <p>This job can be run on any <code>CentOS</code> node with the command:</p> CentOS<pre><code>sbatch my_script.sb\n</code></pre> <p>This job will fail on Ubuntu because <code>R/4.0.2</code> is not available (and even if it were, the name would be different). Instead, use the <code>sbatch_old</code> command:</p> Ubuntu<pre><code>module load powertools\nsbatch_old my_script.sb\n</code></pre> <p>This script should work on jobs that request at most one node and do not use MPI. Jobs using GPUs should still continue to work.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#interactively-using-the-backwards-compatibility-container-with-old_os","title":"Interactively using the backwards compatibility container with <code>old_os</code>","text":"<p>To run commands interactively using the compatibility container, run the command <code>old_os</code> on any Ubuntu node. This will replace your command line with one running inside the compatibility container, and you will have access to the majority of the commands on the CentOS system.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#example_1","title":"Example","text":"<p>Suppose you want to test compiling and running GPU code. On a CentOS node your session might look like</p> CentOS commands<pre><code>getexample helloCUDA\ncd helloCUDA\nmodule load gcccuda/2019\nnvcc Hello.cu -o Hello_CUDA\n./Hello_CUDA\n</code></pre> <p>However, running this on Ubuntu will not work, because <code>gcccuda/2019</code> is not available. You also cannot guarantee that <code>./Hello_CUDA</code> will still work on Ubuntu because the compilers and libraries used to compile it on CentOS are either different or nonexistent on Ubuntu. To get around, this you can start an interactive session in the compatibility container.</p> <p>Note that this will need to run on a node with a GPU, e.g., <code>dev-amd20-v100</code>.</p> Ubuntu commands<pre><code>module load powertools\nold_os  # All following commands are in CentOS compatibility container\n./Hello_CUDA  # Works\nrm Hello_CUDA\nmodule load gcccuda/2019  # Recompiling with CentOS modules\nnvcc Hello.cu -o Hello_CUDA\n./Hello_CUDA\nexit  # Go back to Ubuntu\n</code></pre>","tags":["how-to guide","OS upgrade"]},{"location":"Backwards_Compatibility_with_CentOS/#advanced-manually-using-the-container","title":"Advanced: Manually using the container","text":"<p>If you are comfortable using Singularity directly and would like a more flexible workflow, e.g., to experiment with MPI jobs, you can also invoke the backwards compatibility container directly.</p> <p>We recommend a invoking <code>singularity</code> with the following options:</p> <pre><code>singularity &lt;command&gt; \\\n    --bind /opt/.software-legacy:/opt/software  # Access module system\n    --bind /opt/.modules-legacy:/opt/modules\n    --bind /cvmfs  # Access software provided by CVMFS endpoints (Icecube, CERN, etc.)\n    --cleanenv  # Ensure module system from Ubuntu doesn't conflict\n    --nv  # If using a GPU\n    /mnt/research/helpdesk/ubuntu-compute/centos79.sif  # Use most recent image\n</code></pre> <p>where <code>&lt;command&gt;</code> is one of <code>shell</code> or <code>exec</code>. See <code>less $(which sbatch_old)</code> for an example.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Buy-In_Accounts_with_SLURM/","title":"Buy-In and Account Management","text":"<p>If you want to have priority access to our clusters, you can purchase buy-in nodes.  Users who run on buy-in nodes receive priority access to their nodes within 4 hours and are exempt from the 1 million CPU hour per year limit.</p> <p>Priority access to buy-in nodes is controlled through the SLURM partitioning system. See SLURM Queueing and Partitions for more information. The sections below describe how to manage accounts in SLURM to ensure access to buy-in partitions.</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#checking-default-account","title":"Checking Default Account","text":"<p>When submitting a job without specifying an account, your default account is used. You can check your default account using the <code>buyin_status</code> power tool.</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>User <code>fordste5</code> has a default account of <code>classres</code>. If <code>fordste5</code> submits a job without specifying an account, it will be submitted to the <code>classres</code> partition. If the job requested four hours or less of wall time, it will be submitted to the <code>general-short</code> partition as well. Jobs submitted by users with a default account of <code>general</code> will be queued in the <code>general-long</code> and <code>general-short</code> partitions. See How jobs are assigned to queues for a complete listing of queue assignments.</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#using-non-default-accounts","title":"Using Non-Default Accounts","text":"<p>You can see what accounts you have access to using the <code>buyin_status</code> powertool:</p> <pre><code>$ module load powertools\n$ buyin_status -l\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n</code></pre> <p>This output shows that <code>fordste5</code> is a member of the <code>test1_classres</code>, <code>test1</code>, <code>and\u00a0classres</code> buy-in accounts. Both <code>test1</code> and <code>classres</code> correspond to partitions with the same name.</p> <p>Because <code>fordste5</code> has a default account of <code>classres</code>, if they want\u00a0to submit jobs to <code>test1</code>, they will have to explicitly specify the <code>test1</code> account at submission. This is done by using either the <code>-A test1</code> option for <code>srun/sbatch/salloc</code>, or by adding the <code>#SBATCH -A test1</code> directive to their batch script.</p> <p>If you would like to change your default account, you can do so with <code>sacctmgr</code>. In the example above, if <code>fordste5</code> wants to change their default account to  <code>test1</code>, they can do so with the command</p> <pre><code>sacctmgr modify user name=fordste5 set DefaultAccount=test1\n</code></pre>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#managing-buy-in-account-membership","title":"Managing Buy-In Account Membership","text":"<p>SLURM allows for the configuration of account coordinators. Account coordinators can add and remove users to accounts that they coordinate. Buy-in account owners are configured as coordinators of their buy-in accounts and can request that other users also be added as coordinators.</p> <p>To check who has coordinator privileges on a buy-in account, run:</p> <pre><code>$ sacctmgr show account &lt;account_name&gt; withcoord\n</code></pre> <p>If you had a buy-in account configured in the old Moab scheduler, account coordinators for your SLURM buy-in account were copied from the managers listed on the Moab account.</p> <p>View what users have access to your buy-in account:</p> <pre><code>$ sacctmgr show association where account=&lt;account&gt;\n</code></pre> <p>Add a user to your buy-in account:</p> <pre><code>$ sacctmgr add user name=&lt;userid&gt; account=&lt;account&gt;\n</code></pre> <p>Remove a user from your buy-in account:</p> <pre><code>$ sacctmgr delete user &lt;userid&gt; where account=&lt;account&gt;\n</code></pre>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buy-In_Accounts_with_SLURM/#checking-the-status-of-buy-ins","title":"Checking the Status of Buy-Ins","text":"<p>The status of buy-ins can be viewed with the <code>buyin_status</code> powertool.</p> <pre><code>$ module load powertools\n$ buyin_status -a classres\n\nUser:     fordste5\nAccounts: test1_classres test1 classres\nDefault:  classres\n\n\nBuyin: classres\n  JOBID      STATE      USER       CPUS PRIORITY  TIME_LIMIT           START_TIME\n  17401      RUNNING    fordste5   10   101          1:00:00  2018-08-09T12:55:17\n  17409      RUNNING    changc81   4    100          1:00:00  2018-08-09T12:57:54\n\n  Partition: classres\n    lac-421 (down*)\n    csn-020 (down*)\n    csp-018 (down*)\n    css-034 (down*)\n    css-035 (down*)\n    css-079 (down*)\n    css-080 (down*)\n    css-033 (allocated)\n      JOBID      ACCOUNT    USER       CPUS       TIME  TIME_LEFT\n      17402      general    fordste5   10         4:36      55:24\n      17401      classres   fordste5   10         4:36      55:24\n</code></pre> <p>This tool will list all jobs that are queued for a buy-in account, all nodes associated with the buy-in\u00a0account\u2013including node\u00a0status\u2013and a list of jobs running on each node. Because jobs with a wall time of four hours or less can run on any buy-in node, it is possible to see jobs running on buy-in nodes from other accounts.</p> <p>Buy-in partitions have the same name as a given buy-in account</p>","tags":["how-to guide","buyin","partitions","accounts"]},{"location":"Buyin_Transition/","title":"Buy-in Transition","text":"<p>As of June 17th, most nodes in the HPCC have been transitioned to the new operating system. However, buy-in node upgrades are being delayed until the week of July 17th to allow for extra time to transition. Please see schedule specifics below.</p> <p>At any time, buy-in PIs can contact us using the subject line \"OS Upgrade 2024\" and request that their buy-in be upgraded sooner. This process will take 1-2 business days after confirming with ICER.</p>"},{"location":"Buyin_Transition/#buy-in-transition-schedule","title":"Buy-in transition schedule","text":"<p>Upgrades of buy-in nodes to the new operating system will start on July 17th (unless requested to be earlier) following the approximate schedule below. During and before this period, buy-in nodes may not be available in the scheduler so that jobs are not interrupted by the upgrade.</p> Date Buy-ins July 17 DicksonLab acdcbag albrecht allenmc ansgenetics baek bazil beacon black bmb bradburd ccg cem883 charms chenlab chomiuk christlibuyin classres July 18 cmich data-machine July 19 merzjrke midi_lab multiscaleml oakland-university phani scbbuyin tonggao wang-krishnan weilai July 22 davidroy daylab devolab junlin klausner mitchmcg neiderhu pollyhsu qian seiswei shadeash-colej wmich zayernouri_fmath July 23 deyoungbuyin dufourlab geodesy liulab msumaize planets promislow shachar_hill val-cont walker_lab wang wkerzend wuki July 24 colej jckier dworkin edgerpat egr egle_aqd eisenlohr galaxies gandalf ged geometry hcheng hcy horticulture mendoza_q michigan orgroutines prokoplab schrenklab shade-cole-bonito toulson tsang vanburen_lab vmante woldring July 25 bassolab bioinformaticscore cvmaccess ecode exoplanet_lab frib_na frib_nodes glbrc henbry_lab hsu josephsnodes maghrebi-group mc2 piermaro pls-dept toxicogenomics vermaaslab wang-compbio yueqibuyin"},{"location":"COMSOL/","title":"COMSOL","text":"<p>To use COMSOL with SLURM you should use the slurm option '-mpibootstrap slurm' and omit using srun.</p> <p>Sample run line:\u00a0</p> <pre><code>comsol batch -mpibootstrap slurm -inputfile batchin.mph -outputfile batchout.mph -batchlog batchlog.out\n</code></pre>"},{"location":"Change_Primary_Group/","title":"Change primary group","text":"<p>Generally, HPCC users need to request\u00a0root privilege through the ticket system to change their primary group permanently. However, a user can also use <code>newgrp</code> command to change their primary group temporarily to any one of their groups.</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#primary-group-change-on-command-line","title":"Primary Group Change on Command Line","text":"<p>To change a user's primary group with a command line, simply run</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp &lt;Group Name&gt;\n</code></pre> <p>Note that <code>&lt;Group Name&gt;</code> has to be one of the user's group names. To find out\u00a0all group names, please run <code>groups</code> command:</p> <pre><code>[username@dev-intel18 CurrentPath]$ groups\nchemistry VMD g09 BiCEP education-data ParaView\n</code></pre> <p>Here, the primary group is \"<code>chemistry</code>\", the first group name in the results. After the execution of <code>newgrp</code> command, a new shell session is created and\u00a0the current environment, including the current working directory remains the same.</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp g09\n[username@dev-intel18 CurrentPath]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Note that the primary group is changed to g09. To leave the session, just run \"<code>exit</code>\" command. It will go back to the previous session with the original primary group (<code>chemistry</code>).</p> <p>If a user would like to start the shell session with a new primary group as though the user just logged in, the optional \"<code>-</code>\" flag can be used</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp - g09\n[username@dev-intel18 ~]$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Using the \"<code>-</code>\" flag reinitializes the user's environment and the shell session starts from the home directory.</p> <p>Some users might have the issue of losing the header in front of the prompt after executing the command:</p> <pre><code>[username@dev-intel18 CurrentPath]$ newgrp g09\nbash-4.2$ groups\ng09 chemistry VMD BiCEP education-data ParaView\n</code></pre> <p>Using <code>source</code> on the default bashrc file will get it back.</p> <pre><code>bash-4.2$ source /etc/bashrc\n[changc81@dev-intel18 MyCurrentPath]$ \n</code></pre> <p>Note that, if a job is submitted in the session of a new primary group, the environment variables of the new primary group will be used by the job by default instead of the original environment. For more information, please see <code>--export</code> in the table of List of Job Specifications page.</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#primary-group-change-in-job-script","title":"Primary Group Change in Job Script","text":"<p>Many HPCC users have more than one research spaces. In a job running, the user might need their primary group set to be the group of the research space where the output files are. In order to do this, the job script can use <code>newgrp</code> from the beginning of the command lines. For example, an original job script may look like:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\n</code></pre> <p>If the job script is submitted in a research space, adding a few lines can make the primary group the same as the group of the research space:</p> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00             # limit of wall clock time - how long the job will run (same as -t)\n#SBATCH --ntasks=5                  # number of tasks - how many tasks (nodes) that you require (same as -n)\n#SBATCH --mem-per-cpu=2G            # memory required per allocated CPU (or core) - amount of memory (in bytes)\n\nGroupName=$(readlink -f $PWD|awk -F \"/\" '{ if ($2==\"mnt\" &amp;&amp; $3==\"research\" &amp;&amp; $4!=\"\") system(\"ls -ld /mnt/research/\"$4\"|cut -d \\\" \\\" -f 4\") }')\nnewgrp ${GroupName} &lt;&lt; EOS\n\nmodule purge\nmodule load GCC/6.4.0-2.28 OpenMPI  ### load necessary modules, e.g. \n\nsrun -n 5 &lt;executable&gt;              ### call your executable (similar to mpirun)\n\nscontrol show job $SLURM_JOB_ID     ### write job information to output file\nEOS\n</code></pre> <p>The group name of the research space for job running is automatically determined in line 7. You can also directly set the variable \"<code>GroupName</code>\" to be the group name of your research space in line 7 if you are sure about jobs running in the research space:</p> <pre><code>GroupName=&lt;group name of your research space&gt;\n</code></pre> <p>After the group name of the research space is given, the command lines between line 9 and 16 (inside the two \"<code>EOS</code>\") will be run under the environment of the new primary group (<code>${GroupName}</code>).</p>","tags":["how-to guide","groups"]},{"location":"Change_Primary_Group/#automatic-primary-group-change-after-login","title":"Automatic Primary Group Change after Login","text":"<p>To automatically change your primary group after every login, please submit a ticket to ICER's system administrators with the subject \"Membership Changes\".</p> <p>Previously, this page recommended using the <code>PrimaryGroup</code> powertool in your <code>.bashrc</code>. However, this can cause issues when you SSH to other nodes from a development node, and so we no longer recommend using it.</p>","tags":["how-to guide","groups"]},{"location":"Checkpoint_with_DMTCP/","title":"Checkpoint with DMTCP","text":"<p>Note</p> <p>To run DMTCP correctly, the limit of stack size can not be \"unlimited\". To set it to a number n, run <code>ulimit -s n</code>. For example, <code>ulimit -s 8192</code> would set the stack size as 8192. User could also add it into .bashrc file.\u00a0 It should also be set in the job batch script, as seen below.</p> <p>DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints both single and multi-node computations. It supports MPI (various implementations), OpenMP, MATLAB, Python, Perl, R, and many programming languages and shell scripting languages. DMTCP allows one to checkpoint running programs to disk, restart calculations from a checkpoint, or even migrate the processes to another host by moving the checkpoint files prior to restarting.</p> <p>Each computation you wish to checkpoint requires one DMTCP coordinator.  First, the DMTCP coordinator process is started on one host. Then application binaries are started with the <code>dmtcp_launch</code> command, causing them to connect to the coordinator upon startup. As threads are spawned, child processes are forked, remote processes are spawned via ssh, libraries are dynamically loaded, etc., DMTCP transparently and automatically tracks them. To checkpoint, use <code>dmtcp_coordinator</code> command to start checkpointing. To restart from a checkpoint, use <code>dmtcp_restart</code>.</p> <p>By default, DMTCP uses <code>gzip</code> to compress the checkpoint images. This can be turned off. This will be faster, and if your memory is dominated by incompressible data, this can be helpful. <code>gzip</code> can add seconds for large checkpoint images. Typically, checkpoint and restart is less than one second without <code>gzip</code>.</p>","tags":["tutorial","checkpointing"]},{"location":"Checkpoint_with_DMTCP/#using-dmtcp","title":"Using DMTCP","text":"<p>Running a program with checkpointing usually involves the following 4 steps (option settings may be needed for special cases) :</p> <ol> <li>Start DMTCP coordinater\u00a0<ul> <li><code>$ dmtcp_coordinator --daemon --exit-on-last $@ 1\\&gt;/dev/null     2\\&gt;&amp;1\u00a0#run coordinator as daemon in background</code></li> </ul> </li> <li>Launch program<ul> <li><code>$ dmtcp_launch ./a.out # launch ./a.out</code></li> </ul> </li> <li>Trigger checkpointing. This will     generate a set of checkpointing image files (file type:\u00a0<code>.dmtcp</code>)\u00a0and     a shell script for restart.<ul> <li><code>$ dmtcp_command\u00a0--bcheckpoint # checkpointing</code></li> </ul> </li> <li>Restart: the DMTCP coordinator will write     a script, <code>dmtcp_restart_script.sh</code>, along with a checkpoint file     (file type: <code>.dmtcp</code>) for each client process. The simplest way to     restart a previously checkpointed computation is:<ul> <li><code>$ ./dmtcp_restart_script.sh\u00a0# restart     using script</code></li> <li>Alternatively, if all processes were on the same processor, and there were no <code>.dmtcp</code> files prior to this checkpoint: <code>$ dmtcp_restart ckpt_*.dmtcp</code></li> </ul> </li> </ol>","tags":["tutorial","checkpointing"]},{"location":"Checkpoint_with_DMTCP/#dctmp-example","title":"DCTMP example","text":"<p>The following is the sample script <code>longjob.sb</code> that uses DMTCP for checkpointing a long job. In this way, the job can be run as a sequence of short walltime jobs. To obtain the complete example, run <code>module load powertools; getexample dmtcp_longjob</code>.</p> <pre><code>#!/bin/bash -login\n\n## resource requests for task:\n\n#SBATCH -J count-longjob                  # Job Name\n\n#SBATCH --time=00:06:00                   # Note that 6 min is not enough to complete the job. It enough for checkpointing and resubmit job\n\n#SBATCH -N 1 -c 1 --mem=20MB              # requested resource\n\n#SBATCH --constraint=lac                  # user could add other requests as usual.\n\n# set a limited stack size so DMTCP could work\nulimit -s 8192\n\n# current working directory shuld have source code dmtcp1.c\ncd ${SLURM_SUBMIT_DIR}\n\n# this script file name. This script may be resubmit multiple times until job completed\nexport SLURM_JOBSCRIPT=\"longjob.sb\"\n\n\n######################## start dmtcp_coordinator #######################\n\nfname=port.$SLURM_JOBID                                                                 # to store port number\n\ndmtcp_coordinator --daemon --exit-on-last -p 0 --port-file $fname $@ 1&gt;/dev/null 2&gt;&amp;1   # start coordinater\n\nh=`hostname`                                                                            # get coordinator's host name\n\np=`cat $fname`                                                                          # get coordinator's port number\n\nexport DMTCP_COORD_HOST=$h                                                  # save coordinators host info in an environment variable\n\nexport DMTCP_COORD_PORT=$p                                                  # save coordinators port info in an environment variable\n\n#rm $fname\n\n\n\n\n# uncommand following lines to print out some information if user wish\n\n#echo \"coordinator is on host $DMTCP_COORD_HOST \"\n\n#echo \"port number is $DMTCP_COORD_PORT \"\n\n#echo \" working directory: ${SLURM_SUBMIT_DIR} \"\n\n#echo \" job script is $SLURM_JOBSCRIPT \"\n\n\n\n\n####################### BODY of the JOB ######################\n\n# prepare work environment of the job\n\nmodule swap GNU/6.4.0-2.28 GCC/4.9.2\n\n\n# build the program if executable file does not exist\n\nif [ ! -f count.exe ] \n\nthen\n\n    cc count.c -o count.exe\n\nfi\n\n\n\n\n# run the program count.exe. \n\n# To run interactively: \n\n#    $ ./count.exe n num.odd 1&gt; num.even \n\n# it will count to number n and generate 2 files: \n\n# num.odd contains all the odd number;\n\n# num.even contains all the even number.\n\n\n\n# To run with DMTCP, use dmtcp commamds.\n\n# if first time launch, use \"dmtcp_launch\"\n\n# otherwise use \"dmtcp_restart\"\n\n\n\n\n# set checkpoint interval. This script would wait after dmtcp_launch\n\n# the job for the interval (in seconds), then do start the checkpoint. \n\nexport CKPT_WAIT_SEC=$(( 3 * 60 ))            # checkpointing when program runs for 3 min\n\n\n\n\n# Launch or restart the execution\n\nif [ ! -f ckpt_*.dmtcp ]                      # if no ckpt file exists, it is first time run, use dmtcp_launch\n\nthen\n\n  # first time run, use dmtcp_launch to start the job and run on background */\n\n  dmtcp_launch -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --rm --ckpt-open-files ./count.exe 800 num.odd 1&gt; num.even 10&gt;&amp;- 11&gt;&amp;- &amp;\n\n\n\n\n  #wait for an inverval of checkpoint seconds to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n\n  # start checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files --bcheckpoint\n\n\n  # kill the running job after checkpointing\n  dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n\n  # resubmit the job\n  sbatch $SLURM_JOBSCRIPT\n\n\nelse            # it is a restart run\n\n  # restart job with checkpoint files ckpt_*.dmtcp and run in background\n  dmtcp_restart -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT ckpt_*.dmtcp 1&gt; num.even &amp;\n\n\n  # wait for a checkpoint interval to start checkpointing\n  sleep $CKPT_WAIT_SEC\n\n\n  # if program is still running, do the checkpoint and resubmit\n\n  if dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT -s 1&gt;/dev/null 2&gt;&amp;1\n  then   \n    # clean up old ckpt files before start checkpointing\n    rm -r ckpt_*.dmtcp\n\n    # checkpointing the job\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --ckpt-open-files -bc\n\n    # kill the running program and quit\n    dmtcp_command -h $DMTCP_COORD_HOST -p $DMTCP_COORD_PORT --quit\n\n    # resubmit this script to slurm\n    sbatch $SLURM_JOBSCRIPT\n\n  else\n\n    echo \"job finished\"\n\n  fi\n\nfi\n\n# show the job status info\nscontrol show job $SLURM_JOB_ID\n</code></pre>","tags":["tutorial","checkpointing"]},{"location":"Cluster_Resources/","title":"Cluster resources","text":"<p>HPCC maintains several clusters. They are named according to the year of installation. Each cluster has very similar hardware with specific processors but has some variety in configuration, such as different coprocessors, memory capacity, or number of CPUs. However, with many different kinds of configurations, HPCC uses a single-queue system managed by SLURM, a resource management software. Jobs submitted to SLURM job queue can run on any possible nodes, unless there are specifications on cluster constraints. Users only have to specify resource requirements and our scheduler can assign your job to an appropriate cluster.</p> <p>All clusters currently run Ubuntu 22.04 and use the SLURM resource manager. They are connected to each other with file systems through Infiniband. Your home, research, and scratch is available and identical on all nodes.</p> <p>The following table lists nodes\u00a0that are currently available to run jobs; jobs can be submitted from any of our development nodes. Please note that you may need to scroll the table to see all columns.</p> Cluster Type Node Count Processors (Clock Speed) Cores Memory Disk Size GPUs (Number) Node Name amd22 48 AMD EPYC 7763 Processor (2.445 GHz) 128 493 GB 412 GB acm-[000-047] 18 AMD EPYC 7763 Processor (2.445 GHz) 128 996 GB 412 GB acm-[050-067] 6 AMD EPYC 7763 Processor (2.445 GHz) 128 2005 GB 412 GB acm-[048,049,068-071] amd21 11 AMD EPYC 7713 Processor (2.0 GHz) 128 512 GB 1.92 TB A100 SXM with 81920 MB memory(4) nal-[000-010] intel21 6 Intel Xeon 8358 (2.6 GHz) 64 256 GB 1.92 TB A100 PCIe with 40960 MB memory(4) nif-[000-005] amd20 207 AMD EPYC 7H12 Processor (2.595 GHz) 128 493 GB 412 GB amr-[000-101], amr-[136-238], amr-[252-253] 45 AMD EPYC 7H12 Processor (2.595 GHz) 128 996 GB 412 GB amr-[104-135], amr-[239-251] 2 AMD EPYC 7H12 Processor (2.595 GHz) 128 2005 GB 412 GB amr-[102-103] amd20-v100 20 Intel(R) Xeon(R) Platinum 8260 CPU (2.40 GHz) 48 178 GB 412 GB v100s with 32768 MB memory(4) nvf-[000-016], nvf-[018-020] 1 Intel(R) Xeon(R) Platinum 8260 CPU (2.40 GHz) 48 565 GB 412 GB v100s with 32768 MB memory(4) nvf-017 intel18 112 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 83 GB 413 GB skl-[000-011,013-112] 19 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 178 GB 413 GB skl-[113-131] 28 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 367 GB 413 GB skl-[132-139,148-167] 8 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 745 GB 413 GB skl-[140-147] 1 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 996 GB 413 GB skl-012 intel18-v100 8 Intel(R) Xeon(R) Gold 6148 CPU (2.40 GHz) 40 367 GB 413 GB v100 with 32768 MB memory (8) nvl-[000-007] intel16 26 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 492 GB 190 GB lac-[250-253,256-261,302-317] 39 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 240 GB 190 GB lac-[224-225,228-248, 278-285,294-301] 313 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 115 GB 190 GB lac-[000-023,032-191,200-223, 254,255,276,277,318-341, 350-369,372,372-445] intel16-k80 48 Intel(R) Xeon(R) CPU E5-2680 v4 (2.40 GHz) 28 240 GB 190 GB k80 with 12206 MB memory (8) lac-[024-031,080-087,136-143, 192-199,286-293,342-349] intel16-xl 2 Intel(R) Xeon(R) CPU E7-8867 v3 (2.50 GHz) 64 2.93 TB 860 GB vim-[000,001] 1 Intel(R) Xeon(R) CPU E7-8867 v4 (2.40 GHz) 144 5.86 TB 3.7 TB vim-002 intel14 18 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 240 GB 416 GB csm (11 nodes) &amp; css (10 nodes) 8 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB css-[002-003,020,023,032-035] 65 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 52 GB 416 GB css nodes intel14-k20 37 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB k20 with 4743 MB memory (2) csn-[001-039] intel14-phi 8 Intel(R) Xeon(R) CPU E5-2670 v2 (2.50 GHz) 20 115 GB 416 GB Phi card (2) csp-[006,016-020,025-026] intel14-xl 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 969 GB 1.8 TB qml-003 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 1.45 TB 897 GB qml-[002] 1 Four Intel Xeon CPU E7-8857 v2 (3.00 GHz) 48 2.93TB 1.1 TB qml-000","tags":["reference"]},{"location":"Cluster_amd20_with_AMD_CPUs/","title":"Cluster amd20 with AMD CPUs","text":"<p>In 2020, the HPCC purchased a new cluster amd20 powered by AMD EPYC processors. Each amd20 node has total 128 cores and at least 0.5 TB RAM. Each core has a base clock speed 2.6 GHz, up to 3.3 GHz.\u00a0</p>"},{"location":"Cluster_amd20_with_AMD_CPUs/#basic-architecture-information","title":"Basic architecture information","text":"<p>The AMD CPU contains 2 sockets with CPU packages with 4 NUMA nodes in each socket, and 16 cores in each NUMA node. Each NUMA core contains two \"Core Complex Dies\", which have two four-core \"Core-Complex\" modules. Each four-core Core-Complex shares a 16 MB L3 cache.</p> <p></p> <p>A 7002 series processor (via AMD)</p> <p></p> <p>A Core-Complex (via AMD) logical diagram.</p> <p></p> <p>A 1 TB amd20 node.</p> <p>Generally, cores within the same L3 cache have the lowest latency, followed by in-NUMA cores, other cores on the same socket, and cores on the other socket are slower. The SLURM job scheduler on the HPCC will try to keep within the same NUMA node by default. One option to try during testing is to use OpenMP within the L3 node, and MPI for everything else. When using newer versions of OpenMPI, you can use the following argument with OMP_NUM_THREADS=4 to distribute one 4-thread rank per L3:</p> <pre><code>mpirun -map-by ppr:1:l3cache:pe=4 --bind-to core\n</code></pre> <p>In testing, the Intel Compiler and MKL toolchain works well if you \u201cexport MKL_DEBUG_CPU_TYPE=5\u201d and compile for AVX2 instead of AVX512.</p> <p>If you\u2019re doing single-node scaling work, be aware of memory bandwidth; on these nodes, HPL scales from 1-96 cores linearly but only 3.5 -&gt; 4 TF from 96-&gt;128 when going from 3 cores per L3 to 4.</p> <p>Each node has a 100 gigabit HDR100 connection. There are 52-56 nodes per switch:</p> <p></p> <p>Resources:</p> <p>High Performance Computing: Tuning Guide for AMD EPYC\u2122 7002 Series Processors</p> <p>Compiler Options Quick Ref Guide for AMD EPYC 7xx2 Series Processors</p>"},{"location":"Collaborative_Research/","title":"Collaborative research","text":""},{"location":"Collaborative_Research/#research-space","title":"Research space","text":"<p>To support collaborative research on campus, a PI may request a shared research space where files and softwares can be shared. </p>"},{"location":"Collaborative_Research/#software-installation-request","title":"Software installation request","text":"<p>We suggest the following two steps when you are in need of a particular software package.</p> <ol> <li>Go to <code>/opt/software/</code> on a dev-node. This is where all software packages are installed. Check if your desired software and specific versions are there. Please note there could be some name discrepancy caused by switch between lower and upper case letters. Take BBMap for example, all the versions available can be viewed by \"<code>ls -l /opt/software/BBMap</code>\". Send us a ticket if the software is totally absent in <code>/opt/software</code> or your desired version is missing. If the software and the version are both present in <code>/opt/software</code>, go to step 2.</li> <li>Follow the instructions to try loading software/version. If you can't find the software using <code>module spider</code>, send us a ticket. If you can find the software but not the desired version, send us a ticket too.</li> </ol>"},{"location":"Collaborative_Research/#using-our-academic-research-consulting-services","title":"Using our Academic Research Consulting Services","text":"<p>ICER's Academic Research Consulting service (ARCS) exists to meet the computationally complex needs of the MSU research community. Research groups in need of software and computational workflow development services, dedicated training programs, as well as those that require additional scientific expertise in areas such as bioinformatics, time-series analysis, and computational modeling may request long-term collaborations with ARCS research consultants. ICER ARCS consultants are PhD-level researchers with domain-specific computational expertise and experience at R1 institutions. ARCS collaborations require funding and may be funded internally through startup grants or unit-level support or externally by grant support or industrial partnerships. More information can be found here.</p>"},{"location":"Compilers_and_Libraries/","title":"Compilers and Libraries","text":"<p>The HPCC organizes most compilers and libraries into \"toolchains\". A toolchain is a set of tools bundled together that can be used to build software. This may be as minimal as a single compiler, or as expansive as a compiler with MPI, linear algebra, CUDA, and other helpful libraries.</p> <p>These toolchains are inherited from the program ICER uses to build much of its software, EasyBuild. For more information, please see the EasyBuild documentation pages for common toolchains and all toolchains.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#common-toolchains","title":"Common toolchains","text":"<p>ICER primarily supports two toolchains on the HPCC: <code>foss</code> and <code>intel</code>. These are each numbered by year (with an <code>a</code> or <code>b</code> suffix for midyear and end of year releases respectively) and contain compilers and libraries that are compatible with each other.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-toolchain","title":"<code>foss</code> toolchain","text":"","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-components","title":"<code>foss</code> components","text":"<p>The <code>foss</code> toolchain is currently derived from the following components (and is equivalent to doing a <code>module load</code> on each one individually):</p> <ul> <li><code>GCC</code>: Composed of <ul> <li><code>GCCcore</code>: The core GCC compilers</li> <li><code>binutils</code>: Extra binary tools   for use with GCC</li> </ul> </li> <li><code>OpenMPI</code>: MPI implementation for multi-process   programs</li> <li><code>FlexiBLAS</code>: BLAS and   LAPACK API (using <code>OpenBLAS</code> and reference   <code>LAPACK</code> as backends) for linear   algebra routines</li> <li><code>FFTW</code>: Library for Fast Fourier Transforms</li> <li><code>ScaLAPACK</code>: Parallel/distributed linear   algebra routines</li> </ul>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-versions","title":"<code>foss</code> versions","text":"<p>To see the versions of this toolchain available on the HPCC use</p> <pre><code>module spider foss\n</code></pre> <p>and to see the versions of each component, use</p> <pre><code>module show foss/&lt;version&gt;\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-usage-examples","title":"<code>foss</code> usage examples","text":"CFortran Using OpenMP and shared libraries<pre><code>$ module load foss/2022a\n$ gcc foo.c -o foo \\\n    -fopenmp \\  # For OpenMP \n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using OpenMP and shared libraries<pre><code>$ module load foss/2022a\n$ gfortran foo.f90 -o foo \\\n    -fopenmp \\  # For OpenMP\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load foss/2022a\n$ mpicc foo.c -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load foss/2022a\n$ mpifort foo.f90 -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#foss-sub-toolchains","title":"<code>foss</code> sub-toolchains","text":"<p>If the entire <code>foss</code> toolchain has too many dependencies for your needs, consider one of the sub-toolchains:</p> <ul> <li><code>GCC</code>: composed of<ul> <li><code>GCCCore</code></li> <li><code>binutils</code></li> </ul> </li> <li><code>gompi</code>: composed of<ul> <li><code>GCC</code></li> <li><code>OpenMPI</code></li> </ul> </li> </ul> <p>Or feel free to load the individual modules that you need by themselves.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-toolchain","title":"<code>intel</code> toolchain","text":"","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-components","title":"<code>intel</code> components","text":"<p>The <code>intel</code> toolchain is currently derived from the following components (and is equivalent to doing a <code>module load</code> on each one individually):</p> <ul> <li><code>intel-compilers</code>:   Intel's set of (classic and oneAPI) C/C++ and Fortran compilers in addition   to:<ul> <li><code>GCCcore</code>: The core GCC compilers (used as       dependencies)</li> <li><code>binutils</code>: Extra binary tools       for use with GCC (used as dependencies)</li> </ul> </li> <li><code>impi</code>:   Intel's MPI implementation for multi-process programs</li> <li><code>imkl</code>:   Intel's BLAS and LAPACK implementations with FFT and other math libraries</li> </ul>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-versions","title":"<code>intel</code> versions","text":"<p>To see the versions of this toolchain currently available on the HPCC use</p> <pre><code>module spider intel\n</code></pre> <p>and to see the versions of each component, use</p> <pre><code>module show intel/&lt;version&gt;\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-usage-examples","title":"<code>intel</code> usage examples","text":"CFortran Using OpenMP and shared libraries<pre><code>$ module load intel/2022b\n$ icx foo.c -o foo \\\n    -qopenmp \\  # For OpenMP \n    -qmkl # For BLAS/LAPACK/FFT\n$ ./foo\n</code></pre> Using OpenMP and shared libraries<pre><code>$ module load intel/2022b\n$ ifx foo.f90 -o foo \\\n    -qopenmp \\  # For OpenMP \n    -qmkl # For BLAS/LAPACK/FFT\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load intel/2022b\n$ mpiicc foo.c -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load intel/2022b\n$ mpiifort foo.f90 -o foo\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#intel-sub-toolchains","title":"<code>intel</code> sub-toolchains","text":"<p>If the entire <code>intel</code> toolchain has too many dependencies for your needs, consider one of the sub-toolchains:</p> <ul> <li><code>intel-compilers</code>: composed of<ul> <li><code>GCCCore</code></li> <li><code>binutils</code></li> <li>The Intel compilers themselves</li> </ul> </li> <li><code>iimpi</code>: composed of<ul> <li><code>intel-compilers</code></li> <li><code>impi</code></li> </ul> </li> </ul> <p>Or feel free to load the individual modules that you need by themselves.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#mix-and-match","title":"Mix and match","text":"<p>Certain components can be combined across toolchains. A notable example is the Intel Math Kernel Library (MKL), <code>imkl</code>, that can be loaded with any other compiler.</p> <p>C example mixing GCC and MKL</p> <pre><code>$ module load GCC/11.3.0\n$ module load imkl/2022.2.1\n$ gcc foo.c -o foo \\\n    -lmkl_rt  # MKL single dynamic library\n$ ./foo\n</code></pre> <p>For more information on linking against MKL, see Intel's documentation.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#alternative-compilers-and-toolchains","title":"Alternative compilers and toolchains","text":"<p>In addition to the <code>foss</code> and <code>intel</code> toolchains, select versions of other compilers are available (with limited support) including:</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#aocc","title":"<code>AOCC</code>","text":"<p>AMD optimized compilers for C and Fortran. These use LLVM as a backend and therefore have the same command-line syntax as <code>Clang</code>.</p> <p>See also the documentation for Optimizing for AMD CPUs which gives information on installing AOCL, the AMD Optimizing CPU Libraries for AMD optimizations of math libraries, BLAS, FFTW, and others.</p> CFortran Using OpenMP<pre><code>$ module load AOCC/4.0.0\n$ clang foo.c -o foo.out \\\n    -fopenmp  # For OpenMP\n$ ./foo\n</code></pre> Using OpenMP<pre><code>$ module load AOCC/4.0.0\n$ flang foo.f90 -o foo.out \\\n    -mp  # For OpenMP\n$ ./foo\n</code></pre> CFortran Using AOCL libraries<pre><code># Assuming AOCL libraries are installed into $HOME/amd/aocl/4.0\n$ module load AOCC/4.0.0\n$ export AOCL_ROOT=$HOME/amd/aocl/4.0\n$ clang foo.c -o foo.out \\\n    -I${AOCL_ROOT}/include \\\n    -L${AOCL_ROOT}/lib \\\n    -fopenmp \\  # Required for multithreaded BLAS\n    -lflame \\  # For LAPACK\n    -lblis-mt \\  # For multithreaded BLAS\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using AOCL libraries<pre><code># Assuming AOCL libraries are installed into $HOME/amd/aocl/4.0\n$ module load AOCC/4.0.0\n$ export AOCL_ROOT=$HOME/amd/aocl/4.0\n$ flang foo.f90 -o foo.out \\\n    -L${AOCL_ROOT}/lib \\\n    -fopenmp \\  # Required for multithreaded BLAS\n    -lflame \\  # For LAPACK\n    -lblis-mt \\  # For multithreaded BLAS\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#clang","title":"<code>Clang</code>","text":"<p>LLVM-based C compilers. For math libraries like <code>FlexiBLAS</code> and <code>FFTW</code>, you will also need to load a compatible toolchain.</p> CFortran Using OpenMP<pre><code>$ module load GCCcore/11.3.0\n$ module load Clang/14.0.0\n$ clang foo.c -o foo.out \\\n   -fopenmp  # For OpenMP\n$ ./foo\n</code></pre> Using OpenMP<pre><code>$ module load foss/2022a  # Compatible with GCCcore/11.3.0\n$ module load Clang/14.0.0\n$ clang foo.c -o foo.out \\\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#nvhpc-formerly-pgi","title":"<code>NVHPC</code> (formerly PGI)","text":"<p>The NVIDIA HPC Software Development Kit. This includes C and Fortran CUDA-compatible compilers, an OpenMPI implementation, and GPU accelerated math libraries. It also includes implementations of BLAS and LAPACK, but for other CPU math libraries like <code>FFTW</code>, you will  need to load a compatible toolchain.</p> <p>Note that these compilers are most effective for building software on GPUs.</p> CFortran Using OpenMP and included shared libraries<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -mp \\  # For OpenMP \n    -lblas \\  # For BLAS\n    -llapack  # For LAPACK\n$ ./foo\n</code></pre> Using OpenMP and included shared libraries<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -mp \\  # For OpenMP \n    -lblas \\  # For BLAS\n    -llapack  # For LAPACK\n$ ./foo\n</code></pre> CFortran Using outside CPU libraries<pre><code>$ module load foss/2021a  # Compatible with GCC/10.3.0\n$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -lflexiblas \\  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> Using outside CPU libraries<pre><code>$ module load foss/2021a  # Compatible with GCC/10.3.0\n$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -lflexiblas  # For BLAS/LAPACK\n    -lfftw3  # For FFTW\n$ ./foo\n</code></pre> CFortran Using MPI<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ mpicc foo.c -o foo  # Uses NVHPC's OpenMPI\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre> Using MPI<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ mpifort foo.f90 -o foo  # Uses NVHPC's OpenMPI\n$ srun ./foo  # Use srun for better SLURM integration instead of mpirun\n</code></pre>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#alternative-mpi-implementations","title":"Alternative MPI implementations","text":"<p>Though we recommend OpenMPI or Intel MPI, older installations of MPICH and MVAPICH2 are available. These are generally only compatible with a single compiler module as they were built on a one-off basis.</p> <p>You can see the available versions and how to load them with</p> <pre><code>module spider MPICH\n</code></pre> <p>or</p> <pre><code>module spider MVAPICH2\n</code></pre> <p>If OpenMPI, Intel MPI, or the available versions of <code>MPICH</code> and <code>MVAPICH2</code> will not work for your needs, please contact us to discuss alternatives.</p>","tags":["reference","compilers"]},{"location":"Compilers_and_Libraries/#basic-mathematical-library-benchmark","title":"Basic Mathematical Library Benchmark","text":"<p>Possibly outdated</p> <p>This benchmark was run when the AMD EPYC processors were installed (approximately 2020). The results may be outdated due to compiler improvements or hardware changes.</p> <p>This section shows the results of a basic test of the mathematical libraries with different compilers on the AMD EPYC Processors (i.e., those in the <code>amd20</code> cluster). The test runs many calculations of sine, cosine and\u00a0logarithm functions in parallel. Each of the calculations is independent from the others and finally they get summed up. The test is executed by a C program written with OpenMP multi-threading and compiled with different compilers and libraries. Three letters (A, G and I) followed by a digit (1 or 2) are used to specify different tests:</p> Letters First Second Digit A AMD Compiler AMD\u00a0basic mathematical Library 1: all threads running in one socket G GNU Compiler GNU\u00a0basic mathematical Library (<code>-lm</code>) 2: threads evenly spread to two different sockets I Intel Compiler Intel\u00a0basic mathematical Library (included in compiler) <p>where the letter in the first and second position represents which compiler and basic mathematical library is in use respectively. The performance results are presented in the following figure:</p> <p></p> <p>where all timing values were derived by the average of running ten times. As you can see in the figure, the performance of the parallel scaling is almost linear for all compilers and the scaling efficiency is strong (about 61% for AA1 and AA2). From the comparison of the timing results, Intel compiler with its library shows the best performance. However, GCC and AMD compilers with AMD basic mathematical library also perform well. In the results of 128 threads, the elapsed time of AMD compiler with AMD library are very closed to the time of Intel's. In the tests of spreading threads, we also find out all threads running on one socket has no difference from spreading them on two different sockets.</p> <p>The same C program was also compiled and run on an <code>intel18</code> and an <code>intel16</code> node. The timing of <code>amd20</code> node with 128 threads is about 3 times faster than the performance of\u00a0<code>intel18</code> (with 40 threads) and 4.5 times faster than the performance of\u00a0<code>intel16</code> (with 28 threads). The decrease in timing is well prorated with the increase on thread number.\u00a0</p>","tags":["reference","compilers"]},{"location":"Compiling_for_GPUs/","title":"Compiling for GPUs","text":"<p>There are a few different ways for code to access GPUs. This page will focus only on compiling code that uses those techniques rather than explaining the techniques themselves.</p> <p>For more general information on compilers on the HPCC, see our reference page.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#gpu-compatibility","title":"GPU compatibility","text":"<p>Because GPU hardware can change dramatically from device to device, it's important to keep in mind the GPUs your code will run on when compiling. In practical terms on the HPCC, this means selecting version of CUDA that is best suited for your target GPU.</p> <p>See the table below for the CUDA compute capability and associated CUDA versions of the various GPUs available on the HPCC (additionally, see this CUDA version table and the compute capabilities for GPUs for more information). Using the maximum version or lower will ensure that code compiled with that version of CUDA will be able to run on the associated GPU by default.</p> GPU Compute capability Minimum CUDA version Maximum CUDA version* <code>k20</code> 3.5 4.2 10.2 <code>k80</code> 3.7 4.2 10.2 <code>v100</code> 7.0 9.0 <code>a100</code> 8.0 11.0 <p>*Support beyond the maximum version</p> <p>CUDA versions greater than the maximum CUDA version listed above may still work for certain GPUs (for example, CUDA versions up to 11.4 can work with compute capabilities 3.5 and 3.7), but will not compile for them by default. In these situations, you should explicitly state the minimum compute capability when compiling as discussed below.</p> <p>Compiling for maximum compatibility</p> <p>The minimum version listed in the table above is necessary to support all capabilities of a GPU. However, the drivers for NVIDIA GPUs are generally backwards compatible with earlier versions of PTX code (see below). This means you can use an earlier CUDA version to compile code, and it will be able to run on any newer GPUs. For more information, see Compiling for specific GPUs below.</p> <p>For a good compromise between features and compatibility with all GPUs at ICER, CUDA 9.x or 10.x will work well.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cuda-code","title":"CUDA code","text":"<p>CUDA Fortran (that is, Fortran code with CUDA specific extensions) is compiled using the same <code>nvfortran</code> compiler described in the <code>NVHPC</code> section of our compiler reference.</p> <p>CUDA C/C++ code (that is, C/C++ code with CUDA specific extensions to run kernels on a GPU device), is compiled using the <code>nvcc</code> compiler. The recommended way to access this compiler is to load the <code>NVHPC</code> module as described in our compiler reference or a <code>fosscuda</code> or <code>gcccuda</code> toolchain. Alternatively, <code>nvcc</code> is also included in any of the <code>CUDA</code> modules.</p> <p>Behind the scenes, <code>nvcc</code> will use <code>gcc/g++</code> to compile the C/C++ code itself. The version used is whatever is on your path. This means that if you load <code>CUDA</code> by itself, you will be using the system version of <code>gcc/g++</code> which is much older than the versions available in the module system. For this reason, we recommend loading either the <code>NVHPC</code> modules (which will use the version of <code>gcc/g++</code> suffixing the module version), or loading a <code>fosscuda</code> or <code>gcccuda</code> module that will load an appropriate version of <code>CUDA</code> along with the corresponding <code>foss</code> or <code>GCC</code> toolchain.</p> CFortran CUDA with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.cu -o foo\n$ ./foo\n</code></pre> CUDA with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.cuf -o foo\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#compiling-for-specific-gpus","title":"Compiling for specific GPUs","text":"<p>GPU code is compiled in two stages: </p> <ol> <li>Compiling into a virtual instruction set like assembly code (called PTX)</li> <li>Compiling the virtual instructions into binary code (called a cubin) that actually runs on the GPU</li> </ol> <p>These stages are controlled by the compute capability specified to <code>nvcc</code> (in the previous examples, this is set implicitly to 5.2) and <code>nvcc</code> can embed the results of stage 1, stage 2, or both for various compute capabilities in the final executable. If the stage 2 output is missing for the compute capability of the GPU that the code is executed on, the NVIDIA driver will just-in-time (JIT) compile any stage 1 code it finds at runtime into stage 2 code appropriate for that GPU.</p> <p>In general, you should use the lowest compute capability your code supports in step 1 (for the widest compatibility with future JIT compilation) and the compute capability of the target GPU in step 2 (for the best optimization).</p> <p>To specify compute capability x.y for stage 1, use the <code>-arch=compute_xy</code> flag, and for stage 2, use the <code>-code=sm_xy</code> flag. You can also specify <code>-code=compute_xy</code> to embed the output of stage 1 into the final binary for JIT compilation. Multiple <code>compute_xy</code> and <code>sm_xy</code> values can be supplied to <code>-code</code> in a comma separated list.</p> <p>See NVIDIA's documentation on GPU compilation for more information and examples.</p> <p>Compiling for <code>k20</code> and <code>k80</code> GPUs with CUDA 11.4</p> <p>As discussed above, CUDA 11.4 will not compile for <code>k20</code> and <code>k80</code> GPUs by default. However, we can specify the corresponding compute capabilities explicitly:</p> <pre><code>$ module load NVHPC/21.9-GCC-10.3.0-CUDA-11.4\n$ nvcc foo.cu -o foo \\\n    -arch=compute_35 -code=compute_35,sm_35,sm_37,sm_70\n$ ./foo\n</code></pre> <p>The resulting executable will be able to run on all GPUs at ICER:</p> <ul> <li>The <code>k20</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_35</code>-compatible <code>cubin</code>.</li> <li>The <code>k80</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_37</code>-compatible <code>cubin</code>.</li> <li>The <code>v100</code> by compiling <code>compute_35</code> (and higher)-compatible PTX into an <code>sm_70</code>-compatible <code>cubin</code>.</li> <li>The <code>a100</code> by JIT compiling the embedded <code>compute_35</code> (and higher)-compatible PTX at runtime.</li> </ul>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cuda-libraries","title":"CUDA libraries","text":"<p>The <code>NVHPC</code> and <code>CUDA</code> modules offer many CUDA accelerated math libraries, like cuBLAS, cuSOLVER and cuFFT.</p> <p>For C/C++ code, since using many these libraries do not require writing CUDA code, using the <code>nvcc</code> compiler is optional. We refer to the documentation for the specific libraries for how to link them, but give examples of linking <code>cuBLAS</code> and <code>cuFFT</code> with <code>nvcc</code> and the GNU compilers directly.</p> <p>Take care to link to libraries that are all distributed in the same version of CUDA, to use a version of CUDA compatible with the desired GPUs, and (if using shared libraries) to load the same version of CUDA when running the executable.</p> CFortran Using CUDA shared libraries with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.c -o foo \\\n    -lcublas \\  # For BLAS\n    -lcufft  # For cuFFT\n$ ./foo\n</code></pre> Using CUDA shared libraries with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.f90 -o foo \\\n    -cudalib=cublas \\  # For cuBLAS\n    -cudalib=lcufft  # For cuFFT\n$ ./foo\n</code></pre> <p>Fortran support for linking to the CUDA libraries is limited to NVIDIA's compilers. See NVIDIA's Fortran CUDA interfaces for more information. </p> C Using CUDA shared libraries with fosscuda<pre><code>$ module load fosscuda/2020a\n$ gcc foo.c -o foo \\\n    -lcudart  \\ # For CUDA runtime routines like memory management\n    -lcublas \\  # For cuBLAS\n    -lcufft  # For cuFFT\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#cudnn","title":"cuDNN","text":"<p>A popular set of CUDA libraries not included in the CUDA toolkit is cuDNN. On the HPCC, cuDNN is available as a module. Search for available versions with</p> <pre><code>module spider cuDNN\n</code></pre> <p>and choose one which uses a version of CUDA compatible with any other GPU-based work you may be doing. See NVIDIA's API reference for which libraries to link against.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#gpu-offloading","title":"GPU offloading","text":"<p>Parts of code can be offloaded onto the GPUs using directive-based APIs like OpenMP and OpenACC. Currently, the recommended approach is to use OpenACC with the NVIDIA HPC SDK compilers.</p>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#openacc","title":"OpenACC","text":"<p>Offloading with OpenACC is primarily supported by the NVIDIA's compilers in the <code>NVHPC</code> modules. Using the <code>-acc</code> option will activate OpenACC and run kernels by default on the GPU.</p> <p>The specific compute capabilities of the desired target GPUs can also be passed to compile compatible binaries for the respective GPUs. GPUs with other compute capabilities will incur a slight one-time cost when the executable is run (so that embedded PTX code can be JIT compiled to appropriate binary).</p> CFortran GPU offloading with OpenACC with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvc foo.c -o foo \\\n    -acc \\  # To use OpenACC (default is on GPU)\n    -gpu=cc35,cc37,cc70,cc80  # To embed GPU code for various compute capabilities\n$ ./foo\n</code></pre> GPU offloading with OpenACC with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.c -o foo \\\n    -acc \\  # To use OpenACC (default is on GPU)\n    -gpu=cc35,cc37,cc70,cc80  # To embed GPU code for various compute capabilities\n$ ./foo\n</code></pre> <p>The same versions of the <code>GCC</code> modules discussed in the OpenMP section above support OpenACC, however with the same caveats. Experiment with these versions at your own risk.</p> CFortran GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gcc foo.c -o foo \\\n    -fopenacc \\  # To activate OpenACC instructions\n    -foffload=nvptx-none=\"-lm\"  # To offload code to NVIDIA GPUs, and\n                                # ensure that offloaded code has access\n                                # to math libraries\n$ ./foo\n</code></pre> GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gfortran foo.c -o foo \\\n    -fopenacc \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm -lgfortran\"  # To offload code to NVIDIA\n                                           # GPUs, and # ensure that \n                                           # offloaded code has access \n                                           # to math and Fortran libraries\n$ ./foo\n</code></pre>","tags":["reference","compilers","GPU"]},{"location":"Compiling_for_GPUs/#openmp","title":"OpenMP","text":"<p>Offloading with OpenMP is primarily supported by the NVIDIA's compilers in the <code>NVHPC</code> modules. Using the <code>-mp=gpu</code> option will set OpenMP code to use a GPU as a target device.</p> <p>The specific compute capabilities of the desired target GPUs can also be passed to compile compatible binaries for the respective GPUs. GPUs with other compute capabilities will incur a slight one-time cost when the executable is run (so that embedded PTX code can be JIT compiled to appropriate binary).</p> CFortran GPU offloading with OpenMP with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvcc foo.c -o foo \\\n    -mp=gpu \\  # To use OpenMP on GPU\n    -gpu=cc35,cc37,cc70,cc80  # Embed GPU code for compute capabilities\n$ ./foo\n</code></pre> GPU offloading with OpenMP with NVHPC<pre><code>$ module load NVHPC/21.9-GCCcore-10.3.0-CUDA-11.4\n$ nvfortran foo.c -o foo \\\n    -mp=gpu \\  # To use OpenMP on GPU\n    -gpu=cc35,cc37,cc70,cc80  # Embed GPU code for compute capabilities\n$ ./foo\n</code></pre> <p>A few versions of the <code>GCC</code> modules available on the HPCC have highly experimental support for offloading OpenMP code to GPUs. These versions include a <code>-cuda</code> or <code>-offload</code> suffix in the version name. Use</p> <pre><code>module spider GCC\n</code></pre> <p>to search for versions including these suffixes.</p> <p>Support for these compilers is very limited, and simple tests indicate that they can suffer from reduced performance in comparison to NVIDIA's compilers or running multi-threaded (or, in extreme cases, single-threaded) on the CPU. Experiment with these versions at your own risk.</p> CFortran GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gcc foo.c -o foo \\\n    -fopenmp \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm\"  # To offload code to NVIDIA GPUs, and\n                                # ensure that offloaded code has access\n                                # to math libraries\n$ ./foo\n</code></pre> GPU offloading with OpenMP with GCC<pre><code>$ module load GCC/11.1.0-cuda-9.2.88\n$ gfortran foo.c -o foo \\\n    -fopenmp \\  # To activate OpenMP instructions\n    -foffload=nvptx-none=\"-lm -lgfortran\"  # To offload code to NVIDIA\n                                           # GPUs, and # ensure that \n                                           # offloaded code has access \n                                           # to math and Fortran libraries\n$ ./foo\n</code></pre> <p>Other GPU specific compilation options can be passed in the quotes following <code>-foffload=nvptx-none=</code>, e.g., <code>-foffload=nvptx-none=\"-lm -latomic -O3\"</code>.</p>","tags":["reference","compilers","GPU"]},{"location":"Conditional_Statements/","title":"Conditional statements","text":"<p>Conditional statements help you to execute parts of a shell script only if certain conditions hold. Below, we give the syntax and examples for three common variants of <code>if</code> statements as well as the <code>case</code> statement which acts like switch statements in other languages.</p>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-fi","title":"<code>if ... fi</code>","text":"<p>General syntax:</p> <pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nfi\n</code></pre> <p>Here, <code>expression</code> often takes the form of comparing strings using operators <code>==</code> or <code>!=</code> or comparing numbers using operators <code>-eq</code> (equal), <code>-ne</code> (not equal), <code>-lt</code> (less than), <code>-gt</code> (greater than), or <code>-ge</code> (greater than or equal to).</p> <p>However, there are other conditional expressions, many related to files (for example <code>-e test.txt</code> returns true if the file <code>test.txt</code> exists). For a full listing, see this manual page.</p> <p>Example:</p> test.sh<pre><code>#!/bin/bash\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash ./test.sh\n</code></pre> output<pre><code> is equal to\n</code></pre> input<pre><code>bash ./test.sh 1 2\nbash ./test.sh 12 12\n</code></pre> output<pre><code>12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-else-fi","title":"<code>if ... else ... fi</code>","text":"<p>General syntax: </p> <pre><code>if [ expression ]\nthen\n   Statement(s) to be executed if expression is true\nelse\n   Statement(s) to be executed if expression is not true\nfi\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/bash\n\nif [ $1 == $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelse\n   echo \"$1 is not equal to $2\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash ./test.sh\n</code></pre> output<pre><code> is equal to\n</code></pre> input<pre><code>bash ./test.sh 1 2\n</code></pre> output<pre><code>1 is not equal to 2\n</code></pre> input<pre><code>bash ./test.sh 12 12\n</code></pre> output<pre><code>12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#if-elif-else-fi","title":"<code>if ... elif ... else ... fi</code>","text":"<p>General syntax: </p> <pre><code>if [ expression 1 ]\nthen\n   Statement(s) to be executed if expression 1 is true\nelif [ expression 2 ]\nthen\n   Statement(s) to be executed if expression 2 is true\nelif [ expression 3 ]\nthen\n   Statement(s) to be executed if expression 3 is true\nelse\n   Statement(s) to be executed if no expression is true\nfi\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/bash\n\nif [ $1 -eq $2 ]\nthen\n   echo \"$1 is equal to $2\"\nelif [ $1 -gt $2 ]\nthen\n   echo \"$1 is greater than $2\"\nelif [ $1 -lt $2 ]\nthen\n   echo \"$1 is less than $2\"\nelse\n   echo \"None of the condition met\"\nfi\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash ./test.sh\n</code></pre> output<pre><code> is equal to\n</code></pre> input<pre><code>bash ./test.sh 1 2\n</code></pre> output<pre><code>1 is less than 2\n</code></pre> input<pre><code>bash ./test.sh 12 2\n</code></pre> output<pre><code>12 is greater than 2\n</code></pre> input<pre><code>bash ./test.sh 12 12\n</code></pre> output<pre><code>12 is equal to 12\n</code></pre>","tags":["reference","command line"]},{"location":"Conditional_Statements/#case-esac","title":"<code>case ... esac</code>","text":"<p>General syntax:</p> <pre><code>case word in\n    patterns ) commands ;;\nesac\n</code></pre> <p>Example:</p> test.sh<pre><code>#!/bin/bash\n\nread -p \"Enter a number between 1 and 3 inclusive &gt; \" character\ncase $character in\n    1 ) echo \"You entered one.\"\n        ;;\n    2 ) echo \"You entered two.\"\n        ;;\n    3 ) echo \"You entered three.\"\n        ;;\n    * ) echo \"You did not enter a number between 1 and 3.\"\nesac\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Enter a number between 1 and 3 inclusive &gt; 1\nYou entered one.\n</code></pre> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Enter a number between 1 and 3 inclusive &gt; 2\nYou entered two.\n</code></pre> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Enter a number between 1 and 3 inclusive &gt; 3\nYou entered three.\n</code></pre> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Enter a number between 1 and 3 inclusive &gt; 4\nYou did not enter a number between 1 and 3.\n</code></pre>","tags":["reference","command line"]},{"location":"Connect_over_SSH_with_VS_Code/","title":"Connect over SSH with VS Code","text":"<p>VS Code allows you to work on remote servers from your local machine. </p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install VS Code by following the instructions.</li> <li>(Windows only) Install OpenSSH for Windows by following the instructions.</li> <li>Configure SSH so that you can access development nodes using proxy jumps by following these directions.</li> <li>(Optional) Set up SSH keys to connect to the HPCC without needing to enter a password. For Windows users, use the MobaXterm GUI instructions (or ensure that your private SSH key is saved to <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>).</li> </ol> <p>Warning</p> <p>If you do not add the proxy jumps to the <code>.ssh/config</code> file on your local computer as described in step 3 above, you will not be able to access the HPCC from VS Code.</p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#connecting-to-the-hpcc-with-vs-code","title":"Connecting to the HPCC with VS Code","text":"<p>First, we need the Remote Development extension for VS Code, which can be obtained here</p> <p></p> <p>When the extension is installed, press F1, and select 'Remote-SSH: Connect to Host...' You may need to start typing this option to get it to appear. </p> <p>Then select intel18, k80, or any other development node. </p> <p>Never connect to a gateway node</p> <p>If you have a <code>Host gateway</code> section in your <code>.ssh/config</code> file, <code>gateway</code> will be an option when connecting to a host using VS Code as shown in the screenshot above. Never choose this option. It will run a VS Code server on gateways node which are not meant for running user processes. This can lead to system slowdown and stop users from being able to login to the HPCC altogether.</p> <p>You may be asked for the type of the remote platform, in which case, choose \"Linux\". Accept any other questions, and if you do not have SSH keys setup, you will be asked for your password.</p> <p>Afterwards new VS Code window will pop up. When you are successfully connected, bottom left of the VS Code will show server information such as 'SSH: intel18'. </p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_over_SSH_with_VS_Code/#working-on-the-hpcc-with-vs-code","title":"Working on the HPCC with VS Code","text":"<p>Now that you are connected, you can run commands and codes from VS Code. When you are connected to the HPCC, all files created through VS Code will be saved to the HPCC, not on your local machine.</p> <p>To test the ability to run remote code, let's create a Python file 'hello.py' with VS Code. Click 'New file' and create a file with the following content to the file: </p> <pre><code>print('hello!')\n</code></pre> <p>Now, open a terminal from 'Terminal' menu from VS Code (or by pressing the key sequence CTRL+Shift+`) and run the code.</p> <pre><code>$ python hello.py\n</code></pre> <p>Once you have connected the server, click the Remote Explorer to see the list of servers. Click the icon to connect to host in a new window. Right click the name to see the option.  </p> <p></p>","tags":["how-to guide","VS Code","VScode","ssh"]},{"location":"Connect_to_HPCC_System/","title":"Connect to the HPCC","text":"<p>This tutorial requires that you have an SSH client present on your system. It will walk you through connecting to the HPCC's gateway, moving on to a development node, and testing your system's X Windows server.</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#connect-to-the-gateway","title":"Connect to the gateway","text":"<p>Open the SSH client on your local computer and run <code>ssh -XY &lt;username&gt;@hpcc.msu.edu</code>, where <code>&lt;username&gt;</code> should be your HPCC account name. The first time you connect, SSH will ask if you trust the SSH key fingerprint of the HPCC. Type <code>yes</code> and press enter to continue. SSH will prompt  for your password. Please type the password of your MSU NetID (the password will be invisible). Upon a successful login, you will see a welcome message and current usage load for each development node. </p> <p>Our gateway is for entrance to the HPCC system only. Running programs and other tasks should be done on development nodes (see the next section).</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#ssh-to-a-development-node","title":"SSH to a Development Node","text":"<p>To access a development (dev) node from gateway, pick one and type <code>ssh -X node-name</code> (if you don't need to launch a GUI program, omit  <code>-X</code>).   See here for dev-node usage and policy.</p>","tags":["how-to guide","ssh"]},{"location":"Connect_to_HPCC_System/#testing-your-x-windows-system","title":"Testing your X Windows System","text":"<p>An X Windows System allows graphical user interfaces (GUIs) to be shared over SSH. X Windows Systems can be part of any operating system - Windows, Mac, or Linux. You may also hear references to \"X11\" when people are talking about X Windows Systems, as this is the most common protocol used to implement an X Windows System.</p> <p>If your local computer has an X Windows client installed (such as Xquatz or MobaXterm) and you log into the nodes with the <code>-X</code> option, you can test if GUI will work by running <code>xeyes</code> on the command line. If everything is set correctly, you should see a pop-up window containing a pair of eyes.</p> <p>If you see an error about X11 forwarding when logging into the gateway (e.g. \"Warning: untrusted X11 forwarding setup failed: xauth key data not generated\"), please check your local machine's \"config\" file (<code>$HOME/.ssh/config</code>) setting. It should look like</p> $HOME/.ssh/config<pre><code>Host *\nForwardAgent yes\nForwardX11 yes\nForwardX11Trusted yes\nXAuthLocation /opt/X11/bin/xauth\n</code></pre>","tags":["how-to guide","ssh"]},{"location":"Connections_to_compute_nodes/","title":"Connections to compute nodes","text":"","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#ssh","title":"SSH","text":"<p>When a job is running on a compute node, the owner of the job can access the node using <code>ssh</code> from a development node. This will connect your terminal to the  shell on the compute node where the job is running. </p> <p>If your job is running on  the node <code>amr-070</code> the command would be <code>ssh amr-070</code>. Once on the compute node you can inspect individual CPU and memory usage with <code>top</code> and <code>htop</code>.</p>","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#connecting-from-your-local-machine","title":"Connecting from your local machine","text":"<p>If you have SSH Tunneling set up, we can extend the  ProxyJump framework to connect to compute nodes.</p> <p>To do this, add the following lines to your <code>.ssh/config</code>:  <pre><code>    Host lac* vim* skl* nvl* amr* nvf* nif* nal* acm*\n        User here_you_put_your_net_id\n        ProxyJump intel18\n</code></pre></p> <p>The three-letter names correspond to the kinds of nodes available from each cluster. Notice that we are using the development node <code>intel18</code> to connect to the compute nodes, rather than the gateway.</p> <p>We can then run a command such as <code>ssh amr-070</code> from our local terminal (assuming we have a job running on that node). This will connect us directly to that node.</p>","tags":["ssh","how-to guide"]},{"location":"Connections_to_compute_nodes/#ondemand","title":"OnDemand","text":"<p>Using OnDemand you can access the compute node that your job is running on in the same way by opening a development node from  the Development Nodes menu and then using the <code>ssh</code> command as described above.  You can identify the node for interactive sessions from the My Interactive  Sessions tab. It is located after the word \"Host\". Do not include the <code>&gt;_</code>  characters in your <code>ssh</code> command. The image below shows a job running on the  node <code>amr-070</code>.</p> <p></p>","tags":["ssh","how-to guide"]},{"location":"Containers_Overview/","title":"Containers (Singularity and Docker)","text":"<p>Suppose that you develop or use some software on your computer that you would like to use on the HPCC. If that software requires many dependencies or a tricky setup to work properly, it may be difficult to transfer over to the HPCC. This is a prime opportunity to employ containers to simplify your workflow and get you started on the HPCC!</p> <p>A Linux container provides an environment that's different from the host computer you may be running on (e.g., your laptop or the HPCC). For example, you could run a different version of Linux (e.g., running Alpine on our Ubuntu system). One advantage of containers is if your software requires a newer version of system libraries (e.g. glibc) than is available in our operating system, then you can run your software in a container. Another advantage is that many pieces of software provide containers that you can download and start working with immediately, no matter what computer/operating system/software stack you may be using. </p> <p>The following three pages explain container-usage:</p> <ul> <li>Docker: Docker is the most popular tool to work with containers. This tutorial walks you through an example of running containers and building your own.</li> <li>Singularity Introduction: Though Docker is the most popular way to work with containers, it requires permissions that users are not allowed on the HPCC. Instead, HPCC systems use a (Docker compatible) tool called Singularity. This tutorial walks you through the basics of using Singularity to run containers (created with Docker or Singularity) on the HPCC.</li> <li>Singularity Advanced Topics: This page serves as reference for some more advanced topics related to tweak your usage of Singularity on the HPCC.</li> </ul>","tags":["explanation","containers","Docker","Singularity"]},{"location":"Dashboard/","title":"Dashboard","text":"<p>Toggle Fullscreen</p>","tags":["dashboard"]},{"location":"Data_Display_Debugger/","title":"Data Display Debugger","text":"<p>DDD stands for 'Data Display Debugger'. It is a GUI front end of GDB, the GNU debugger. The main advantage of DDD over GDB is that DDD offers a GUI. In this tutorial, we will learn about</p> <ul> <li>setting and removing breakpoints</li> <li>tracing through programs</li> <li>examining data at various points in execution.</li> </ul>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#starting-ddd","title":"Starting DDD","text":"<p>Since DDD is a graphical debugger, you will either need to SSH with X forwarding or use OnDemand.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#ssh-with-x","title":"SSH with X","text":"<p>If you want to debug from a dev node, first make sure that you SSH using the <code>-X</code> option. You can then start DDD using <code>ddd</code> which will open a DDD window on your computer:</p> <pre><code>$ ssh -X user@hpcc.msu.edu\n$ ssh dev-amd20  # only the first ssh needs -X\n$ ddd\n</code></pre> <p>If you would like to run your debugging job interactively on a compute node, please see the instructions for running an interactive graphical application.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#ondemand","title":"OnDemand","text":"<p>You can also use DDD in your browser through OnDemand. Log in using your MSU credentials on the OnDemand site. Then from the Jobs menu, select Interactive Desktop. Once your job starts, you can open your Interactive Desktop and choose \"Data Display Debugger\" from the Applications menu under Programming.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#the-ddd-interface","title":"The DDD interface","text":"<p>When you start DDD, you will see a DDD window like this:</p> <p></p> <p>The DDD window consists of 4 sections:</p> <ul> <li>data window</li> <li>source window</li> <li>machine code window</li> <li>GDB console</li> </ul> <p>You can show/hide each of them in View menu.</p> <p>You can customize the DDD environment in Edit \u2192 Preferences menu.</p> <p>For example, to display line numbers in source window, under Edit \u2192 Preferences \u2192 Source, check 'Display Source Line Numbers'.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#opening-a-program","title":"Opening a program","text":"<p>To use DDD, we need a program to debug. Let's use the following code.</p> debug_ex.c<pre><code>#include &lt;stdio.h&gt;\n\nint main(int argc, char** argv){\n  for(int i = 0; i &lt; 10; i++){\n    int j = i*i;\n    printf(\"%d \", j);\n  }\n  printf(\"\\n\");\n}\n</code></pre> <p>First, you need to compile this code with <code>-g</code> option to include the debug symbols. Run this on the command line:</p> <pre><code>gcc -g debug_ex.c -o debug_ex\n</code></pre> <p>Now, to open this in DDD, you can either select it in the File \u2192 Open Program menu or you can run DDD with this executable from the command line like</p> <pre><code>ddd debug_ex\n</code></pre> <p>Even though you open an executable such as <code>debug_ex</code>, DDD will show the source file name such as <code>debug_ex.c</code>.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#setting-breakpoints","title":"Setting breakpoints","text":"<p>Breakpoints stop your program in the middle of running to examine the current state of variables and data structures.  You can continue from where you set the breakpoint to finish program execution.</p> <p>To set a breakpoint, double click to the left of the source line in the source window. A STOP icon will appear next to it.</p> <p>Click Run at the top of the window to start execution or type <code>run</code> in the GDB console. A green arrow will appear as soon as you hit the breakpoint.</p> <p></p> <p>The breakpoints you set can be deleted or disabled by right-clicking on the line with the breakpoint and choosing either the \"Disable Breakpoint\" or \"Delete Breakpoint\" options.</p> <p>In order to set breakpoints in other files (i.e., not in the <code>main()</code> function), choose the \"Open Source\" option from the File menu of DDD. The file dialog should appear. </p> <p>The example in the figure above has a breakpoint at line 6. The program runs to the line number 5 and waits your input. You can run the code a line by line with 'next' command (by either clicking the button at the top of the screen or typing <code>next</code> into the GDB console). </p> <p>To see a variable's value, type <code>print &lt;variable_name&gt;</code> in the GDB console. For example, <code>print i</code> will show the value of the variable <code>i</code>. You can also right click the variable in the source code choose <code>Print &lt;variable_name&gt;</code>.</p> <p>To go to the next break point, click Cont button or type <code>cont</code> on GDB console.</p> <p>When you find bugs, edit your source code in your editor of choice and recompile the code. Reload the new source code into DDD using the Source menu: Source \u2192 Reload Source.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#common-commands","title":"Common commands","text":"<p>DDD offers command buttons, but you can also type commands directly on GDB console.  </p> command Description <code>help</code> help documentation for topics and commands <code>help breakpoint</code> Lists help information about breakpoints <code>break</code> sets breakpoint <code>break line_number</code> Sets breakpoint at a line number <code>break function_name</code> Sets breakpoint at the begining of function name <code>enable</code>, <code>disable</code>, <code>delete</code>/<code>clear</code> Enable, disable, or delete one or more breakpoints. <code>disable 3</code> Disables breakpoint number 3 <code>clear line_number</code> Clears breakpoint at line_number <code>delete 3</code> Deletes breakpoint number 3 <code>delete</code> Deletes all beakpoints <code>run</code> Starts program running from the begining. <code>continue</code> (or <code>cont</code>) Continues execution from the current line to the next breakpoint <code>step</code> (or <code>s</code>) Execute next line(s) of program <code>step</code> Executes one line of a program <code>step number</code> Executes next number of lines of program next (or n) Like step, but treats a function as a single line. <code>next</code> Execute the next line <code>next number</code> Executes next number of lines of program <code>until line_number</code> Executes program until line number <code>quit</code> quit DDD <code>list</code> Lists program source code <code>condition</code> Conditional breakpoints <code>print</code> Display program values, results of expressions <code>whatis</code> List type of an expression <code>whatis j</code> Shows data type of expression <code>j</code> <code>info</code> Get information <code>info locals</code> Shows local variables in current stack frame <code>info args</code> Shows the argument variable of current stack frame <code>info break</code> Show breakpoints <code>set</code> Change values of variables, memory, registers <code>set x = 123*y</code> Set variable <code>x</code>'s value to <code>123*y</code>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#examining-data","title":"Examining data","text":"<p>While the program is running, you may want to examine the contents of variables. You can do this by right-clicking on a variable name in the DDD window.</p> <p>Upon right-clicking, select \"Display\". If you want to display the value of a pointer, use the \"Display*\" menu item.</p> <p></p> <p>Right-clicking on a variable name offers other capabilities such as Print, Lookup, What Is (showing the data type), Break, and Clear.</p> <p>Instead of right-clicking, you can peek at memory contents also. To do that, click Data \u2192 Memory.</p>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Display_Debugger/#some-useful-resources","title":"Some useful resources","text":"<ul> <li>The official DDD Manual</li> <li>A good debugging tutorial using DDD</li> </ul>","tags":["tutorial","DDD","Debugging"]},{"location":"Data_Machine/","title":"The Data Machine","text":"<p>ICER is excited to offer a new computational resource called the \u201cData Machine\u201d. Many research areas are now faced with large amounts of data thanks in large part to the growth of available datasets. Manipulating, analyzing, and visualizing  large amounts of data requires specialized computational resources that are not typically offered by more traditional high performance computing systems. Additionally, this \u201cdata explosion\u201d is occurring primarily in fields where research computing has historically not been widely used.</p> <p>To meet these hardware and educational needs, ICER is developing the Data Machine and associated outreach and training programs. Though the machine is not yet available to the broader research community (both at MSU and beyond), ICER is looking to connect with researchers who are interested in this new initiative and willing to experiment with the machine and offer feedback. ICER would also like to develop relationships with instructors hoping to integrate data-intensive computation (including demonstrations, assignments, and projects) into their classrooms.</p> <p>Researchers and instructors looking for more information are encouraged to read below and submit short proposals. For the time being, access to the Data Machine is only provided upon request.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#would-you-benefit-from-the-data-machine","title":"Would you benefit from the Data Machine?","text":"<p>The Data Machine is structured to benefit users whose research and/or instruction is described by many or all of the following characteristics:</p> <ul> <li>Datasets that are many GB in size or larger</li> <li>A combination of datasets of varying types and provenances</li> <li>Datasets that require many read &amp; write (I/O) operations, including those composed of many small files</li> <li>Desire for interactive data analysis or workflow development</li> <li>Desire to incorporate data from publicly available repositories or Cloud-based networks</li> <li>Desire to share data products with researchers at other institutions</li> <li>Use of machine learning and artificial intelligence techniques</li> </ul> <p>In particular, the Data Machine is intended to be accessible to researchers (particularly students) with little background in programming or high performance computing.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#example-use-cases","title":"Example Use Cases","text":"<p>Below are some example use cases for the new Data Machine drawn from early users. We hope these examples will illuminate how the Data Machine might benefit your research.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#genomics-of-microbial-communities","title":"Genomics of Microbial Communities","text":"<p>A DNA sequencing machine can produce millions of short raw sequences that need to be combined into larger assemblies and genomes in order to be useful. This requires accessing many small files and holding lots of data in memory. These assemblies are then compared to data available through public repositories and inform ecological and evolutionary modeling.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#agent-based-modeling-of-social-systems","title":"Agent-Based Modeling of Social Systems","text":"<p>Agent-based models (ABMs) simulate thousands of individual actors to uncover large scale behavioral patterns. Each of these agents require information from a large combination of datasets from archival maps to near real-time GPS and social media data. Aggregating these datasets for use in ABMs benefits from interactive development and testing, requiring the data to be available in memory. Production runs of ABMs produce many terabytes of data that similarly benefit from interactive workflow development as well as high-end GPUs to accelerate machine learning and visualization.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#spatial-and-community-ecology","title":"Spatial and Community Ecology","text":"<p>Data from a breadth of biological, ecological, and earth science disciplines can be combined to provide insights on large-scale ecological patterns and their drivers. This requires large-scale statistical analysis of combined datasets. These datasets and analysis tools can then be made available to external collaborators. Additionally, these data can be used to parameterize mathematical models that simulate ecological interactions over long periods of time.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-driven-turbulence-modeling","title":"Data-Driven Turbulence Modeling","text":"<p>Simulations capturing the behavior of turbulent flows at both small and large scales are computationally expensive, taking months to run and producing many terabytes of data. Machine learning algorithms can be trained to emulate small scale behavior of turbulence for more efficient modeling of large scale systems. These models must be trained on large quantities of high-fidelity simulation data, requiring high-end GPUs.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#design-of-the-data-machine","title":"Design of the Data Machine","text":"<p>The characteristics outlined above can be translated into hardware constraints, particularly large amounts of memory per core, ample low latency data storage, GPUs, and networks capable of transferring large volumes of data. The Data Machine will have a total of 8 nodes, 4 focused on CPU-intensive jobs and 4 for jobs requiring GPUS.</p> <p>The Data Machine will be connected to the existing HPCC file systems and compute resources. Though the CPU and GPU hardware is similar to what is offered by the current amd21 and amd22 clusters, what sets the Data Machine apart is the amount of memory available to CPU cores, the amount of local data storage available to the node, and the way users will interface with the machine.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#cpu-nodes","title":"CPU Nodes","text":"<p>Each of the 4 CPU-focused nodes will have the following:</p> <ul> <li>128 CPU cores</li> <li>2 TB of memory</li> <li>32 TB of local high speed SSD storage</li> </ul>","tags":["explanation","data machine"]},{"location":"Data_Machine/#gpu-nodes","title":"GPU Nodes","text":"<p>Each of the 4 GPU nodes will have the following:</p> <ul> <li>128 CPU cores</li> <li>512 GB of memory</li> <li>32 TB of local high speed SSD storage</li> <li>4 NVIDIA A100 GPUs with 80 GB of memory per GPU</li> </ul> <p>The A100 GPUs have a combination of tensor processing units and mixed-precision arithmetic units, making them ideal for machine learning and artificial intelligence applications. A job may utilize multiple GPUs per node, or the GPUs may be partitioned for use in interactive data exploration or for course work.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#the-advantage-of-solid-state-drives","title":"The Advantage of Solid State Drives","text":"<p>A unique feature of the Data Machine is that each node (CPU or GPU) is directly connected to 32 TB of solid state drive (SSD) storage. This local storage allows for more efficient read and write (I/O) access than the existing disk-based file systems used by the HPCC.</p> <p>These SSDs are accessed following the NVMe specification, which allows for many possible data access options. Some of these options include direct filesystem access from the GPUs (bypassing the CPU) or using a portion of the SSD as virtual memory to allow access to datasets larger than the memory limits of the node. Users interested in these alternative configurations are encouraged to contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#running-on-the-data-machine","title":"Running on the Data Machine","text":"<p>Though the Data Machine is not yet ready for general access, users can anticipate the following workflow features. Please note that the Data Machine will be separate from ICER\u2019s buy-in program and is currently only accessible upon request.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#interactive-jobs","title":"Interactive Jobs","text":"<p>The structure of ICER\u2019s HPCC is oriented towards submitting batch jobs to a system queue, which has been the traditional access pattern for high performance computing (HPC) resources. The Data Machine will instead prioritize interactive usage through OnDemand. Via OnDemand, users will have access to tools such as RStudio, Jupyter notebooks, Matlab, and Stata. Other tools can be added to OnDemand upon request; if this is desired, please fill out a ticket.  When the Data Machine is experiencing high demand, interactive jobs will be able to preempt lower-priority workloads.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#containerization","title":"Containerization","text":"<p>Research groups who can make efficient use of the Data Machine for their research may have complex and specialized software needs. This includes software with large sets of dependencies or which expect a particular runtime environment. The difficulties associated with building and deploying such software can be greatly alleviated by containers. Containers bundle together the user\u2019s software and its minimum set of dependencies into a single executable that will behave consistently on any system.</p> <p>ICER will develop containers for the most common anticipated use cases (e.g. ML/AI with Python, bioinformatics) and make these container images available to users. ICER will also work with users looking to design images for their own research groups, courses, etc. Users interested in developing containers to match their needs should contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-sharing-and-cloud-integration","title":"Data Sharing and Cloud Integration","text":"<p>The Data Machine will be connected to ICER\u2019s existing infrastructure, including the HPCC file systems and MSU's High-Speed Research Network. The latter will allow researchers to share their datasets and other products with collaborators outside of MSU via Globus.</p> <p>Researchers will also be able to move data into and out of the Data Machine via cloud systems. High speed access to the main cloud providers (AWS, Google Cloud, and Microsoft Azure) is a priority for the Data Machine so that researchers can leverage datasets regardless of where they are stored.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#educational-support","title":"Educational Support","text":"<p>ICER will be offering dedicated training for the Data Machine in addition to its traditional offerings. These trainings will be available both synchronously and asynchronously via workshops, web-based tutorials, and self-paced training modules. All new users will be required to take an orientation workshop prior to being granted access to the Data Machine. This will ensure users have the knowledge and skills to make efficient use of the machine.</p> <p>Users of the Data Machine will be able to access ICER support staff through the existing ticket system and weekly office hours on Microsoft Teams. Researchers requiring additional assistance with, for example, particularly complex research needs or workflow development are encouraged to leverage the ARCS program.</p> <p>Support will also be offered to instructors looking to integrate computational exercises into their courses. Instructors interested in initiating such projects should submit an abstract to this form.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#data-machine-user-advisory-board","title":"Data Machine User Advisory Board","text":"<p>Once the Data Machine becomes widely available, ICER will be looking for graduate students, postdocs, and faculty who use the Data Machine to join the User Advisory Board (UAB). This UAB will meet regularly to discuss the current state of the machine and user experiences, and to make recommendations about software and policy changes. The UAB will also make recommendations as to user training and support for the Data Machine.</p> <p>A typical UAB term is two years, and researchers may serve consecutive terms. Researchers interested in serving on the UAB should contact ICER.</p>","tags":["explanation","data machine"]},{"location":"Data_Machine/#request-access-to-the-data-machine","title":"Request Access to the Data Machine","text":"<p>Currently, the Data Machine is only available upon request. Users interested in using the Data Machine will need to submit short proposals for research and instructional support using this form with the subject line \"Data Machine\". Note that usage of the Data Machine is subject to providing a short annual report detailing accomplishments using the machine.</p> <p>Instructional use of the Data Machine should be requested by the course's lead instructor. Research use should be requested by the Principal Investigator (PI).</p> <p>For both research and instructional use, the requestor must supply the following information:</p> <ul> <li>The NetID of the user(s) they are making the request for</li> <li>A short abstract describing the research or course work that will be done on the data machine</li> <li>A brief justification for why the resources of the Data Machine will benefit this work</li> <li>Acknowledgement that the requestor agrees to provide a short annual report detailing accomplishments using the machine (if requested by ICER)</li> </ul>","tags":["explanation","data machine"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/","title":"Display Compute Nodes and Job Partitions by sinfo command","text":"","tags":["reference"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#information-of-compute-nodes","title":"Information of Compute Nodes","text":"<p>If you would like to run a job with a lot of resources, it is a good idea to check available resources, such as which nodes are available as well as how many cores and how much memory is available on those nodes,  so the job will not wait for too much time. Users can use SLURM command sinfo to get a list of nodes controlled by the job scheduler. Such as, running the command <code>sinfo -N -r -l</code>, where the specifications <code>-N</code> for showing nodes, <code>-r</code> for showing nodes only responsive to SLURM and <code>-l</code> for long description are used.</p> <p>However, for each node, <code>sinfo</code> displays all possible partitions and causes repetitive information. Here, the powertools command <code>node_status</code> can be used to display much better results:</p> <pre><code>$ node_status                       # powertools command\n\nWed Apr 22 11:14:40 EDT 2020\n\nNodeName       Account         State     CPU(Load:Aloc Idl:Tot)    Mem(Aval:Tot)Mb   GPU(I:T)   Reason\n----------------------------------------------------------------------------------------------------------\ncsm-001        general       ALLOCATED      13.61: 20    0: 20       45186: 246640      N/A\ncsm-002       albrecht         MIXED        10.14: 15    5: 20        1072: 246640      N/A\ncsm-003         colej        ALLOCATED       7.45: 20    0: 20       50032: 246640      N/A\n......\ncsn-005        general         MIXED         9.92: 12    8: 20       16160: 118012    k20(0:2)\n......\ncs*      =&gt;   33.3%(buyin)   91.4%(162)     43.6%: 59.5%( 3240)      69.9%(17.0Tb)    97%( 78)   Usage%(Total)\n......\n......\nlac-078        general         MIXED        11.38:  8   20: 28       69884: 118012      N/A\nlac-079          ptg         ALLOCATED      22.37: 28    0: 28       15612: 118012      N/A\nlac-080       merzjrke         MIXED         2.48: 16   12: 28       50032: 246640    k80(0:8)\n......\n......\nvim-002          ccg           MIXED        66.14: 63   81:144     5427008:6145856      N/A\n\nintel14  =&gt;   34.5%(buyin)   91.7%(168)     47.8%: 62.7%( 3576)      60.1%(31.1Tb)    97%( 78)   Usage%(Total)\nintel16  =&gt;   69.0%(buyin)   98.8%(429)     55.2%: 65.1%(12200)      76.6%(79.9Tb)    70%(384)   Usage%(Total)\nintel18  =&gt;   63.6%(buyin)   99.4%(176)     45.8%: 55.8%( 7040)      77.1%(31.3Tb)    55%( 64)   Usage%(Total)\n\nSummary  =&gt;   60.3%(buyin)   97.4%(773)     51.2%: 61.9%(22816)      73.1%( 142Tb)    72%(526)   Usage%(Total\n</code></pre> <p>The result of <code>node_status</code> is a good reference to find out how many nodes available for your  jobs as it displays important information including node names, buyin accounts, node states,  CPU cores, memory, GPU, and the reason the node is unavailable.</p> <p>If you need more complete details of a particular node, you can use <code>scontrol show node -a &lt;node_name&gt;</code> command:</p> <pre><code>$ scontrol show node -a skl-166\nNodeName=skl-166 Arch=x86_64 CoresPerSocket=20\n   CPUAlloc=0 CPUTot=40 CPULoad=0.01\n   AvailableFeatures=skl,gbe,intel18,ib,edr18\n   ActiveFeatures=skl,gbe,intel18,ib,edr18\n   Gres=(null)\n   NodeAddr=skl-166 NodeHostName=skl-166 Version=18.08\n   OS=Linux 3.10.0-693.21.1.el7.x86_64 #1 SMP Wed Mar 7 19:03:37 UTC 2018\n   RealMemory=376162 AllocMem=0 FreeMem=382562 Sockets=2 Boards=1\n   State=DOWN ThreadsPerCore=1 TmpDisk=174080 Weight=103 Owner=N/A MCS_label=N/A\n   Partitions=general-short,general-short-18,general-long,general-long-18,qian-18,nvl-benchmark-18,piermaro-18,vmante-18,liulab-18,devolab-18,tsangm-18,plzbuyin-18,chenlab-18,shadeash-colej-18,allenmc-18,cmse-18,seiswei-18,niederhu-18,daylab-18,junlin-18,mitchmcg-18,pollyhsu-18,davidroy-18,yueqibuyin-18,eisenlohr-18\n   BootTime=2019-02-11T15:07:38 SlurmdStartTime=2019-02-11T15:08:44\n   CfgTRES=cpu=40,mem=376162M,billing=57176\n   AllocTRES=\n   CapWatts=n/a\n   CurrentWatts=0 LowestJoules=0 ConsumedJoules=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n   Reason=Currently being imaged [fordste5@2019-02-11T09:49:30]\n</code></pre>","tags":["reference"]},{"location":"Display_Compute_Nodes_and_Job_Partitions_by_sinfo_command/#slurm-partitions-for-jobs","title":"SLURM Partitions for Jobs","text":"<p>One of the important details about a node is what kind of jobs can run on it. For example, if a node is a buy-in node, only jobs with walltime equal to or less than 4 hours can run for a non-buyin users. We can check the summary of all partitions using <code>sinfo</code> with  the <code>-s</code> specification:</p> <pre><code>$ sinfo -s\nPARTITION           AVAIL  TIMELIMIT   NODES(A/I/O/T)  NODELIST\ngeneral-short          up    4:00:00    729/26/16/771  csm-[001-005,007-010,017-022],csn-[001-039],csp-[006-007,016-020,025-026],css-[001-003,007-012,014,016-020,023,032-036,038-045,047-050,052-067,071-072,074-076,079-085,087-095,097-103,106-109,111-127],lac-[000-225,228-247,250-261,276-369,372,374-445],nvl-[000-007],qml-[000-005],skl-[000-167],vim-[000-002]\ngeneral-long           up 7-00:00:00      269/0/8/277  csm-001,csn-020,csp-[006-007,016-018,020,025],css-[008-012,014,016-019,023,032,034-036,038-045,047-050,052-066,071,075-076,079-080,083,087-089,092-095,097-099,107,118,121,124,126],lac-[038-044,078,123,209,217,225,228,230-235,246-247,276-284,300-301,336-339,353-360,363-364,372,374-399,401-420,422-445],skl-[023,026-112]\ngeneral-long-bigmem    up 7-00:00:00        17/0/0/17  lac-[252-253,306],qml-[000,005],skl-[143-147,162-167],vim-001\ngeneral-long-gpu       up 7-00:00:00       46/12/0/58  csn-[001-019,021-036],lac-[030,087,137,143,192-199,287-290,292-293,342,348],nvl-[005-007]\n</code></pre> <p>where the list of job partitions and their setup for walltime limit and nodes are shown. More detailed information for each job partition can also be found by <code>-p</code> specification:</p> <pre><code>$ sinfo -p general-long -r -l\nMon Jul 13 12:22:16 2020\nPARTITION    AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE NODELIST\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      2    draining lac-[231,247]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all      1     drained css-053\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all    217       mixed csm-001,csp-[006,017-018,020,025],css-[010,018-019,023,032,034-035,038,044,047-049,052,055-056,061-066,075,088-089,098-099,107,118,126],lac-[038-044,078,123,209,217,225,228,230,232,234-235,276-280,282-284,300-301,336-337,339,353-360,363,372,374-382,384-399,401-420,423,427-445],skl-[023,026,028-029,031,033-034,036-042,044-046,048,050-067,069-079,081-094,096-106,108-112]\ngeneral-long    up 7-00:00:00 1-infinite   no       NO        all     50   allocated csn-020,csp-016,css-[008-009,011,016-017,036,039-043,045,050,054,057-060,083,087,092-095,097,121,124],lac-[233,246,281,338,364,383,422,424-426],skl-[027,030,032,035,043,047,049,068,080,095,107]\n</code></pre> <p>Users can also show nodes only allowed for specific job partitions by using <code>-N</code> and <code>-p</code>:</p> <pre><code>$ sinfo -N -l -r -p general-short,general-long\nMon Jul 13 12:25:58 2020\nNODELIST   NODES     PARTITION       STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON\ncsm-001        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-001        1  general-long       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-002        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-003        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-004        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\ncsm-005        1 general-short       mixed   20   2:10:1 246640   174080    101 gbe,ib,i none\n...\n...\nskl-166        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nskl-167        1 general-short       mixed   40   2:20:1 376162   174080    103 skl,gbe, none\nvim-000        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-001        1 general-short       mixed   64   4:16:1 306780   174080    102 gbe,inte none\nvim-002        1 general-short   allocated  144   8:18:1 614585   174080    102 gbe,inte none\n</code></pre> <p>For a complete instruction of <code>sinfo</code>, please refer to the SLURM web page.</p>","tags":["reference"]},{"location":"Docker/","title":"Docker","text":"<p>What is Docker? Docker is a tool to make it easier to create, deploy and run applications by using containers. Containers allow developers to package up an application with all of the dependencies such as libraries and tools, and deploy it as one package. The application will run on most operating systems (Mac/Windows/Linux) regardless of any customized settings. This page covers how you can run a development environments using Docker containers and package up your own code into a portable container.</p> <p>Warning</p> <p>This tutorial is meant to be run on your personal computer, not the HPCC. Docker does not work on the HPCC since it requires super user (<code>sudo</code>) permissions that users do not have access to. To run containers on the HPCC, you will need to use Singularity.</p> <p>Nevertheless, many of the skills taught in this tutorial transfer over to using Singularity, and most Docker containers can be used without modification on the HPCC through Singularity. However, if you just want to get started running containers on the HPCC, start with the Singularity Introduction.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#docker-installation","title":"Docker installation","text":"<p>Docker can be installed on all major operating systems. However, note that installation on Windows requires the Windows Subsystem For Linux (WSL).</p> <p>For detailed installation instructions depending on operating system, click here:\u00a0Mac/Windows/Linux.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#testing-docker-installation","title":"Testing Docker installation","text":"<p>When you have installed Docker, test your Docker installation by opening a terminal (if you are running Windows, this should be a WSL terminal) and running the following command:</p> <pre><code>$ docker --version\nDocker version 19.03.8, build afacb8b\n</code></pre> <p>When you run the <code>docker</code> command without <code>--version</code>, you will see the options available with docker. Alternatively, you can test your installation by running the following (you have to log into Docker to use this test):</p> <pre><code>$ docker run hello-world\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n0e03bdcc26d7: Pull complete\nDigest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n(amd64)\n3. The Docker daemon created a new container from that image which runs the\nexecutable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\nto your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n$ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\nhttps://hub.docker.com/\n\nFor more examples and ideas, visit:\nhttps://docs.docker.com/get-started/\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#running-docker-containers-from-prebuilt-images","title":"Running Docker containers from prebuilt images","text":"<p>Now, you have setup everything, and it is time to use Docker seriously. You will run a container from the Alpine Linux image on your system and will learn the\u00a0<code>docker run</code>\u00a0command. However, you should first know what containers and images are, and the difference between containers and images.</p> <p>Images: The file system and configuration of applications which are created and distributed by developers. Of course, you can create and distribute images.</p> <p>Containers: Running instances of Docker images. You can have many containers for the same image.</p> <p>Now that you know what containers and images are, let's get some practice, by running the <code>docker run alpine ls -l</code> command in your terminal.</p> <pre><code>$ docker run alpine ls -l\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\ncbdbe7a5bc2a: Pull complete\nDigest: sha256:9a839e63dad54c3a6d1834e29692c8492d93f90c59c978c1ed79109ea4fb9a54\nStatus: Downloaded newer image for alpine:latest\ntotal 56\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 bin\ndrwxr-xr-x    5 root     root           340 May 26 17:11 dev\ndrwxr-xr-x    1 root     root          4096 May 26 17:11 etc\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 home\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 lib\ndrwxr-xr-x    5 root     root          4096 Apr 23 06:25 media\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 mnt\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 opt\ndr-xr-xr-x  187 root     root             0 May 26 17:11 proc\ndrwx------    2 root     root          4096 Apr 23 06:25 root\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 run\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 sbin\ndrwxr-xr-x    2 root     root          4096 Apr 23 06:25 srv\ndr-xr-xr-x   12 root     root             0 May 26 17:11 sys\ndrwxrwxrwt    2 root     root          4096 Apr 23 06:25 tmp\ndrwxr-xr-x    7 root     root          4096 Apr 23 06:25 usr\ndrwxr-xr-x   12 root     root          4096 Apr 23 06:25 var\n</code></pre> <p>When you run the <code>docker run alpine ls -l</code> command, it searches for the <code>alpine:latest</code> image from your system first. If your system has it (i.e. if you downloaded it previously), Docker uses that image.</p> <p>If your system does not have that image, then Docker\u00a0fetches the <code>alpine:latest</code> image from Docker Hub first, saves it in\u00a0your system, then runs a container from the saved image.\u00a0Docker Hub is a huge repository of images people have uploaded so that others can download and run their code in containers. Though Docker Hub is the most popular place to find Docker images, there are other sources that work just as well (for example, Quay.io).</p> <p><code>docker run alpine</code> starts a container, and then <code>ls -l</code> will be a command which is fed to the container, so Docker starts the given command and results show up.</p> <p>To see a list of all images on your system, you can use the <code>docker images</code> command.</p> <pre><code>$ docker images\nalpine                     latest              f70734b6a266        4 weeks ago         5.61MB\nhello-world                latest              bf756fb1ae65        4 months ago        13.3kB\n</code></pre> <p>Next, let's try another command.</p> <pre><code>$ docker run alpine echo \"Hello world\"\nHello world\n</code></pre> <p>In this case, Docker ran the <code>echo</code> command in your <code>alpine</code> container, and then exited it. Exit means the container is terminated after running the command.</p> <p>Let's try another command.</p> <pre><code>docker run alpine sh\n</code></pre> <p>It seems nothing happened. In fact, docker ran the <code>sh</code> command in your alpine container, and exited it. If you want to be inside the container shell, you need to use <code>docker run -it alpine sh</code>. The <code>-i</code> flag tells Docker you want to run the container interactively and the <code>-t</code> flag tells it you want to start a terminal in that image to run your command. You can find more help on the <code>run</code> command\u00a0with <code>docker run --help</code>.</p> <p>Let's run a few commands inside the <code>docker run -it alpine sh</code> container.</p> <pre><code>$ docker run -it alpine sh\n/ # ls\nbin    etc    lib    mnt    proc   run    srv    tmp    var\ndev    home   media  opt    root   sbin   sys    usr\n/ # uname -a\nLinux c1552c9b6cf0 4.19.76-linuxkit #1 SMP Fri Apr 3 15:53:26 UTC 2020 x86_64 Linux\n/ # exit\n</code></pre> <p>You are inside of the container shell and you can try out a few commands like <code>ls</code> and <code>uname -a</code> and others. To quit the container, type <code>exit</code> on the terminal. If you use the <code>exit</code> command, the container is terminated. If you want to keep the container active, then you can use keys <code>Ctrl-p</code> followed by <code>Ctrl-q</code> (you don't have to press these key combinations simultaneously). If you want to go back into the container, you can type <code>docker attach &lt;container_id&gt;</code>, such as <code>docker attach\u00a0c1552c9b6cf0</code>. You can find container id with <code>docker ps -all</code>. This command will be explained next.</p> <p>Now, let's learn about the <code>docker ps</code> command which shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS\n</code></pre> <p>In this case, you don't see any container because no containers are running. To see a list of all containers that you ran, use <code>docker ps --all</code>. You can see that STATUS says that all containers exited. \u00a0</p> <pre><code>$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     6 minutes ago       Exited (0) 2 minutes ago                        wonderful_cori\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     18 minutes ago      Exited (0) 18 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  31 minutes ago      Exited (0) 31 minutes ago                       fervent_gould\n6dbe999044b4        hello-world         \"/hello\"                 3 hours ago         Exited (0) 3 hours ago\n</code></pre> <p>When Docker containers are created, the Docker system automatically assign a universally unique identifier (UUID) number to each container to avoid any naming conflicts. CONTAINER ID is a shortform of the UUID. Each container also has a randomly generated name. You can usually use this name in place of the CONTAINER ID to make typing a bit easier.</p> <p>You can also assign names to your Docker containers when you run them, using the <code>--name</code> flags. In addition, you can rename your Docker container's name with <code>rename</code> command. For example, let's rename \"wonderful_cori\" to \"my_container\" with\u00a0<code>docker rename</code> command.</p> <pre><code>$ docker rename wonderful_cori my_container\n$ docker ps --all\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\nc1552c9b6cf0        alpine              \"sh\"                     10 minutes ago      Exited (0) 6 minutes ago                        my_container\n5de22ab86f2a        alpine              \"echo 'Hello world'\"     22 minutes ago      Exited (0) 22 minutes ago                       goofy_visvesvaraya\ndf35ee7df7e3        alpine              \"ls -l\"                  35 minutes ago      Exited (0) 35 minutes ago                       fervent_gould\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#build-docker-images-which-contain-your-own-code","title":"Build Docker images which contain your own code","text":"<p>Now you are ready to use Docker to create your own applications! First, you will learn more about Docker images. Then you will build your own image and use that image to run an application on your local machine.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#docker-images","title":"Docker images","text":"<p>Docker images are basis of containers. In the above example, you pulled the alpine image from Docker Hub and ran a container based on that image. To see the list of images that are available on your local machine, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nalpine        latest    9ed4aefc74f6   4 weeks ago     7.05MB\nhello-world   latest    feb5d9fea6a5   19 months ago   13.3kB\n</code></pre> <p>The TAG refers to a particular snapshot of the image and the ID is the corresponding UUID of the image. Images can have multiple versions. When you do not assign a specific version number, the client defaults to latest. If you want a specific version of the image, you can use docker pull command as follows:</p> <pre><code>$ docker pull ubuntu:22.04\n7: Pulling from library/ubuntu\n2d473b07cdd5: Pull complete \nDigest: sha256:0eb0f877e1c869a300c442c41120e778db7161419244ee5cbc6fa5f134e74736\nStatus: Downloaded newer image for ubuntu:22.04\ndocker.io/library/ubuntu:22.04\n</code></pre> <p>Notice that here we pulled the image without running it. When we run a container with the <code>ubuntu:22.04</code> image in the future, it will use this downloaded copy.</p> <p>You can search for images from a repository's website (for example, searching Docker hub for CentOS) or directly from the command line using <code>docker search</code>.</p> <pre><code>$ docker search centos\nNAME                               DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\ncentos                             The official build of CentOS.                   6014                [OK]\nansible/centos7-ansible            Ansible on Centos7                              129                                     [OK]\nconsol/centos-xfce-vnc             Centos container with \"headless\" VNC session\u2026   115                                     [OK]\njdeathe/centos-ssh                 OpenSSH / Supervisor / EPEL/IUS/SCL Repos - \u2026   114                                     [OK]\ncentos/mysql-57-centos7            MySQL 5.7 SQL database server                   76\nimagine10255/centos6-lnmp-php56    centos6-lnmp-php56                              58                                      [OK]\ntutum/centos                       Simple CentOS docker image with SSH access      46\ncentos/postgresql-96-centos7       PostgreSQL is an advanced Object-Relational \u2026   44\nkinogmt/centos-ssh                 CentOS with SSH                                 29                                      [OK]\npivotaldata/centos-gpdb-dev        CentOS image for GPDB development. Tag names\u2026   12\nguyton/centos6                     From official centos6 container with full up\u2026   10                                      [OK]\ncentos/tools                       Docker image that has systems administration\u2026   6                                       [OK]\ndrecom/centos-ruby                 centos ruby                                     6                                       [OK]\npivotaldata/centos                 Base centos, freshened up a little with a Do\u2026   4\npivotaldata/centos-mingw           Using the mingw toolchain to cross-compile t\u2026   3\ndarksheer/centos                   Base Centos Image -- Updated hourly             3                                       [OK]\nmamohr/centos-java                 Oracle Java 8 Docker image based on Centos 7    3                                       [OK]\npivotaldata/centos-gcc-toolchain   CentOS with a toolchain, but unaffiliated wi\u2026   3\nmiko2u/centos6                     CentOS6 \u65e5\u672c\u8a9e\u74b0\u5883                                   2                                       [OK]\nblacklabelops/centos               CentOS Base Image! Built and Updates Daily!     1                                       [OK]\nindigo/centos-maven                Vanilla CentOS 7 with Oracle Java Developmen\u2026   1                                       [OK]\nmcnaughton/centos-base             centos base image                               1                                       [OK]\npivotaldata/centos7-dev            CentosOS 7 image for GPDB development           0\nsmartentry/centos                  centos with smartentry                          0                                       [OK]\npivotaldata/centos6.8-dev          CentosOS 6.8 image for GPDB development         0\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#building-your-first-docker-image","title":"Building your first Docker image","text":"<p>In this section, you will build a simple Docker image with writing a Dockerfile, and run it. For this purpose, we will create a Python script and a Dockerfile.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#creating-working-directory","title":"Creating working directory","text":"<p>Let's create a working directory where you will make the following files: <code>hello.py</code>, <code>Dockerfile</code>.</p> <pre><code>cd ~\nmkdir my_first_Docker_image\ncd my_first_Docker_image\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#python-script","title":"Python script","text":"<p>Create the <code>hello.py</code> file with the following content.</p> hello.py<pre><code>print(\"Hello world!\")\nprint(\"This is my 1st Docker image!\")\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a text file which has a list of commands that Docker calls while creating an image. The Dockerfile is similar to a job batch file, and contains all information that Docker needs to know to to run the application package.</p> <p>In the <code>my_first_Docker_image</code> directory, create a file, called Dockerfile, which has the content below.</p> Dockerfile<pre><code># our base image. The latest version will be pulled.\nFROM alpine\n\n# install python and pip\nRUN apk add --update py3-pip\n\n# copy files required to run\nCOPY hello.py /usr/src/my_app/\n\n# run the application\nCMD python3 /usr/src/my_app/hello.py\n</code></pre> <p>Now, let's learn the meaning of each line.</p> <p>The first line means that we will use Alpine Linux as a base image. No version is specified, so the latest version will be pulled. Use the <code>FROM</code> keyword.</p> <pre><code>FROM alpine\n</code></pre> <p>Next, the Python <code>pip</code> package is installed using the Alpine Package Keeper (<code>apk</code>). Use the <code>RUN</code> keyword.</p> <pre><code>RUN apk add --update py3-pip\n</code></pre> <p>Next, copy the file to the image. <code>/usr/src/my_app</code> will be created while the file is copied. Use the <code>COPY</code> keyword.</p> <pre><code>COPY hello.py /usr/src/my_app/\n</code></pre> <p>The last step is run the application with the <code>CMD</code> keyword.\u00a0<code>CMD</code>\u00a0tells the container what the container should do by default when it is started.</p> <pre><code>CMD python3 /usr/src/my_app/hello.py\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#build-the-image","title":"Build the image","text":"<p>Now you are ready to build your first Docker image. The <code>docker build</code> command will do most of the work.</p> <p>To build the image, use the following command.</p> <pre><code>docker build -t my_first_image .\n</code></pre> <p>The client will pull all necessary images and create your image. If everything goes well, your image is ready to be used! Run the <code>docker images</code> command to see if your image <code>my_first_image</code> is shown.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#run-your-image","title":"Run your image","text":"<p>When you successfully create your Docker image, test it by starting a new container from the image.</p> <pre><code>docker run my_first_image\n</code></pre> <p>If everything went well, you will see this message.</p> <pre><code>Hello world!\nThis is my 1st Docker image!\n</code></pre>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#connecting-docker-and-your-computer","title":"Connecting Docker and your computer","text":"<p>Containers are great for keeping all the parts of a piece of software isolated together. But this means that there are a few extra steps necessary to share information from that container with the computer you're running it on.</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#sharing-data","title":"Sharing data","text":"<p>Let's pretend that you have a container that runs a long analysis and outputs the results in some data file. We'll mimic this by just creating an empty file with the <code>touch</code> command. Let's make a new directory for our output and create our \"important data\" in our alpine image:</p> <pre><code>mkdir results\nls results\n\ndocker run alpine touch data.dat \nls results\n</code></pre> <p>Nothing happened! You can look in your current directory with <code>ls</code>, and you won't see anything either. We can even run an interactive alpine container and look around:</p> <pre><code>docker run -it alpine sh\n/ # ls\nbin    dev    etc    home   lib    media  mnt    opt    proc   root   run    sbin   srv    sys    tmp    usr    var\n/ # exit\n</code></pre> <p>No results directory and no <code>data.dat</code> file...</p> <p>What happened is that the file was created and locked away in the previous container. When we start a new container, we start fresh from whatever the image specified. Nothing sticks around! So we need a way to get that data out of the container we're working in.</p> <p>The way Docker does this is through \"bind mounts\". It's like we are \"binding\" a directory on our computer to a directory that's \"mounted\" in the container. Let's try it interactively first:</p> <pre><code>$ docker run -it -v ./results:outside_world alpine sh\n/ # ls\nbin            etc            lib            mnt            outside_world  root\nsbin           sys            usr            dev            home           media\nopt            proc           run            srv            tmp            var\n/ # exit\n</code></pre> <p>The <code>-v</code> command tells Docker that I want to connect the <code>./results</code> directory on my computer to a directory called <code>/outside_world</code> inside the container.</p> <p>Now we can put it all together:</p> <pre><code>$ docker run -v ./results:/outside_world alpine touch /outside_world/data.dat\n$ ls results\ndata.dat\n</code></pre> <p>Notice that we had to write to the container version of our directory with the <code>touch</code> command, but it's now visible on the computer in the <code>results</code> directory.</p> <p>This is a contrived example, but in real life, you could replace <code>touch ...</code> with any command that can run in your container, including heavy duty data analysis. You just need to make sure that there is a bind mount between wherever that data is being written inside the container and wherever you want it outside the container (and/or vice versa if you want to input data into your container).</p>","tags":["tutorial","containers","Docker"]},{"location":"Docker/#exposing-ports-and-jupyter-example","title":"Exposing ports (and Jupyter example)","text":"<p>Software that connects to your web browser uses network ports to share information. These ports are just a number that tell your web browser where to access the content shared by the software, and are usually setup by the software (though there are often options a user can set to change the port number).</p> <p>In the context of research computing, one of the most popular examples of this setup is Jupyter Notebook. When a Jupyter Notebook is running, it usually is available on port 8888, meaning you can access it from your web browser with the URL http://127.0.0.1:8888. Here, 127.0.0.1 will always be the IP address of your own computer which in this case is running the Jupyter Notebook on port 8888.</p> <p>Since containers are meant to be an isolated computing environment, network ports are not accessible by default from outside the container. We will now go through an example of exposing ports from a Docker container using Jupyter Hub (a more full-featured version of Jupyter Notebooks). This example will also show an alternate way to include files in a Docker container, albeit, in more of a read-only way.</p> <p>First, let's check the Jupyter images available on Docker Hub. We will use minimal-notebook.</p> <pre><code>$ docker search jupyter\nNAME                                    DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\njupyter/datascience-notebook            Jupyter Notebook Data Science Stack from htt\u2026   666\njupyter/all-spark-notebook              Jupyter Notebook Python, Scala, R, Spark, Me\u2026   301\njupyterhub/jupyterhub                   JupyterHub: multi-user Jupyter notebook serv\u2026   248                                     [OK]\njupyter/scipy-notebook                  Jupyter Notebook Scientific Python Stack fro\u2026   241\njupyter/tensorflow-notebook             Jupyter Notebook Scientific Python Stack w/ \u2026   218\njupyter/pyspark-notebook                Jupyter Notebook Python, Spark, Mesos Stack \u2026   157\njupyter/base-notebook                   Small base image for Jupyter Notebook stacks\u2026   106\njupyter/minimal-notebook                Minimal Jupyter Notebook Stack from https://\u2026   105\n...\n</code></pre> <p>Let's start by creating a directory <code>my_notebook</code>. Copy <code>hello.py</code>, which we used for the Python image to the <code>my_notebook</code> directory. Then\u00a0create a Dockerfile in the <code>my_notebook</code> directory with the following content:</p> Dockerfile<pre><code># base image\nFROM jupyter/base-notebook\n\n# copy files\nCOPY hello.py /home/jovyan/work\n\n# the port number the container should expose\nEXPOSE 8888\n</code></pre> <p>The <code>COPY</code> line moves our <code>hello.py</code> script into the container directory <code>/home/jovyan/work</code> which is where the Jupyer instance inside the container can access files.</p> <p>The <code>EXPOSE</code> line specifies the port number which needs to be exposed. The default port for Jupyter is 8888, and therefore, we will expose that port.</p> <p>Now, build the image using the following command:</p> <pre><code>$ docker build -t mynotebook .\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM jupyter/base-notebook\n ---&gt; 6494235c84ec\nStep 2/3 : COPY hello.py /home/jovyan/work\n ---&gt; 6e22bc10eee0\nStep 3/3 : EXPOSE 8888\n ---&gt; Running in 2d754a40aa2b\nRemoving intermediate container 2d754a40aa2b\n ---&gt; 464731f2e3a7\nSuccessfully built 464731f2e3a7\nSuccessfully tagged mynotebook:latest\n</code></pre> <p>Now, everything is ready. You can run the image using the <code>docker run</code> command. We use the <code>-p 8888:8888</code> option to tell Docker that we'd like to bind the 8888 port in the container to the 8888 port on your host computer.</p> <pre><code>$ docker run -p 8888:8888 mynotebook\nEntered start.sh with args: jupyter lab\nExecuting the command: jupyter lab\n[I 2023-05-02 17:45:26.694 ServerApp] Package jupyterlab took 0.0000s to import\n...\n[I 2023-05-02 17:45:26.941 ServerApp] Serving notebooks from local directory: /home/jovyan\n[I 2023-05-02 17:45:26.941 ServerApp] Jupyter Server 2.5.0 is running at:\n[I 2023-05-02 17:45:26.941 ServerApp] http://ad7f42d45370:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n[I 2023-05-02 17:45:26.941 ServerApp]     http://127.0.0.1:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n[I 2023-05-02 17:45:26.941 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 2023-05-02 17:45:26.943 ServerApp] \n\n    To access the server, open this file in a browser:\n        file:///home/jovyan/.local/share/jupyter/runtime/jpserver-7-open.html\n    Or copy and paste one of these URLs:\n        http://ad7f42d45370:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n        http://127.0.0.1:8888/lab?token=b76dcd0514e9fe7f60b145e936852ab7836df45e6e4b0879\n</code></pre> <p>If you navigate to one of the URLs that Jupyter outputs, you will see your containerized Jupyter Hub ready to go. If you look inside the <code>work</code> directory, you'll even see our <code>hello.py</code> script!</p>","tags":["tutorial","containers","Docker"]},{"location":"EasyBuild_Reference/","title":"EasyBuild Reference","text":"<p>EasyBuild can install software on the HPCC relying on a huge library of on user-contributed recipes called EasyConfigs. This page collects some of the most useful commands and topics for using EasyBuild at ICER. See our tutorial to learn how to use EasyBuild.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#loading-easybuild","title":"Loading EasyBuild","text":"<p>EasyBuild is a module available on the HPCC. Make sure to remove all other modules before using it.</p> <pre><code>module purge\nmodule load EasyBuild\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#configuration-and-options","title":"Configuration and options","text":"<p>Use the <code>--show-config</code> option to see all of the configuration options and their values.</p> <pre><code>eb --show-config\n</code></pre> <p>Any option can be changed by passing it as a command line argument when using <code>ebS</code>. Key examples are given below.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#software-installation-directory-installpath-software","title":"Software installation directory: <code>--installpath-software</code>","text":"<p>Set to the directory where the final software is installed (default: <code>$HOME/.local/easybuild/software</code>).</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#module-directory-installpath-module","title":"Module directory: <code>--installpath-module</code>","text":"<p>Set to the directory where the software's module files are stored (default: <code>$HOME/.local/easybuild/modules</code>).</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#temporary-build-directory-buildpath","title":"Temporary build directory: <code>--buildpath</code>","text":"<p>Set to the directory the software is downloaded and compiled (default: <code>$HOME/.local/easybuild/build</code>).</p> <p>Warning</p> <p>We highly recommend setting <code>--buildpath=/tmp/$USER/EASYBUILD</code> so that you don't download the temporary files to your home directory!</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#number-of-cores-allowed-in-parallel-installation-parallel","title":"Number of cores allowed in parallel installation: <code>--parallel</code>","text":"<p>Set to the number of cores you wish to use (default: all cores on node)</p> <p>Warning</p> <p>When installing software on a development node, please use the <code>--parallel</code> option to restrict your usage and ensure that the node stays usable for other users. When in doubt, set <code>--parallel=8</code>.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#easyconfigs","title":"EasyConfigs","text":"<p>EasyConfigs are the recipes used to install software. There is a large collection (focusing on research software) already available on the HPCC contributed by many users and software developers across the world.</p>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#searching-with-search-or-m","title":"Searching with <code>--search</code> or <code>-M</code>","text":"<p>Use the <code>--search</code> or <code>-S</code> option to <code>eb</code> to find an EasyConfig available on the HPCC.</p> <p>Example:</p> <pre><code>eb -S zfp\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#viewing-with-show-ec","title":"Viewing with <code>--show-ec</code>","text":"<p>Use the <code>--show-ec</code> option to view an EasyConfig based on its name (not its full path).</p> <p>Example:</p> <pre><code>eb --show-ec zfp-1.0.1-GCCcore-12.3.0.eb\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#installing-software","title":"Installing software","text":"<p>Run <code>eb</code> with any options you wish to set followed by the name of the EasyConfig you wish to install.</p> <p>Example:</p> <pre><code>eb --parallel=8 --buildpath=/tmp/$USER/EASYBUILD --installpath-software=$SCRATCH/software zfp-1.0.1-GCCcore-12.3.0.eb\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Reference/#accessing-installed-software-with-the-module-system","title":"Accessing installed software with the <code>module</code> system","text":"<p>Add the location where your module files are installed (i.e., the value of <code>--installpath-modules</code> in your EasyBuild configuration to your <code>$MODULEPATH</code> with <code>module use</code> so modules can be found by further <code>module</code> commands.</p> <p>Note that EasyBuild adds an extra <code>all</code> directory inside your <code>installpath-modules</code> that you should use with <code>module use</code>:</p> <p>Example:</p> input<pre><code>module use $HOME/.local/easybuild/modules/all\nmodule spider zfp\n</code></pre> output<pre><code>----------------------------------------------------------------------------\n  zfp: zfp/1.0.1-GCCcore-12.3.0\n----------------------------------------------------------------------------\n...\n    This module can be loaded directly: module load zfp/1.0.1-GCCcore-12.3.0\n...\n</code></pre> input<pre><code>module load zfp/1.0.1-GCCcore-12.3.0\n</code></pre>","tags":["reference","EasyBuild"]},{"location":"EasyBuild_Tutorial/","title":"EasyBuild Tutorial","text":"<p>One of the most complex parts of using an HPCC can often be installing the software you want to use. EasyBuild is a piece of software that helps simplify the process. It compiles well-tested recipes contributed by people installing this software (often on HPCCs just like MSU's) all over the world. It's also what ICER uses to install all of the modules you use on the HPCC!</p> <p>A warning: while \"easy\" is in the name, don't ever expect installing software on the HPCC to be easy... But EasyBuild is probably the closest you'll get.</p> <p>What kind of software do you need?</p> <p>EasyBuild can help you install all kinds of software, but there are other options that we recommend for things like Python and R. For Python, we recommend Conda, and for R we recommend using the built-in <code>install.packages</code> command. EasyBuild will be most helpful if you need to compile a piece of software from scratch that somebody else has created a recipe for.</p> <p>In this tutorial, we are going to try to install a piece of software that's not already on the HPCC called <code>zfp</code>.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#loading-easybuild","title":"Loading EasyBuild","text":"<p>To get started, we load the EasyBuild module:</p> <pre><code>module purge\nmodule load EasyBuild\n</code></pre> <p>We now have access to the <code>eb</code> command that does everything you need in EasyBuild.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#configuring-easybuild","title":"Configuring EasyBuild","text":"<p>We can first check our global EasyBuild configuration using</p> <p>input<pre><code>eb --show-config\n</code></pre> output<pre><code>#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath      (D) = /mnt/home/k0068027/.local/easybuild/build\ncontainerpath  (D) = /mnt/home/k0068027/.local/easybuild/containers\ninstallpath    (D) = /mnt/home/k0068027/.local/easybuild\nrepositorypath (D) = /mnt/home/k0068027/.local/easybuild/ebfiles_repo\nrobot-paths    (D) = /opt/software-current/2023.06/x86_64/generic/software/EasyBuild/4.9.2/easybuild/easyconfigs\nsourcepath     (D) = /mnt/home/k0068027/.local/easybuild/sources\n</code></pre></p> <p>Let's highlight the most important ones:</p> <code>installpath</code> <p>This is the root directory for your software installation. In this case, it's inside my (<code>k0068027</code>) home directory in the hidden directory <code>.local/easybuild</code>. By default, the software and modules both fall under this directory, but can be set separately (see the next two options).</p> <code>installpath-software</code> <p>This is where all of your software is actually installed. When your installation is finished, you should be able to find it under its name in this directory.</p> <code>installpath-modules</code> <p>This is where the module files are stored for your software installations. What's a module file? It's how <code>module load mysoftware</code> works! So after you install something with EasyBuild, you'll have built your own personal module that you can load like anything else on the HPCC!</p> <code>buildpath</code> <p>This is where the software is compiled. Usually, setting it as a <code>/tmp</code> directory is good since it's fast storage on the node for lots of small reading and writing. Once it's built, it gets moved to your <code>installpath-software</code> directory anyways, so it really is temporary.</p> <p>These are usually good defaults, but you might want to change them. For example, what if you need to install a piece of software in a research space so everyone in your group can access it? Or what if your home directory is filling up, you only need the software temporarily, and are okay installing it into your scratch space?</p> <p>For the sake of this tutorial, we'll practice by using a directory in <code>/tmp</code> as the build directory, installing the software into our scratch space, and leaving the module files in our home directory.</p> <p>To change the configuration, we can do it by passing the new value as a command line argument to any <code>ebS</code> command:</p> <p>input<pre><code>eb --buildpath=/tmp/$USER/EASYBUILD --installpath-software=$SCRATCH/software --show-config\n</code></pre> output<pre><code>#\n# Current EasyBuild configuration\n# (C: command line argument, D: default value, E: environment variable, F: configuration file)\n#\nbuildpath            (C) = /tmp/k0068027/EASYBUILD\ncontainerpath        (D) = /mnt/home/k0068027/.local/easybuild/containers\ninstallpath          (D) = /mnt/home/k0068027/.local/easybuild\ninstallpath-software (C) = /mnt/gs21/scratch/k0068027/software\nrepositorypath       (D) = /mnt/home/k0068027/.local/easybuild/ebfiles_repo\nrobot-paths          (D) = /opt/software-current/2023.06/x86_64/generic/software/EasyBuild/4.9.2/easybuild/easyconfigs\nsourcepath           (D) = /mnt/home/k0068027/.local/easybuild/sources\n</code></pre></p> <p>Great! It also tells us that the options were set using a command line argument by the <code>(C)</code>. We'll have to make sure to include this option when we actually try to install the software: it's not \"set and forget\"!</p> <p>As you can see from the output, there are multiple ways to configure EasyBuild. If you want to make more lasting changes that are \"set and forget\", try setting up a configuration file.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#finding-our-easyconfig","title":"Finding our EasyConfig","text":"<p>So now that we're happy with and (mostly) understand our <code>eb --show-config</code> results, we can try finding the recipe for the software we'd like to install. These recipes are called EasyConfigs and there's a good chance that someone has already created one for the software you're trying to install.</p> <p>The list of EasyConfigs is stored on the HPCC and we can search through it using the <code>-S</code> option of <code>ebS</code>:</p> <p>input<pre><code>eb -S zfp\n</code></pre> output<pre><code>CFGS1=/opt/software-current/2023.06/x86_64/generic/software/EasyBuild/4.9.2/easybuild/easyconfigs/z/zfp\n * $CFGS1/zfp-0.5.5-GCCcore-10.2.0.eb\n * $CFGS1/zfp-1.0.0-GCCcore-9.3.0.eb\n * $CFGS1/zfp-1.0.0-GCCcore-10.3.0.eb\n * $CFGS1/zfp-1.0.0-GCCcore-11.3.0.eb\n * $CFGS1/zfp-1.0.1-GCCcore-12.3.0.eb\n</code></pre></p> <p>This tells us that there a few different EasyConfigs available to help us install different versions of <code>zfp</code> under different toolchains.</p> What is a toolchain? <p>A toolchain is a set of software dependencies used to install new software. Most often, this is a compiler like GCC or a compiler/MPI pair like GCC and OpenMPI. The most basic toolchains are just single compilers and are labeled using their software version (like <code>GCCcore-12.3.0</code>).</p> <p>Some of these are so commonly used that EasyBuild groups dependency software into larger toolchains like foss and intel that contain a compiler/MPI pair and a number of other common dependencies. These are labeled by their year and an <code>a</code> or <code>b</code> for the first or second half of the year. You can check what's in them by searching for their EasyConfig and showing it with <code>eb --show-ec</code>:</p> <p>input<pre><code>eb -S ^foss\n</code></pre> output<pre><code>...\nk0068027@dev-intel14-k20:~$ eb -S ^foss-\nCFGS1=/opt/software-current/2023.06/x86_64/generic/software/EasyBuild/4.9.2/easybuild/easyconfigs\n...\n * $CFGS1/f/foss/foss-2022a.eb\n * $CFGS1/f/foss/foss-2022b.eb\n * $CFGS1/f/foss/foss-2023.09.eb\n * $CFGS1/f/foss/foss-2023a.eb\n * $CFGS1/f/foss/foss-2023b.eb\n * $CFGS1/f/foss/foss-2024.05.eb\n</code></pre> input<pre><code>eb --show-ec foss-2023a.eb\n</code></pre> output<pre><code>easyblock = 'Toolchain'\n\nname = 'foss'\nversion = '2023a'\n\nhomepage = 'https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain'\ndescription = \"\"\"GNU Compiler Collection (GCC) based compiler toolchain, including\n OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.\"\"\"\n\ntoolchain = SYSTEM\n\nlocal_gccver = '12.3.0'\n\n# toolchain used to build foss dependencies\nlocal_comp_mpi_tc = ('gompi', version)\n\n# we need GCC and OpenMPI as explicit dependencies instead of gompi toolchain\n# because of toolchain preparation functions\ndependencies = [\n    ('GCC', local_gccver),\n    ('OpenMPI', '4.1.5', '', ('GCC', local_gccver)),\n    ('FlexiBLAS', '3.3.1', '', ('GCC', local_gccver)),\n    ('FFTW', '3.3.10', '', ('GCC', local_gccver)),\n    ('FFTW.MPI', '3.3.10', '', local_comp_mpi_tc),\n    ('ScaLAPACK', '2.2.0', '-fb', local_comp_mpi_tc),\n]\n\nmoduleclass = 'toolchain'\n</code></pre></p> <p>We can see that <code>foss</code> includes <code>GCC</code>, <code>OpenMPI</code>, <code>FlexiBLAS</code>, <code>FFTW</code>, <code>FFTW.MPI</code>, and <code>ScaLAPACK</code>.</p> <p>We'll try to install the newest version of <code>zfp</code> with the newest compiler there's a corresponding EasyConfig for: <code>zfp-1.0.1-GCCcore-12.3.0.eb</code>.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#checking-dependencies","title":"Checking dependencies","text":"<p>One of the great part of EasyBuild is that it will handle the dependencies of the software you're using for you. If they're not already installed, it will use other EasyConfigs to install them.</p> <p>We can check to see if we're missing any of <code>zfp</code>'s dependencies on the system using <code>eb -M</code>:</p> <p>input<pre><code>eb -M zfp-1.0.1-GCCcore-12.3.0.eb\n</code></pre> output<pre><code>...\n1 out of 13 required modules missing:\n\n* zfp/1.0.1-GCCcore-12.3.0 (zfp-1.0.1-GCCcore-12.3.0.eb)\n...\n</code></pre></p> <p>We're only missing one module: the software itself! The remaining twelve modules are already available on the HPCC. If there were other dependencies that weren't installed, EasyBuild will install those for us, so long as we use the <code>--robot</code> option when installing <code>zfp</code>. </p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#installing","title":"Installing","text":"<p>Now we're ready to install. We just use the <code>eb</code> alias with our EasyConfig, and hope things go well!</p> <p>We can run shorter installs like this on a development node. By default, EasyBuild will try to parallelize compilation using all of the cores on the machine. To be a good dev node neighbor, we can use the <code>--parallel</code> option to only use a few of the cores, and leave the rest of the machine useable for everyone. For longer builds, you might consider running EasyBuild through a batch or interactive job.</p> <p>And don't forget to change your build and software install paths!</p> <p>input<pre><code>eb --parallel=8 \\\n    --buildpath=/tmp/$USER/EASYBUILD --installpath-software=$SCRATCH/software \\\n    zfp-1.0.1-GCCcore-12.3.0.eb\n</code></pre> output<pre><code>...\n== processing EasyBuild easyconfig /opt/software-current/2023.06/x86_64/generic/software/EasyBuild/4.9.2/easybuild/easyconfigs/z/zfp/zfp-1.0.1-GCCcore-12.3.0.eb\n== building and installing zfp/1.0.1-GCCcore-12.3.0...\n== fetching files...\n== ... (took 1 secs)\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== ... (took 1 secs)\n== configuring...\n== building...\n== ... (took 10 secs)\n== testing...\n== installing...\n== taking care of extensions...\n== restore after iterating...\n== postprocessing...\n== sanity checking...\n== ... (took 1 secs)\n== cleaning up...\n== creating module...\n== ... (took 1 secs)\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully (took 16 secs)\n== Results of the build can be found in the log file(s) /mnt/gs21/scratch/k0068027/software/zfp/1.0.1-GCCcore-12.3.0/easybuild/easybuild-zfp-1.0.1-20240815.131013.log\n== Build succeeded for 1 out of 1\n...\n</code></pre></p> <p>This process should less than a minute in total.</p> <p>There's a lot of output here, but we can see that it completed steps like configuring, building, testing, installing, and checking that important files are where they're supposed to be. And not only would it do this for the software you requested, but for all of the dependencies too. You would usually have to do this all manually!</p> <p>We can check that the software is where it's supposed to be.</p> <p>input<pre><code>ls $SCRATCH/software/zfp/1.0.1-GCCcore-12.3.0\n</code></pre> output<pre><code>bin  easybuild  include  lib  lib64\n</code></pre> input<pre><code>ls $SCRATCH/software/zfp/1.0.1-GCCcore-12.3.0/bin\n</code></pre> output<pre><code>testviews testzfp  zfp\n</code></pre></p> <p>Note that the installation is stored under the <code>zfp</code> directory in a directory labeled with the software and toolchain versions. If, in the future, we wanted a new version or one built with a different compiler, these two versions can coexist in different directories.</p>","tags":["tutorial","EasyBuild"]},{"location":"EasyBuild_Tutorial/#using-the-software","title":"Using the software","text":"<p>Now that it's installed, we can try to test it with the <code>testzfp</code> executable. But just because we know where the software is, doesn't mean we're ready to run it! When we install software, it needs to be linked to the proper libraries that also need to be available when the software runs. This includes all the dependencies of <code>zfp</code>!</p> <p>The right way to do this is to use the module file that comes when we install the software. EasyBuild installs modules into <code>$HOME/.local/easybuild/modules/all</code> by default. We can add these to our \"module path\" so that they show up when we try to do <code>module load</code>:</p> <p>input<pre><code>module use $HOME/modules/.local/easybuild/modules/all\necho $MODULEPATH\n</code></pre> output<pre><code>/mnt/home/k0068027/.local/easybuild/modules/all:/opt/software-current/2023.06/x86_64/generic/modules/all:/usr/modulefiles/Linux:/usr/modulefiles/Core:/usr/lmod/lmod/modulefiles/Core\n</code></pre></p> <p>Let's try searching for it:</p> <p>input<pre><code>module spider zfp\n</code></pre> output<pre><code>---------------------------------------------------------------------------------------------------------------------------------------------\n  zfp: zfp/1.0.1-GCCcore-12.3.0\n---------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      zfp is a compressed format for representing multidimensional\n      floating-point and integer arrays. zfp provides compressed-array classes\n      that support high throughput read and write random access to individual\n      array elements. zfp also supports serial and parallel (OpenMP and CUDA)\n      compression of whole arrays, e.g., for applications that read and write\n      large data sets to and from disk.\n\n\n    This module can be loaded directly: module load zfp/1.0.1-GCCcore-12.3.0\n\n    Help:\n\n      Description \n      ===========\n      zfp is a compressed format for representing\n      multidimensional floating-point and integer arrays. zfp provides\n      compressed-array classes that support high throughput read and write\n      random access to individual array elements. zfp also supports serial and\n      parallel (OpenMP and CUDA) compression of whole arrays, e.g., for\n      applications that read and write large data sets to and from disk.\n\n\n      More information\n      ================\n       - Homepage: https://github.com/LLNL/zfp\n...\n</code></pre></p> <p>Notice that the version is followed by the toolchain it depends on. Now we can load it using the command <code>module spider</code> gave.</p> <p>input<pre><code>module load zfp/1.0.1-GCCcore-12.3.0\nmodule list\n</code></pre> output<pre><code>Currently Loaded Modules:\n  1) EasyBuild/4.9.2   2) GCCcore/12.3.0   3) zfp/1.0.1-GCCcore-12.3.0\n</code></pre></p> <p>This does a few things including </p> <ul> <li>adding <code>zfp</code>'s <code>bin</code> directory to our path,</li> <li>adding <code>zfp</code>'s <code>lib</code> directory to our <code>LD_LIBRARY_PATH</code> (so we can compile new programs using the libraries it provides in the future),</li> <li>and loading the modules for its runtime dependencies.</li> </ul> <p>This means we can run <code>testzfp</code> without using it's absolute path:</p> <p>input<pre><code>testzfp\n</code></pre> output<pre><code>zfp version 1.0.1 (December 15, 2023)\nlibrary version 4112\nCODEC version 5\ndata model LP64\n\ntesting 1D array of floats\n  compress:   rate= 2                                                 OK \n  decompress: rate= 2 1.626e+01 &lt;= 1.627e+01                          OK \n...\nall tests passed\n</code></pre></p> <p>Though this was a small example, this workflow should get you through most EasyBuild installations. Checkout our EasyBuild reference in the future if you need a quick refresher of the most important commands.</p> <p>(Oh, and you can delete the <code>$HOME/.local/easybuild/modules</code> and <code>$SCRATCH/software</code> directories to start fresh for your real installations.)</p>","tags":["tutorial","EasyBuild"]},{"location":"Editing_Text_with_Nano/","title":"Editing Text with <code>nano</code>","text":"<p>Life on the HPCC is filled with text files: batch scripts, configuration files, source code... Sooner or later you will have to create or edit one!</p> <p>While there are options like VS Code that are closer to the types of text editors used on a normal desktop computer, they can be difficult to setup on the HPCC, and if you're just moving around using the command line, you might want to quickly drop in and edit a file without having to switch programs.</p> <p>We recommend that new users try <code>nano</code> to edit files on the command line. While there are other text editors you can use directly from the shell (e.g., <code>vim</code> or <code>emacs</code>), <code>nano</code> is much more user-friendly, and easier to start with.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#opening-a-file","title":"Opening a file","text":"<p>To open a file using <code>nano</code>, type</p> <pre><code>nano &lt;filename&gt;\n</code></pre> <p>into the shell, replacing <code>&lt;filename&gt;</code> with the file you'd like to edit. You can either give the full path to the file like</p> <pre><code>nano ~/my/important/text/file.txt\n</code></pre> <p>or if you're already in the directory where your file is, you can just use the filename, like</p> <pre><code>cd ~/my/important/text\nnano file.txt\n</code></pre> <p>If the file doesn't exist, <code>nano</code> will create it for you.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#editing-a-file","title":"Editing a file","text":"<p>Once the file is open, you can use the arrow keys, page up/down, and home/end to move around. You can type anywhere to add text.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#copying-and-pasting","title":"Copying and pasting","text":"<p>To copy, first use the mouse to highlight text. Right click, and choose Copy from the menu.</p> <p>To paste, move the cursor to the position you want to paste using the arrow keys. Right click, and choose Paste from the menu.</p> <p>The keyboard shortcuts to copy and paste depend on your operating system and terminal program:</p> WindowsMacLinux <p>In MobaXterm:</p> Command Shortcut Copy Text selected with the mouse will be copied by default Paste <code>shift + insert</code> Command Shortcut Copy <code>cmd + c</code> Paste <code>cmd + v</code> <p>This depends on your terminal, but usually the following work:</p> Command Shortcut Copy <code>ctrl + shift + c</code> Paste <code>ctrl + shift + v</code> <p>If you need to scroll through a file to select all of the text you need to copy, the shortcuts in <code>nano</code> become more complicated. A simpler way to do this is to exit <code>nano</code>, and enter</p> <pre><code>cat &lt;filename&gt;\n</code></pre> <p>on the command line.</p> <p>This will output all contents of the filename you give to your command line, and you can select the part you want to copy with the mouse. The copy and paste shortcuts above will work in the same way.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#shortcuts","title":"Shortcuts","text":"<p>At the bottom of the screen, you will see some helpful key combinations:</p> <pre><code>^G Get Help  ^O WriteOut  ^R Read File ^Y Prev Page ^K Cut Text  ^C Cur Pos\n^X Exit      ^J Justify   ^W Where Is  ^V Next Page ^U UnCut Text^T To Spell\n</code></pre> <p>Each of these combinations starts with <code>^</code> symbolizing the <code>ctrl</code> key on your keyboard. For example, to exit, hold <code>ctrl</code> on your keyboard and press <code>x</code>. Even though <code>nano</code> shows an upper case <code>X</code>, you don't need to use the <code>shift</code> key.</p> <p>The shortcut descriptions are meant to be mnemonic and match the key used, which can lead to some confusion. Some helpful shortcuts in plain language include:</p> Task Shortcut Get help <code>^G</code> Exit <code>nano</code> <code>^X</code> Save a file <code>^O</code> Search for text <code>^W</code> Delete a line <code>^K</code> Paste line that was deleted <code>^T</code> Spell check <code>^T</code>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#saving-a-file","title":"Saving a file","text":"<p>To save a file, use <code>^O</code> (that is, hold the <code>ctrl</code> key and press <code>o</code>). You will be asked what filename to save your file to, with the default being the already existing filename. Press the <code>enter</code> or <code>return</code> key without making any changes to the filename to save.</p> <p>If you'd like to make a copy or save your edits in a new place, change the filename before accepting.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Editing_Text_with_Nano/#exiting-nano","title":"Exiting <code>nano</code>","text":"<p>To exit, use <code>^X</code> (that is, hold the <code>ctrl</code> key and press <code>x</code>). If you exit before having saved the file, <code>nano</code> will ask if you would like to save before exiting. Press the <code>y</code> key to save changes, the <code>n</code> to discard your changes, or <code>^C</code> to cancel.</p>","tags":["how-to guide","nano","text editors"]},{"location":"Example_SLURM_scripts/","title":"Example SLURM scripts","text":"<p>This page contains example job scripts for several kinds of jobs that you can modify for your own needs:</p> <ul> <li>Single core on a single node</li> <li>Single node, single core job with a GPU</li> <li>Multiple threads and cores on a single node</li> <li>Multiple nodes (e.g., an MPI program)</li> <li>Multiple threads and cores across multiple nodes (e.g., a hybrid MPI-OpenMP program)</li> </ul> <p>See the tutorial on writing and submitting job scripts for more guidance on using job scripts. </p> <p>More information on SLURM job specifications is also available.</p> <p>Users looking for example SLURM scripts for Singularity jobs should see this page.</p> <p>Different kinds of executables (compiled C/C++ code, Python and R scripts) are demonstrated across these examples for variety. How an executable should be run and the resources required depends on the details of that program.</p>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-single-core","title":"Single node, single core","text":"<p>This example demonstrates a simple R script job but is suited to any serial job. The <code>Rscript</code> command does not necessarily have to be wrapped with the <code>srun</code> command because this job uses no parallelism.</p> basic_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=Rscript\n\n# Number of tasks (processes)\n# SLURM defaults to 1 but we specify anyway\n#SBATCH --ntasks=1\n\n# Memory per node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=20M\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 powertools\n\n# Run our job\ncd /mnt/home/user123\nsrun Rscript myscript.R\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-single-core-with-gpu","title":"Single node, single core with GPU","text":"<p>Jobs that use GPUs must request these resources within their SLURM script. SLURM will automatically allocate the job to a node with the appropriate GPU.  Multiple GPUs may be available per node.</p> <p>Note</p> <p>This example requests only a single GPU node.  Users looking to use multiple nodes, each with their own GPU(s), should replace the <code>--gpus</code> option with <code>--gpus-per-node</code>. See the list of SLURM specifications for more.</p> <p>The fake Python script in this example would use PyTorch to define and train a neural network. The script loads our conda environment following the recommended setup in the conda usage guide.</p> gpu_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=pytorch\n\n# Number of processes.\n# Unless programmed using MPI,\n# most programs using GPU-offloading only need\n# a single CPU-based process to manage the device(s)\n#SBATCH --ntasks=1\n\n# Type and number of GPUs\n# The type is optional.\n#SBATCH --gpus=v100:4\n\n# Total CPU memory\n# All available memory per GPU is allocated by default.\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\nmodule load Conda/3\nconda activate myenv\n\n# Run our job\ncd /mnt/home/user123\nsrun python train_neural_network.py\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#single-node-multiple-cores","title":"Single node, multiple cores","text":"<p>There are two ways a job could use multiple CPU cores (also known as processors) on a single node: each CPU could work independently, or they could work collaboratively on tasks (also called processes). The latter style is appropriate for jobs written with OpenMP. Examples of each are shown below.</p>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#independent-cpus","title":"Independent CPUs","text":"<p>The fake Python script in this example would use a pool of processes independently completing tasks. The script loads our conda environment following the recommended setup in the conda usage guide.</p> pleasantly_parallel.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=python-pool\n\n# Number of nodes\n#SBATCH --nodes=1\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=16\n\n# Memory per CPU\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem-per-cpu=20M\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\nmodule load Conda/3\nconda activate myenv\n\n# Run our job\ncd /mnt/home/user123\nsrun python my_processor_pool.py\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#collaborative-cpus","title":"Collaborative CPUs","text":"<p>This example is suited to software written with OpenMP, where multiple CPU cores are needed per task (also called a process).</p> <p>Warning</p> <p>For this kind of job, users must specify <code>--cpus-per-task</code> at the top of their job file and when calling <code>srun</code>. See the Lab Notebook on changes to <code>srun</code> for more.</p> <p>Warning</p> <p>When using <code>srun -c</code> around your job's executable as in this example, it is usually not necessary to specify <code>OMP_NUM_THREADS</code>; however, setting <code>OMP_NUM_THREADS</code> will override any options passed to <code>srun</code>.</p> threaded_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=openmp-threaded\n\n# Number of nodes\n#SBATCH --nodes=1\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=1\n\n# Number of CPUs per task\n#SBATCH --cpus-per-task=32\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0\n\n# Run our job\ncd $SCRATCH\n# You MUST specify the number of CPUs per task again.\n# Alternatively, you can set OMP_NUM_THREADS\nsrun -c $SLURM_CPUS_PER_TASK my_openmp\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#multiple-nodes","title":"Multiple nodes","text":"<p>When your required resources exceed those available on a single node,  you can request multiple nodes while specifying per-node resources.</p> <p>Note</p> <p>In most cases, the <code>srun</code> command takes the place of <code>mpirun</code>. The <code>srun</code> command does not require an argument specifying the number of processes to be used for the job.</p> mpi_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=mpi-parallel\n\n# Number of nodes\n#SBATCH --nodes=4\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=32\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\n# Run our job\ncd $SCRATCH\nsrun my_mpi_job\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Example_SLURM_scripts/#multiple-nodes-with-multiple-cores-per-task-hybrid-jobs","title":"Multiple nodes with multiple cores per task (hybrid jobs)","text":"<p>Some programs can use, for example, both MPI and OpenMP to execute what  is called \"hybrid\" parallelism where each node runs one or more threaded processes.</p> <p>Warning</p> <p>For this kind of job, users must specify <code>--cpus-per-task</code> at the top of their job file and when calling <code>srun</code>. See the Lab Notebook on changes to <code>srun</code> for more.</p> <p>Warning</p> <p>When using <code>srun -c</code> around your job's executable as in this example, it is usually not necessary to specify <code>OMP_NUM_THREADS</code>; however, setting <code>OMP_NUM_THREADS</code> will override any options passed to <code>srun</code>.</p> hybrid_job.sb<pre><code>#!/bin/bash --login\n# Job name:\n#SBATCH --job-name=mpi-hybrid\n\n# Number of nodes\n#SBATCH --nodes=8\n\n# Number of tasks to run on each node\n#SBATCH --ntasks-per-node=6\n\n# Number of CPUs per task\n#SBATCH --cpus-per-task=4\n\n# Memory per Node\n# Specify \"M\" or \"G\" for MB and GB respectively\n#SBATCH --mem=2G\n\n# Wall time\n# Format: \"minutes\", \"hours:minutes:seconds\", \n# \"days-hours\", or \"days-hours:minutes\"\n#SBATCH --time=01:00:00\n\n# Mail type\n# e.g., which events trigger email notifications\n#SBATCH --mail-type=ALL\n\n# Mail address\n#SBATCH --mail-user=yournetid@msu.edu\n\n# Standard output and error to file\n# %x: job name, %j: job ID\n#SBATCH --output=%x-%j.SLURMout\n\n# Purge current modules and load those we require\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\n# Run our job\ncd $SCRATCH\n# You MUST specify the number of CPUs per task again.\n# Alternatively, you can set OMP_NUM_THREADS\nsrun -c $SLURM_CPUS_PER_TASK my_hybrid_program\n\n# Print resource information\nscontrol show job $SLURM_JOB_ID\njs -j $SLURM_JOB_ID\n</code></pre>","tags":["reference","slurm","job script"]},{"location":"Expansion/","title":"Expansion","text":"<p>When we type a command with arguments/inputs and press the enter key, the shell does several things to the arguments/input text before it actually carries out the command. This action is called expansion. With expansion, the arguments expands into something else before the shell acts on it with the command. Let's see an example with <code>echo</code> command which prints out the text arguments given on standard output.</p> input<pre><code>echo hello world!\n</code></pre> output<pre><code>hello world!\n</code></pre> <p>Let's use <code>echo</code> with <code>*</code>:</p> input<pre><code>echo *\n</code></pre> output<pre><code>hello.c hello.qsub hello.sb README\n</code></pre> <p>Instead of printing <code>*</code>, it prints out all file names in the directory because shell expands <code>*</code> into something else before the <code>echo</code> command acts on the argument (in this case <code>*</code>).</p> input<pre><code>ls\n</code></pre> output<pre><code>hello.c hello.qsub hello.sb README\n</code></pre> <p>The <code>*</code> character is called a \"wildcard\" is replaced by everything in the current directory. We can also modify it so that it's only replaced by certain things. For example, <code>h*</code> is replaced everything that starts with <code>h</code> in the current directory.</p> input<pre><code>echo h*\n</code></pre> output<pre><code>hello.c hello.qsub hello.sb\n</code></pre> <p><code>~</code> is another special character with a special meaning. It expands into the name of the home directory of the user:</p> input<pre><code>echo ~\n</code></pre> output<pre><code>/mnt/home/temp_user_01\n</code></pre> <p>The shell will also expand arithmetic. Arithmetic expansion uses the form <code>$((expression))</code>. Look at the example.</p> input<pre><code>echo $((1 + 1))\n</code></pre> output<pre><code>2\n</code></pre> <p>Please keep in mind that arithmetic expansions allows only integers. Arithmetic expression can be nested, and spaces are allowed.</p> input<pre><code>echo $(( 7*( 2 + 2 ) ))\n</code></pre> output<pre><code>28\n</code></pre> input<pre><code>echo $((7*(2+2)))\n</code></pre> output<pre><code>28\n</code></pre> <p>Brace expansion is useful when you write a shell script or batch script. The brace expression can contain a comma separated list of characters, strings, or integers that will be expanded into multiple expressions. Here are a few examples.</p> input<pre><code>echo srt-{a,b,c}-end\n</code></pre> output<pre><code>srt-a-end srt-b-end srt-c-end\n</code></pre> input<pre><code>echo number_{1..5}\n</code></pre> output<pre><code>number_1 number_2 number_3 number_4 number_5\n</code></pre> input<pre><code>echo {A..Z}\n\n``` output title=\"output\"\nA B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n</code></pre> input<pre><code>echo a{A{1,5},B{6..10}}b\n</code></pre> output<pre><code>aA1b aA5b aB6b aB7b aB8b aB9b aB10b\n</code></pre> <p>The next example creates multiple directories with a brace expansion.</p> input<pre><code>mkdir Photos\ncd Photos/\nmkdir {2020..2021}-{01..12}\nls\n</code></pre> output<pre><code>2020-01/  2020-03/  2020-05/  2020-07/  2020-09/  2020-11/  2021-01/  2021-03/  2021-05/  2021-07/  2021-09/  2021-11/\n2020-02/  2020-04/  2020-06/  2020-08/  2020-10/  2020-12/  2021-02/  2021-04/  2021-06/  2021-08/  2021-10/  2021-12/\n</code></pre> <p>Expansion also allows us to use the output of a command in different places. The syntax is the same as arithmetic expansion, except the command is placed inside <code>$(...)</code></p> input<pre><code>echo $(ls | grep 2020)\n</code></pre> output<pre><code>2020-01/ 2020-02/ 2020-03/ 2020-04/ 2020-05/ 2020-06/ 2020-07/ 2020-08/ 2020-09/ 2020-10/ 2020-11/ 2020-12/\n</code></pre> <p>Next, let's learn how to control expansion.\u00a0Consider the following two examples where automatic expansion doesn't do what we want.</p> input<pre><code>echo This is a      test\n</code></pre> output<pre><code>This is a test\n</code></pre> input<pre><code>echo The total is $100.00\n</code></pre> output<pre><code>The total is 00.00\n</code></pre> <p>In the first example, the shell removes extra space from the <code>echo</code> command's argument. In the second example, <code>$1</code> is interpreted as the first input parameter which is not defined here, and therefore, it is replaced as empty string. With quoting, we can suppress unwanted expansions.</p> <p>First, let's learn about double quotes. If we place text inside double quotes, parameters are not split by white space and all special characters lose their special meaning, and are treated as ordinary characters. However <code>$</code>, <code>\\</code> (backslash), and <code>`</code> (back quote) are exceptions.</p> input<pre><code>echo \"This is a     test\"\n</code></pre> output<pre><code>This is a     test\n</code></pre> input<pre><code>echo \"The total is $100.00\"\n</code></pre> output<pre><code>The total is 00.00\n</code></pre> <p>Here is another example where the shell splitting the parameters by white space would cause a problem. Suppose that our working directory has the file <code>two words.txt</code> in it, where the filename has a space.</p> input<pre><code>ls two words.txt\n</code></pre> output<pre><code>ls: cannot access two: No such file or directory\nls: cannot access words.txt: No such file or directory\n</code></pre> <p>The shell splits the two words and looks for them each separately with <code>ls</code>. Quoting fixes this.</p> input<pre><code>ls \"two words.txt\"\n</code></pre> output<pre><code>two words.txt\n</code></pre> <p>If you want to suppress all expansions, you need to use single quotes.</p> input<pre><code>echo 'This is a     test'\n</code></pre> output<pre><code>This is a     test\n</code></pre> input<pre><code>echo 'The total is $100.00'\n</code></pre> output<pre><code>The total is $100.00\n</code></pre> <p>The next three examples show how quoting gives different results.</p> input<pre><code>echo text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\n</code></pre> output<pre><code>text /mnt/home/user_name/hostfile.txt /mnt/home/user_name/powertools.txt \n1 2 3 4 5 foo 4 Tue Jan 19 15:10:00 EST 2021\n</code></pre> input<pre><code>echo \"text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\"\n</code></pre> output<pre><code>text ~/*.txt {1..5} foo 4 Tue Jan 19 15:10:09 EST 2021\n</code></pre> input<pre><code>echo 'text ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)'\ntext ~/*.txt {1..5} $(echo foo) $((2+2)) $(date)\n</code></pre> <p>A backslash is useful when we want to quote a single character. A backslash is called the escape character. Next example shows how quoting and an escape character work.</p> input<pre><code>echo The balance of $(date) is $100\n</code></pre> output<pre><code>The balance of Tue Jan 19 15:15:36 EST 2021 is 00\n</code></pre> input<pre><code>echo \"The balance of $(date) is $100\"\n</code></pre> output<pre><code>The balance of Tue Jan 19 15:15:48 EST 2021 is 00\n</code></pre> input<pre><code>echo 'The balance of $(date) is $100'\n</code></pre> output<pre><code>The balance of $(date) is $100\n</code></pre> input<pre><code>echo \"The balance of $(date) is \\$100\"\n</code></pre> output<pre><code>The balance of Tue Jan 19 15:16:09 EST 2021 is $100\n</code></pre> input<pre><code>echo 'The balance of $(date) is \\$100'\n</code></pre> output<pre><code>The balance of $(date) is \\$100\n</code></pre> <p>The table show the most frequently used escape characters.</p> Escape Character Name usage \\n newline Adding blank lines to text \\t tab Inserting horizontal tabs to text \\a alert Making the user terminal beep \\\\ backslash Inserting a backslash","tags":["tutorial","command line"]},{"location":"External_Resources/","title":"External resources","text":"<p>In addition to ICER's workshops and D2L training materials, there are many trainings, tutorials, and resources online created by other high-performance computing centers and education groups. The links below include just a few of these materials that are mostly oriented towards general high-performance computing tasks.</p> <p>If you have found reference materials online that have helped you use ICER's HPCC more effectively, please share them by filling out the documentation feedback form.</p>","tags":["reference","training"]},{"location":"External_Resources/#introductory-materials","title":"Introductory materials","text":"<ul> <li>Introductory Supercomputing slides from Pawsey Supercomputing Research   Center</li> <li>HPC Carpentry lessons for basic skills in high-performance   computing</li> </ul>","tags":["reference","training"]},{"location":"External_Resources/#intermediate-materials","title":"Intermediate materials","text":"<ul> <li>Intermediate Supercomputing slides from Pawsey Supercomputing Research   Center</li> </ul>","tags":["reference","training"]},{"location":"External_Resources/#advanced-materials","title":"Advanced materials","text":"<ul> <li>Best Practices for HPC Software Developers webinar series from   IDEAS</li> </ul>","tags":["reference","training"]},{"location":"File-Permission-in-Research-Space_34963746.html/","title":"File Permission in Research Space","text":""},{"location":"File-Permission-in-Research-Space_34963746.html/#file-permission-in-research-space","title":"File Permission in Research Space","text":"<p>A user with account name <code>User1</code> is not able to access a directory <code>Dirct</code> in his research space <code>Group1</code>. The following is the result of the <code>ls</code> command:</p> <pre><code>[User1@dev-intel18 ~]$ ls -la /mnt/research/Group1\ntotal 98\ndrwxrwS---   3 ProjInvs Group1 8192 Aug  6 08:53 .\ndrwxr-xr-x 391 root     root      0 Sep  9 07:34 ..\n-rwx------   1 User2    Group2 4299 Jul  2  2018 file1\n-rwx------   1 User3    Group3 2452 Jul  2  2018 file2\ndrwxrwS---   2 User2    Group2 8192 May 22 11:31 Dirct\n-rw-rw-r--   1 User1    Group1  263 Aug  6 08:54 file3\n</code></pre> <p>Q: How to make <code>User1</code> able to access the directory <code>Dirct</code>?</p> <p>A: Since <code>User2</code> is the owner of the directory, <code>User2</code> can run a\u00a0 command to change the group ownership:</p> <pre><code>[User2@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/Dirct\n</code></pre> <p>Q: In order for all group users able to access files and directories in the research space, what should they do?</p> <p>A: They should run the following commands:</p> <ol> <li> <p>Change the group ownership of all files and directories to the research group <code>Group1</code>:</p> <pre><code>[UserID@dev-node ~]$ chgrp -R Group1 /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Open the permissions of all files and directories to be readable (<code>r</code>) and writable (<code>w</code>) to group users:</p> <pre><code>[UserID@dev-node ~]$ chmod -R g+rw /mnt/research/Group1/ 2&gt;/dev/null\n</code></pre> </li> <li> <p>Make all directories sticky to the group ownership for any file generated inside (turn on group sticky bits):</p> <pre><code>[UserID@dev-node ~]$ chmod g+s $(find /mnt/research/Group1/ -type d -user $USER 2&gt;/dev/null)\n</code></pre> </li> <li> <p>Group users should not copy files to their research space with preserving the ownership, such as using command \"<code>cp -p ...</code> \".</p> </li> </ol>"},{"location":"File_Count/","title":"<code>file-count</code>","text":"<p><code>file-count</code> is a powertools program that shows you the number of files in your home directory or any directory that you pass as an argument. This can be useful for comparing against the number of files from the <code>quota</code> command or determining where you may have a large number of files (especially if they are hidden).</p>","tags":["reference","files","quota"]},{"location":"File_Count/#examples","title":"Examples","text":"<p>Below are some examples of it's usage. Since <code>file-count</code> is a powertool, the <code>powertools</code> module must be loaded:</p> <pre><code>module load powertools\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-home-directory","title":"List file count of home directory","text":"input<pre><code>file-count\n</code></pre> output<pre><code>385162  /mnt/home/user/\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-all-subdirectories-in-home-directory","title":"List file count of all subdirectories in home directory","text":"<p>This includes hidden directories that may contain large numbers of files, like <code>.conda</code>.</p> input<pre><code>file-count --detail \n</code></pre> output<pre><code>4   /mnt/home/user/.thumbnails\n12  /mnt/home/user/.jupyter\n22950   /mnt/home/user/R\n5   /mnt/home/user/.ssh\n446 /mnt/home/user/.config\n39  /mnt/home/user/.vnc\n...\n53  /mnt/home/user/scripts\n54356   /mnt/home/user/.cache\n6209    /mnt/home/user/.vscode-server\n385162  /mnt/home/user/\n</code></pre> <p>Note that a summary of all files is given at the bottom</p>","tags":["reference","files","quota"]},{"location":"File_Count/#list-file-count-of-a-research-directory","title":"List file count of a research directory","text":"input<pre><code>file-count /mnt/research/my_group\n</code></pre> output<pre><code>400124 /mnt/research/my_group/\n</code></pre>","tags":["reference","files","quota"]},{"location":"File_Permissions_on_HPCC/","title":"File Permissions on HPCC","text":"<p>The HPCC offers several different types of storage for users. All of these filesystems make use of standard UNIX file permissions. Understanding how standard UNIX permissions and ownership works is an important way to control access to your files.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#unix-users-and-groups","title":"UNIX users and groups","text":"<p>Every user has a unique username on HPCC systems. This is typically your MSU NetID. Every user is also a member of at least one group. This group is typically the department the user is in (such as <code>cse</code> or <code>plb</code>). An user can be a member of additional groups. To see what groups you are a member of, run the <code>groups</code> command. If you feel you are in the wrong group, please contact HPCC staff.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#unix-file-ownership","title":"UNIX file ownership","text":"<p>Every file and directory has two sets of ownership, the user and the group. The user owner is normally set to the user that created the file. Normally, the user owner of a file or directory is the only user that is able to change permissions or group ownership.</p> <p>The group owner of a file or directory allows a user owner to grant permissions to a group of users for a particular file or directory. The user owner of a file can change the group ownership of a file to any group that they are a member of. Any file created by a user normally defaults to group owner being set to the user's primary group, unless the user or directory owner has changed the behavior (using procedures described here.)</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#the-three-types-of-basic-unix-permissions","title":"The three types of basic UNIX permissions","text":"","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#read","title":"Read","text":"<p>Read permission on a file allows the contents of a file to be read. The read permission, when applied to a directory, allows the contents of a directory to be listed. Referred to as \"r\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#write","title":"Write","text":"<p>Write permission on a file allows the file to be modified or deleted. Write permissions in a directory allow the creation of additional files in that directory. Referred to as \"w\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#execute","title":"Execute","text":"<p>The execute permission allows a file to be run as an executable. When applied to a directory it allows traversal of that directory: the ablility to access files or subdirectories in that directory. Referred to as \"x\" in the output of the <code>ls -l</code> command.</p>","tags":["explanation","files","groups"]},{"location":"File_Permissions_on_HPCC/#other-resources","title":"Other resources","text":"<p>This just covers the basic ideas of UNIX file permissions. In order to work with permissions on the HPCC most effectively, see the page on managing file permissions. Here are some other resources for more in-depth information: </p> <ul> <li>Software Carpentry - Permissions</li> <li>Linux Cookbook, 2nd ed., Chapter 9</li> <li>Computer Hope - Linux <code>umask</code> command</li> </ul>","tags":["explanation","files","groups"]},{"location":"File_transfer/","title":"File transfer","text":"<p>This document highlights several simple methods to transfer files to the HPCC home and research directories. There are two main gateway systems for copying files.\u00a0</p> <ol> <li> <p><code>hpcc.msu.edu</code>: This is our login gateway. While it can be used for file transfer, it's not intended for high volumes of files. More importantly, the scratch space is not mounted there and so you can't access your files on scratch.</p> </li> <li> <p><code>rsync.hpcc.msu.edu</code>: It has access to scratch, and is dedicated to file transfer. Although this gateway is named by the popular Linux \"rsync\" command, it can be used for \"sftp\" or \"scp\" as well. Starting in October 2022, login to the rsync gateway will accept SSH keys as the ONLY authentication method. Username/password won't work. Please refer to the SSH key tutorial for setting up your keypair.</p> </li> </ol>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"File_transfer/#all-operating-systems","title":"All operating systems","text":"<p>Note</p> <p>The OnDemand portal is best for transferring files less than ~1 GB in size. For transferring larger files to and from the HPCC, see Large file transfer (Globus)</p> <p>The most straightforward way to transfer files to and from the HPCC is via our OnDemand web portal. Log in with your NetID at https://ondemand.hpcc.msu.edu and click \"Files\" to access your different user spaces.</p> <p></p> <p>On this page, you can upload, download, rename, and modify most files in storage locations you have access to.</p>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"File_transfer/#specific-operating-systems","title":"Specific operating systems","text":"<p>Use the tabs below to view the relevant options for your system.</p> Linux and Mac commandsWindows <p>A number of different command-line utilities are available to OS X and Linux users. Each of them has its own advantages.</p> <p>Warning</p> <p>Using the commandline to connect to the <code>rsync.hpcc.msu.edu</code> gateway requires SSH key setup. Please refer to the SSH key tutorial for setting up your keypair.</p> <ol> <li> <p>Basic file copy (<code>scp</code>)</p> <p>A simple command for transferring files between the cluster and another host is <code>scp</code>. To copy a file from a local directory to file space on the cluster, run a command such as</p> <pre><code>scp example.txt username@rsync.hpcc.msu.edu:example_copy.txt\n</code></pre> <p>This will copy the file named <code>example.txt</code> in the local host's current directory to the user's home directory on the cluster, with the copy having the name <code>example_copy.txt</code>. Leaving the space after the colon blank gives the new file the same name as the original.\u00a0 Note: To transfer a file name with spaces you must put a backslash before each space in your file name, i.e. <code>scp \"My File Name\" username@hpcc.msu.edu:\"My\\ File\\ Name\"</code>.</p> <p>To copy a file from the cluster to your local directory,</p> <pre><code>scp username@rsync.hpcc.msu.edu:example.txt ./example_copy.txt\n</code></pre> <p>will copy the file named <code>example.txt</code> from the user's home directory on the cluster to the home directory of the local host, naming the new file <code>example_copy.txt</code>. Leaving the space after the slash blank gives the new file the same name as the original. The <code>-r</code> option can be used to copy entire directories recursively.\u00a0</p> </li> <li> <p>Synchronize directories (<code>rsync</code>)</p> <p>If you are an advanced LINUX/Mac user, there is a useful utility that makes mirroring directories simple. The syntax looks very similar to <code>scp</code>.</p> <ul> <li> <p>To mirror <code>&lt;local_dir&gt;</code> on my local computer to <code>&lt;hpcc_dir&gt;</code> on hpcc, the following command can be run:</p> <pre><code>rsync -ave ssh &lt;local_dir&gt; username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt;\n</code></pre> <p>In the above command, rsync will scan through both directories. If any files in the <code>&lt;local_dir&gt;</code> are newer, they will be uploaded to <code>&lt;hpcc_dir&gt;</code>. (It is also possible to get rsync to upload ALL different files, regardless of which is newer).</p> </li> <li> <p>To mirror the HPCC directory to your local system, call</p> <pre><code>rsync -ave ssh username@rsync.hpcc.msu.edu:&lt;hpcc_dir&gt; &lt;local_dir&gt;\n</code></pre> </li> <li> <p>Please use the <code>rsync</code> command with the option <code>--chmod=Dg+s</code> to transfer files from a local computer to your research space.     See the following example:</p> <pre><code>rsync -ave ssh TestDir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;GroupName&gt;/\n</code></pre> </li> </ul> <p>Note</p> <p>the first time you use <code>rsync</code>, you might want to add the <code>-n</code> flag to do a dry run before any files are copied.</p> </li> <li> <p>Interactive file copy (<code>sftp</code>)</p> <p>When performing several data transfers between hosts, the <code>sftp</code> command may be preferable, as it allows the user to work interactively. Running</p> <pre><code>sftp username@rsync.hpcc.msu.edu\n</code></pre> <p>from a local host establishes a connection between that host and the cluster. Both hosts can be navigated. For the local file system, <code>lcd</code> changes to the specified directory, <code>lpwd</code> prints the working directory, and <code>lls</code> prints a list of files in the current directory. For the remote file system, the same three commands are available, minus the leading <code>l</code>. Also available are commands to change permissions, rename files, and manipulate directories on the remote host. The two key commands are <code>get &lt;file&gt;</code>, which copies the file in the remote working directory to the local working directory, and <code>put &lt;file&gt;</code>, which copies the file in the local working directory to the remote working directory. The <code>quit</code> command closes the connection between hosts.</p> </li> <li> <p>Copy files from Internet (<code>wget</code>)</p> <p><code>wget</code> is a simple command useful for copying files from the Internet to a user's file space on the cluster.\u00a0 Running the line</p> <pre><code>wget http://www.examplesite.com/examplefile.txt\n</code></pre> <p>downloads <code>examplefile.txt</code> to the user's working directory.</p> </li> </ol> <p>An alternate method for transferring files on Windows is MobaXTerm. Installation and setup instructions are available here. Once you are connected to the HPCC, you can use the MobaXterm SCP interface tab to upload and download files, available on the left side of the window.</p> <p></p> <p>You can type a path at the top of the SCP interface tab to access anywhere you have permissions on the HPCC e.g. your scratch and research spaces.</p> <p>MobaXTerm can also be set up for use as a SFTP client to transfer data with the <code>rsync.hpcc.msu.edu</code> gateway. As with other uses of the <code>rsync.hpcc.msu.edu</code> gateway, you must have an SSH key pair. Please refer to the SSH key tutorial for setting up your keypair.</p> <p>The configuration for the SFTP session should look like this:</p> <p></p> <p>The settings should be:</p> <ul> <li>Remote host: <code>rsync.hpcc.msu.edu</code></li> <li>Username: your HPCC username.</li> <li>Select the Advanced Sftp settings tab.</li> <li>Use private key: check the box. Click the small file icon to open a file browser and select your private key. Alternatively, type in the path to your private key.</li> <li>All other settings can remain default.</li> </ul>","tags":["how-to guide","rsync","scp","OnDemand"]},{"location":"Frequently_Asked_Questions_FAQ_/","title":"Frequently Asked Questions (FAQ)","text":"<p>This page lists many of our frequently asked questions. Please search for keywords related to an issue by using Ctrl+F (on Windows/Linux) or Cmd+F (on Mac), or scroll through the list of questions in the table of contents to the right.</p> <p>If you don't see an answer to your question, please contact us.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#table-of-contents","title":"Table of contents","text":"<ul> <li>Logging in and accessing the HPCC</li> <li>Limits and usage</li> <li>Storage and files</li> <li>Submitting jobs and running code</li> <li>Software and modules</li> <li>OnDemand</li> <li>Python and Conda</li> <li>R and RStudio Server</li> <li>Getting help</li> </ul>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#logging-in-and-accessing-the-hpcc","title":"Logging in and accessing the HPCC","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-my-hpcc-user-namepassword","title":"What is my HPCC user name/password?","text":"<p>If you are affiliated with MSU, then your MSU NetID is your user name, and your NetID password is your HPCC password. This is the same as those for all the MSU online services. An HPCC account must be requested by an MSU faculty member at\u00a0https://contact.icer.msu.edu/account</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-reset-my-password-on-the-hpcc-because-my-login-got-denied-after-multiple-failed-attempts","title":"Can I reset my password on the HPCC because my login got denied after multiple failed attempts?","text":"<p>There are two ways you can be blocked by entering an incorrect password too many times. The authentication on the HPCC is directly tied to MSU. If you attempt an incorrect password too many times, you may need to request a password reset at https://netid.msu.edu/netid/password/index.html. The HPCC also maintains blocks from hosts with too many failed attempted SSH connections. Users that can log into other MSU resources (Spartan365, D2L, EBS) but are unable to connect to the HPCC should submit a ticket on our contact forms. Be sure to include your external IP address; you can check it with Google.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-used-to-be-able-to-connect-to-the-hpcc-server-but-now-i-cant-why","title":"I used to be able to connect to the HPCC server, but now I can't. Why?","text":"<p>There can be multiple reasons for this, such as system downtime (so please check the ICER blog first). Another common reason is account expiry. The HPCC periodically disables users who are no longer affiliated with the university or registered with a class for which the instructor has created temporary student accounts. To re-activate your HPCC account, please have your PI submit a sponsoring form at https://contact.icer.msu.edu/sponsoredrenewal</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-get-a-permission-denied-error-but-i-put-in-the-right-password-whats-wrong","title":"I get a \"Permission denied\" error, but I put in the right password. What's wrong?","text":"<p>If you are attempting to connect to the <code>rsync.hpcc.msu.edu</code> server, this requires a SSH key pair. See our documentation for how to generate a key pair here. Otherwise, see the question above.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-get-an-error-like-warning-remote-host-identification-has-changed","title":"I get an error like \"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\"","text":"<p>The following errors can occur when the HPCC upgrades a development node or changes it's identifying information (called \"host keys\"):</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nPlease contact your system administrator.\nAdd correct host key in &lt;location&gt;/.ssh/known_hosts to get rid of this message.\nOffending RSA key in &lt;location&gt;/.ssh/known_hosts:6\nYou can use following command to remove the offending key:\nssh-keygen -R dev-amd20 -f &lt;location&gt;/.ssh/known_hosts\nHost key for dev-amd20 has changed and you have requested strict checking.\nHost key verification failed.\n</code></pre> <p>or:</p> <pre><code>The authenticity of host 'dev-amd20 (&lt;no hostip for proxy command&gt;)' can't be established.\nRSA key fingerprint is SHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>To fix the first error, run the command</p> <pre><code>ssh-keygen -R &lt;dev-node-name&gt; -f ~/.ssh/known_hosts\n</code></pre> <p>on your local computer, not the HPCC. Alternatively, you can also delete the file <code>~/.ssh/known_hosts</code> to reset all host keys. This will result in receiving the second message anytime you SSH to any other computer (even ones outside of ICER, like GitHub) until you've accepted the connection again.</p> <p>To accept the new connection and fix the second error, enter \"yes\" and hit enter.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-i-use-hpc-through-web-browsers","title":"Can I use HPC through web browsers?","text":"<p>Yes, we provide Open OnDemand, a web portal for easy web access to the HPCC. Check out this tutorial.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-and-usage","title":"Limits and usage","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#are-there-any-limits-per-user-on-using-the-hpcc-resources","title":"Are there any limits per user on using the HPCC resources?","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#dev-node-limits","title":"Dev node limits","text":"<p>Each process on a dev-node is limited to 2 CPU hours. If you are running a multi-threaded program, the wall time limit would be (roughly) 2 hours divided by the number of threads.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-on-storage","title":"Limits on storage","text":"<ol> <li>Each user has up to 1 TB of storage for free and 1 million files, for each    of the home and research directories. Beyond 1 TB, the cost is $89 per TB    per year for MSU users.</li> <li>For scratch space (i.e. <code>/mnt/scratch/&lt;your_user_name&gt;</code>), 50 TB is the    maximum; more may be requested via contact    forms with center director approval.).</li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#limits-on-cluster-usage","title":"Limits on cluster usage","text":"<ol> <li>the longest wall time you can request is 7 days; </li> <li>the maximum number of CPU cores you can use is 1040\u00a0at any one time (see    SLURM variable <code>QOSMaxCpuPerUserLimit</code>), unless you have a larger buy-in and    your PI has requested that your buy-in account only run on the buy-in nodes;</li> <li>the maximum number of jobs that can be queued is 1000 and 520 running at any    one time (except in the scavenger queue); </li> <li>non-buyin users have a maximum of 500,000 CPU hours per year.</li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-would-like-to-know-more-about-the-dev-node-limit","title":"I would like to know more about the dev-node limit?","text":"<p>When you connect to any\u00a0of the HPCC's dev-nodes, you will see the following\u00a0message:</p> <p><code>processes on development nodes are limited to two hours of CPU time.</code></p> <p>The two hour\u00a0CPU time limit is for each process you run on that dev-node.\u00a0If one process uses CPU time greater\u00a0than 2 hours, then only that\u00a0process will be killed. You can, however, still connect to that dev-node, and run another process.\u00a0Additionally, if your process uses 100% CPU (1 core), it will be terminated in two hours. If your process uses 200% CPU (2 cores), it will be terminated in one hour, and so on.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-check-my-cpu-or-gpu-time-usage","title":"How do I check my CPU or GPU time usage?","text":"<p>Run the command <code>SLURMUsage</code> for both CPU and GPU. </p> <p>NOTE: This time usage does not include time that was submitted to a buyin node.</p> <ul> <li>If you would like to get full usage data, including all buyin usage, you can   run <code>sreport</code> to get the information for specific date ranges: <code>sreport job   SizesByAccount Users=$USER start=2023-01-01 end=now -t hour</code>. This report is   broken down by job size and all columns should be summed for the total usage   in hours. </li> <li>A detailed accounting report can be generated with <code>sacct -X --duplicates -u   $USER -S 2023-01-01 -E 2023-04-03 -o jobid,ncpus,elapsedraw,CPUTimeRaw</code>.   This output should be saved to a file and the CPUTimeRaw column summed for   total hours.  CPUTimeRaw is equal to ncpus * elapsedraw.</li> </ul>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-to-check-the-hpcc-node-usage","title":"How to check the HPCC node usage?","text":"<p>Users can see this information by simply running the <code>node_status</code> command on any dev node. We also offer a web-based dashboard at https://icer.msu.edu/dashboard.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#storage-and-files","title":"Storage and files","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quota","title":"Quota","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quota-issues-writing-to-research-spaces","title":"Quota issues writing to research spaces","text":"<p>Many users have reported problems copying or transferring files to their research space. Although their research space still has plenty of space, they still get the following error message:</p> <p>failed to ... Disk quota exceeded</p> <p>This problem may occur because you do not have your primary group set to match the research space or the folders which you copy or transfer files to have\u00a0incorrect group ownership or no set-group-ID. Please read the instructions for using a research space, in particular, point 5.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#quotafile-limit-exceeded-or-general-issues-related-to-writing-files-especially-in-home-directories","title":"Quota/file limit exceeded or general issues related to writing files (especially in home directories)","text":"<p>Begin by checking your storage usage with the <code>quota</code> command. Then compare with the results from running the <code>file-count</code> powertool:</p> <pre><code>module load powertools\nfile-count\n</code></pre> <p>For more detailed information including a count of files in each subdirectory, use</p> <pre><code>file-count --detail\n</code></pre> <p>If you find that you are over quota, please delete files, move them to another location (like a research space or if they are temporary, scratch space), or move them off of the HPCC. If this resolves the issue, then you may consider keeping your files in a different location or asking for more space in your home directory.</p> <p>If you don't see a change in your quota, the number reported by <code>quota</code> and <code>file-count</code> are extremely different, or your quota is showing unrealistic numbers like negative or extremely large file counts, please contact ICER as this is likely the result of an unresolved issue with one of the HPCC's storage systems.</p> <p>Sometimes exceeding your quota can stop you from being able to login or access systems like OnDemand because they require writing to a small file. If this is the case, please contact ICER</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#my-files-in-the-scratch-space-are-gone","title":"My files in the scratch space are gone?","text":"<p>Files in scratch are automatically purged if the last changed time is older than 45 days. Note that the scratch spaces are not intended for long-term storage. Files saved in scratch have no back-up.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-copy-files-fromto-my-ms-one-drivegoogle-drive","title":"How do I copy files from/to my MS One Drive/Google Drive?","text":"<p>Rclone is currently installed on the HPCC. This software supports research in the cloud and helps HPCC users to sync files and directories between MSU\u2019s HPCC and their cloud storage, including OneDrive and Google Drive. Please refer to Rclone</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-hpccs-data-protection-policy","title":"What is HPCC's data protection policy?","text":"<p>All of the HPCC's shared storage systems are protected against individual drive and storage node failure (using RAID and highly availabile, active-active servers.) </p> <p>We maintain an offsite disaster recovery system for users' home and research directories. We do not archive users' scratch spaces nor the persistent 'nodr' space. </p> <p>Our goal is to maintain hourly snapshots for the last 24 hours and 60 days of file history on the disaster recovery servers. However, when there is a significant amount of data written, there may be a delay in copying updated data to the disaster recovery servers. Users that have hard requirements for should consider using MSU's Data Storage Finder.</p> <p>Users may request older versions of their files via the contact forms.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#does-hpcc-offer-a-cheaper-long-term-archiving-plan","title":"Does HPCC offer a cheaper long-term archiving plan?","text":"<p>We do not. However, MSU offers the Data Storage Finder (https://data-storage-finder.tech.msu.edu, on-campus only). There are several possible options for data archiving.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#submitting-jobs-and-running-code","title":"Submitting jobs and running code","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-buyin-account-do-i-need-to-specify-it-when-i-submit-jobs","title":"I have a buyin account, do I need to specify it when I submit jobs?","text":"<p>No, unless your PI has requested that it be opt-in instead of the default. When submitting a job without specifying an account, your default account is used. You can check your default account using the \"buyin_status -l\" command; buyin user's default is their buyin account. We recommend you read this if you have purchased buyin nodes.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#do-you-support-running-gpu-jobs","title":"Do you support running GPU jobs?","text":"<p>Yes. There are three GPU dev-nodes and a series of compute nodes in the cluster; see Cluster resources.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-does-the-message-nodes-required-for-job-are-down-drained-or-reserved-for-jobs-in-higher-priority-partitions-mean-after-my-job-is-submitted","title":"What does the message \"Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions\" mean after my job is submitted?","text":"<p>Once a job is submitted the scheduler adds it to the calculations and continues to update the status of the job as the system works. The status for a job will reflect the current state of the scheduler, so you will see this message update once the scheduler has found a place to put the job. There are always some nodes which are down or drained in the cluster due to normal maintenance, but the \"reserved for jobs in higher priority partitions\" is the important part, and simply indicates that the scheduler has not yet found a time to schedule the job. This will update as the scheduler continues to function.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-i-get-an-illegal-instruction-error","title":"Why did I get an \"Illegal Instruction\" error?","text":"<p>This is usually because a program was compiled on a newer CPU architecture (e.g., intel18) but then run on an older one (e.g., intel14). Our system has a range of CPUs, and the newest versions support new instructions not available on the older CPUs. One short-term fix is to run programs on the same CPU that they were compiled on. Based on our experience, this error has occurred only on intel14 nodes and therefore you need to avoid them. That is, for dev-node testing, pick one from dev-intel16, dev-intel16-k80 and dev-intel18. For job submission, add <code>#SBATCH--constraint=\"[intel16|intel18]\"</code>in your SLURM script.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#why-do-i-get-a-module-not-found-error-in-my-slurm-output","title":"Why do I get a <code>module: not found</code> error in my slurm output?","text":"<p>Job scripts that start with #!/bin/sh will result in errors like  <pre><code>/var/lib/slurmd/jobXXXXXXXX/slurm_script: XX: module: not found. \nPlease change this line to #!/bin/bash.\n</code></pre> /bin/sh is a symbolic link in modern Linux distributions and does not always link to the same shell.  A better method on current Linux distributions is to explicitly call the bash shell with #!/bin/bash in your script if needed.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#software-and-modules","title":"Software and modules","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-want-to-install-software-packages-what-should-i-do","title":"I want to install software packages, what should I do?","text":"<p>The HPCC has a lot of software installed already. Search for the software you  want to install using <code>module spider &lt;software name&gt;</code>, then follow the instructions provided by the output to use <code>module load</code> and use the software.  See our documentation on this subject here. We have additional documentation on the module system  here.</p> <p>If the software is not present, you can submit a ticket. However, we encourage  users to install software on their own, if possible. The HPCC has provided  numerous versions of compilers and libraries which should accommodate the vast  majority of software across different fields.</p> <p>If you are thinking of requesting the system-wide installation of a piece of software, we strongly recommend you check the following factors when submitting a request for software installation:\u00a0</p> <ol> <li> <p>How popular is the software? If it is not a popular software, are there    other users on HPCC who would also be using it? If you are the only one    using it, we would recommend it be installed in your home directory.\u00a0</p> </li> <li> <p>What type of license agreement does the software have? Some software    licenses may restrict use even when they are free. Examples include software    with export control, specific end-user license agreement, etc. When software    licenses restrict use, we typically recommend the user directly make an    agreement with the software provider to obtain and install it in their home    directory. If it will be used by a group of people, HPCC system    administrators can help with setting up the group access in compliance with    the license agreement.\u00a0</p> </li> <li> <p>Is the software well maintained and up-to-date? If the software you wish    to install is legacy software or is not being well maintained, chances are    its installation will require an older version of its dependencies as well.    The effort to install this software may then be greater than the effort    required to find an up-to-date software with the same, similar, or even    better functionality. It may be time to consider transitioning to using a    newer software.\u00a0\u00a0</p> </li> <li> <p>Is the software available through EasyBuild with an ICER supported    toolchain? ICER uses a tool called EasyBuild to    install software that is provided via \"recipe\" files or EasyConfigs. See the    entire list of available    EasyConfigs.    Note that software is installed with a \"toolchain\" (see our EasyBuild    tutorial    for more information). ICER only officially supports software installed    under the following toolchains or    subtoolchains    (including <code>gfbf</code>, <code>gompi</code>, <code>iimpi</code>, <code>iimkl</code>):</p> <ul> <li><code>foss/2022b</code></li> <li><code>foss/2023a</code></li> <li><code>foss/2023b</code></li> <li><code>intel/2022b</code></li> <li><code>intel/2023a</code></li> <li><code>intel/2023b</code></li> </ul> </li> </ol>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#why-did-my-module-load-command-output-errors","title":"Why did my \"module load\" command output errors?","text":"<p>There are many reasons that errors occur when you try loading a module. However, the most common cause is that you have forgotten to run <code>module purge</code>. Sometimes, <code>module spider</code> can also fail to find the module. Most likely it's because your personal module cache is out of date. To clear it, run <code>rm ~/.cache/lmod/spider*</code>.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-should-i-do-when-i-cannot-load-modules","title":"What should I do when I cannot load modules?","text":"<p>See How to find and load software modules.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#the-module-command-is-missing-in-vs-code","title":"The <code>module</code> command is missing in VS Code","text":"<p>Try running <code>source /etc/profile</code> on login. See also this issue on VS Code's GitHub.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#what-is-powertools","title":"What is powertools?","text":"<p>The powertools module is a collection of software tools and examples that allows researchers to better utilize HPC systems. Powertools was created to help advanced users use the HPCC more effectively. To learn more about powertools, run the command <code>powertools</code>.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#ondemand","title":"OnDemand","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#the-ondemand-job-composer-doesnt-work","title":"The OnDemand job composer doesn't work","text":"<p>The OnDemand server is in the process of being upgraded to match the new operating system. Until then, the job composer is not functional. Please contact us for help writing and submitting job scripts in the meantime.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#when-i-use-the-ondemand-interactive-desktop-i-get-the-error-the-panel-encountered-a-problem-while-loading-indicatorappletcompletefactoryindicatorappletcomplete-do-you-want-to-delete-the-applet-from-your-configuration","title":"When I use the OnDemand Interactive Desktop, I get the error \"The panel encountered a problem while loading 'IndicatorAppletCompleteFactory::IndicatorAppletComplete'. Do you want to delete the applet from your configuration?\"","text":"<p>Click the \"Delete\" option, and this error will not return in the future.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-cant-open-firefox-from-an-interactive-desktop-in-ondemand","title":"I can't open Firefox from an Interactive Desktop in OnDemand","text":"<p>Run <code>mv ~/.mozilla ~/.mozilla_backup</code>.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#python-and-conda","title":"Python and Conda","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-use-python-on-the-hpcc","title":"How do I use Python on the HPCC?","text":"<p>There are two methods: users can install their own version of Python with Conda or use the versions of Python installed on the HPCC system. See here.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-have-a-python-conflict-what-should-i-do-to-resolve-it","title":"I have a Python conflict. What should I do to resolve it?","text":"<p>Upon login to a dev-node, a default module list will load automatically. Since Python/3.6.4 is included in the list, it can interfere with a user's conda environment. As a consequence, your program may not be able to find packages installed in your conda environment even if it has been activated. In other words, the program still picks up Python/3.6.4 in the module system. The solution is to run <code>module unload Python</code> before activating the conda environment.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#how-do-i-deactivate-conda-base-environment","title":"How do I deactivate Conda base environment?","text":"<p>Many users have reported that after a local installation of Conda on the HPCC, their login prompt changes\u00a0to something starting with <code>(base) -bash-4.2$</code>. This is because conda activates the default environment, <code>base</code>, upon startup. To disable this behavior, which often results in conflicts with system defaults, users can run the following command:</p> <pre><code>conda config --set auto_activate_base False\n</code></pre>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-start-a-jupyter-notebook-through-ondemand-but-my-job-will-not-start-or-will-not-recognize-my-conda-environment","title":"I tried to start a Jupyter Notebook through OnDemand, but my job will not start or will not recognize my Conda environment","text":"<p>All Conda environments used with the Jupyter Notebook OnDemand app must have Jupyter installed. Without this, the OnDemand job status will stay stuck on </p> <p>Your session is currently starting... Please be patient as this process can take a few minutes.</p> <p>before moving to</p> <p>For debugging purposes, this card will be retained for 6 more days</p> <p>without giving the chance to start the notebook. Depending on the setup, the job may start, but the environment will not be properly recongized and the app will fall back to the default version installed on the HPCC.</p> <p>To install Jupyter in your Conda environment on the command line, activate it first by running</p> <pre><code>module load Conda/3\nconda activate &lt;environment_name&gt;\n</code></pre> <p>and then run</p> <pre><code>conda install jupyter\n</code></pre>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-tried-to-use-python-matplotlib-to-plot-but-got-an-error-of-no-module-named-_tkinter","title":"I tried to use python matplotlib to plot, but got an error of \"No module named '_tkinter'\"","text":"<p>If you use the default python module (<code>/opt/software/Python/3.6.4-foss-2018a/bin/python</code>) on a dev-node, you need to load the Tkinter module before using python in order to proceed without errors. Run: <code>module load Tkinter/3.6.4-Python-3.6.4</code></p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#r-and-rstudio-server","title":"R and RStudio Server","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#when-i-start-rstudio-server-all-i-see-is-a-gray-blank-screen","title":"When I start RStudio Server, all I see is a gray blank screen.","text":"<p>Open a command line and run</p> <pre><code>mv .local/share/rstudio .local/share/rstudio.backup\n</code></pre> <p>This will move your RStudio configuration files to a backup. Note that this will likely reset your RStudio session, so you may need to reopen previous projects and files, and could lose any unsaved work. See this Lab Notebook for more details.</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-cant-install-the-r-package-matrix-or-other-packages-that-need-it-like-ggplot2","title":"I can't install the R package \"Matrix\" or other packages that need it like \"ggplot2\"","text":"<p>The R package <code>Matrix</code> (which is a dependency for many other packages including <code>ggplot2</code>) is incompatible with versions of R earlier than 4.4.0 (i.e., all of the versions installed on the new operating system. We recommend using the <code>R-bundle-CRAN</code> module instead of <code>R</code> which includes a pre-installed version of <code>Matrix</code>. If you need to install it yourself using <code>install.packages</code>, use the command</p> <pre><code>install.packages(https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz, repos=NULL, type=\"source\")\n</code></pre>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#getting-help","title":"Getting help","text":"","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#can-you-keep-me-posted-on-the-current-status-of-the-hpcc","title":"Can you keep me posted on the current status of the HPCC?","text":"<p>Yes. Users are encouraged to follow the HPCC Announcements blog to keep updated on the status of HPCC (such as scheduled downtimes and urgent notices).</p>","tags":["reference","FAQ"],"boost":5},{"location":"Frequently_Asked_Questions_FAQ_/#i-am-looking-for-help-to-troubleshoot-my-problem-how-do-i-share-my-codefiles-with-you","title":"I am looking for help to troubleshoot my problem. How do I share my code/files with you?","text":"<p>We do not go to your directory to view files or test your code for that matter. Please send your files along with your reply to the ticket email.</p>","tags":["reference","FAQ"],"boost":5},{"location":"GATK4/","title":"GATK4","text":"<p>Be sure to read this Quick Start before using GATK4. In particular, note the following statement from the developers:</p> <p>Once you have downloaded and unzipped the package (named <code>gatk-[version]</code>), you will find four files inside the resulting directory:</p> <p><code>gatk</code> <code>gatk-package-[version]-local.jar</code> <code>gatk-package-[version]-spark.jar</code> <code>README.md</code></p> <p>Now you may ask, why are there two jars? As the names suggest, <code>gatk-package-[version]-spark.jar</code> is the jar for running Spark tools on a Spark cluster, while <code>gatk-package-[version]-local.jar</code> is the jar that is used for everything else (including running Spark tools \"locally\", i.e. on a regular server or cluster).</p> <p>So does that mean you have to specify which one you want to run each time? Nope! See the gatk file in there? That's an executable wrapper script that you invoke and that will choose the appropriate jar for you based on the rest of your command line. You could still invoke a specific jar if you wanted, but using gatk is easier, and it will also take care of setting some parameters that you would otherwise have to specify manually.</p> <p>On the HPCC, after login to a dev-node, run: <code>module load GATK/4.0.5.1-Python-3.6.4</code>. As a tip, if you happen to run a <code>module purge</code> command in the middle of your work, and want to go back to the original login environment, please type the command: <code>exec bash -l</code></p> <p>A simple test on the HPCC is provided below.</p> <pre><code>module load GATK/4.0.5.1-Python-3.6.4\ngatk --java-options \"-Xmx8G\" HaplotypeCaller -R /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleFASTA.fasta -I /opt/software/GATK/3.3-0-Java-1.7.0_80/resources/exampleBAM.bam -O gatk_test.vcf\n</code></pre>"},{"location":"GPU_resources/","title":"The HPCC's GPU Resources","text":"<p>The HPCC offers several generations of GPUs as noted in the general Cluster Resources page. More information about these devices is provided in the table below. The cluster types that each GPU is associated with correspond to the cluster types listed in the Cluster Resources table.</p> GPU Cluster Type Number per Node GPU Memory Architecture Compute Capability Connection Type NVLink <code>a100</code> amd21* 4 81920 MB Ampere 8.0 SXM Yes intel21 4 40960 MB Ampere 8.0 PCIe No <code>v100</code> amd20 4 32768 MB Volta 7.0 PCIe Mixed intel18 8 32768 MB Volta 7.0 PCIe Yes <code>k80</code> intel16 8 12206 MB Kepler 3.7 PCIe No <code>k20</code> intel14 2 4743 MB Kepler 3.5 PCIe No <p>*The amd21 cluster contains some nodes that belong the to Data Machine.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#architecture-compute-capability","title":"Architecture &amp; Compute Capability","text":"<p>Currently, all of the HPCC's GPUs are manufactured by NVIDIA. They are designed following multiple architectures. Knowing a GPU's architecture aids in researching their technical specifications. A GPU's architecture is abbreviated in it's name; for example, the V100 GPUs follow the Volta architecture. </p> <p>Specific architectures and models of GPUs are able to meet certain compute capabilities (CC): sets of features that applications can leverage when executing on that GPU. Newer GPUs offer more advanced features and therefore adhere to a newer version of NVIDIA's compute capabilities. An explanation of what features are available for each compute capability can be found on both as part of the CUDA documentation (CC &gt; 5.0 only) and compiled on Wikipedia.</p> <p>Developers may use the CUDA programming language to utilize our GPUs in their software applications. See our page on Compiling for GPUs for more information on which versions of CUDA may be used for each of the HPCC's GPUs and their respective compute capabilities.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#connection-type","title":"Connection Type","text":"<p>Most of the HPCC's GPUs communicate with the CPUs of their host node via the PCIe (Peripheral Component Interconnect Express) bus. This bus is the primary channel by which data and instructions are transferred to and from the GPU. As such, the speed of this bus can affect the speed of GPU applications where large amounts of data transfer are a concern. In contrast, the A100 GPUs associated with the amd21 clusters are connected using SXM (Server PCI Express Module) sockets which offer higher connection speeds. Research the specifications of the particular GPU you are planning to use to learn more specifics about their bus's bandwidth.</p>","tags":["reference","GPU"]},{"location":"GPU_resources/#nvlink","title":"NVLink","text":"<p>While PCIe and SXM refer to the connection between the CPU and GPU, some of the HPCC's V100 and A100 GPUs are also connected to each other using NVIDIA's NVLink technology. NVLink allows GPUs to directly share data with each other. Without NVLink, transferring data from one GPU to another would require that the data first pass through the CPU. Using the CPU as a data transfer \"middleman\" adds to overall time the transfer takes and may also delay the CPU from communicating additional data and instructions to the GPUs. If you plan to use multiple GPUs for your job, consider requesting resources that support NVLink as indicated in the table above.</p> <p>Some of the amd20 nodes support NVLink while others do not. You can check whether or not a given node supports NVLink by requesting a job on that node and connecting to it. Specific nodes can be requested with the <code>-w</code> or <code>--nodelist</code> option; see the list of job specifications for more. Then, once connected, run <code>nvidia-smi nvlink -s</code> to check the status of the node's NVLink connection.</p>","tags":["reference","GPU"]},{"location":"Gaussian_Access/","title":"Gaussian Access","text":"<p>To obtain access to this software on HPCC complete\u00a0and sign the appropriate\u00a0Gaussian Confidentiality Agreement. Please ensure that all required sections are filled out legibly or else the form will be returned to you.</p> <ul> <li>Research Group Leader Form\u00a0\u2014\u00a0Only one form needs     to be completed per research group.</li> <li>User/Researcher Form\u00a0\u2014 Each user of the software     needs to complete this form.\u00a0Please ensure that your group leader     has completed the Research Group Leader form before submitting     this.\u00a0</li> <li>Coursework Form\u00a0\u2014 This form is to be completed by     the each student enrolled in an MSU course. Please note that the     course instructor should have completed the Research Group Leader     Form above. Access ends after the class is completed.\u00a0</li> </ul> <p>Email the completed form to\u00a0general@rt.hpcc.msu.edu.\u00a0 Please make the subject of your email, \"Gaussian Confidentiality Agreement.\" Your completed agreement will then be sent to Gaussian for the final approval. Once approved, the HPCC System Admin team will act upon the request and grant access.</p> <p>Note: This process can take up to two weeks from the date of submission of the Confidentiality Agreement to Gaussian giving approval of the software so we ask that you please be patient. Without a personally signed Confidentiality Agreement, you will not be granted access to Gaussian.</p>"},{"location":"Gaussian_Job_Script/","title":"Gaussian Job Script","text":"<p>Here is a simple job script <code>g16.sb</code> for running Gaussian job <code>g16.com</code>:</p> <p>g16.sb</p> <pre><code>#!/bin/bash --login\n\n#SBATCH \u2013-job-name=GaussianJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=4\n#SBATCH --mem=7G\n#SBATCH \u2013-time=00:10:00 \n\nInputFile=g16.com\nOutputFile=g16.log\n\nmodule load Gaussian/g16 powertools\n# GAUSS_SCRDIR=&lt;your preferred Gaussian scratch space&gt;\n# mkdir -p ${GAUSS_SCRDIR}\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n### write job information to SLURM output file\nscontrol show job $SLURM_JOB_ID \n\n# Print out resource usage  \njs -j $SLURM_JOB_ID           ### powetools command\n</code></pre> <p>where the Gaussian input file <code>g16.com</code> can be found from the\u00a0previous section Running Gaussian by Command Lines.</p> <p>For the resource request (<code>#SBATCH</code> lines) above, since Gaussian can only run in parallel with shared memory in HPCC system, only 1 task (with 1 node) is requested in line 2. The number of CPUs requested in line 3 is the same as the setting of \"<code>%NProcShared</code>\" (<code>=4</code>) in the Gaussian input file. The memory request in line 4 should be larger than the setting of \"<code>%Mem</code>\" (<code>=5GB</code>) in the Gaussian input file in case the job runs out of memory. Please also make sure the walltime request in line 5 is longer enough to finish the job.</p> <p>In the command line, you need to make sure Gaussian/g16 is loaded as in line 10. If you would like to use scratch directory other than <code>/mnt/scratch/$USER</code> for the Gaussian scratch files, you could set up a different one with line 11 and 12. The calculation of the Gaussian job is executed in line 14 with input file <code>g16.com</code> and output file <code>g16.log</code>. Once the calculation is done, line 17 and 20 will be executed to print out the job information and resource usage respectively to the SLURM output file ( with file name: <code>slurm-&lt;JobID&gt;.out</code>).</p>"},{"location":"Gaussian_Job_with_Checkpointing_Run/","title":"Gaussian Job with Checkpointing Run","text":"<p>For running a large system with Gaussian, it usually takes a long time and many resources to complete. It is a good idea to set up checkpointing so the calculation can keep going in case of any interruption due to walltime limit or possible system malfunction. The checkpointing function can save a snapshot of a Gaussian running state so it can restart from the previous calculation. Users can also divide a long-time job into many 4-hour short jobs since jobs with walltime less than or equal to 4 hours can use the buy-in nodes (55% of all nodes) on the HPCC.</p> <p>In order to have an appropriate checkpointing run with Gaussian, an unified read-write file setting (<code>%RWF</code>) should be in the Link 0 section of the input file. An example <code>water.gjf</code> is in the following:</p> <p>water.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=water.rwf\n%NoSave\n%chk=water.chk\n#P opt b3lyp/aug-cc-pVTZ\n\nwater molecules\n\n0 1\nO   -2.12123400  1.99409800 -1.27381200\nH    1.52438600  0.53672100  0.67508800\nH    1.76493000 -0.81527300 -0.18137000\nO   -1.12977500 -0.31430400 -0.37860700\nH   -1.76492800 -0.81528500  0.18137200\nH   -1.52439700  0.53670800 -0.67510100\nO    2.89125300 -1.69896600 -1.06351900\nO    1.12976700 -0.31428900  0.37859300\nH    2.99568600 -1.73945400 -2.01677200\nH    3.39746100 -2.42787400 -0.69708600\nO   -2.89123000 -1.69896400  1.06353700\nH   -2.99563400 -1.73945600  2.01679300\nH   -2.43456700  2.07972500 -2.17761600\nH   -2.58174600  2.66131900 -0.75942800\nH   -3.39743400 -2.42788000  0.69711700\n</code></pre> <p>The input file requests geometry optimization of 5 water molecules with a very large basis set <code>aug-cc-pVTZ</code>. It will take about 25 CPU hours to finish the whole calculation. We have the setting on <code>%RWF</code> which specifies <code>water.rwf</code> file for the checkpointing function besides the <code>water.chk</code> file. Since the specification <code>%RWF</code> is placed before the <code>%NoSave</code> line, the rwf file will be deleted if the calculation is normally completed without any error.</p> <p>In order to have several restarts running after the first run stops, we can build a restart Gaussian input file <code>restart.gjf</code> simply as</p> <p>restart.gjf</p> <pre><code>%NProcShared=2\n%Mem=3GB\n%RWF=waters.rwf\n%NoSave\n%chk=waters.chk\n#P Restart\n</code></pre> <p>Since all information about the calculation is recorded in the rwf file, a line with \"Restart\" is enough for Gaussian to restart from the previous job. This restart input file can also be created by the commands:</p> <pre><code>grep '^%' waters.gjf &gt; restart.gjf\necho -e '#P Restart\\n' &gt;&gt; restart.gjf\n</code></pre> <p>where we simply \"<code>grep</code>\" the lines starting with \"<code>%</code>\" sign in <code>water.gjf</code> and put them in the Gaussian restart file with \"<code>#P Restart</code>\" line in the end.</p> <p>Now we need a job script to submit the Gaussian calculation. The script needs to keep submitting jobs to restart the previous calculation until it is completed. Here is a job script <code>water.sb</code> which can do the work:</p> <p>water.sb</p> <pre><code>#SBATCH \u2013-job-name=LongJob\n#SBATCH \u2013-ntasks=1\n#SBATCH \u2013-cpus-per-task=2\n#SBATCH --mem=5G\n#SBATCH \u2013-time=04:00:00\n\nmodule load Gaussian/g16 powertools\nOutputFile=\"water-${SLURM_JOBID}.log\"             # Gaussian output file name for each job\n\n# How many seconds before end of job to submit another\nBeforeEnd=300                                       # 5 minutes\n\n# The background script to keep job submission until calculation is completed\n(sleep $((4*60*60 - BeforeEnd))                     # sleep until the time before end of job\njs -j ${SLURM_JOBID}                                # print out resource usage\ncat ${OutputFile} &gt;&gt; water.log                      # collect Gaussian outputs into one file\necho -e \"\\n\\n====== Gaussian calculation on job ${SLURM_JOBID} stops. ======\\n\\n\" &gt;&gt; water.log\necho \"The Gaussian calculation has not completed. Submit another job to keep doing it.\"\nsbatch water.sb                                     # submit another job\nscancel ${SLURM_JOBID}  )&amp;                          # job stops if g16 command is not finished\n\n# Whether this is a restart job or not\nif [ -f water.rwf ] &amp;&amp; [ -f water.chk ]; then\n   InputFile=\"restart.gjf\"\nelse\n   InputFile=\"water.gjf\"\nfi\n\ng16 &lt; ${InputFile} &gt; ${OutputFile}\n\n# The following commands are not executed unless g16 command is completed.\n# Print out resource usage \njs -j $SLURM_JOB_ID           ### powetools command\n\ncat ${OutputFile} &gt;&gt; water.log \necho -e \"\\n\\n====== Gaussian calculation is completed on job ${SLURM_JOBID}. ======\\n\\n\" &gt;&gt; water.log\n</code></pre> <p>where a background script in <code>(---)&amp;</code> from line 14 to 20 is added to keep submitting job<code>s</code>.</p> <p>Once the job is started, the background script is running at the same time as the foreground script. The background script is in sleep for 3 hours and 55 minutes first. During this time, the foreground script runs the Gaussian calculation or restarts the previous calculation if the checkpointing files <code>water.rwf</code> and <code>water.chk</code> exist. After 5 minutes before the end of the job, the background is awake to print out the resource usage and Gaussian output. It submits another job and stops the current running job in line 19 and 20 if the g16 command in line 29 is not completed. If the g16 command is finished before the background script is awake, the job will keep executing all command lines after line 30 and finish. There will be no more jobs submitted.</p> <p>Since the rwf file usually takes a lot of file space, it is suggested to run checkpointing jobs in scratch space in case your home or research space is over quota. Users can create a directory in their scratch space. Copy all files (<code>water,gjf</code>, <code>restart.gjf</code> and <code>water.sb</code>) and submit the job script there. Please check your job status frequently. Make sure to copy necessary files back to your home or research directory from time to time since files on scratch will be purged if they have not been modified for 45 days.</p> <p>Note</p> <p>The time for running the background script needs to be longer than the time needed for a cycle of Gaussian analysis to avoid restarting from the point of privious run again. The checkpointing is done between cycles.  </p>"},{"location":"Gaussian_on_HPCC/","title":"Gaussian on HPCC","text":"<p>Here is the slides pdf for the Gaussian workshop.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/","title":"Getting started with Grace Hopper and Grace Grace","text":"<p>ICER has received four NVIDIA Grace Hopper systems that MSU researchers have purchased as well as one NVIDIA Grace Grace CPU system.</p> <p>All nodes are available in SLURM and are available to the research community under the same buy-in rules as the rest of our buy-in hardware. In particular, users that are not part of a node's buy-in are restricted to submitting jobs less than four hours.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#node-listing","title":"Node listing","text":"Cluster Type Node Count Processors Cores Memory Disk Size GPUs (Number) Node Name Grace Hopper 3 Grace CPU (Arm Neoverse v2) 72 480 GB 1.5TB GH200 96 GB (1) <code>nch-[000-002]</code> Grace Hopper 1 Grace CPU (Arm Neoverse v2) 72 480 GB 3.2TB GH200 96 GB (1) <code>nch-003</code> Grace Grace 1 Grace CPU (Arm Neoverse v2) 144 480 GB 3.5TB <code>ncc-000</code>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#differences-from-other-nodes","title":"Differences from other nodes","text":"<p>The Grace systems are currently considered \"beta\" with very minimal installations. Users should be aware:</p> <ol> <li>These are ARM-based (<code>aarch64</code>) systems. Existing code compiled for our <code>x86_64</code> nodes (including conda environments) will not work on this system. To see if an executable is compiled, use <code>file executablename</code>. If it mentions <code>x86_64</code> it is not compatible with the Grace Hopper systems.</li> <li>These nodes have slower-than-normal home directory and research space access. Researchers may wish to stage data or code to the node in <code>/tmp</code>. </li> <li> <p>Software pre-installed by ICER is different than what is available on other nodes. To access the <code>module</code> command and all software currently built for the Grace Hopper systems, run </p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash &amp;&amp; module use /opt/modules/all\n</code></pre> <p>Please note that you may see references to modules on the main cluster being inactive, as SLURM copies these references over before the line above deactivates them. These modules are non-functional on the Grace nodes.</p> </li> <li> <p>Before running code that has been compiled on the Grace nodes, you need to load the <code>Compiled</code> module after all other dependent modules. For example, assuming that a code is compiled using the <code>GCC/12.2.0</code>, <code>Cabana/0.6.1-foss-2022b-CUDA-12.1.1</code>, and <code>CUDA/12.1.1</code> modules, you should use the following lines to run your code:</p> <pre><code>module purge\nmodule load GCC/12.2.0 Cabana/0.6.1-foss-2022b-CUDA-12.1.1 CUDA/12.1.1\nmodule load Compiled\n\n./my-compiled-code\n</code></pre> </li> </ol> <p>We will update this page as we address these issues.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#submitting-jobs","title":"Submitting jobs","text":"<p>The primary mechanism to schedule jobs on Grace nodes is to use the SLURM option <code>--constraint NOAUTO:grace</code>.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#interactive-jobs","title":"Interactive jobs","text":"<p>To start an interactive job, use the template</p> <pre><code>salloc --constraint=NOAUTO:grace --time=3:00:00 --gpus=1 --cpus-per-task=72 --mem=10G\n</code></pre> <p>If you would like to avoid using your default shell environment (due to any of the potential incompatibilities, see above), you should use <code>srun</code> with option <code>--pty</code> and command <code>/bin/bash</code>,  e.g.,</p> <pre><code>srun --constraint=NOAUTO:grace --time=3:00:00 --gpus=1 --cpus-per-task=72 --mem=10G --pty /bin/bash\n</code></pre> <p>If you have buy-in access to a Grace node, you should additionally add the option <code>--account=&lt;buy-in-name&gt;</code>. By using the <code>--gpus:1</code> option, jobs are restricted to only running on Grace Hopper nodes. Removing this options allows jobs to run on any Grace node.</p>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#job-script-template","title":"Job script template","text":"grace_hopper_template.sb<pre><code>#!/bin/bash\n\n#SBATCH --constraint=NOAUTO:grace  # Only run on Grace nodes\n#SBATCH --time=3:00:00             # Run for three hours\n#SBATCH --gpus=1                   # Request one GPU (restricts to Grace Hopper)\n#SBATCH --cpus-per-task=72         # Request all CPUs on a Grace Hopper node \n#SBATCH --mem=10GB                 # Request 10GB of (non-GPU) memory\n\n# Gain access to the module system\nsource /cvmfs/software.eessi.io/versions/2023.06/init/bash &amp;&amp; module use /opt/modules/all\n\n# Load modules\nmodule load CUDA-Samples\n\n# Run code (GPU examples from CUDA-Samples)\nmatrixMul\nmatrixMulCUBLAS\n\n# Output debugging information\nscontrol show job $SLURM_JOB_ID\n</code></pre>"},{"location":"Getting_Started_with_Grace_Hopper_and_Grace_Grace/#see-also","title":"See also","text":"<p>Users may wish to refer to NVIDIA's documentation:</p> <ul> <li>NVIDIA Grace Docs </li> <li>NVIDIA Grace Hopper Docs</li> </ul>"},{"location":"Guidelines_for_Choosing_File_Storage_and_I_O/","title":"Guidelines for Choosing File Storage and I/O","text":"<p>HOME, RESEARCH and SCRATCH  are referred to as networked file systems.\u00a0Each node must go through the network switch to access these spaces. The LOCAL storage options at <code>/tmp</code> and <code>/mnt/local</code> are locally accessible in the hard drive of each node and are not affected by the network. All of these are larger than RAMDISK (<code>/dev/shm</code>) which is located inside the node\u2019s RAM. However, RAMDISK is the closest (and therefore fastest) storage location for files. Files stored here take up some of the node\u2019s memory space and are counted when Slurm calculates the memory a job is using.</p> <p>The table below provides detailed information about each type of storage\u00a0on HPCC. (<code>$USER</code> is your login username and GROUP is your research group name). Please use the table below to choose which file system is best for your job.\u00a0The HOME and RESEARCH systems are the  only systems with automatic offsite disaster recovery protection. The SCRATCH, LOCAL, and RAMDISK systems all have automatic purge policies. </p> <p>The \"nodr portion of HOME/RESEARCH\" column is similar to the \"HOME\" or \"RESEARCH\" columns, but refers to the portion of those directories that have been requested to move to <code>nodr</code> space. Note that this space is NOT protected by automatic DR protection! See the Home space or Research space pages for more information.</p> HOME RESEARCH nodr portion of HOME/RESEARCH SCRATCH LOCAL RAMDISK Primary Private files or data storage for each user Shared files or data storage for group users same as the standard HOME/RESEARCH Temporary large files or data storage for users and groups Temporary small files or data usage for job running same as LOCAL with very fast I/O Access location Automatic login <code>$HOME</code>\u00a0 or\u00a0<code>/mnt/home/$USER</code> <code>/mnt/research/GROUP</code> <code>/mnt/ufs18/nodr</code> <code>$SCRATCH</code> or <code>/mnt/scratch/$USER</code> <code>/mnt/local</code> or <code>/tmp</code> (at each node) <code>$TMPDIR</code> (used in a job as <code>/tmp/local/$SLURM_JOBID</code>) <code>/dev/shm</code> (at each node) Size 50GB upto 1TB, 1 million files, ($90/year for each additional TB) 50GB upto 1TB, 1 million files, ($90/year for each additional TB) as a portion of HOME or RESEARCH by user's request. No limit on the number of files 50TB and 1 million files ~400GB for intel14, ~170GB for intel16, ~400GB for intel18 Note: userIDs are restricted from consuming no more than 95% of the total available space in <code>/tmp</code> \u00bd of RAM I/O Best Practice low I/O using\u00a0 single or multiple nodes Same as HOME same as HOME or RESEARCH heavy I/O on files of large size using single or multiple nodes frequent I/O operations on many files in one node frequent and fast I/O operations on small files in one node Careful with Watch for quota. Avoid heavy parallel I/O. Same as HOME. In addition, need to set umask or file permission so files can be shared in group. Be aware of no automatic DR protection.\u00a0 Avoid frequent I/O on many small files(&lt; 1MB), such as untarring a tar file to create many small files in a short time. Move files to HOME or RESEARCH before purge period elapses. Need to copy or move files to HOME or RESEARCH before job completes. Only local access available. Users are not able to store files in one node and gain I/O access to them from other nodes. Same as LOCAL. Request extra memory in your job script so you'll have enough space for file storage. Command to check quota <code>quota</code> <code>quota</code> <code>quota</code> <code>quota</code> <code>#SBATCH --tmp=20gb</code> to reserve 20gb in <code>$TMPDIR</code>. \u00bd of the memory requested by job Disaster Recovery Yes Yes No No No No Purge policy No No No Yes. (Files not accessed or modified for more than 45 days may be removed) Yes (at completion of job) Yes (RAM may be reused by other jobs)","tags":["reference","files","I/O"]},{"location":"HPCC-Job-Submission-Workflow_40337480.html/","title":"HPCC Job Submission Workflow","text":"<p>Goal: Understand basic workflow for submitting jobs to the clusters.</p> <p>Task: Run hello.c on one core and one compute node.</p> <ul> <li>Login to HPCC</li> <li>Load the powertools module.</li> <li>Create a copy the helloworld getexample in your current directory.</li> <li>Change into the helloworld directory created.</li> <li>View the submission script in the helloworld directory.</li> <li>Compile the hello C program on the development node.</li> <li>Submit the script to the SLURM scheduler.</li> <li>Check the queue for your job.</li> <li>Examine your job</li> <li>Check the output of your job</li> <li>View the output of your job</li> </ul> <p>Answer \u00a0Expand source</p> <pre><code>#Login to HPCC\nssh -XY msu_netid@hpcc.msu.edu\nssh dev-intel18\n\n#Load powertools module\nmodule load powertools\n\n#Copy the helloworld getexample\ngetexample helloworld\nls -l\n\n#Change\u00a0into the helloworld directory\ncd helloworld\nls -l\n\n#View the submission script in the helloworld directory\ncat hello.sb\n\n#Compile the C program &lt;hello&gt; on the development node\ngcc hello.c -o hello\n\n#Submit the jobscript hello.sb to the SLURM scheduler\nsbatch hello.sb\n\n#Check the queue for your job\nqstat \u2013u $USER\n\n#Examine your job\nscontrol &lt;job_id&gt;\n\n#View the output of your job\nless &lt;slurm-jobid.out&gt;\n</code></pre>"},{"location":"HPC_Glossary/","title":"HPC Glossary","text":"<p>Program \u2013 code stored on a computer intended to fulfill a certain task</p> <ul> <li>There are many types of programs:<ul> <li>Part of the operating system and help computer function</li> <li>Fulfill a particular job are called applications</li> </ul> </li> <li>Typically stored on disk (g., hard drive)</li> <li>A program needs memory and various operating system resources     (g., peripheral interfaces) to run</li> </ul> <p>Operating System \u2013 manages all resources needed for a program (e.g., macOS)</p> <p>Process \u2013 program with all necessary resources loaded into memory</p> <ul> <li>When a program is run, it is loaded into memory which makes it     accessible for processing by the computer\u2019s central processing unit     (CPU)</li> <li>There can be multiple instances of a single program, and each     instance of that running program is a process</li> <li>Each process has a separate memory address space, which means that a     process runs independently and is isolated from other processes</li> <li>This independence of processes is valuable so that a problem with     one process cannot cause havoc with another process</li> </ul> <p>Central processing unit (CPU) - logical hardware unit capable of executing a single process (i.e., gets instructions then performs calculations)</p> <ul> <li>Made up of:<ul> <li>Processor is a device that processes program instructions to     manipulate data</li> <li>Socket is an array of pins that connect processor to     motherboard</li> </ul> </li> <li>Individual CPU processors now contain multiple cores for more     efficient multi-tasking and parallel computing<ul> <li>Core is the smallest hardware unit capable of performing a     processing task</li> <li>Ex: dual-core processor has two cores</li> </ul> </li> </ul> <p></p> <p></p> <p>Thread (of execution) is the smallest set of programmed instructions that can be managed independently by an operation system. In general, one thread is handled by one core.</p> <p>As video gaming popularity increased, so did the need for more computing power. To accomplish this a CPU can work with a graphics processing unit (GPU), usually found on a graphics card docked into the motherboard, to quickly render high-resolution images and video concurrently. A GPU gets its intense computing power from hundreds of smaller cores capable of crunch application data in parallel. Multiple GPUs can be installed on one graphics card or multiple graphics card can be installed in one node to further improve computation power through parallelism. After GPUs became popular for gaming, they were made fully programmable to be useful in processing big science data. The resulting general-purpose graphics processing unit (GPGPU) is used extensively in supercomputing to increase speed and improve analysis of scientific data.</p> <p>Node is a single computer comprised of 1+ CPUs, memory, network interfaces, etc.</p> <p>Cluster is a group of nodes networked together so that a program can run on them in parallel</p> <p>Parallel computing is an umbrella term describing the use of multiple computers or computers made up of multiple processors in combination to solve a single problem</p> <p>Within HPCC there are different types of nodes:</p> <ul> <li>Gateway nodes are nodes used to enter the computer system:<ul> <li>Login - login and non-intensive compute tasks (e., moving     files)</li> <li>rsync - data transfer to/from HPCC</li> </ul> </li> <li>Development (dev) nodes are nodes used to navigate file systems     and compile, test, and schedule heavy computational tasks (e.,     jobs)</li> <li>Compute nodes are clusters that perform scheduled jobs</li> <li>Accelerator nodes are nodes equipped with accelerated cards     (g., GPU or phi nodes)</li> </ul> <p>Secure Shell (SSH) - network protocols and implementing suite of utilities that provide a secure way to access and execute commands on a remote computer over an unsecured network</p> <p>Remote synchronization (rsync) - software utility for Linux systems that efficiently sync files and directories between two hosts or machines making ideal for transferring large files</p> <p>File system \u2013 tree-like directory organization for storing many files</p>"},{"location":"HPC_Glossary/#for-more-information-on-these-terms-check-out-the-following-videos","title":"For more information on these terms, check out the following videos:","text":"<p>What is ICER\u2019s HPCC (11min)</p> <p>HPCC System Layout (7min)</p>"},{"location":"HPC_s_entire_layout_at_ICER/","title":"The HPCC's Layout","text":"<p>The HPCC is comprised of three different kinds of nodes: the \"gateway\" and \"rsync\" entry nodes, development nodes, and compute nodes. In a typical workflow, users will connect to the HPCC through the an entry node, then connect to a development node to compile and test code before submitting jobs to the SLURM queue to be run on a compute node. This workflow is demonstrated in the diagram below. </p> <p>Each node type is explained in more detail in the following sections. Information on the HPCC's filesystems is available within a separate section.</p> <p></p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#entry-nodes","title":"Entry Nodes","text":"<p>The gateway and rsync nodes are the only nodes directly accessible over the internet. Users connect to these nodes from their personal computers using <code>ssh</code> before accessing other parts of the HPCC.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#gateway-nodes","title":"Gateway Nodes","text":"<p>These nodes are the default accessed via <code>ssh &lt;username&gt;@hpcc.msu.edu</code> as in the top fork of the diagram above. The gateway nodes are not meant for compilig or running software, accessing the scratch space, or connecting to compute nodes. Users should only use the gateway nodes to <code>ssh</code> to development nodes. Alternatively, users may set up SSH tunneling to automate the process of passing through the gateway to a development node.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#rsync-nodes","title":"Rsync Nodes","text":"<p>These nodes are accessed via <code>ssh &lt;username&gt;@rsync.hpcc.msu.edu</code> as in the bottom fork of the diagram above. Unlike the gateway nodes, the scratch file system is accessbile from the rsync nodes. This is because these nodes are primarily intended for file transfer. Large amounts of data should be transferred via the rsync nodes to avoid slowing down the gateway nodes.</p> <p>These nodes are named for the popular command line file transfer utility <code>rsync</code>. Users can use this utility to transfer files via the rsync gateway by following this command pattern: <code>rsync &lt;local path&gt; &lt;username&gt;@rsync.hpcc.msu.edu:&lt;remote path&gt;</code>. Other commands such as <code>scp</code> may also be used with the rsync gateway, or a GUI such as MobaXterm may be used instead.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#development-nodes","title":"Development Nodes","text":"<p>From the gateway node, users can connect to any development node to compile their jobs or and run short tests. They may also access files on the scratch file system. Jobs on the development nodes are limited to two hours of CPU time. More information is available on the development node page.</p> <p>Each development node is configured to match the compute nodes of the same cluster. If you would like your job to be able to run on any cluster (as is the default for the queue; see the section on Automatic Job Constraints) you should not compile with  architecture-specific tuning (e.g. <code>-march</code> or <code>-x</code>).</p> <p>Warning</p> <p>Code compiled on older development nodes (dev-intel14 and dev-intel14-k20) may have errors when running on the latest clusters due to an outdated instruction set. To avoid this, compile your code on a newer development node or specify <code>--constraint=intel14</code> in your SLURM batch script.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#compute-nodes","title":"Compute Nodes","text":"<p>ICER maintains several clusters worth of compute nodes. Users submit jobs to the SLURM scheduler which assigns compute nodes based on the resources requested. </p> <p>A user may see which nodes their job is running on using <code>squeue -u &lt;username&gt;</code>. Not providing a username to <code>squeue</code> will show all jobs currently running on the system. Users may <code>ssh</code> directly to a compute node only if they have a job running on that node. See our page on connecting to compute nodes for more.</p>","tags":["explanation"]},{"location":"HPC_s_entire_layout_at_ICER/#comparison-to-a-personal-computer","title":"Comparison to a personal computer","text":"Laptop/Desktop HPCC Clusters Number of Nodes 1 979 Sockets per node 1 2, 8 Cores per node 4, 8, or 16 20, 28, 40, 128 or 144 Cores total 4, 8, or 16 50,084 Core Speed 2.7 - 3.5 ghz 2.5-3 ghz RAM memory 8, 16 or 32 GB 64, 128, 92, 500 GB or 6TB File Storage 250, 500 GB or 1TB 1TB(Home), 50TB(Scratch) Connection to other computers Campus ethernet1 Gbit/sec \"Infiniband\" 100 Gbit/sec Users 1 ~2,000 Schedule On Demand 24/7 via queue","tags":["explanation"]},{"location":"HTSeq/","title":"HTSeq","text":"<p>HTSeq is a Python package for analysis of high-throughput sequencing data. It's available upon a fresh login to a dev-node. For example, you can call command <code>htseq-count</code> directly without needing to load specific modules. This is because HTSeq is installed to the default Python distribution (you can run module list\u00a0to see the list of modules loaded by default).</p> <p>If you happen to run a module purge\u00a0command in the middle of your work, you need to load the default Python back again in order to use HTSeq. To do so:</p> <pre><code>module purge\nmodule load GCC/6.4.0-2.28 OpenMPI/2.1.2 Python/3.6.4\n</code></pre> <p>Tip: if you want to go back to the original shell environment immediately after your login, you can run: <code>exec bash -l</code></p>"},{"location":"Home_Space/","title":"Home Space","text":"<p>Each user account is given a home space for personal file storage located at <code>/mnt/home/$USER</code>, where <code>$USER</code> is the environment variable of the user's login name. Alternatively, it can be accessed at <code>~/</code>, though this shorthand may not work in some scripts. By default,\u00a0it is only accessible to the user.  It is often referred to as the \u201chome directory\u201d since this is the beginning directory after login of any HPCC node.</p> <p>Every home space starts with a 50 GB limit for file storage space and can not contain more than 1 million files. To check the quota and used space of your home directory, see the Space quota section.  You can request to increase your quota up to 1TB by completing Quota Increase Request form.  Storage space greater than 1TB is available for an annual fee paid through a MSU financial account. Users can find the fee and submit their request by completing the Large Quota Increase Request form.   If you would like to store more than 1 million files in your home space, please refer to the section Limit on number of files.</p> <p>All home directories are stored in the IBM General Parallel File System (GPFS). It is automatically backed up except files saved in the <code>nodr</code> space. To restore any file from backup, please submit a ticket and let us know the paths to the files or the directory with the time frame you would like them restored.</p> <p>For the system security and user data privacy, we recommend that users do NOT open home directory access permission to others. When you report an issue about files saved in home, please attach them to your message for reference. ICER staff cannot access any files or directories in your home directory.</p> <p>Warning</p> <p>Currently our home file system check quota function will sometimes cause a user's directory\u00a0over the quota due to incorrect calculation of used space.\u00a0 If you see this please open a ticket and we will work with you to temporarily increase your quota.\u00a0 We continue to work with our vendor to correct this issue.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#space-quota","title":"Space quota","text":"<p>The only way to get quota information of home space is to run the command <code>quota</code>:</p> <pre><code>$ quota\nhome directory: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n/mnt/home/$USER  50G      32G     18G       64%       1048576   432525    616051    59%\n</code></pre> <p>where all file spaces accessible to the user are listed, including home, research, and scratch. In each space, the information of quota, usage and availability on space size and number of files can be found. If \"Free\" or \"Available\" is a negative value (such as the \"Space Available\" column in the above example), the usage is over the quota, please remove, transfer or compress some files so the used space or the file count can be lower than the \"Quota\" value.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#actual-disk-usage-du-different-from-quota-results","title":"Actual disk usage (<code>du</code>) different from quota results","text":"<p>The GPFS system set the smallest file block size to 64KiB. This means that files between 2KiB and 64KiB will occupy 64KiB of space. This causes space usage to be greatly inflated for users with large amounts of small files. One suggested solution would be to compress many small files into one large file (larger than 64KiB). In this way, the number of files can also be reduced. If you still have any difficulty, a temporarily larger quota can be requested if your quota is at 1TB with many small files.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#limit-on-number-of-files","title":"Limit on number of files","text":"<p>Besides the quota on the size of space, users are also limited to 1 million files in their home or research directory. We need to set this limit because, with a great number of files, the file system will spend too much time on backups to be able to function normally. If possible, users can compress many files into one to reduce the file number.</p> <p>If users do not wish to have the limit, they can request to have a portion of their home quota (or research space) moved under <code>nodr</code> space (<code>/mnt/ufs18/nodr/</code> or <code>/mnt/ufs18/nodr/research/</code>) by submitting a  help ticket  with the drop-down subject 'other' and secondary subject 'nodr request'.  There is no limit on the file count, but there is no backup of these files either.  Users will be responsible for their own backup of files in the <code>nodr</code> space.</p> <p>By default, one half of the current home quota is assigned to the requested <code>nodr</code> space. The original quota under <code>/mnt/home/</code> (or <code>/mnt/research/</code>) is then reduced to half the original quota so the total space quota remains the same. A different size of <code>nodr</code> space can also be assigned based on the user's request. Once this space is created, the path and the quota information on both home and nodr space  can be found using <code>quota</code> command mentioned above.</p>","tags":["explanation","quota","files"]},{"location":"Home_Space/#backups","title":"Backups","text":"<p>All home space files are periodically, automatically backed up (except  those files that a user has opted to store in a specially requested  <code>nodr</code> space).\u00a0To access file backups, please submit a  help ticket containing the file paths and the period, i.e. the time frame, from  which the files should be restored.  </p>","tags":["explanation","quota","files"]},{"location":"How_Jobs_are_Scheduled/","title":"How Jobs are Scheduled","text":"","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#schedulers","title":"Schedulers","text":"<p>SLURM schedules jobs in two ways: the main scheduler and the backfill scheduler. The main scheduler constantly tries to start high priority jobs. The backfill scheduler considers all jobs, and starts any jobs that won't defer the start time of a higher priority job.</p> Scheduler Function When it Runs Run Time Main Launches high priority jobs that can start immediately. Stops evaluating jobs once it encounters a job that cannot be started.\u00a0 About every 2 seconds 0.08-2 seconds Backfill Evaluates the entire queue. Launches jobs that won't interfere with the start time of a higher priority job. Sets jobs'\u00a0StartTime and SchedNodeList. 20 seconds after the last backfill cycle completes 2-15+ minutes","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#starttime-and-schednodelist","title":"StartTime and SchedNodeList","text":"<p>The backfill scheduler sets the StartTime and SchedNodeList parameters on jobs that can start within the next 7 days. These parameters can be viewed in the output of <code>scontrol show job &lt;jobid&gt;</code>. StartTime estimates when a job will start and SchedNodeList shows the nodes this job might start on. StartTime is only an estimate. These values are updated every time the backfill scheduler runs and may change as running jobs complete and new jobs are submitted.</p>","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#minimum-job-requirements-to-avoid-deferment","title":"Minimum Job Requirements to Avoid Deferment","text":"<p>Jobs must meet certain criteria before the backfill scheduler will avoid potentially deferring them through starting lower priority jobs. These thresholds allow the backfill scheduler to cycle faster and maintain high system utilization.</p> Criteria Minimum Description Priority 3000 Jobs require a minimum priority of 3000 is require to avoid potential deferment in scheduling. Buy-in account jobs are never below this threshold. Age 30 minutes Jobs must be queued for at least 30 minutes to avoid potential deferment in scheduling. This applies to all jobs.","tags":["reference","slurm"]},{"location":"How_Jobs_are_Scheduled/#job-priority-factors","title":"Job Priority Factors","text":"<p>A job's priority is determined by a combination of several priority factors. Age, size, fairshare, and whether it was submitted to a buy-in account all contribute to the job\u2019s priority.</p> Priority Factor Description Maximum Contribution to Priority Age Starts at zero at job submission, then increases linearly to a maximum of 60000 after 30 days 60000 after 30 days Fairshare Starts at 60000 and decreases and users' recent usage goes up. Usage for this calculation is decayed 50% each day 60000 for no recent cluster usage Size Scales linearly with the amount of CPU and memory requested by a job. 100 per CPU, 20 per GB. 52000+ depending on memory requested QOS Adds 3000 to buy-in jobs to ensure they are always above backfill schedulers minimum priority for reserving resources 3000","tags":["reference","slurm"]},{"location":"How_to_find_and_load_software_modules/","title":"Searching software modules","text":"<p>This page contains a basic overview of common interactions with the module system. For a guided tutorial, please see our Introduction to the Module System.</p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#searching-in-docs","title":"Searching in docs","text":"<p>Please refer to the Available Software page for a searchable list of available modules and how to load them. </p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#command-line-search-using-module-spider","title":"Command line search using <code>module spider</code>","text":"<p>To search for a particular software module (e.g. \"ABC\"), you would run</p> <pre><code>module spider ABC # can also be abc, ABc...\n</code></pre> <p>Once you find it, and want to load a specific version (say 1.1.1), run</p> <pre><code>module spider ABC/1.1.1-foss-2023a # should only be ABC\n</code></pre> <p>The resulting output information will tell you what prerequisites modules are needed before loading your <code>ABC/1.1.1-foss-2023a</code>. </p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#command-line-searching-with-a-partial-name","title":"Command line searching with a partial name","text":"<p>You don't need to know the full name of the software. For example, let's search for all modules related to VCF:</p> <pre><code>$ module spider vcf\n\n------------------------\n  VCFtools: VCFtools/0.1.16-GCC-12.3.0\n------------------------\n    Description:\n      The aim of VCFtools...\n\n    This module can be loaded directly: module load VCFtools/0.1.16-GCC-12.3.0\n\n    Help:\n      ...\n\n------------------------\n  vcfR:\n------------------------\n    Versions:\n        vcfR/1.14.0 (E)\n        vcfR/1.15.0 (E)\n\nNames marked by a trailing (E) are extensions provided by another module.\n\n------------------------\nFor detailed information about a specific \"vcfR\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider vcfR/1.15.0\n------------------------\n</code></pre>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#other-ways-to-search","title":"Other ways to search","text":"<p>The output of <code>module spider</code> may also suggest other ways to search for modules.</p> <p>See the example for PCRE below, which suggests both \"other possible module matches\" and \"other possible module matches.\"</p> <pre><code>$ module spider PCRE\n\n--------------------------\n  PCRE:\n--------------------------\n    Description:\n      The PCRE library...\n\n     Versions:\n        PCRE/8.44-GCCcore-10.2.0\n        ...\n        PCRE/8.45-GCCcore-13.2.0\n     Other possible modules matches:\n        PCRE2\n\n--------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*PCRE.*'\n\n--------------------------\n</code></pre>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#loading-a-specific-version","title":"Loading a specific version","text":"<p>If a module is loaded without specifying a version - e.g. <code>module load HDF5</code> - the version of that module from the FOSS 2023a toolchain (or a sibling toolchain) will be loaded. Such modules will be compatible with the HPCC's default modules. In this case, <code>HDF5/1.14.0-gompi-2023a</code> will be loaded.</p> <p>A specific version of a module can be loaded using its full name. Users must run <code>module purge</code> before loading a module from a different toolchain than their currently loaded modules. Otherwise, an error about incompatible dependencies will be displayed.</p> <p>Note that <code>module purge</code> is always needed before you start loading your own modules. This command will clear the default modules and prevent version conflicts.</p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#saving-and-restoring-module-sets","title":"Saving and restoring module sets","text":"<p>Your currently loaded modules can be saved for easy access at a later time.</p> <p>To save your modules, run <pre><code>module save &lt;collection_name&gt;\n</code></pre> where <code>&lt;collection_name&gt;</code> is replaced with your desired name.</p> <p>You can then re-load this collection of modules with <pre><code>module restore &lt;collection_name&gt;\n</code></pre></p> <p>To see all saved collections, use <pre><code>module savelist\n</code></pre></p> <p>To see the contents of a collection, use <pre><code>module describe &lt;collection_name&gt;\n</code></pre></p> <p>A saved collection can be removed with <pre><code>module disable &lt;collection_name&gt;\n</code></pre></p>","tags":["reference"]},{"location":"How_to_find_and_load_software_modules/#troubleshooting","title":"Troubleshooting","text":"<p>Sometimes, <code>module spider</code> doesn't work because your personal module cache is out of date. To clear it, do <code>rm ~/.cache/lmod/spider*</code></p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/","title":"Classroom Support","text":"<p>This document provides a defined pathway for instructors at MSU to utilize ICER/HPCC resources for classroom education.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#services-available","title":"Services Available","text":"<p>ICER will work with instructors to provide the following services during the class:</p> <ul> <li> <p>Training workshops on using the HPCC, via our asynchronous Desire2Learn-based training modules.</p> </li> <li> <p>HPCC student accounts and research space for use in classes and for sharing data files</p> </li> <li> <p>Access to OnDemand, a web-based portal to use Python via Jupyter notebooks,  RStudio, Matlab, Stata, and an interactive Linux desktop. OnDemand has access to all of the HPCC's file systems.</p> </li> <li> <p>Software installation for teaching purposes</p> </li> <li> <p>Reservations for computing resources in the cluster so that the students can instantly have access to the resources to finish the assignments</p> </li> <li> <p>Helpdesk tickets (submitting your questions via our online form)</p> </li> </ul>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-classroom-support","title":"Request Classroom Support","text":"<p>Requests should be submitted two weeks in advance, to allow for time for account creation, specialized software installation, etc.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-individual-student-user-accounts","title":"Request individual student user accounts","text":"<p>Instructors need to submit a New Account Request online form for all students enrolled in the course. Please note that instructors are also responsible for requesting the accounts be closed when students are no longer enrolled in the course or when the course is finished. The following information should also be included in the request.</p> <ul> <li>MSU NetIDs of students enrolled in the course</li> <li>Course name for the group name of the student accounts</li> <li>Software installation, CPU/GPU core reservation (as needed)</li> <li>Start and end dates of the course</li> </ul>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#request-a-research-space-for-course-use","title":"Request a research space for course use","text":"<p>Instructors can also submit a Research request to create a research space for the course group and add all students enrolled in the request form.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#note-on-storage-quota","title":"Note on storage quota","text":"<p>Both the home directory of a user and the research directory of a group are initialized with 50 GB storage quota. Additional space up to 1 TB may be requested for home directory or research space. Beyond 1 TB, ICER will need to charge for the additional storage.</p>","tags":["reference"]},{"location":"ICER_HPC_Classroom_Support/#class-account-agreement","title":"Class Account Agreement","text":"<p>Instructors are expected to make good faith efforts towards implementing the following policies:</p> <ul> <li> <p>Awareness of ICER and MSU policies -- Instructors using ICER     resources are expected to have an understanding of ICER and MSU IT     policies. Broadly, they should be cognizant of MSU's data sharing     policies, wait times on queued jobs and the possibility of     unscheduled outages. These factors should be taken into     consideration while designing assignments and projects that utilize     ICER resources. ICER/HPCC policies are emailed to users upon     account creation.</p> </li> <li> <p>Contacting ICER -- Instructors should provide clear policies in     their syllabus about when students should contact ICER staff. \u00a0ICER     will provide account, hardware and system software support. Very limited applications software support for the course     TAs and instructors is available. However, so as not to overwhelm     the ticketing system with course-specific questions, we ask that all     student questions be routed through the TA or course instructor who     will then determine whether to forward these to ICER's ticketing     system. While students are encouraged to visit ICER research     consultants during office hours, these hours are meant for research     support and are not designed to be used as a means of TA support. In     particular, students should be aware that submitting queries to ICER     that seek answers to homework problems will be considered cheating     and a violation of the Honor Code.</p> </li> <li> <p>Planning for outages -- ICER resources may become unavailable as     a result of an unscheduled system outage. Instructors are advised     not to depend on ICER resources for final exams and/or projects that     require a short turnaround time.</p> </li> <li> <p>Storing data -- Instructors should advise students against     storing data on the \"scratch\" space. Files are typically purged on     scratch after 45 days if no modification has been made, and cannot be recovered. Instructors may     request a research space for students to store their class related     data for the duration of the semester. Students may also store their     data in their home directory. Accounts for course work are typically     limited to 50 GB.</p> </li> <li> <p>Terminating educational accounts -- Student HPCC accounts, linked     home folders, and group folders created for courses will be removed     30 days after the end of the class. This applies only to     education-sponsored accounts and NOT research-sponsored student     accounts. However, students who wish to convert their     educated-sponsored account to a research-sponsored account after the     course is completed must have their research supervisor submit a     request for membership change no later than 30 days after the     semester in which the course was completed in order to retain the     data saved in the education-sponsored student account.</p> </li> <li> <p>Acknowledging ICER -- Instructors and students are encouraged to     acknowledge the use of ICER/HPCC upon publication of data related to the     resources used during the course.</p> </li> </ul> <p>Last updated: Aug 2022</p>","tags":["reference"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/","title":"Installing Local Perl Modules with CPAN","text":"<p>CPAN\u00a0is a convenient way to build and install perl modules, but many people have difficulty knowing how to do this if they lack \"root\" permissions. \u00a0This tutorial will demonstrate how to install Perl modules to a local user space using CPAN.</p>","tags":["tutorial","Perl"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/#procedure","title":"Procedure","text":"<p>First start the CPAN shell:</p> <pre><code>$ cpan\n\nTerminal does not support AddHistory.\n\ncpan shell -- CPAN exploration and modules installation (v1.9402)\n\nEnter 'h' for help.\n\ncpan[1]&gt;\n</code></pre> <p>Next determine where you want to install your local Perl modules. Let's assume we are going to place them in: /mnt/home/myUid/perlmods</p> <p>Now from within the CPAN shell, enter the following three (3) commands:</p> <pre><code>cpan[1]&gt; o conf mbuildpl_arg \"--install_base /mnt/home/myUid/perlmods\"\n\n     mbuildpl_arg       [--install_base /mnt/home/myUid/perlmods]\n\n   Please use 'o conf commit' to make the config permanent!\n\n\ncpan[2]&gt; o conf makepl_arg \"PREFIX=/mnt/home/myUid/perlmods\"\n\n    makepl_arg         [PREFIX=/mnt/home/myUid/perlmods]\n\n  Please use 'o conf commit' to make the config permanent!\n\n\ncpan[3]&gt; o conf prefs_dir \"/mnt/home/myUid/.cpan/prefs\"\n\n    prefs_dir          [/mnt/home/myUid/.cpan/prefs]\n\n  Please use 'o conf commit' to make the config permanent!\n</code></pre> <p>If you want to make the settings above permanent, enter \"o conf commit\". \u00a0Otherwise, bear in-mind you will need to reset this value every time you restart CPAN. If you do make the setting permanent, you can always change it later and re-commit as shown above.</p> <p>Now to install a module (lets assume we want to build \"Math::GMP\") simply enter:</p> <pre><code>cpan[4]&gt; install Math::GMP\n</code></pre> <p>Respond to any prompts for information that might be requested. When you are finished, enter:</p> <pre><code>cpan[5]&gt; quit\n</code></pre>","tags":["tutorial","Perl"]},{"location":"Installing_Local_Perl_Modules_with_CPAN/#setting-the-perl5lib-path","title":"Setting the PERL5LIB Path","text":"<p>Now that you've successfully installed a local Perl module, you will need to tell Perl where to find them. \u00a0This can be easily accomplished by setting the environmental path variable \"PERL5LIB\". For example:</p> <pre><code>$ export PERL5LIB=/mnt/home/myUid/perlmods:$PERL5LIB\n</code></pre> <p>You can add this export to your .bashrc file if you'd like to ensure it is always loaded upon login. In addition, for any scripts that you write that utilize these local Perl modules that you run on the HPCC cluster, you should add this export statement to your job script, or create a\u00a0custom user module\u00a0that does that for you, and which can be loaded from within your job script.</p>","tags":["tutorial","Perl"]},{"location":"Installing_TensorFlow_using_anaconda/","title":"Installing TensorFlow using anaconda","text":"<p>Warning</p> <p>TensorFlow requires specific instructions for a fully functional installation. As such, the instructions and recommendations on this page may differ slightly from other pages in ICER's documentation, but have been fully tested as of March 2023. For more general Conda and Python usage, please see our page on Using Conda.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#install-tensorflow-using-conda","title":"Install TensorFlow using conda","text":"<p>In this tutorial, we will first install Anaconda in our home directory, then install TF in a newly created conda environment, and finally run a few TF commands to verify the installation.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#installing-anaconda-in-your-home-directory","title":"Installing Anaconda in your home directory","text":"<p>A full guide of downloading anaconda and installing it in your home directory is here. Following the guide, below we show a sequence of commands that will download and configure conda in one's home directory on the HPCC (say <code>/mnt/home/user123/</code>).</p> <pre><code># Install anaconda3 in /mnt/home/user123/ (replace user123 with your HPCC account name)\nwget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh\nbash Anaconda3-2022.10-Linux-x86_64.sh\nsource /mnt/home/user123/anaconda3/bin/activate\nconda init\nconda config --set auto_activate_base false\n</code></pre> <p>Notes</p> <ul> <li> <p>We recommend downloading Anaconda 3 which corresponds to Python 3. </p> </li> <li> <p>In the guide, step 8, it says Anaconda recommends entering \"yes\".  However, we recommend a \"No\" so as to not modify your <code>~/.bashrc</code>. After that, you will need to run  <code>source /mnt/home/user123/anaconda3/bin/activate</code> and <code>conda init</code> as shown above.</p> </li> <li> <p>The last command above is to disable automatic base environment activation. This is necessary.</p> </li> <li> <p>By default, your anaconda will be installed in <code>/mnt/home/user123/anaconda3/</code>. You can specify an alternate installation path during this interactive process.</p> </li> <li> <p>Above, the link after <code>wget</code> can be replaced by a more recent version of script in https://repo.anaconda.com/archive/</p> </li> <li> <p>If you encounter any errors, check your quota first, by running <code>quota</code>. Make sure your home directory has enough space. Always fully delete previously installed anaconda if you are going to re-install by repeating the steps.</p> </li> </ul>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#installing-tf-in-a-conda-environment","title":"Installing TF in a conda environment","text":"<p>After you've successfully installed Anaconda in your home directory, you can follow the commands below to install TF and troubleshoot some errors. After initial login,  run  <code>ssh dev-amd20-v100</code> to log into our GPU dev-node.</p> <p>Warning</p> <p>Installing TensorFlow while on <code>dev-amd20-v100</code> will restrict you to amd20 nodes with GPUs. You must specify <code>amd20</code> as a constraint when submitting a batch job or starting an OnDemand session.</p> <p>If you are not familiar with basic conda commands (e.g., <code>conda create/activate/install/deactivate</code>), check out this conda cheatsheet. After creating a new conda environment (namely <code>tf_gpu_Feb2023</code> below) and activating it, the environment variable <code>$CONDA_PREFIX</code> will point to <code>/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023</code>. </p> <p>Minimally, you only need to modify the first line below, that is, <code>export PATH=...</code>, so that the path points to the <code>bin</code> folder in your anaconda installation. The rest of the commands can be directly copied and run in your terminal.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda create --name tf_gpu_Feb2023 python=3.9\nconda activate tf_gpu_Feb2023\n\nconda install -c conda-forge cudnn=8.1.0 --yes\nconda install -c nvidia cuda-nvcc --yes \n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\npip install --upgrade pip\npip install tensorflow==2.11.0 # compatible with CuDNN v8.1.0\npip install nvidia-pyindex\npip install nvidia-tensorrt\n\n# To fix the error of \"Could not load dynamic library 'libnvinfer.so.7'\". The trick is to create a symlink.\ncd $CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nln -s libnvinfer.so.8 libnvinfer.so.7\nln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7\n\n# To fix the error of \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\" \nmkdir -p $CONDA_PREFIX/lib/nvvm/libdevice\ncp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice\n\nconda deactivate\n</code></pre>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#verifying-the-installation-using-simple-commands","title":"Verifying the installation using simple commands","text":"<p>Now we'll run a few one-liners to test out, right from the shell command line. Again, you need to be logged onto <code>dev-amd20-v100</code> the GPU dev-node. If no errors pop up when executing these commands, you should be all set.</p> <p>Note</p> <p>You'll need to run the first four lines every time you want to start using TensorFlow. This includes any SLURM scripts you write to launch TF jobs.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda activate tf_gpu_Feb2023\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\n\n# Simple one-liner test commands\npython3 -c \"import tensorflow as tf; print (tf.__version__)\" # check TF version\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" # verify GPU devices\npython3 -c \"import tensorrt; print(tensorrt.__version__); assert tensorrt.Builder(tensorrt.Logger())\" # test TensorRT installation\n\nconda deactivate\n</code></pre> <p>More complicated testing code can be found in our TensorFlow model training examples.</p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_anaconda/#using-tensorflow-in-an-ondemand-jupyter-notebook","title":"Using TensorFlow in an OnDemand Jupyter notebook","text":"<p>If you would like to use TensorFlow from an Open OnDemand Jupyter notebook, you'll first need to install Jupyter.</p> <pre><code>export PATH=/mnt/home/user123/anaconda3/bin:$PATH\nconda activate tf_gpu_Feb2023\n\nconda install jupyter\n</code></pre> <p>Then, you need to edit a particular file to set up the <code>LD_LIBRARY_PATH</code> and <code>XLA_FLAGS</code> environment variables in the same way they are set above. First, we'll make a backup of this file as demonstrated below.</p> <pre><code>cd $CONDA_PREFIX/share/jupyter/kernels/python3/\n\ncp kernel.json kernel.json.bak\n</code></pre> <p>With your favorite text editor, open <code>kernel.json</code>. Look for the following pattern at the end of the file <pre><code> }\n}\n</code></pre> and add the following (note the commas!) <pre><code> },\n \"env\": {\n  \"XLA_FLAGS\":\"--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib\",\n  \"LD_LIBRARY_PATH\":\"$LD_LIBRARY_PATH:/lib/:/lib64/:$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt_libs\"\n }\n}\n</code></pre></p> <p>Note</p> <p>If you open a notebook and get a message about no kernel being available, make sure you added the comma after the first curly bracket.</p> <p>When you request a Jupyter notebook through OnDemand, make sure to  do the following:</p> <ul> <li>Request more than the minimum amount of memory (on the order of GB)</li> <li>Select \"Launch Jupyter Notebook using the Anaconda installation in my home directory\"</li> <li>Enter the full path to your Anaconda installation; e.g, <code>/mnt/home/user123/anaconda3</code></li> <li>Enter the name of your TF Conda environment; e.g., <code>tf_gpu_Feb2023</code></li> <li>Select \"Advanced Options\"</li> <li>Set the node type to <code>amd20</code></li> <li>Request 1-4 GPUs</li> </ul> <p>Even if you have requested less than 4 hours of wall time, your  job may spend more time in the queue than you may used to. This is normal given the specific resources we have requested.</p> <p>You can test that TensorFlow will run in your notebook by running the following: <pre><code>import tensorflow as tf\nimport tensorrt\n\nprint(\"TF Version:\", tf.__version__)\nprint(\"GPUs:\\n\", tf.config.list_physical_devices('GPU'))\nprint(\"TensorRT Version:\", tensorrt.__version__)\n\nassert tensorrt.Builder(tensorrt.Logger())\n</code></pre></p>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_TensorFlow_using_miniforge/","title":"Install TensorFlow using conda","text":"<p>We assume that you've already installed conda using miniforge locally, e.g., in your home directory on the HPCC.</p> <p>While the official TF installation guide provides a one-shot method to install TF by  <code>pip install tensorflow[and-cuda]</code> without needing to manually pre-install packages such as CUDA and cuDNN, they have issues with registering cuDNN, cuFFT, cuBLAS, and TensorRT. To resolve these library-related issues,  we will choose to install them manually inside a conda environment. To learn more, you can read  this blog where they show different results from using the two methods.</p> <p>First off,  run  <code>ssh dev-amd20-v100</code> to log into our GPU dev-node.</p> <p>Note</p> <p>If you are not familiar with basic conda commands (e.g., <code>conda create/activate/install/deactivate</code>), check out the conda cheatsheet. After creating a new conda environment (namely <code>tf_Jul2024</code> below) and activating it, the environment variable <code>$CONDA_PREFIX</code> will be set to <code>/mnt/home/user123/miniforge3/envs/tf_Jul2024/</code>. </p> <p>Once logged in, run the installation script below in your terminal to complete the GPU-based TF installation in your conda environment.</p> <pre><code># README\n# - Below we assume your miniforge is installed in /mnt/home/user123/miniforge3/; change user123 to your real account\n# - You need to load conda first, by following the \"Using Conda\" tutorial https://docs.icer.msu.edu/Using_conda/\n\nconda create -n tf_Jul2024 python=3.10\nconda activate tf_Jul2024\nconda install -c conda-forge cudatoolkit=11.8 cudnn=8.8\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\npython -m pip install tensorrt==8.5.3.1\nTENSORRT_PATH=$(dirname $(python -c \"import tensorrt; print(tensorrt.__file__)\"))\necho $TENSORRT_PATH # output is used for composing the next command\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/mnt/home/user123/miniforge3/envs/tf_Jul2024/lib/python3.10/site-packages/tensorrt\npython -m pip install tensorflow==2.13\nconda deactivate\n</code></pre> <p>Now we'll run a few simple one-liner commands to verify the installation.</p> <pre><code>conda activate tf_Jul2024\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib:/mnt/home/user123/miniforge3/envs/tf_Jul2024/lib/python3.10/site-packages/tensorrt\n\npython3 -c \"import tensorflow as tf; print (tf.__version__)\" # check TF version\npython3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" # verify GPU devices\npython3 -c \"import tensorrt; print(tensorrt.__version__); assert tensorrt.Builder(tensorrt.Logger())\" # test TensorRT installation\nconda deactivate\n</code></pre>","tags":["how-to guide","TensorFlow","Conda"]},{"location":"Installing_pytorch_using_anaconda/","title":"Installing Pytorch/Pytorch Lightning Using Conda","text":"<p>This guide will walk you through installing Pytorch and/or Pytorch Lighting using conda. It assumes you have already installed either Miniforge. See the guide on using conda for more.</p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#setup-checking-python","title":"Setup - Checking Python","text":"<p>If you installed conda on your own and not following our using conda guide, the HPCC may be trying to use the system python installation instead of your own. To test if this is the case, run <pre><code>which python\n</code></pre> If the output starts with <code>/opt/software</code>, you will need to run <code>module unload Python</code>. If the output starts with the path to your Miniforge, you don't need to do anything else.</p> <p>Note</p> <p>If you are affected by the above issue, you will have to run <code>module unload Python</code> every time you wish to use your own python installation. You may wish to add the <code>module unload Python</code> command to your <code>$HOME/.bashrc</code> file.</p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#installing-pytorch","title":"Installing Pytorch","text":"<p>Since Pytorch works best when using a GPU, it needs to be installed on a development node with a GPU. We recommend using <code>dev-amd20-v100</code> for the latest hardware. Run <pre><code>ssh dev-amd20-v100\n</code></pre></p> <p>Note</p> <p>You will be restricted to running Pytorch on nodes with v100 GPUs. See the page on cluster resources and SLURM job specifications for more.</p> <p>You will also need the CUDA compiler, so load this using our module system: <pre><code>module load CUDA/12.1.1\n</code></pre></p> <p>It's best practice to use Conda environments to organize your Python packages. Create a new conda environment with the name <code>pytorch</code> run <pre><code>conda create --name pytorch\n</code></pre></p> <p>To switch to this new environment, run <pre><code>conda activate pytorch\n</code></pre></p> <p>Now that you are on a GPU development node, have loaded the CUDA module, and activated your new environment, you can install Pytorch with the following command: <pre><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n</code></pre></p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Installing_pytorch_using_anaconda/#installing-pytorch-lightning","title":"Installing Pytorch Lightning","text":"<p>It's best to install Pytorch following the instructions above before installing Pytorch Lightning, or GPU-support may not function correctly.</p> <p>After Pytorch has been installed, Pytorch Lightning can be installed to the same <code>pytorch</code> environment using <pre><code>conda install pytorch-lightning -c conda-forge\n</code></pre></p>","tags":["how-to guide","Conda","PyTorch"]},{"location":"Interactive_Job/","title":"Interactive Job","text":"<p>It is helpful to run your work and see the response of the commands right away to check if there is any error in your work flow. To use the interactive mode with resources more than the limit imposed on the dev nodes, HPCC users can submit an interactive job using the <code>interact</code>  powertool, or the <code>salloc</code>/<code>srun</code> commands, with options of resource requests.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#interact-powertool","title":"<code>interact</code> powertool","text":"<p>The <code>interact</code> powertool provides sensible defaults to launch interactive jobs on the HPCC. It is loaded as part of the default modules. Alternatively you can access it with <code>module load powertools</code>. The default resource request when you  run <code>interact</code> is 1 task on 1 core on 1 node for 1 hour. Your job will be queued.  Do not close your terminal. Once the job has queued, you will be transported to a  command prompt on the compute node assigned to your job. You can close your  terminal and reconnect to the interactive session following the information at  Connections to compute nodes.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#specifying-other-resources","title":"Specifying other resources","text":"<p>To request resources beyond the defaults, you can use the following options:</p> Option Alternate Description <code>-t &lt;Time&gt;</code> <code>--time</code> Set a limit on the total run time. <code>-gpu</code> <code>--gpu</code> Allocate 1 gpu and 16 CPUs <code>-N &lt;Nodes&gt;</code> <code>--nodes</code> Number of nodes <code>-c &lt;ncores&gt;</code> Number of cores <code>-n &lt;ntasks&gt;</code> Number of tasks (spread over all Nodes) <code>--ntasks-per-node=&lt;ntasks&gt;</code> Number of tasks, 1 per core per node. <code>--mem=&lt;MB&gt;</code> Real memory required per node in MegaBytes <p>You can view the other options that <code>interact</code> accepts using the command <code>interact -h</code>. </p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#salloc-command","title":"salloc command","text":"<p>For salloc, the command line</p> <pre><code>salloc -N 1 -c 2 --time=1:00:00\n</code></pre> <p>will allocate a job with resources of 1 node, 2 cores and walltime 1 hour. The execution will first wait until the job controller can provide the resources.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\n</code></pre> <p>Once that happens, the terminal will be transported to a command prompt on a compute node assigned to the job.</p> <pre><code>[username@dev-intel18 WorkDir]$ salloc -N 1 -c 2 --time=1:00:00\nsalloc: Required node not available (down, drained or reserved)\nsalloc: job 7625 queued and waiting for resources\nsalloc: job 7625 has been allocated resources\n[username@test-skl-000 WorkDir]$\n</code></pre> <p>where \"test-skl-000\" after the symbol\u00a0@ is the name of the assigned compute node.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#salloc-and-gpus","title":"salloc and GPUs","text":"<p>GPUs requested for an interactive job can now be used without submitting an additional srun. See our pages on our GPU resources and requesting GPUs for more information.</p> <pre><code>[username@dev-intel18 ~]$ salloc --gpus=k80:1 --time=1:00:00\nsalloc: Pending job allocation 28241766\nsalloc: job 28241766 queued and waiting for resources\nsalloc: job 28241766 has been allocated resources\nsalloc: Granted job allocation 28241766\nsalloc: Waiting for resource configuration\nsalloc: Nodes lac-195 are ready for job\n[username@lac-195 ~]$ nvidia-smi\nThu Jan  4 14:34:03 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 00000000:0B:00.0 Off |                    0 |\n| N/A   40C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>","tags":["reference","slurm"]},{"location":"Interactive_Job/#srun-command","title":"srun command","text":"<p>A similar way can also be used with srun command:</p> <pre><code>[username@dev-intel18 WorkDir]$ srun -N 4 --ntasks-per-node=2 -t 00:60:00 --mem-per-cpu=1000M --pty /bin/bash\nsrun: Required node not available (down, drained or reserved)\nsrun: job 7636 queued and waiting for resources\nsrun: Granted job allocation 7636\n[username@test-skl-000 WorkDir]$ \n</code></pre> <p>As we can see, the specification \"--pty /bin/bash\" is required for srun command to request an interactive mode. Any command executed in this kind of interactive jobs will be launched parallelly with the number of task requested. srun can also be used in a command line without the specification \"--pty /bin/bash\". You may refer to the srun web site for more details.</p>","tags":["reference","slurm"]},{"location":"Interactive_Job/#job-with-graphical-application","title":"Job with graphical application","text":"<p>To schedule an interactive job able to use graphical user interface (GUI) software, the specification <code>--x11</code> for X11 forwarding needs to be specified with the command (salloc or srun).\u00a0\u00a0You must use the <code>-X</code> parameter with <code>ssh</code> to allow X11 forwarding when connecting to both gateway  and development nodes prior to running the salloc command.\u00a0 If you are using Mac Terminal, you must have Xquartz installed. If you are on Windows and using Moba Xterm to log in, these instructions will work with the <code>-X</code> parameter.\u00a0 Putty does not support X11 and so this will not work with putty.\u00a0</p> <p>The other option is to first log into our web-based remote desktop, and run the terminal there. See Web Site Access to HPCC for GUI software.\u00a0</p> <pre><code>[username@gateway-03 ~]$ ssh -X dev-intel18\n[username@dev-intel18 ~]$ cd WorkdDir  # this is optional, but you may want to select your work directory, for example\n[username@dev-intel18 WorkDir]$ salloc --ntasks=1 --cpus-per-task 2 --time 00:30:00 --x11\nsalloc: Granted job allocation 7708\nsalloc: Waiting for resource configuration\nsalloc: Nodes css-076 are ready for job\n[username@css-076 WorkDir]$ module load MATLAB\n[username@css-076 WorkDir]$ matlab\nMATLAB is selecting SOFTWARE OPENGL rendering.\nOpening log file:  /mnt/home/username/java.log.7159\n</code></pre> <p></p>","tags":["reference","slurm"]},{"location":"Intro_to_MobaXterm/","title":"Intro to MobaXterm","text":""},{"location":"Intro_to_MobaXterm/#setting-up-mobaxterm-on-your-local-computer","title":"Setting up MobaXterm on your local computer","text":"<p>To obtain a copy of MobaXterm, go to the MobaXterm website and selected 'Download' at the top of the page</p> <p></p> <p>On the next page, choose \"Download Now\" under Home Edition</p> <p></p> <p>Finally, click the blue \"Portable Edition\" button. </p> <p></p> <p>The Portable Edition is convenient in that all you will need to do is unzip the file once the download is finished, no need to install anything.</p> <p></p> <p>Go into the unzipped folder and click on the MobaXterm application to run the program (which should be named something like \"MobaXterm_Personal_XX.Y\" where XX.Y is the verison number)</p> <p></p>"},{"location":"Intro_to_MobaXterm/#setting-up-and-ssh-connection-session-to-hpcc","title":"Setting up and SSH connection session to HPCC","text":"<p>Inside MobaXterm, you can define a 'SSH Session' to simplify the process of connecting to HPCC. First, start MobaXterm, then click on the \"Session\" button in the upper left. </p> <p></p> <p>Next, choose the \"SSH\" option from pop-up menu</p> <p></p> <p>In the following menu, fill in \"hpcc.msu.edu\" as the remote host. Then check the \"Specify username\" box and fill in your MSU NetId. Also, make sure the X11 fowarding box is checked under the \"Advanced SSH Settings\" tab, as this will allow to launch GUI programs through your session. Click \"OK\" once you are finsihed. You may get a pop-up window asking if you trust the host you have entered; click \"Accept.\"</p> <p></p> <p>Now, if you click the yellow star on the left pannel, you should see a line in the 'User sessions' column named something like \"hpcc.msu.edu ()\". <p></p> <p>Click this and you  will be prompted to enter your NetId password, after which you will be logged into one of the gateway nodes on HPCC. From here you can connect to any of the development nodes.</p>"},{"location":"Intro_to_modules/","title":"Module System Tutorial","text":"<p>The HPCC has a large amount of software installed in order to support its diverse users. This can include multiple versions of the same software. The module system exists to manage all of this by making software available to users and preventing version conflicts.</p> <p>In this tutorial, you'll learn how to use the module system to:</p> <ul> <li>See which modules are currently loaded</li> <li>Search for available software versions</li> <li>Check requirements for particular modules</li> <li>Loading modules</li> <li>Saving currently loaded modules to easily reload</li> </ul> <p>For the purposes of this tutorial, we'll be trying to load version 3.6.3 of the R interpreter.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#viewing-currently-loaded-modules","title":"Viewing Currently Loaded Modules","text":"<p>Several modules are already loaded by default once you log on to a development node. These include several commonly used packages such as Python, MATLAB, and the GNU compiler.</p> <p>Run <code>module list</code> to see all currently available modules. There is a long list of modules available by default. If the output of <code>module list</code> is too big for your screen, it will be displayed using a program called <code>less</code>. </p> <p>The <code>less</code> program will let you scroll up and down through the list with the arrow keys. You can press <code>q</code> to exit <code>less</code> at any time. You will also automatically exit once you scroll to the end of the <code>module list</code> output. To search within <code>less</code>, type the <code>/</code> character followed by the string you want to search for, then press enter. Use the <code>n</code> and <code>p</code> keys to jump between the next and previous matches respectively.</p> <p>If we search through the output of <code>module list</code>, we'll see that the default version of R is <code>4.3.2-gfbf-2023a</code>. This label means the module loads version 4.3.2 of R where R has been compiled with the 2023a version of the <code>gfbf</code> toolchain. However, recall that we want to load R version 3.6.3 for this tutorial. Let's learn more about the module system to see how to accomplish this.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#searching-for-available-modules","title":"Searching for Available Modules","text":"<p>In addition to searching the Available Software in this documentation, users can use the <code>module</code> command accepts a variety of \"sub-commands\". The example we used above, <code>list</code>, is an example of a sub-command.</p> <p>What other sub-commands are available? Run <code>module</code> by itself to find out. </p> <p>You'll see a long list of available sub-commands printed to the screen. Scroll up until you see the portion on listing and searching: <pre><code>Listing / Searching sub-commands:\n---------------------------------\n  list                              List loaded modules\n  list                s1 s2 ...     List loaded modules that match the pattern\n  avail | av                        List available modules\n  avail | av          string        List available modules that contain \"string\".\n  spider                            List all possible modules\n  spider              module        List all possible version of that module file\n  spider              string        List all module that contain the \"string\".\n  spider              name/version  Detailed information about that version of the\n                                    module.\n  whatis              module        Print whatis information about module\n  keyword | key       string        Search all name and whatis that contain \"string\".\n</code></pre></p> <p>Here we see the sub-command <code>list</code>, which we've already encountered. We'll cover <code>avail</code> and <code>keyword</code> in other documentation. For now, let's focus on the <code>spider</code> sub-command.</p> <p>The <code>spider</code> sub-command is the most useful way to search through available modules. Its name isn't obvious, but you can think of sending a spider to walk through a tangled web of modules to find the ones matching your request.</p> <p>The list of sub-commands from <code>module</code> shows four options for the <code>module spider</code> sub-command:</p> Argument Output None All possible modules <code>module</code> All versions of that module <code>string</code> All modules containing <code>string</code> <code>name/version</code> Details about a module version <p>We'll cover <code>module</code> and <code>string</code> search in the next section on searching by module name. After that we'll cover the <code>name/version</code> search for loading a specific version.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#searching-by-module-name","title":"Searching by Module Name","text":"<p>For this tutorial we want to search for the versions of the R interpreter. Within the module system on the HPCC, \"R\" with a capital R is the formal name of the module.</p> <p>Run <code>module spider R</code>. An abbreviated output is reproduced below: <pre><code>----------\n\n  R:\n----------\n    Description:\n      R is a free software environment for statistical computing and graphics.\n\n     Versions:\n        R/3.6.3-foss-2022b\n        R/4.2.2-foss-2022b\n        R/4.3.2-gfbf-2023a\n        R/4.3.3-gfbf-2023b\n     Other possible modules matches:\n        ADMIXTURE  AOFlagger  APR  APR-util  Amber  Armadillo  Arrow  Avogadro2  ...\n\n----------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*R.*'\n\n----------\n  For detailed information about a specific \"R\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider R/4.3.3-gfbf-2023b\n----------\n</code></pre></p> <p>We see a full list of all available R versions as well as some other helpful information:</p> <ul> <li>Other possible module names we may have been searching for</li> <li>How to search for all modules containing the string \"R\"</li> <li>How to get detailed information on a specific version</li> </ul> <p>The first two points reference searching by <code>string</code> in the table above, rather than searching by <code>module</code>. The third point references the <code>name/version</code> search.</p> <p>What's the difference between searching by <code>module</code> and searching by <code>string</code>? As mentioned at the start of this section, \"R\" is the formal name of the module. The module system tries to be case insensitive, but it can have odd results. </p> <p>Run <code>module spider r</code> with a lowercase r and you'll see we've executed a <code>string</code> search, returning all modules that include the letter R!</p> <p>This is probably more information than you would like. Press <code>q</code> to quit and return to the terminal.</p> <p>That said, searching by <code>string</code> is powerful if you don't know the precise name of the module you are looking for.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#details-on-a-module-version","title":"Details on a Module Version","text":"<p>All modules listed by <code>module spider</code> follow the <code>name/version</code> format. We can use this format to get more information on a specific version. As with the default R module, the <code>version</code> tag contains information about the toolchain with which that version of R was compiled. </p> <p>We're interested in using R version 3.6.3, so run <code>module spider R/3.6.3-foss-2022b</code>.</p> <p>You'll again see a long list of output. We are interested in the following portions: <pre><code>This module can be loaded directly: module load R/3.6.3-foss-2022b\n\nThis module provides the following extensions:\n\n  base (E), compiler (E), datasets (E), graphics (E), grDevices (E), grid (E), methods (E), parallel (E), splines (E), stats (E), stats4 (E), tcltk (E), tools (E), utils (E)\n</code></pre></p> <p>The phrase \"this module can be loaded directly\" means that we don't have to load any additional modules before loading R version 3.6.3. Any of its dependencies - such as the FOSS 2022b toolchain it was built with - will be loaded automatically when we load this one module.</p> <p>It is also worth noting that R provides a number of extensions. These are packages that are installed alongside R. In this case, these packages can be loaded within R. Some modules may also have extensions that provide additional command line tools.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#loading-modules","title":"Loading Modules","text":"<p>As we saw in the last section, when checking module details, R version 3.6.3 can be loaded \"directly\" without first loading any dependencies. This is because any necessary dependencies will be loaded automatically, including those from its compiler toolchain.</p> <p>Let's attempt to load our desired version of R: <pre><code>module load R/3.6.3-foss-2022b\n</code></pre></p> <p>This results in an error! The first two sentences are the most informative part of this error: <pre><code>Lmod has detected the following error:  The previous module command attempted to\nload \"R/3.6.3-foss-2022b\" while \"R/4.3.2-gfbf-2023a\" was already loaded. This is likely due\nto loading a module with incompatible dependencies from the one currently loaded.\n</code></pre></p> <p>Since <code>R/4.3.2-gfbf-2023a</code> and all of its dependencies are already loaded as default modules, we'll need to clear out our currently loaded modules if we want to load a module from a different toolchain. Run <code>module purge</code> to remove all currently loaded modules. You can confirm all modules have been unloaded with <code>module list</code>.</p> <p>Now, we can run <code>module load R/3.6.3-foss-2022b</code> again. You will not see any output after this command. Run <code>module list</code> if you would like to confirm it worked.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#saving-and-restoring-loaded-modules","title":"Saving and Restoring Loaded Modules","text":"<p>We often use the same pieces of software over and over on the HPCC. Remembering all the modules we need every time we log in can be a hassle.</p> <p>Thankfully, the module system lets us save and restore different configurations.</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#saving-a-configuration","title":"Saving a Configuration","text":"<p>Now that we only have GCC, OpenMPI, and R version 4.1.0 loaded, let's save a configuration to easily access these modules later. We'll name it <code>R-example</code>.</p> <p>Type <code>module save R-example</code>. You should see the following confirmation message: <pre><code>Saved current collection of modules to: \"R-example\"\n</code></pre></p>","tags":["tutorial"]},{"location":"Intro_to_modules/#restoring-a-configuration","title":"Restoring a Configuration","text":"<p>Log out of the HPCC and log back in to a development node. This will reset your loaded modules to the default; run <code>module list</code> to confirm this is the case.</p> <p>Run <code>module savelist</code> to see our stored configurations. Confirm that you see <code>R-example</code>: <pre><code>Named collection list :\n  1) R-example \n</code></pre></p> <p>Now, run <code>module restore R-example</code>. The existing modules will automatically be purged and your desired modules loaded!</p>","tags":["tutorial"]},{"location":"Intro_to_modules/#further-resources","title":"Further Resources","text":"<p>You should now understand the basics of the module system. For a refresher on searching for modules, see Searching software modules. You can also look through available modules online. Please note that searching via the command line will always yield the most up-to-date results.</p>","tags":["tutorial"]},{"location":"Job-Arrays---Run-multiple-similar-jobs-simultaneously_40337504.html/","title":"Job Arrays - Run multiple similar jobs simultaneously","text":"<p>Job array is an efficient way to submit and manage a collection of jobs that differ from each other by only a single index parameter. All jobs in a job array should have the same resource request (ex. size, time, etc.). For example, in the task below we wish to run python script \"hello.py\" ten times, each with a different input parameter for.  Instead of creating 10 separate Slurm job scripts and submitting them separately, we can create an array job in a script and submit it once for all.</p> <ul> <li> <p>Creat Python script file \"hello.py\" as show below.</p> <p>hello.py</p> </li> </ul> <pre><code>    #!/usr/bin/python\n\n    # import sys library (needed for accepted command line args)\n    import sys\n\n    # example of \"hello world!\" in Python\n    print('Hello World! This is the task number', sys.argv[1])\n</code></pre> <ul> <li> <p>Creat a job array script file \"hello.sb\" as show below.</p> <p>hello.sb </p> </li> </ul> <pre><code>    #!/bin/bash\n\n    # Example of running python script in a batch mode\n\n    #SBATCH -J hello.py                     # job name\n    #SBATCH -c 1                            # use one CPU core\n    #SBATCH --mem=1M                        # request memory\n    #SBATCH -t 10:00                        # 10 minutes time limit\n    #SBATCH -o hello-%A_%a.out              # output file name pattern\n                                            # %A is jobID and %a is task index.\n\n    #SBATCH --array=1-10                    # run array of 10 tasks with index 1, 2, ... 10.\n\n    # Load default version of Python\n    module purge\n    module load Python\n\n    # Run array of python script\n    python hello.py $SLURM_ARRAY_TASK_ID\n</code></pre> <ul> <li>Submit the job array as shown in following line: </li> </ul> <pre><code>    sbatch hello.sb\n</code></pre> <p>Now you can see there are 10 jobs submitted into your job queue. </p> <p>This example was modified from the example at:\u00a0https://rcpedia.stanford.edu/topicGuides/jobArrayPythonExample.html</p> <p>User can also download job array examples from our collection of examples by running following commands:</p> <pre><code>    module load powertools\n    getexample basic_array_job\n</code></pre> <p>A directory named \"basic_array_job\" containing two simple examples of job array will be downloaded to user's current directory.</p>"},{"location":"Job_Constraints/","title":"Job Constraints","text":"<p>Constraints are set to restrict which node features are required for a given job. Use\u00a0-C\u00a0or\u00a0--constraint = &lt;list&gt;\u00a0when submitting your job to specify a constraint.\u00a0</p>"},{"location":"Job_Constraints/#operators","title":"Operators","text":"<p>The following operators can be used to combine multiple features when specifying constraints.</p> Operator Function Description Example feature&amp;feature AND Nodes allocated for the job must have both features lac&amp;ib feature|feature OR Each node allocated for the job can have one feature or the other. lac|vim [feature|feature] XOR All nodes allocated for the job must have one feature or the other. Useful for multi-node shared memory jobs. [intel16|intel18]"},{"location":"Job_Constraints/#multi-node-jobs","title":"Multi-Node Jobs","text":"<p>Multi-node jobs that use constraints to control which cluster hardware they run on must use the XOR syntax to specify multiple clusters, e.g. <code>[intel18|intel16]</code>, and not the OR syntax, e.g. <code>intel18|intel16</code>. Multi-node jobs using OR instead of XOR may experience a considerable performance decrease or may be killed by HPCC staff without warning.</p> <p>In most cases where OR is used, the job would be better served by XOR. For this reason, constraints using OR are automatically re-written to use XOR. If an OR constraint is required, prepend the constraint request with <code>NOAUTO:</code>.</p>"},{"location":"Job_Constraints/#automatic-job-constraints","title":"Automatic Job Constraints","text":"<p>When no constraints are specified, a job will get a default constraint of\u00a0<code>[intel14|intel16|intel18|(amr|acm)|nvf|nal|nif]</code>. This\u00a0ensures the job runs on only one type of cluster hardware. </p> <p>User specified constraints that don't use the <code>feature|feature</code> or <code>[feature|feature]</code> syntax are automatically combined with\u00a0<code>[intel14|intel16|intel18]</code>. If a more complex constraint is required, <code>NOAUTO:</code> must be prepended to the constraint.</p> User Specified Constraint Literal Constraint Effective Constraint Result None [Default] [Default] Run this job on nodes that are all the same cluster type <code>--constraint=lac</code> lac&amp;[Default] lac Run this job on only Laconia nodes <code>--constraint=</code><code>\"lac&amp;ib\"</code> lac&amp;ib&amp;[Default] lac&amp;ib Run this job on only nodes with both lac and ib features <code>--constraint=</code><code>\"intel16\\|intel18\"</code> [intel16|intel18] [intel16|intel18] Run this job on nodes that are either all intel16 or all intel18 <code>--constraint=</code><code>\"NOAUTO:lac\\|vim\"</code> lac|vim lac|vim Run this job on nodes the each have either the lac or vim feature <code>--constraint=</code><code>\"\\[intel16&amp;ib\\|intel18&amp;ib\\]\"</code> [intel16&amp;ib|intel18&amp;ib] [intel16&amp;ib|intel18&amp;ib] Run this job on nodes that are either all intel16&amp;ib or all intel18&amp;ib"},{"location":"Job_Script_and_Job_Submission/","title":"Writing and submitting job scripts","text":"<p>The HPCC uses the SLURM system to manage computing resources. Users access these resources by submitting batch jobs.</p> <p>This tutorial will walk you through the process of writing and submitting a job submission script for a parallel job that uses multiple cores across several nodes.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#setup","title":"Setup","text":"<p>First, make sure you're on a development node and in your home directory.</p> <p>Clone and compile the example we're going to use: <pre><code>getexample MPI_OpenMP_GPU\ncd MPI_OpenMP_GPU\n\nmodule purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\nmake\n</code></pre></p> <p>This directory contains example C++ codes using several forms of parallelism. These examples may be useful if you find yourself developing your own software, and interested users should read the accompanying README.</p> <p>For now we'll just use the <code>hybrid</code> example. This example combines MPI and OpenMP. MPI allows multiple processes to communicate with each other, while OpenMP allows multiple CPUs to \"collaborate\" on the same process.</p> <p>We would like to run 4 processes, each on their own node, with 2 CPUs per process. That means we'll need a total of 8 CPUs.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#writing-a-job-script","title":"Writing a job script","text":"<p>A job script is a plain text file. It's composed of two main parts:</p> <ul> <li>The resource request</li> <li>The commands for running the job</li> </ul> <p>Using <code>nano</code> or your preferred text editor, create and open <code>hybrid.sb</code>: <pre><code>nano hybrid.sb\n</code></pre></p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#resource-request","title":"Resource request","text":"<p>The <code>hybrid</code> example likely uses a more complex set of resource requests than you will need for your own jobs, but it useful for illustrative purposes.</p> <p>Recall from the previous section that we'd like to run <code>hybrid</code> over 4 processes with 2 CPUs per process. Each process will also run on its own node. This outlines the resources we want to request.</p> <p>Let's type up the first part of the job script, the resource request. </p> <p>The very first line specifies the interpreter we want to use for our commands; in this case, it's the bash shell.</p> <p>Then, each resource request line begins with <code>#SBATCH</code>. All resources must be requested at the top of the file, before any commands, or they will be ignored.</p> <p>The request lines are as follows:</p> <ol> <li>Wall clock limit - how long will the job run? This job will run for 10 minutes.</li> <li>The number of nodes; here, 4</li> <li>The number of tasks, also known as processes, running on each node. Here we want 1.</li> <li>The number of CPUs per task. The default is one, but we've requested 2.</li> <li>The amount of memory to use per CPU. We are requesting 1 GB each.</li> <li>The name of the job, so we can easily identify it later.</li> </ol> <pre><code>#!/bin/bash --login\n\n#SBATCH --time=00:10:00\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=1G\n#SBATCH --job-name hybrid_example\n</code></pre>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#job-commands","title":"Job commands","text":"<p>The resource request is just one part of writing a job script. The second part is running the job itself.</p> <p>To run our job we need to:</p> <ol> <li>Load required modules</li> <li>Change to the appropriate directory</li> <li>Run <code>hybrid</code></li> </ol> <p>We'll add the following lines to <code>hybrid.sb</code>:</p> <pre><code>module purge\nmodule load GCC/10.3.0 OpenMPI/4.1.1\n\ncd ${HOME}/MPI_OpenMP_GPU\n\nsrun -c 2 hybrid\n</code></pre> <p>Notice that we use the <code>srun</code> command to run our <code>hybrid</code> executable. This command prepares the parallel runtime environment, setting up the requested 4 processes across 4 nodes and their associated CPUs.</p> <p>You may already be familiar with <code>mpirun</code> or <code>mpiexec</code>. While <code>srun</code> is similar to these commands it is preferred for use on the HPCC because of its connection to the SLURM scheduler.</p> <p>We can also add a couple of optional commands that will save data about our job:</p> <pre><code>### write job information to SLURM output file.\nscontrol show job $SLURM_JOB_ID\n\n### write resource usage to SLURM output file (uses a powertools command).\nmodule load powertools\njs -j $SLURM_JOB_ID\n</code></pre> <p>You have now completed your job script. </p> <p>If you used <code>nano</code> to write it, hit Ctrl+X followed by Y to save, then press Enter to accept the filename.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#final-notes","title":"Final notes","text":"<p>As was previously said, your job is most likely going to use much simpler resource specifications than shown above. You can see our example job scripts for more ideas.</p> <p>By default, SLURM will try to use the settings if overriding commands aren't specified: </p> <ul> <li><code>--nodes=1</code></li> <li><code>--tasks-per-node=1</code></li> <li><code>--cpus-per-task=1</code></li> <li><code>--time=00:01:00</code></li> <li><code>--mem-per-cpu=750M</code> </li> </ul> <p>See the curated List of Job Specifications or the <code>sbatch</code> Documentation for more options.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#batch-job-submission","title":"Batch job submission","text":"<p>Now that we have our job script, we need to submit it to the SLURM scheduler. For this, we use the <code>sbatch</code> command:</p> <pre><code>$ sbatch hybrid.sb\nSubmitted batch job 8929\n</code></pre> <p>If the command has been submitted successfully, the job controller will issue a job ID on the screen. This ID can be used with, for example, <code>scancel</code> to cancel the job or <code>sacct</code> to look up stats about the job after it ends.</p> <p>Note the <code>sbatch</code> command only runs on development and compute nodes - it will not work on any gateway node.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#checking-our-job-status","title":"Checking our job status","text":"<p>Once the job has been submitted, we can see it in the queue with <code>sq</code> powertool: <pre><code>module load powertools\nsq\n</code></pre></p> <p>This will show us the following information:</p> <ol> <li>The job's ID number</li> <li>The job's name, which we specified in the script</li> <li>The job's submitting user (should be your username)</li> <li>The job's state (pending, running, or completed)</li> <li>The job's current walltime</li> <li>The job's allowed walltime</li> <li>The number of nodes requested and/or allocated to the job</li> <li>The reason why the job has the status it has</li> </ol>","tags":["tutorial","slurm","job script"]},{"location":"Job_Script_and_Job_Submission/#viewing-job-outputs","title":"Viewing job outputs","text":"<p>Every SLURM job creates a file that contains the standard output and standard error from the job.</p> <p>The default name is <code>slurm-&lt;jobid&gt;.out</code> where <code>&lt;jobid&gt;</code> is the job ID assigned when the job was submitted.</p> <p>Find the output log from your job and view it with <code>less &lt;filename&gt;</code>. You should see several lines printing the thread and process information for each CPU involved.</p> <p>The SLURM log files are essential for investigating whether or not your job ran successfully and for finding out why it failed.</p>","tags":["tutorial","slurm","job script"]},{"location":"Job_with_Checkpointing_Run/","title":"Job with Checkpointing Run","text":"<p>Checkpointing is a function to save a snapshot of an application's running state, so it can restart from the saved point in case job running fails or reaches the time limit. Some applications might already have this feature for long-term computation. If users develop their own program, it is encouraged to implement checkpointing as a part of their codes. They can develop a function to write result variables to file systems at regular intervals and a function to read those variables in when restart.</p> <p>However if the program you used does not and can not include the feature, you may consider using \"Distributed MultiThreaded CheckPointing\" (or DMTCP) installed on HPCC nodes. DMTCP is a tool for transparently checkpointing the state of a distributed program spread across many machines without modifying the user's program or the operating system kernel.</p> <p>On the HPCC, you can use DMTCP directly or use the <code>longjob</code> powertool to automate this process for you. For more details about DMTCP, please refer to their web site.</p>","tags":["explanation","checkpointing"]},{"location":"Jupyter_Notebook_in_VS_Code/","title":"Running Jupyter Notebooks on the HPCC through VS Code","text":"<p>VS Code supports Jupyter notebooks. This how-to guide will show you how to run these notebooks on the HPCC while using VS Code on your personal/local computer.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#prerequisites","title":"Prerequisites","text":"<ol> <li>You'll need to connect VS Code to the HPCC over SSH. Pay special note that this requires setting up SSH tunneling.</li> <li>Building off the SSH tunneling needed for the previous step, you'll also need to set up tunneling to compute nodes.</li> <li>(Optional) Set up SSH keys to connect to the HPCC without needing to enter a password. For Windows users, use the MobaXterm GUI instructions (or ensure that your private SSH key is saved to <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>).</li> <li>Install either the <code>jupyter</code> package or (if you don't plan on using Jupyter through OnDemand) the <code>ipykernel</code> package to the Python environment you plan on using. We recommend installing <code>jupyter</code> to avoid future confusion.</li> <li>Install the Python and Jupyter extensions in VS Code.</li> </ol>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#start-an-interactive-job","title":"Start an Interactive Job","text":"<p>If we want to run Python and/or Jupyter on the HPCC, it's best to do so from an interactive job. This will give us a compute node on which to run our calculations. While a development node may seem more convenient, remember that jobs using more than 2 hours of CPU time are automatically cancelled! For example, we can request a single CPU for four hours: <pre><code>salloc -t 04:00:00 --mem=4GB\n</code></pre> It is strongly suggested that you request a few GB of memory. The default is only 750 MB.</p> <p>Once your interactive job starts, run the command <code>hostname</code>; e.g. <pre><code>bash-4.2$ hostname\nlac-046\n</code></pre> to get the name of the compute node you have been allocated. Copy or write down the output of this command for later.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#connect-vs-code-to-the-interactive-host","title":"Connect VS Code to the Interactive Host","text":"<p>Inside VS Code, press F1 and select 'Remote-SSH: Connect to Host...' You may need to start typing this option to get it to appear.</p> <p></p> <p>Note</p> <p>If this option is not available, please follow the steps in this how-to guide.</p> <p>Instead of selecting a host from the drop down list, enter the host of your interactive job into the textbox.</p> <p></p> <p>A new VS Code window will pop up. Follow any of the prompts that follow, selecting \"Continue\" and \"Linux\" as applicable. When you are successfully connect, the bottom left of the VS Code window will show the hostname you just connected to. You may open files and folders as you normally would when connecting VS Code with SSH.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Jupyter_Notebook_in_VS_Code/#set-your-python-kernel","title":"Set your Python Kernel","text":"<p>Open an existing Jupyter Notebook file or create one by going to the File menu, selecting \"New File..\", and choosing the \"Jupyter Notebook\" option.  With your Notebook open, click \"Select Kernel\" in the upper right and then choose \"Python Environments.\" If a Kernel was automatically chosen for you, you'll first need to choose \"Select Another Kernel.\"</p> <p></p> <p>Pay attention to the paths given for each entry.  In the screenshot below, for example, the \"Recommended\" Python interpreter corresponds to the base Conda environment. We can also see our other Conda environments (in this example, there is only one called \"astro-analysis\") as well as system installations of Python in <code>/bin</code>, <code>/usr/bin</code>, and <code>/opt/software</code>. These system installation are likely not the Python versions you want to use.</p> <p></p> <p>You may now execute cells in your Jupyter Notebook while running on the HPCC! VS Code will remember your choice of kernel next time you open that workspace.</p>","tags":["how-to guide","VS Code","VScode","Jupyter"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/","title":"Linux Command Line Interface for Beginners I","text":"<p>The operating system of the MSU HPC is Ubuntu, which is a distribution of Linux. So if you want to use our system, it is essential to equip some basic knowledge of Linux. Even though Linux supports a GUI, most works are done on a terminal via text. The Linux command line is a text interface to Linux.\u00a0</p> <p>We will walk through some practical exercises to become familiar with a few basic commands and concept.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#navigation","title":"Navigation","text":"<p>Let's run the first command. Type <code>pwd</code> and pressing the Enter or Return key to run it (From now, I'll not mention pressing Enter/Return key to run a command).</p> input<pre><code>pwd\n</code></pre> output<pre><code>/mnt/home/iamsam/\n</code></pre> <p>You will see a path such as <code>/mnt/home/user/your_id</code></p> <p><code>pwd</code>\u00a0is an abbreviation of 'print working directory'. It prints out the shell's current working directory. You can change the working directory using the <code>cd</code> command, an abbreviation for 'change directory'.</p> input<pre><code>cd /\npwd\n</code></pre> output<pre><code>/\n</code></pre> <p>Now your working directory is '/' which is the root directory.\u00a0There is nothing much you can do on the root directory, so let's go to your 'home' directory.</p> input<pre><code>cd \npwd\n</code></pre> output<pre><code>/mnt/home/iamsam/\n</code></pre> <p>Regardless of your location, when you just type <code>cd</code>, you will be home. You can also type <code>cd \\~</code> instead of <code>cd</code> to be back your home.</p> <p>To go the previous directory, type <code>cd -</code></p> input<pre><code>cd -\npwd\n</code></pre> output<pre><code>/\n</code></pre> <p>The root directory has many subdirectories including your home directory. Let's go to the 'bin' directory.</p> input<pre><code>cd bin\npwd\n</code></pre> output<pre><code>/bin\n</code></pre> <p>To go up to the parent directory (it is / for us now), use the special syntax of two dots with <code>cd</code> such as</p> input<pre><code>cd ..\npwd\n</code></pre> output<pre><code>/\n</code></pre> <p>To go up to the previous directory, use <code>-</code> with <code>cd</code> such as</p> input<pre><code>cd -\npwd\n</code></pre> output<pre><code>/bin\n</code></pre> <p>You can use <code>..</code> more than once if you have to move up multiple levels of parent directories.</p> input<pre><code>cd ../..\npwd\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#relative-and-absolute-paths","title":"Relative and absolute Paths","text":"<p>A path is an address of a directory. Most of the examples we've looked at so far use relative paths. So your final location is decided based on your current working directory. However, sometimes you want to use an absolute path than a relative one. Your home's absolute path at HPC is <code>/mnt/home/your_id</code>. See the example to find how to use a relative and absolute path.</p> input<pre><code>cd /\npwd\n</code></pre> output<pre><code>/\n</code></pre> input<pre><code>cd -\npwd\n</code></pre> output<pre><code>/mnt/home/iamsam\n</code></pre> input<pre><code>cd /\ncd /mnt/home/iamsam\npwd\n</code></pre> output<pre><code>/mnt/home/iamsam\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-directories","title":"Creating and removing directories","text":"<p>To make a directory, use <code>mkdir</code> (short for 'make directory') such as</p> input<pre><code>mkdir temp01\nls\n</code></pre> output<pre><code>temp01\n</code></pre> <p><code>ls</code> is a command to list files and folder. We will learn it in a minute. You can create multiple directories as well.</p> input<pre><code>mkdir temp02 temp03 temp04\nls\n</code></pre> output<pre><code>temp01 temp02 temp03 temp04\n</code></pre> <p>To make a subdirectory, use with <code>-p</code> option such as</p> input<pre><code>mkdir -p temp04/temp05/temp06\nls temp04\n</code></pre> output<pre><code>temp05\n</code></pre> <p>To remove directory use <code>rm</code> command with <code>-r</code>. Without <code>-r</code> option, rm will not delete directories (but you can delete files). <code>-r</code> means recursive.</p> input<pre><code>rm temp04\n</code></pre> output<pre><code>rm: cannot remove 'temp04': Is a directory\n</code></pre> input<pre><code>rm -r temp04\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#creating-and-removing-files","title":"Creating and removing files","text":"<p>You can use editors to create files, but it is out of scope of this tutorial. Let's use <code>ls</code> and a pipe <code>&gt;</code>\u00a0(we will explain pipes later).</p> input<pre><code>ls\n</code></pre> output<pre><code>temp01 temp02 temp03\n</code></pre> input<pre><code>ls &gt; list.txt\nls\n</code></pre> output<pre><code>list.txt temp01 temp02 temp03\n</code></pre> <p>Now we have three directories (<code>temp01</code>, <code>temp02</code>, <code>temp03</code>) and one file (<code>list.txt</code>). We already know how to delete directories. To delete files, we use command <code>rm</code> such as</p> input<pre><code>rm list.txt\nls\n</code></pre> output<pre><code>temp01 temp02 temp03\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#copying-and-renaming-directories-and-files","title":"Copying and renaming directories and files","text":"<p>To copy files or directories, use <code>cp</code> such as</p> input<pre><code>cp list.txt list2.txt\nls\n</code></pre> output<pre><code>list.txt list2.txt temp01 temp02 temp03\n</code></pre> <p>With the <code>-r</code> option, you can copy files and directories recursively, i.e., copy subdirectories and files.</p> <p>The <code>mv</code> command moves files/directories or renames them.</p> input<pre><code>mv list2.txt temp01\nls\n</code></pre> output<pre><code>list.txt temp01 temp02 temp03\n</code></pre> input<pre><code>ls temp01\n</code></pre> output<pre><code>list2.txt\n</code></pre> input<pre><code>cd temp01\nmv list2.txt list3.txt\nls\n</code></pre> output<pre><code>list3.txt\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#listing-directories-and-files","title":"Listing directories and files","text":"<p>We already used ls. To list directories and files, us\u00a0ls (short for 'list').\u00a0</p> input<pre><code>ls\n</code></pre> output<pre><code>temp01 temp02 temp03\n</code></pre> <p>There are many options for <code>ls</code>. Most frequently used options are</p> <ul> <li><code>-a</code>: list all files and directories including hidden contents</li> <li><code>-h</code>: print sizes in human readable format (e.g.: 1K, 2.4M, 3.1G)</li> <li><code>-l</code>: list with a long listing format</li> <li><code>-t</code>: sort my modification time</li> </ul> <p>You can use options separately like <code>ls -l -a -t</code> or together like <code>ls -lat</code>.</p> input<pre><code>ls -l -a -t\n</code></pre> output<pre><code>total 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n</code></pre> input<pre><code>ls -lat\n</code></pre> output<pre><code>total 8\n-rw-r--r--    1 iamsam  staff    30 Mar 23 15:37 list.txt\ndrwxr-xr-x    6 iamsam  staff   192 Mar 23 15:37 .\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp03\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp02\ndrwxr-xr-x    2 iamsam  staff    64 Mar 23 15:37 temp01\ndrwxr-xr-x+ 113 iamsam  staff  3616 Mar 23 15:21 ..\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_I/#exercises","title":"Exercises","text":"<p>Now let's do some exercises.\u00a0</p> <ul> <li>Log in your MSU HPC account and go to any dev-node.\u00a0</li> <li>Create a <code>linux_tutorial</code>\u00a0dir on your home.</li> <li>Copy a folder and contents for this tutorial from\u00a0</li> <li><code>/mnt/research/common-data/workshops/intro2Linux_iamsam</code> to   <code>linux_tutorial</code>\u00a0dir on your home</li> <li>Go to <code>linux_tutorial</code></li> <li>Find a hidden directory and rename it to <code>not_hidden</code></li> <li>Check the contents of <code>not_hidden</code></li> <li>Create a new directory called <code>new_dir</code></li> <li>Copy the file <code>youfoundit.txt</code> into <code>new_dir</code></li> <li>Remove <code>garbage</code> dir</li> </ul> This is an answer (not including the login process). input<pre><code>mkdir linux_tutorial\ncp -r /mnt/research/common-data/workshops/intro2Linux_iamsam linux_tutorial\ncd linux_tutorial\nls -a\nmv .hidden not_hidden\nls not_hidden\nmkdir new_dir\ncp not_hidden/youfoundit.txt new_dir\nrm -r garbage\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/","title":"Linux Command Line Interface for Beginners II","text":"","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#files-and-folders","title":"Files and folders","text":"<p>Linux has a single directory 'tree', separated by slash. The top of the tree is the 'root' directory . All additional disks are connected on <code>/mnt</code> ('mounted'). In Linux, there is no concept of drive letters.</p> <p><code>tree</code> is a recursive directory listing program which prints a depth indented listing of files. With no arguments, tree lists the files in the current directory. You can change the depth with <code>-L</code> argument.</p> <p>The example shows the structure of the <code>linux_tutorial</code> directory which we used for an exercise.</p> input<pre><code>tree\n</code></pre> output<pre><code>.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n</code></pre> input<pre><code>tree -L 1\n</code></pre> output<pre><code>.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n</code></pre> <p><code>tree</code> command does not show hidden files/directory by default. To see hidden files/directories, use <code>-a</code> argument.</p> input<pre><code>tree -a\n</code></pre> output<pre><code>.\n\u251c\u2500\u2500 000.txt\n\u251c\u2500\u2500 001.txt\n\u251c\u2500\u2500 002.txt\n\u251c\u2500\u2500 009.txt\n\u251c\u2500\u2500 010.txt\n\u251c\u2500\u2500 a.txt\n\u251c\u2500\u2500 A.txt\n\u251c\u2500\u2500 b.txt\n\u251c\u2500\u2500 B.txt\n\u251c\u2500\u2500 c.txt\n\u251c\u2500\u2500 C.txt\n\u251c\u2500\u2500 garbage\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 garbage_02\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 garbage_03\n\u251c\u2500\u2500 .hidden\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 youfoundit.txt\n\u251c\u2500\u2500 my_data.dat\n\u251c\u2500\u2500 my-data.txt\n\u251c\u2500\u2500 z.txt\n\u2514\u2500\u2500 Z.txt\n\n2 directories, 19 files\n</code></pre>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#file-permissions","title":"File permissions","text":"<p>Linux has a method to keep files private and safe. When you type\u00a0<code>ls -l</code> you will get a lot of information of your directory including files such as</p> <p></p> <p>The first letter shows a type of a file.</p> <ul> <li><code>-</code>: normal file</li> <li><code>d</code>: directory</li> <li><code>l</code>: symbolic link</li> </ul> <p>The next nine characters shows permission; first three is for you, next three is for your group members, and last three is for a whole world. Let's look at the first file in the picture which shows <code>d rwxr-xr-x</code>.</p> <p>The first character is 'd', and therefore, even though the name is 'hello.txt', it is not a file, but a directory. Next nine characters represent the settings for the three sets of permissions.</p> <ul> <li>The first three characters show the permissions for the user who   owns the file (user permissions).</li> <li>The middle three characters show the permissions for members of the   file\u2019s group (group permissions).</li> <li>The last three characters show the permissions for anyone not in the   first two categories (other permissions).</li> </ul> <p>The letters represent:</p> <ul> <li><code>r</code>: Read permissions. The file can be opened, and its content viewed.</li> <li><code>w</code>: Write permissions. The file can be edited, modified, and deleted.</li> <li><code>x</code>: Execute permissions. If the file is a script or a program, it can   be run (executed).</li> </ul> <p>In our screenshot, the first three characters are <code>rwx</code> which means you (owner) can read, write/modify or execute (it means you can go inside the directory here). Next\u00a0three characters are <code>r_x</code> which means your group members can read, or execute, but can not modify the contents. Last\u00a0three characters are <code>r_x</code> which means everyone else can read, and execute.</p> <p>Another examples:</p> <ul> <li><code>---------</code>: it means no permissions have been granted at all.</li> <li><code>rwxrwxrwx</code>: it means full permissions have been granted to everyone.</li> </ul> <p>Permissions can be set with <code>chmod</code> command, and ownership set with <code>chown</code>. You can only change these for files you are the owner of. To use <code>chmod</code>, we need to tell it 'who' we are setting permissions for, 'what' change we are making, 'which' permissions we are setting.</p> <p>The \u201cwho\u201d values can be:</p> <ul> <li><code>u</code>: User - the owner of the file.</li> <li><code>g</code>: Group - members of the group the file belongs to.</li> <li><code>o</code>: Others - people not governed by the u and g permissions.</li> <li><code>a</code>: All - all of the above.</li> </ul> <p>If none of these are used, <code>chmod</code> behaves as if <code>a</code> had been used.</p> <p>The \u201cwhat\u201d values can be:</p> <ul> <li><code>\u2013</code>: Minus sign. Removes the permission.</li> <li><code>+</code>: Plus sign. Grants the permission. The permission is added to the   existing permissions. If you want to have this permission and only   this permission set, use the = option, described below.</li> <li><code>=</code>: Equals sign. Set a permission and remove others.</li> </ul> <p>The \u201cwhich \u201d values can be:</p> <ul> <li><code>r</code>:\u00a0 The read permission.</li> <li><code>w</code>: The write permission.</li> <li><code>x</code>: The execute permission.</li> </ul> <p>You can also use a three three-digit numbers (total nine) to provide the permission with\u00a0<code>chmod</code>. The leftmost digit represents the permissions for the owner. The middle digit represents the permissions for the group members. The rightmost digit represents the permissions for the others. <code>x</code> is 1, <code>w</code> is 2, and <code>r</code> is 4, and the some of these three numbers are the permission.</p> <p>The digits you can use and what they represent are listed here:</p> <ul> <li>0: (000) No permission.</li> <li>1: (001) Execute permission.</li> <li>2: (010) Write permission.</li> <li>3: (011) Write and execute permissions.</li> <li>4: (100) Read permission.</li> <li>5: (101) Read and execute permissions.</li> <li>6: (110) Read and write permissions.</li> <li>7: (111) Read, write, and execute permissions.</li> </ul> <p>The tables shows a summary of the <code>chmod</code> command.</p> User Type Permission u - \u00a0user + add r - read (4) g - group - delete w - write (2) o - others = set exactly x - execute (1) a - all","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#examples","title":"Examples","text":"<p><code>chmod u=rx, og=r my_file1.txt</code> : set the user has read, and executable permissions; group/other have read permission only for <code>my_file1.txt</code>.</p> <p><code>chmod 544 my_file1.txt</code>: same as\u00a0<code>chmod u=rx, og=r my_file1.txt</code>.\u00a0</p> <p><code>chmod 750 my_file1.txt</code>:\u00a0set the user has read, write, and executable permissions; group has read/executable permission; others no permission.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#concatenate-files","title":"Concatenate files","text":"<p>The <code>cat</code> (short for \"concatenate\") command is one of the most frequently used command in Linux. <code>cat</code> command allows us to create single or multiple files, view contain of file, concatenate files and redirect output in terminal or files.</p> <ul> <li>To display contents of a file: <code>cat filename</code></li> <li>To view contents of multiple files: <code>cat file1 file2</code></li> <li>To create a file: <code>cat\u00a0&gt; file2.txt</code></li> </ul> <p>If file content is large, and won't fit in a terminal screen, you can use <code>more</code> and <code>less</code> with <code>cat</code> command such as (we are using pipes,\u00a0<code>|</code>, which will be covered later)</p> <ul> <li><code>cat file2.txt | more</code></li> <li><code>cat file2.txt | less</code></li> </ul> <p><code>more</code>, <code>less</code>, and <code>most</code>\u00a0(<code>most</code> has more features than <code>more</code> and <code>less</code> commands) are commands to open a given file for interactive reading.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#redirection","title":"Redirection","text":"<p>Redirection is a feature such that when you execute a command with it, you can change the standard input/output devices. The standard input (stdin) device is the keyboard, and the standard output (stdout) device is the screen. With redirection, stdin/stdout can be changed.</p> <ul> <li><code>&gt;</code>: Output redirection (overwriting)</li> <li>e.g. <code>ls -la\u00a0&gt; list.txt</code> (<code>ls -la</code> results save as <code>list.txt</code> instead of       standard output (screen))</li> <li><code>&gt;&gt;</code>: Output redirection (appending)</li> <li>e.g.\u00a0<code>ls -l\u00a0&gt;&gt; list.txt</code> (<code>ls -l</code> result will be appended at the end of       the <code>list.txt</code>. If there is no <code>list.txt</code>, it works as <code>ls -la\u00a0&gt; list.txt</code>)</li> <li><code>&lt;</code>: Input redirection</li> <li><code>&gt;&amp;</code>: Writing the output from one file to the input of another file</li> <li>e.g. <code>matlab\u00a0&gt; outfile 2&gt;&amp;1</code> : send stdout and stderr to <code>outfile</code>. Here       1 and 2 are file descriptors. File descriptor 1 is the standard output       (stdout), and\u00a02 is the standard error (stderr).\u00a0<code>&gt;</code> is redirection,       and <code>&amp;</code> indicates that what follows and precedes is a file descriptor       and not a filename.</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#wildcards","title":"Wildcards","text":"<p>Wildcards are symbols or special characters that represent other characters. You can use them with any command such as <code>ls</code>/<code>rm</code>/<code>cp</code> etc.</p> <ul> <li><code>*</code>: anything or nothing</li> <li><code>?</code>: single character</li> <li><code>[ ]</code>: any character or range of characters</li> <li><code>[! ]</code>: inverse the match of <code>[ ]</code></li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#examples_1","title":"Examples","text":"<ul> <li><code>ls *.txt</code>: list all txt files</li> <li><code>ls *-?.txt</code>: list all files with <code>-</code> and with one character in front of <code>.txt</code></li> <li><code>ls [0-9]*.txt</code>: list all files starting with a number</li> <li><code>ls [A-Z]*.txt</code>: list all files starting with a capital letter?\u00a0<code>[A-Z]</code> can be different based on <code>LC_COLLATE</code> value. For further discussion, check here. In HPC at MSU, the default of\u00a0<code>[A-Z]</code> is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</li> <li>Try <code>ls [[:lower:]].txt</code>;\u00a0 <code>ls [[:upper:]].txt</code>;\u00a0 <code>ls [[:lower:][:upper:]].txt</code></li> <li><code>ls [!a-Z]*.txt</code>: list txt files that don\u2019t begin with any letter.</li> </ul> <p>For more information, refer to\u00a0Regular Expressions.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#manuals","title":"Manuals","text":"<p>Linux includes a built in manual for nearly all commands. Type <code>man</code> followed by the commands, e.g., <code>man man</code>.</p> <p>To navigate the man pages use the arrow keys to scroll up and down or use the enter key to advance a line, space bar to advance a page, letter u to go back a page. Use the q key to quit out of the display.</p> <p>The manual pages often include these sections:</p> <ul> <li>Name: a one line description of what it does</li> <li>Synopsis: basic syntax for the command line.</li> <li>Description: describes the program\u2019s functionalities.</li> <li>Options: lists command line options available for this program.</li> <li>Example: examples of some of the options available.</li> <li>See Also: list of related commands.</li> </ul> <p>Note that options can be with single dash <code>-</code> or double dash <code>--</code></p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#commands-for-monitoring-files","title":"Commands for monitoring files","text":"<p>Following commands are used for monitoring files. </p> <ul> <li><code>wc</code>: count words, lines or characters</li> <li>e.g. <code>who | wc -l</code>: number of users logged in</li> <li><code>grep</code>: find patterns in files or data, returns just matching lines</li> <li>e.g. <code>who | grep $USER</code>: find your username in users logged in</li> <li><code>sort</code>: given a list of items, sort in various ways</li> <li>e.g. <code>who | sort</code>: sort the users logged in</li> <li><code>head</code>: list only top n (5 by default) lines of file</li> <li>e.g. <code>who &gt; who.txt; head who.txt</code>: save the users logged in to a file and show the first five</li> <li><code>tail</code>: list only last n (5 by default) lines of file</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#archiving-and-compression","title":"Archiving and compression","text":"<p>Following commands are used for archiving and compression: <code>zip</code>, <code>unzip</code>, <code>tar</code>.</p> <ul> <li><code>unzip</code>: unzip a file</li> <li><code>tar</code>: create (<code>tar -c</code>) or extract (<code>tar -x</code>) \u2018tape archive\u2019 file.</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#exercise","title":"Exercise","text":"<p>Using the man pages, find out what the default number of lines that head and tail will display, and how to limit those to just one line.</p> <p>Can you <code>tar</code> all files and folders in workshop folder? Question? Use <code>man</code>.\u00a0</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#environment-variables","title":"Environment variables","text":"<p>Shell maintains and you can set \u2018variables\u2019 that the shell uses for configuration and in your script.\u00a0Variables start with <code>$</code>, and can be seen with <code>echo $VARNAME</code>.</p> <p>Explore common variables with the echo command and list what they are\u00a0<code>$HOME</code>, <code>$USER</code>, <code>$SHELL</code>, <code>$PATH</code>.</p> <p>For more information, please refer to\u00a0Variables I.</p>","tags":["tutorial","command line"]},{"location":"Linux_Command_Line_Interface_for_Beginners_II/#see-also","title":"See also","text":"<ul> <li>Introduction to UNIX Course Online</li> <li>Unix Tutorial for Beginners</li> </ul>","tags":["tutorial","command line"]},{"location":"Linux_Shell/","title":"Linux shells","text":"<p>Note</p> <p><code>bash</code> is the only officially supported shell for the HPCC. For information about running shell scripts written for other shells (zsh, csh, etc) or using an alternative shell, see this Lab Notebook.  </p>","tags":["reference"]},{"location":"Linux_Shell/#linux-shells","title":"Linux shells","text":"<p>A Unix/Linux shell is a command-line interpreter which provides a user interface for the Unix/Linux operating system. Users control the operation of a computer by submitting single commands or by submitting one or more commands via a shell script. </p> <p>The default shell provided to HPCC users is the <code>bash</code> shell.</p> <p>To find out your current shell run <code>echo $SHELL</code>.</p>","tags":["reference"]},{"location":"Linux_Shell/#environment-variables","title":"Environment variables","text":"<p>Environment variables are a set of dynamically named values which can control the way running processes will behave on a computer. Many of the Unix commands and tools require certain environment variables to be set. Many of these are set automatically for the users when they log in or load applications via the module command. </p> <p>To view your current set of environment variables run <code>env</code>. </p> <p>To assign a new value to an environment variable in either <code>bash</code>: <code>export &lt;name&gt;=&lt;value&gt;</code></p> <p>To print the value of a variable: <code>echo $&lt;name&gt;</code></p>","tags":["reference"]},{"location":"Linux_Shell/#commonly-used-environment-variables","title":"Commonly used environment variables","text":"<p>Bash variables are preceded with $ and optionally enclosed in brackets when used e.g. <code>$USER</code> or <code>${USER}</code>.</p> <ul> <li><code>$HOSTNAME</code>:  Name of the computer currently running the shell or script. If used in a SLURM job, this should be one of the nodes listed in the variable <code>$SLURM_JOB_NODELIST</code> </li> <li><code>$USER</code>: User's name (NetID). Useful if you would like to dynamically generate a directory on some scratch space.</li> <li><code>$HOME</code>: User's home directory. Can also use <code>~/</code> symbol.</li> <li><code>$SCRATCH</code>: Your folder on the scratch disk.                                                   </li> <li><code>$TMPDIR</code>: Temporary working folder of a running job at <code>/mnt/local/$SLURM_JOBID</code>. </li> </ul> <p>To see all variables in the context of your job, add this line to your job script, which will list all variables that contain the word 'SLURM'</p> <pre><code>$ env | grep SLURM\n</code></pre>","tags":["reference"]},{"location":"List_Jobs_by_squeue_sview/","title":"List Jobs by squeue &amp; sview","text":""},{"location":"List_Jobs_by_squeue_sview/#squeue-command","title":"squeue command","text":"<p>After you submit jobs, you can check their information and status in the queue. The simplest way is to use squeue command to list all of your jobs:</p> <pre><code>$ squeue -l -u $USER\nThu Aug  2 14:45:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1   nobody  RUNNING       0:32  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes   nobody  RUNNING       0:11     30:00      2 lac-[386-387]\n              9290 general-l  LongJob   nobody  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where the argument <code>-l</code> reports more of the available information (i.e. <code>l</code> for long format) and <code>-u</code> specifies which user's jobs to show. You may find a complete squeue specifications from the SLURM web site.</p> <p>HPCC staff also wrote some powertools commands so users can see their jobs conveniently. Before using\u00a0powertools commands, please make sure powertools module is loaded by using moudle list (By default,  the powertools module should be loaded unless the user has purged modules  with module purge)</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) powertools/1.2\n</code></pre> <p>One of the commands sq works the same as the above squeue command:</p> <pre><code>$ sq                         # powertools command\nThu Aug  2 14:48:51 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              9405 general-s     Job1 username  RUNNING       0:35  01:00:00      1 lac-198\n              9406 general-s Rmpi_tes username  RUNNING       0:14     30:00      2 lac-[386-387]\n              9290 general-l  LongJob username  PENDING       0:00  36:00:00     40 (Resources)\n</code></pre> <p>where it shows the job IDs, job partition, job name, username, job state, elapsed time, walltime limit, total number of nodes and the node list or the waiting reason for the user's each jobs. Users can also use qs command to see more information:</p> <pre><code>$ qs                         # powertools command\nThu Aug  2 14:49:27 2019\n                                                                                        Start_Time/\n     JobID         User     Account     Name   Node CPUs  TotMem  Tres   WallTime  ST  Elapsed_Time  NodeList(Reason)\n-----------------------------------------------------------------------------------------------------------------------\n    60889405    MyHPCCAcc   general       Job1    1   16      2G  gpu:1    3:55:00  R      1:49:24   lac-198\n    60889496    MyHPCCAcc   general  Rmpi_test    2    8   1500M   N/A       30:00  R        29:46   lac-[386-387]         \n    60889290    MyHPCCAcc  classres    LongJob   40   80    750M  k80:1 3-00:00:00 PD 08-03T10:29:41 (Resources)\n</code></pre> <p>where more items, such as, total number of CPUs, memory, gpu per node and job start time (for pending jobs) or elapsed time (for running jobs) are shown. For a complete usage of squeue command, please refer to the SLURM web site.</p>"},{"location":"List_Jobs_by_squeue_sview/#sview-command","title":"sview command","text":"<p>Besides the text listing of the jobs, SLURM also offer a command to show the squeue information with a graphical interface. Use the command sview:</p> <pre><code>$ sview\n</code></pre> <p>You will see an image of a job list displaying all jobs in the queue:</p> <p></p> <p>Click on each job, it will pop out another window and show the detailed information. For a complete usage of sview command, please refer to the SLURM web site.</p>"},{"location":"List_of_Job_Specifications/","title":"List of Job Specifications","text":"<p>The following are lists of basic <code>#SBATCH</code> specifications, broken up by purpose. To see the complete list of options, please refer to the SLURM <code>sbatch</code> command page, but be aware that not all options are implemented for the HPCC.</p>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#resource-requests","title":"Resource Requests","text":"<p>These options specify the computing resources needed for your job. Not all options need to be specified. See our example batch scripts.</p> Option Description Examples <code>-A</code>,<code>--account=&lt;account&gt;</code> This option tells SLURM to use the specified buy-in account. Unless you are an  authorized user of the account, your job will not run. <code>#SBATCH   -A &lt;account&gt;</code> <code>-C</code>,<code>--constraint=&lt;list&gt;</code> Request node feature. May be specified with symbol <code>&amp;</code> for and, <code>|</code> for or, etc.<p>Constraints using <code>|</code> must be prepended with <code>NOAUTO:</code>. See the Job Constraints page for more information. <code>#SBATCH -C NOAUTO:intel16|intel14</code> <code>-c</code>,<code>--cpus-per-task=&lt;ncpus&gt;</code> Require <code>&lt;ncpus&gt;</code> number of processors per task. Without this option, the controller will just try to allocate one processor per task.<p>Due to changes with <code>srun</code>, this option must also be specified when calling <code>srun</code> inside your script. <code>#SBATCH   -c 3</code> (3 cores per node) <code>-G</code>,<code>--gpus=[&lt;type&gt;:]&lt;number&gt;</code> Specify the total number of GPUs required for the job. An optional GPU type specification can be supplied. Valid GPU types are <code>k20</code>, <code>k80</code>, <code>v100</code> and <code>a100</code>. Note that type is optional, but the number of GPUs is necessary. The allocation has to contain at least one GPU per node. <code>#SBATCH   --gpus=k80:2</code> (request 2 k80 GPUs for entire job) <code>#SBATCH --gpus=2</code> (request 2 GPUs for entire job) <code>--gpus-per-node=[&lt;type&gt;:]&lt;number&gt;</code> Specify the number of GPUs required for the job on each node included in the job's resource allocation. GPUs are specified in the same format as <code>--gpus</code>. <code>#SBATCH   --gpus-per-node=v100:8</code> (request 8 v100 GPUs for each node requested by job) <code>#SBATCH --gpus-per-node=8</code> (request 8 GPUs for each node requested by job) <code>--gpus-per-task=[&lt;type&gt;:]&lt;number&gt;</code> Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. GPUs are specified in the same format as <code>--gpus</code>.<p>This option requires an explicit task count, e.g. <code>-n</code>, <code>--ntasks</code> or <code>--gpus=X --gpus-per-task=Y</code>. This option will implicitly set <code>--gpu-bind=per_task:&lt;gpus_per_task&gt;</code>, but that can be overridden (see the full documentation). <code>#SBATCH   --gpus-per-task=k80:2</code> (request 2 k80 GPUs for each task requested by job)<code>#SBATCH --gpus-per-task=2</code> (request 2 GPUs for each task requested by job) <code>--mem=&lt;size[units]&gt;</code> Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem-per-cpu</code> and <code>--mem-per-gpu</code>. <code>#SBATCH   --mem=2G</code> <code>--mem-per-cpu=&lt;size[units]&gt;</code> Memory required per allocated CPU. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem</code> and <code>--mem-per-gpu</code>. <code>#SBATCH   --mem-per-cpu=2G</code> <code>--mem-per-gpu=&lt;size[units]&gt;</code> Memory required per allocated GPU. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. <p> Mutually exclusive with <code>--mem</code> and <code>--mem-per-cpu</code>. Does not impact the amount of GPU memory users may access. See Requesting GPUs for more. <code>#SBATCH   --mem-per-gpu=2G</code> <code>-N</code>,<code>--nodes=&lt;minnodes[-maxnodes]&gt;</code> Request that a minimum of <code>minnodes</code> nodes be allocated to this job. A maximum node count may also be specified with <code>maxnodes</code>. If only one number is specified, this is used as both the minimum and maximum node count.<p>The job will be allocated as many nodes as possible within the range specified and without delaying the initiation of the job.  Note that the environment variable <code>SLURM_JOB_NUM_NODES</code> will be set to the count of nodes actually allocated to the job. If not specified, the default behavior is to allocate enough nodes to satisfy the other requested resources as expressed by per-job specification options. <code>#SBATCH   --nodes=2-4</code> (Request 2 to 4 different nodes) <code>-n</code>,<code>--ntasks=&lt;number&gt;</code> This option advises the Slurm controller that job steps run within the allocation will launch a maximum of <code>number</code> tasks and to provide for sufficient resources. Actual tasks must be launched by application within the job script.<p>The default is one task per node, but note that the <code>--cpus-per-task</code> option will change this default. Use options such as <code>--ntasks-per-node</code> for finer control over the distribution of tasks. <code>#SBATCH   -n 4</code> (All tasks could be in 1 to 4 different nodes) <code>--ntasks-per-node=&lt;ntasks&gt;</code> Specify the number of tasks to run per node. Meant to be used with the <code>--nodes</code> option.<p>This is related to <code>--cpus-per-task</code>, but does not require knowledge of the actual number of cpus on each node. If used with <code>--ntasks</code>, the <code>--ntasks</code> option will take precedence and <code>--ntasks-per-node</code> will be treated as a maximum count of tasks per node. <code>#SBATCH --ntasks 4</code> <code>-q</code><code>--qos=&lt;qos&gt;</code> Request a quality of service. The HPCC allows you to specify the scavenger QOS. <code>#SBATCH --qos=scavenger</code> <code>-t</code>,<code>--time=&lt;time&gt;</code> Set a limit on the total run time of the job allocation. The total run time in the form: HH:MM:SS or DD-HH:MM:SS <code>#SBATCH   -t 00:20:00</code> <code>-w</code>,<code>--nodelist=&lt;node name list&gt;</code> Request a specific list of your buy-in nodes. The job will contain all of these hosts and possibly additional hosts as needed to satisfy resource requirements.<p>The list may be specified as a comma-separated list of hosts, a range of hosts, or a filename. The host list will be assumed to be a filename if it contains a <code>/</code> character. <code>#SBATCH -w host1,host2,host3,...</code><code>#SBATCH -w host[1-5,7,...]</code><code>#SBATCH -w /mnt/home/userid/nodelist</code> <code>-x</code>,<code>--exclude=&lt;node name list&gt;</code> Explicitly exclude certain nodes from the resources granted to the job. The syntax follows that of <code>--nodelist</code>. <code>#SBATCH -x host[1-5]</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#job-environment","title":"Job Environment","text":"<p>As opposed to the computing resources needed by a job, these parameters impact the computing environment in which a SLURM job is running. </p> Option Description Examples <code>-a</code>,<code>--array=&lt;indexes&gt;</code> Submit a job array with multiple jobs to be executed; that is, a group of jobs requiring the same set of resourcs. Each job has the same job ID (<code>$SLURM_JOB_ID</code>) but different array ID (<code>$SLURM_ARRAY_TASK_ID</code>). The <code>indexes</code> passed to the argument identify what array ID values should be used.<p>The indicies can be a mix of lists or ranges. A <code>:</code> can be used to identify the step for a range. The <code>%</code> separator specifies the maximum number of jobs running simultaneously. <code>#SBATCH -a 0-15</code>,<code>#SBATCH -a 0,6,16-32</code>,<code>#SBATCH -a 0-15:4</code> (same as <code>#SBATCH \u2013a 0,4,8,12</code>),<code>#SBATCH --array=0-15%4</code> (max 4 jobs running simultaneously) <code>-D</code>,<code>--chdir=&lt;directory&gt;</code> Set the working directory of the batch script to <code>&lt;directory&gt;</code> before it is executed. The path can be specified as full path or relative path to the directory where the command is executed. <code>#SBATCH   -D /mnt/scratch/username</code> <code>--export=[ALL,]&lt;environment variables&gt;</code><code>--export=NONE</code> Identify which environment variables are propagated to the launched application. By default, all are propagated. Multiple environment variable names should be comma-separated. If <code>NONE</code>, SLURM will attempt to load the user's environment on the node the job is being executed. <code>#SBATCH --export=ALL,EDITOR=/bin/emacs</code><code>#SBATCH --export=NONE</code> <code>-J</code>,<code>--job-name=&lt;jobname&gt;</code> Specify a name for the job allocation. <code>#SBATCH   -J MySuperComputing</code> <code>-L</code>,<code>--licenses=&lt;license&gt;</code> Specification of licenses (or other resources available on all nodes of the cluster) which must be allocated to this job. <code>#SBATCH   -L comsol@1718@lm-01.i</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#job-io-and-notifications","title":"Job I/O and Notifications","text":"<p>When running interactively, users have access to  standard input, output, and error via the command line. When running as a batch job, SLURM handles each of these streams as files. By default, standard output and error are combined into the same <code>output</code> file.</p> Option Description Examples <code>-e</code>,<code>--error=&lt;filename&gt;</code> Instruct SLURM to connect the batch script's standard error directly to the file name specified.<p>By default both standard output and standard error are directed to the same file. See <code>-o</code>, <code>--output</code> for the default file name. <code>#SBATCH   -e /home/username/myerrorfile</code> <code>-i</code>,<code>--input=&lt;filename pattern&gt;</code> Instruct Slurm to connect the batch script's standard input directly to the file name specified in the \"filename pattern\". <code>#SBATCH -i /mnt/home/username/myinputfile</code> <code>-o</code>,<code>--output=&lt;filename pattern&gt;</code> Instruct Slurm to connect the batch script's standard output directly to the file name specified in the <code>&lt;filename pattern&gt;</code>.<p>The default file name is <code>slurm-%j.out</code>, where the <code>%j</code> is replaced by the job ID. For job arrays, the default file name is <code>slurm-%A_%a.out</code>, <code>%A</code> is replaced by the job ID and <code>%a</code> with the array index. Needs a file name or filename pattern; not just a directory. <code>#SBATCH   -o /home/username/output-file</code> <code>--mail-type=&lt;type&gt;</code> Notify user by email when certain event types occur. Valid type values are NONE, BEGIN, END, FAIL, REQUEUE, ALL (equivalent to BEGIN, END, FAIL, REQUEUE, and STAGE_OUT), STAGE_OUT (burst buffer stage out and teardown completed), TIME_LIMIT, TIME_LIMIT_90 (reached 90 percent of time limit), TIME_LIMIT_80 (reached 80 percent of time limit), TIME_LIMIT_50 (reached 50 percent of time limit) and ARRAY_TASKS (send emails for each array task). <code>#SBATCH   --mail-type=BEGIN,END</code> <code>--mail-user=&lt;user&gt;</code> User to receive email notification of state changes as defined by <code>--mail-type</code>. The default value is the submitting user. <code>#SBATCH   --mail-user=user@msu.edu</code> <code>-v</code>,<code>--verbose</code> Increase the verbosity of sbatch's informational messages. Multiple <code>v</code> will further increase sbatch's verbosity. By default only errors will be displayed.","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#start-conditions","title":"Start Conditions","text":"Option Description Examples <code>--begin=&lt;time&gt;</code> Submit the batch script to the Slurm controller immediately, like normal, but tell the controller to defer the allocation of the job until the specified time. Time may be of the form HH:MM:SS to run a job at a specific time of day (seconds are optional). <code>#SBATCH --begin=16:00</code><code>#SBATCH --begin=now+1hour</code> (default unit is seconds) <code>-d</code>,<code>--dependency=&lt;dependency_list&gt;</code> Defer the start of this job until the specified dependencies have been satisfied completed. <code>&lt;dependency_list&gt;</code> can have many forms:<p>- <code>after:job_id[:jobid...]</code> This job can begin execution after the specified jobs have begun execution.- <code>afterany:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated.- <code>afterburstbuffer:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated and any associated burst buffer stage out operations have completed.- <code>aftercorr:job_id[:jobid...]</code> A task of this job array can begin execution after the corresponding task ID in the specified job has completed successfully (ran to completion with an exit code of zero).- <code>afternotok:job_id[:jobid...]</code> This job can begin execution after the specified jobs have terminated in some failed state (non-zero exit code, node failure, timed out, etc).- <code>afterok:job_id[:jobid...]</code> This job can begin execution after the specified jobs have successfully executed (ran to completion with an exit code of zero).- <code>expand:job_id</code> Resources allocated to this job should be used to expand the specified job. The job to expand must share the same QOS (Quality of Service) and partition. Gang scheduling of resources in the partition is also not supported.- <code>singleton</code> This job can begin execution after any previously launched jobs sharing the same job name and user have terminated. <code>#SBATCH   -d after:&lt;JobID1&gt;:&lt;JobID2&gt;,afterok:&lt;JobID3&gt;</code> <code>-H</code>,<code>--hold</code> Specify the job is to be submitted in a held state (priority of zero). A held job can now be released using scontrol to reset its priority (e.g. <code>scontrol release &lt;job_id&gt;</code>). <code>#SBATCH -H</code> <code>--no-requeue</code> Request that a job not be requeued under any circumstances. Jobs are requeued by default if a node they are running on fails. This options may be useful for jobs in the scavenger queue that will not run properly after having run partially and failing. <code>#SBATCH   --no-requeue</code>","tags":["reference","slurm","job script"]},{"location":"List_of_Job_Specifications/#overriding-the-job-script","title":"Overriding the Job Script","text":"<p>Optionally, any job specification (by #SBATCH line) can also be requested by <code>sbatch</code> command line with an equivalent option. For instance, the #SBATCH\u00a0 --nodes=1-5 line could be removed from the job script,  and instead be specified from the command line:</p> <pre><code>$ sbatch --nodes=1-5 myjob.sb\n</code></pre> <p>Command line specifications take precedence over those in the job script.</p>","tags":["reference","slurm","job script"]},{"location":"Load_the_software/","title":"Load the software","text":"<p>When running <code>module spider orthomcl</code> on a dev-node, you will see the following:</p> <pre><code>----------\nOrthoMCL:\n----------\n    Description:\n      OrthoMCL is a genome-scale algorithm for grouping orthologous protein\n      sequences.\n\n     Versions:\n        OrthoMCL/2.0.9-custom-Perl-5.24.0\n        OrthoMCL/2.0.9-Perl-5.24.0\n</code></pre> <p>They are the same version of OrthoMCL, except that the custom one has fixed a possible error due to sequence identifier. See the\u00a0issue\u00a0reported in GitHub.</p> <p>Since OrthoMCL uses BLAST, it needs to be loaded as well. An example of loading OrthoMCL would be</p> <pre><code>module purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\n</code></pre> <p>The next thing you need to do is to request and set up your MySQL configuration file, see\u00a0https://docs.icer.msu.edu/MySQL_configuration/</p> <p>After you have your config file (say <code>orthomcl.config</code>) ready, you need to run <code>orthomclInstallSchema</code> to install the required schema into the database:</p> <pre><code>orthomclInstallSchema orthomcl.config install_schema.log\n</code></pre> <p>For the rest of your analysis steps, please read\u00a0http://orthomcl.org/common/downloads/software/v2.0/UserGuide.txt</p>"},{"location":"Loading_Modules_in_a_Jupyter_Notebook/","title":"Loading Modules in a Jupyter Notebook","text":"<p>Interacting with the module system in Jupyter does not always work as expected. Common tactics like opening a shell and loading modules or using Jupyter's \"bang\" syntax to call a shell command</p> <pre><code>!module load TensorFlow\n</code></pre> <p>will not change Python's environment. Instead, you will need to interact with the module system more directly.</p>"},{"location":"Loading_Modules_in_a_Jupyter_Notebook/#configuration-if-using-conda","title":"Configuration if Using Conda","text":"<p>Due to the way ICER's <code>Conda/3</code> module works, you will need to make one small configuration change. Assuming you have set the <code>CONDA3PATH</code> environment variable, from a command line, run the following code:</p> <pre><code>cd $CONDA3PATH\nln -s conda.sh conda.python\n</code></pre>"},{"location":"Loading_Modules_in_a_Jupyter_Notebook/#loading-modules-in-python","title":"Loading modules in Python","text":"<p>Similarly to how the module system is made available to the bash shell, you can make the module system available in Python directly with an import statement. You will need to add the location of the module to your system path.</p> <pre><code>import sys\nsys.path.append(\"/usr/lmod/lmod/init\")\nfrom env_modules_python import module\n</code></pre> <p>After this, the <code>module</code> command can be used as a Python function:</p> <pre><code>module(\"list\")\nmodule(\"load TensorFlow\")\n</code></pre>"},{"location":"Loading_Modules_in_a_Jupyter_Notebook/#loading-modules-in-jupyter-with-jupyter-lmod","title":"Loading modules in Jupyter with <code>jupyter-lmod</code>","text":"<p>You can also integrate the module system directly in Jupyter using the <code>jupyter-lmod</code> extension. You can install this on the command line into a virtual or Conda environment you are loading the Jupyter from with</p> <pre><code>pip install jupyterlmod\n</code></pre> <p>Then, when you start the Jupyter notebook, you will have an extra tab on the right where you can search and load modules into your Python execution environment.</p>"},{"location":"Local_File_Systems/","title":"Local File Systems","text":"<p>The local file systems are available on each cluster compute node and development node. These file systems are directly connected to each node and may be faster than the home, research, and scratch filesystems which must be accessed over the network.</p> <p>There are two kinds of local file systems: the local hard drive (accessible from either <code>/tmp</code> or <code>/mnt/local</code>) and RAMDISK space (accessible from <code>/dev/shm</code>). Read more about each type of storage below.</p> <p>Warning</p> <p>Please limit the use of the local spaces. It is also used for MPI by the MPI runtime to implement fast communication between MPI processes. Users are advised to clean up the space after use. Files that over 2 weeks old will be removed without notice. If the space is over 90% full, we may clean up unused files without notice.</p>","tags":["reference"]},{"location":"Local_File_Systems/#local-hard-drive","title":"Local Hard Drive","text":"<p>Each node has local hard drive storage accessible from either <code>/tmp</code> or <code>/mnt/local</code>. The files on this space can be accessed locally on each node without going through network.  This space is a good choice for jobs using a single node or multiple nodes where I/O is processed only on each node's local file. \u00a0When network traffic is high, using this space will likely allow your program to run faster than running on Home, Research or Scratch space. </p> <p>Please note that this local space is\u00a0shared with all processes running on the same node\u00a0and there is\u00a0no direct I/O from other nodes. The space also has\u00a0no auto backup. It should be used as temporary storage space. When the execution of programs in a job is completed, any useful files in this space should be saved back to Home or Research space.</p> <p>To ensure the continued, reliable operation of each node, all user accounts will be prevented from writing data to local storage that consumes more than 95% of the <code>/tmp</code> directory.  Once a user account's usage of the <code>/tmp</code> directory exceeds 95% or the total available space for that  directory, it will be locked and prevented from writing any additional data to the <code>/tmp</code> directory until files and data are removed bringing the user account back under 95% space utilization.</p> <p>Note</p> <p>In addition to the space restrictions on <code>/tmp</code>, local user account quotas are also in place for another shared directory, the  <code>/var</code> directory. This directory is rarely accessed directly by user accounts, and shouldn't be a concern for  most users of the HPCC; however, if you receive any errors that your user account has exceeeded the <code>/var</code> directory  quota, please contact us right away, and we'd be glad to help troubleshoot the error with you.</p>","tags":["reference"]},{"location":"Local_File_Systems/#tmpdir","title":"$TMPDIR","text":"<p>The environment variable <code>$TMPDIR</code> references a directory that is automatically created when your job starts on the local node. This directory is automatically deleted after the job finishes. It is accessible from both <code>/mnt/local/$SLURM_JOBID</code> and <code>/tmp/local/$SLURM_JOBID</code>. Since local hard drive storage is shared amongst all jobs on a node, using <code>$TMPDIR</code> is a convenient way to keep your files separated and organized.</p> <p>Any files saved to <code>$TMPDIR</code> should be transferred to a Home or Research space before the job ends.</p>","tags":["reference"]},{"location":"Local_File_Systems/#ramdisk","title":"RAMDISK","text":"<p>RAMDISK space is a \u201clogical\u201d storage space; it sits inside a node\u2019s RAM, not the hard drive. Linux supports a system tool that provides an interface for users to intercept the I/O requests to <code>/dev/shm</code> with memory operations. \u00a0We may think of it as a virtual disk in memory. </p> <p>Due to this nature, access to this space is actually access to RAM. Since the bandwidth of the access is much higher, \u00a0the I/O operations are considerably faster than the local hard drive space. However, since programs take up some of the node's memory, the usable RAM space for program execution becomes less. </p> <p>This space is good for programs that do not require large memory and perform very frequent I/O on small files.</p>","tags":["reference"]},{"location":"Loops/","title":"Loops","text":"<p>Loops allow you to execute the same code on different values of a variable Below, we give the syntax and examples for <code>for</code> and <code>while</code> loops.</p>","tags":["reference","command line"]},{"location":"Loops/#for-loops","title":"<code>for</code> loops","text":"<p>General syntax:</p> <pre><code>for var in list\ndo\n   Statement(s) to be executed using $var\ndone\n</code></pre> <p>Here, <code>var</code> is a variable name, and <code>list</code> is a list of items separated by white space that <code>var</code> will take on in the loop iterations.</p> <p>Example:</p> test.sh<pre><code>#!/bin/bash\nfor i in 1 2 3 4 5\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Looping ... number 1\nLooping ... number 2\nLooping ... number 3\nLooping ... number 4\nLooping ... number 5\n</code></pre> <p>The looping expression can also use a format similar to a C for loop. For example, this script also gives a same results.</p> <pre><code>#!/bin/bash\nfor ((i=1 ; i&lt;6; i++))\ndo\n  echo \"Looping ... number $i\"\ndone\n</code></pre>","tags":["reference","command line"]},{"location":"Loops/#while-loops","title":"<code>while</code> loops","text":"<p>General syntax:</p> <pre><code>while [ expression ]\ndo\n   Statement(s) to be executed\ndone\n</code></pre> <p>Here, <code>expression</code> is a conditional expression as described in Conditional statements.</p> <p>Example:</p> test.sh<pre><code>#!/bin/bash\nINPUT_STRING=hello\nwhile [ \"$INPUT_STRING\" != \"bye\" ]\ndo\n  echo \"Please type something in (bye to quit)\"\n  read INPUT_STRING\n  echo \"You typed: $INPUT_STRING\"\ndone\n</code></pre> <p>This is the result of a sample run.</p> input<pre><code>bash test.sh\n</code></pre> output<pre><code>Please type something in (bye to quit)\nhello\nYou typed: hello\nPlease type something in (bye to quit)\nhi\nYou typed: hi\nPlease type something in (bye to quit)\nbye\nYou typed: bye\n</code></pre>","tags":["reference","command line"]},{"location":"Managing_File_Permissions_on_HPCC/","title":"Managing file permissions on HPCC","text":"<p>This is a list of techniques to manage file permissions and groups on the HPCC. For background on the concepts, please see the page on file permissions.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#displaying-permissions-of-files-and-directories-with-ls-l","title":"Displaying permissions of files and directories with <code>ls -l</code>","text":"<p>To display permissions in the current directory, run:</p> <pre><code>ls -l\n</code></pre> <p></p> <p>You can also display the permissions of an individual file or directory by running:</p> <pre><code>ls -ld filename\n</code></pre> <p>For example, you can check the permissions of your home directory:</p> <pre><code>ls -ld ~\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-file-permissions-with-chmod","title":"Changing file permissions with <code>chmod</code>","text":"<p>In the normal UNIX security model, there are three levels that are evaluated when considering file or directory access: user owner, group owner, and everyone else on the system. These types are typically referred to as user (<code>u</code>), group (<code>g</code>) and other (<code>o</code>). Only the owner of a file or a directory is allowed to change its permissions or the group name (to one of the owner's groups).</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#change-user-permissions","title":"Change user permissions","text":"<p>To add all permissions for the user owner, run the following command:</p> <pre><code>chmod u+rwx FileName\n</code></pre> <p>Note that any file you create will already have the <code>rw</code> permission for your user account so that you will have the \"read\" and \"write\" permissions respectively. However to have a program script able to be run from the command line, you need to change the 'execute', or <code>x</code>,  permission with</p> <pre><code>chmod u+x FileName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-group-and-other-permissions","title":"Changing group and other permissions","text":"<p>To allow anyone in the group that owns the file to be able to read that file, change the group read permission:</p> <pre><code>chmod g+r FileName\n</code></pre> <p>To allow anyone in the group to read and write the file, you can change the read and write permission</p> <pre><code>chmod g+wr FileName\n</code></pre> <p>If you have a file that is currently read and writeable by the group (g+wr) and you want to make it private, remove those permissions:</p> <pre><code>chmod g-rw FileName\n</code></pre> <p>To add the ability for other users to write to a file or directory (this allows all users on the HPC to see and read this file if it's in a shared folder which we don't recommend).</p> <pre><code>chmod o+w FileName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-group-ownership-with-chgrp","title":"Changing group ownership with <code>chgrp</code>","text":"<p>To change the group ownership of a file or a directory, simply run</p> <pre><code>chgrp &lt;GroupName&gt; &lt;FileName&gt;\n</code></pre> <p>where <code>&lt;GroupName&gt;</code> is the group name which you would like to change to and <code>&lt;FileName&gt;</code> is the name and path of the file or directory.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#working-with-non-primary-groups-and-permissions","title":"Working with non-primary groups and permissions","text":"","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#switching-groups-with-newgrp","title":"Switching groups with <code>newgrp</code>","text":"<p>If you have more than one group associated with your account, you can switch which group owns the files created by default with the <code>newgrp</code> command: <code>newgrp myothergroup</code>. If you need to do this frequently, you can contact HPCC staff to change your primary group or see the page on changing your primary group. </p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#changing-default-group-for-new-files-with-the-set-group-id-bit-and-chmod","title":"Changing default group for new files with the set-group-ID bit and <code>chmod</code>","text":"<p>You can also change the default group for new files created in a directory by setting the set-group-ID setting. The <code>/mnt/research</code> HPCC Research file share spaces have this setting set by default. To set the set-group-ID bit on a directory:</p> <pre><code>chmod g+s DirectoryName\n</code></pre> <p>To remove the set-group-ID bit on a directory:</p> <pre><code>chmod g-s DirectoryName\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#other-special-permissions","title":"Other special permissions","text":"<p>There are other group permissions beyond the scope of this document, primarily the set-user-ID bit and the \"sticky\" bit. For more information about special permissions, please review the GNU documentation, available on any HPCC system:</p> <pre><code>info chmod\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#filesystem-specific-differences","title":"Filesystem-specific differences","text":"","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#home","title":"Home","text":"<p>Your home directory has default permissions that allow only you to have access. Other users, whether they are in your primary group or not, are not allowed access to the contents of your home directory by default. If you wish to allow other users access to your home directory, you will need to change permissions on it.  </p> <p>To allow every member of a group access to read your home directory, use:</p> <pre><code>chmod g+rx ~\n</code></pre> <p>It is strongly recommended that you do not allow all users to read the contents of your home directory.  Instead, to allow users outside of your UNIX group to read contents within your home directory, it is suggested  that you create a subdirectory within your home directory that is readable by all users, and then set the execute permission on your home directory to allow users to access the subdirectory. For example:</p> <pre><code>#Create a subdirectory in your home directory called \"my_sub_directory\"\nmkdir ~/my_sub_directory\n#Make the subdirectory readable by everyone\nchmod a+rx ~/my_sub_directory\n#Set the execute permission on your home directory to allow others to access the new subdirectory\nchmod o+x ~/\n</code></pre> <p>To allow every user outside your UNIX group to read your home directory, use (NOT RECOMMENDED):</p> <pre><code>chmod o+rx ~\n</code></pre> <p>To allow world-wide read access to your home directory (NOT RECOMMENDED):</p> <pre><code>chmod a+rx ~\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#sharing-a-single-directory-inside-your-home-directory","title":"Sharing a single directory inside your home directory","text":"<p>If you wish to share only a single directory in your home directory and keep all other contents private,\u00a0 you can use the following technique (This is the recommended method for sharing home directory contents with users outside of your UNIX group):</p> <pre><code># create the shared folder\ncd ~\nmkdir shared\nchmod o+rwx shared\n# create a shared file in the shared folder\necho \"hello, iCER\" &gt; shared/sharefile.txt\nchmod o+rw shared/sharefile.txt\n# anyone can read this file using\ncat /mnt/home/&lt;netid&gt;/shared/sharefile.txt\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#scratch","title":"Scratch","text":"<p>Directories are created as private to you by default. If you do not wish this to be the case, you can use the technique for sharing a single directory in your home directory above. Note if there are other directories above your shared directory (e.g. it's a sub-sub-directory like <code>~/project/data/shared</code>), then every directory in the path will need the execute bit set for everyone.</p>","tags":["how-to guide","files","groups"]},{"location":"Managing_File_Permissions_on_HPCC/#tmpdir-space","title":"TMPDIR space","text":"<p>Directories are created as world-readable by default, but the scheduler deletes the contents of <code>$TMPDIR</code> after a job exits. If you require additional security for this temporary space, manually setting the permissions of <code>$TMPDIR</code> is necessary. Here is an example to mimic the security of home directory space:</p> <pre><code>chmod go-rwx $TMPDIR\n</code></pre>","tags":["how-to guide","files","groups"]},{"location":"Mapping_HPC_drives_with_SSHFS/","title":"Mapping HPC drives with SSHFS","text":"<p>Besides mapping HPCC drives with SMB,  SSHFS can also enable mounting of HPCC file systems on a local computer. Different from SMB mapping, which can only map to home or research space via the MSU campus network, this method can also work on your scratch space and uses any internet network.</p> <p>Warning</p> <p>You will need to generate an authentication key by following the  direction of SSH Key-Based Authentication if you do not already have one set up. </p> Mac OSXLinuxWindows <ol> <li> <p>Download and install (or upgrade) the most recent versions of the following packages: FUSE for macOS and SSHFS from https://osxfuse.github.io.</p> </li> <li> <p>Reboot (required)</p> </li> <li> <p>Using the Terminal, create a directory (as <code>&lt;local_mount_point&gt;</code> in step 4) for each filesystem you wish to mount. \u00a0If you are creating the folder outside of your home directory, you may need to use <code>sudo</code> before each command (sudo = superuser do ).\u00a0</p> <pre><code>[MacBook-Pro:~ icer2]$ mkdir &lt;local_mount_point&gt;\n/* begin example home directory */\n[MacBook-Pro:~ icer2]$ mkdir /Users/icer2/hpcc_home\n/* end example home directory */\n/* begin example scratch directory in the Mac /Volumes folder where drives are mounted */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n/* end example scratch directory */\n</code></pre> </li> <li> <p>Mount the directory using  the <code>sshfs</code> command. We suggest you add     these additional flags to the command to make it be more \"Mac-like\"     :\u00a0<code>-ocache=no</code> , \u00a0<code>-onolocalcaches</code> \u00a0and <code>-o volname=hpcc_home</code> .\u00a0     For the last option, '<code>-o volname</code>' is the name that displays     in the Finder title bar, so change it for difference file     folders (e.g. use\u00a0<code>-o volname=hpcc_scratch</code> for your scratch     folder). After running the command, enter the password for logging     into HPCC and the FUSE drive icon will show on the desktop of     your local Mac computer.</p> <pre><code>[MacBook-Pro:~ icer2]$ sshfs &lt;user_id&gt;@rsync.hpcc.msu.edu:&lt;remote_directory_to_mount&gt; &lt;local_mount_point&gt; -ovolname=hpcc -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\n/* begin example hpc's home directory, using /mnt/home/hpc/ */\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/home/hpc/ /Users/icer2/hpcc_home -o volname=hpcc_home -o allow_other,defer_permissions,follow_symlinks,reconnect -ocache=no -onolocalcaches\n/* end example home directory */\n/* begin example hpc's scratch directory with authorized key file ~/.ssh/id_rsa, using /mnt/gs18/scratch/users/hpc */\n[MacBook-Pro:~ icer2]$ mkdir /Volumes/scratch\n[MacBook-Pro:~ icer2]$ sshfs hpc@rsync.hpcc.msu.edu:/mnt/gs18/scratch/users/hpc /Volumes/scratch -o volname=hpcc_scratch -o allow_other,defer_permissions,follow_symlinks,reconnect,IdentityFile=~/.ssh/id_rsa -ocache=no -onolocalcaches\n(No password input)\n/* end example scratch directory */\n</code></pre> <p>If <code>&lt;remote_directory_to_mount&gt;</code> is a static link, please make sure</p> </li> </ol> <p>to put <code>/</code> at the end of the directory path:</p> <pre><code>-   For your home space, please use `/mnt/home/&lt;user_id&gt;/` instead\u00a0 of\n`/mnt/home/&lt;user_id&gt;`.\n-   For your research space, please use `/mnt/research/&lt;group_name&gt;/`\ninstead of `/mnt/research/&lt;group_name&gt;`.\n\nAs the above example (starting from line 5), home space\n`/mnt/home/hpc/` is used on line 7 instead of `/mnt/home/hpc`.\n</code></pre> <ol> <li> <p>To unmount a filesystem, use the <code>umount</code> command. </p> <p>Note</p> <p>It's just the letter u before mount, NOT <code>unmount</code>.</p> <pre><code>    umount &lt;local_mount_point&gt;\n    /* begin example */\n    umount /Users/icer2/hpcc_home\n    /* end example */\n</code></pre> <p>You'll see these folders in the finder if you use the Go menu, but you</p> </li> </ol> <p>won't see them listed in the left side with the other mounted drives. \u00a0You must use the terminal and <code>umount</code> command to disconnect.</p> <p>References:</p> <p>https://www.digitalocean.com/community/tutorials/how-to-use-sshfs-to-mount-remote-file-systems-over-ssh</p> <p>Please refer to this web site:</p> <p>https://tecadmin.net/install-sshfs-on-linux-and-mount-remote-filesystem/</p> <p>for how to mount remote filesystem over SSH on Linux.</p> <p>We no longer recommend using SSHFS on Windows, please see instructions for using Samba instead </p>","tags":["tutorial","sshfs","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/","title":"Mapping HPC drives with Samba","text":"<p>Warning</p> <ol> <li> <p>Samba now uses campus AD for user authentication, if you are unable to login and have not updated your NetID password recently please try updating your NetID password before opening a ticket</p> </li> <li> <p>This will only work if your computer has a university IP address. If you are off campus, you can use the MSU VPN  to obtain an MSU IP, which is available to all graduate students, staff and faculty.\u00a0</p> </li> <li> <p>If file transfer speed is a concern please use our Globus endpoint.</p> </li> </ol> <p>The following tutorial will show you how to map your HPC home or research directory using SMB or CIFS File Sharing.</p>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#determining-your-network-path","title":"Determining your Network Path","text":"<p>We have the powertools command <code>show-samba-paths</code> to show all paths of your home and research space. Run these commands in your SSH client after logging on to the HPCC:</p> <pre><code>$ ml powertools                    # if powertools is not loaded\n$ show-samba-paths\n\n  HOME      |        Samba Path\n===================================================================\n  username  |  \\\\ufs.hpcc.msu.edu\\home-021\\username      (Windows)\n            |  smb://ufs.hpcc.msu.edu/home-021/username      (Mac)\n\n\n  RESEARCH        |        Samba Path\n===============================================================================\n  helpdesk        |  \\\\ufs.hpcc.msu.edu\\rs-011\\helpdesk              (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-011/helpdesk              (Mac)\n------------------+------------------------------------------------------------\n  common-data     |  \\\\ufs-12-b.hpcc.msu.edu\\common-data             (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/common-data             (Mac)\n------------------+------------------------------------------------------------\n  BiCEP           |  \\\\ufs.hpcc.msu.edu\\rs-001\\BiCEP                 (Windows)\n                  |  smb://ufs.hpcc.msu.edu/rs-001/BiCEP                 (Mac)\n------------------+------------------------------------------------------------\n  education-data  |  \\\\ufs-12-b.hpcc.msu.edu\\education-data          (Windows)\n                  |  smb://ufs-12-b.hpcc.msu.edu/education-data          (Mac)\n</code></pre> <p>where the paths are the same for\u00a0Mac and Window computers but with different formats.</p>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#windows-10","title":"Windows 10","text":"","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#step-1-enable-netbios-over-tcpip-on-windows","title":"Step 1. Enable NetBIOS over TCP/IP on Windows:","text":"<ul> <li>Click on the Network icon on the taskbar at the right hand side and     click on \"Network &amp; Internet settings\" </li> <li>Click on Change adapter options     </li> <li>Right click on your Network interface that is used to connect to the internet      (here, Ethernet 2) and click on Properties     </li> <li>Select the Internet Protocol version 4 (TCP/IPv4) </li> <li>Click the Advanced button under the General tab.     </li> <li>Click the WINS tab.  </li> <li>Click the Enable NetBIOS Over TCP/IP button.     </li> <li>Click Ok</li> </ul>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#step-2-disable-smb1","title":"Step 2. Disable SMB1","text":"","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#disable-samba-v1-protocol-with-powershell","title":"Disable Samba V1 protocol with PowerShell","text":"This step must be completed or your client will not be able to map the drive. If you have other mounts on the HPC cluster and they are using samba V1 they will stop working.   <ol> <li> <p>Press the Windows start button</p> </li> <li> <p>In the search box type \"power shell\" </p> </li> <li> <p>Right click on the \"Windows PowerShell\" icon and select \"Run as     Administrator\" </p> </li> <li> <p>Select Ok when security warning appears </p> </li> <li> <p>Disable Samba V1 by entering the following command into the windows     power shell. </p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB1 -Type DWORD -Value 0 -Force\n</code></pre> </li> <li> <p>Ensure SMB V2 and SMB V3 are enabled by entering the following     command.\u00a0 In the past, on some versions of windows and for some file     systems we recommended the opposite of this setting.\u00a0 Running this     ensures it's enabled again.\u00a0</p> <pre><code>Set-ItemProperty -Path \"HKLM:\\SYSTEM\\CurrentControlSet\\Services\\LanmanServer\\Parameters\" SMB2 -Type DWORD -Value 1 -Force\n</code></pre> </li> <li> <p>Navigate to \"This PC\" and click on the text labeled \"Map Network     Drive\" at the top of the screen, located under the \"Computer\" tab.</p> <p></p> <p>From this menu you need to type your Network Path. Please see\u00a0#Determining your Network Path\u00a0for help</p> </li> <li> <p>Once you have typed in your Network Path you need to click on     the box \"Connect using different credentials.\" This will open a     window where you type in your MSU NetID and     password:     </p> </li> </ol> <p>Warning</p> <p>If you aren't able to sign in,  You will need to add \"CAMPUSAD\\\" to the beginning of your username. An indicator of this issue is if Windows displays the error \"The specified network password is not correct\" in the username dialog window.</p> <p>For example: substitute \"CAMPUSAD\\sparty\" for username \"sparty\" in the username field. The slash character is a backslash. A forward slash character will not work.</p> <ol> <li> <p>Finally, select \"Finish\" and you will see your system trying to     connect</p> <p></p> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#command-line-windows-netbios-commands","title":"Command-line Windows NetBIOS Commands","text":"<p>If you're working in Windows, you can use command line tools to manage your drive mapping.\u00a0 These commands also work in .bat files, if you're so inclined to connect/disconnect drives in that manner.\u00a0 Note you may also have to Disable SMBV1 and Enable SMB2 per instructions above.\u00a0</p> <ol> <li>From the Start Menu -&gt; Run -&gt; type 'cmd' in the box and hit enter,     the command shell should open.\u00a0 You can then use the following     commands to diagnose, disconnect and connect drives.</li> </ol> <pre><code># to show all mapped connections/drives\n# use 'net use'\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# to map a drive use:\n# net use &lt;drive_letter&gt;: \\\\&lt;hostname&gt;\\&lt;mount&gt; /user:hpcc\\&lt;net_id&gt;\n# where &lt;hostname&gt; and &lt;mount&gt; were determined from above 'Determining Your Network Path' and &lt;net_id&gt; is your MSU Net ID\n\nC:\\Documents and Settings\\Administrator&gt;net use m: \\\\ufs-10-a.hpcc.msu.edu\\jal /user:hpcc\\jal\nThe command completed successfully.\n\n# You can use 'net use' again to show the drive mapping status.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nOK           M:        \\\\ufs-10-a.hpcc.msu.edu\\jal\n                                                 Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n# To disconnect a drive use:\n# net use &lt;drive_letter&gt;: /delete\n## NOTE: this will only drop the connection, not delete the share or any data on the share ##\n\nC:\\Documents and Settings\\Administrator&gt;net use m: /delete\nm: was deleted successfully.\n\n# you can now use 'net use' again to show your mapping status after the delete to ensure it's gone.\n\nC:\\Documents and Settings\\Administrator&gt;net use\nNew connections will not be remembered.\n\n\nStatus       Local     Remote                    Network\n\n-------------------------------------------------------------------------------\nUnavailable  J:        \\\\MachineA\\consult$    Microsoft Windows Network\nUnavailable  P:        \\\\MachineB\\everyone$    Microsoft Windows Network\n             Z:        \\\\vmware-host\\Shared Folders\n                                                 VMware Shared Folders\nThe command completed successfully.\n\n\nC:\\Documents and Settings\\Administrator&gt;\n</code></pre>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#macos-example","title":"MacOS Example","text":"<ol> <li>Open the Finder. </li> <li>Under \"GO\" click on \"Connect to Server\" </li> <li>From this menu you need to type your Network Path. Please     see\u00a0#Determining your Network     Path\u00a0for     help. </li> <li>Enter your MSU NetID and password for authentication and click     \"Connect\". </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#linux","title":"Linux","text":"<ol> <li> <p>Install smb-client</p> <p>Ubuntu / Debian</p> <pre><code>apt install smbclient\n</code></pre> <p>Red Hat / Fedora</p> <pre><code>yum install samba-client\n</code></pre> </li> <li> <p>Edit /etc/samba/smb.conf</p> <pre><code>sudo vi  /etc/samba/smb.conf\n</code></pre> </li> <li> <p>Add the following lines to disable samba V1</p> <p>This step must be completed or your client will not be able to map the drive. If you have other mounts on on the HPC cluster and they are using samba V1 they will stop working. In this case please use SSHFS.</p> <pre><code>client min protocol = SMB2\nclient max protocol = SMB3\n</code></pre> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#ubuntu-mount-example","title":"Ubuntu Mount Example","text":"<ol> <li>Open a File Browser window. In the left pane select \"Other Locations\" </li> <li> <p>Type your network path in the server address box. Remember to get     your network path using\u00a0<code>show-samba-paths</code> on the HPCC      (Format is the same as the Mac format)  </p> <p> 3.  An authentication window will open. Select \"Registered User\". Enter your user ID and password (Domain can remain WORKGROUP)  and click connect. </p> <p>Note</p> <p>Your username must include @msu.edu.</p> <p> 4.  If connected properly the drive should appear in the file manager screen.  </p> <p></p> </li> <li> <p>After successfully mounting, you can unmount using the eject button in the file manager, and reconnect later using the dropdown list next to the Connect to Server box.</p> </li> </ol>","tags":["tutorial","samba","drive mapping"]},{"location":"Mapping_HPC_drives_with_Samba/#more-information","title":"More Information","text":"<ul> <li>http://us3.samba.org/samba/</li> </ul>","tags":["tutorial","samba","drive mapping"]},{"location":"Matlab/","title":"MATLAB","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#about-matlab","title":"About MATLAB","text":"<ul> <li> <p>There is a Matlab portal for Michigan State University users. Users are encouraged to visit it for information and support provided by Mathworks.</p> </li> <li> <p>Several versions of\u00a0MATLAB are installed on the cluster. By default, MATLAB/2018a is loaded. Other available version of MATLAB can be discovered by typing</p> </li> </ul> <pre><code>hpc@dev-intel18:~&gt; module spider MATLAB\n</code></pre> <p>and then switching to a different version, for example, to switch from 2018a to 2019a, one can directly load the the version as</p> <pre><code>hpc@dev-intel18:~&gt; module load MATLAB/2019a \n</code></pre> <ul> <li>MATLAB's many\u00a0built-in functions\u00a0have multi-threaded capability. On     your personal computer, when a MATLAB\u00a0function with multi-threads is     called,\u00a0MATLAB\u00a0will automatically spawn as many threads as the     number of cores on the machine. To avoid over utilizing compute     nodes on HPCC, user should set the max number of threads by using     <code>maxNumCompThreads(N)</code> in matlab where N is the maximum number of     threads matlab would use in the session. User could also use option     <code>-SingleCompThread</code> to launch\u00a0matlab session that would only use a     single thread. Without these option, matlab\u00a0session will potentially     spawn as many as 28 (on intel16 nodes) or 20 (on intel14 nodes)     threads when a built-in multi-threaded function is called. To allow     the multi-thread functions in matlab, users need to do the     following:<ol> <li>Specify the maximum number of compute threads to be used with     \u00a0<code>maxNumCompThreads(N)</code>\u00a0at the beginning of the matlab     program where N is the maximum number of threads in the program.     For example,\u00a0<code>maxNumCompThreads(4)</code>\u00a0will set the maximum     number of threads used in the program to four. \u00a0</li> <li>If submitting to run as batch job, specify <code>--cpus-per-task=N</code>     in your job script where N should match the maximum number of     compute threads set in <code>maxNumCompThreads(N)</code>.</li> </ol> </li> </ul> <p>Warning</p> <p>Starting Nov. 1, 2018 on new HPCC system, the matlab default setting of using a single compute thread is changed. <code>matlab-mt</code> and <code>matlab</code> commands would be the same for 2018 and older versions and <code>matlab-mt</code> will no longer exist from later versions starting 2019.</p> <ul> <li>HPCC has many toolboxes installed. To see a list of installed     toolboxes and licenses, type</li> </ul> <pre><code>hpc@dev-intel18:~&gt; module load powertools\nhpc@dev-intel18:~&gt; licensecheck matlab\nChecking Licenses for matlab\nlmstat -a -c 27000@lm-02.i\n\nUsers of MATLAB:  (Total of 10000 licenses issued;  Total of 44 licenses in use)\nUsers of SIMULINK:  (Total of 10000 licenses issued;  Total of 1 license in use)\nUsers of Bioinformatics_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\nUsers of Communication_Toolbox:  (Total of 10000 licenses issued;  Total of 0 licenses in use)\n\n......\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-on-scratch-space","title":"Running MATLAB on Scratch Space","text":"<p>It is strongly recommended that iCER users run programs on the scratch space. This may improve the speed of job execution as well as the whole system's performance. MATLAB users should carefully check whether any temporary files involved in the program execution need to be stored in scratch space. For example, if you use the Matlab compiler to make a Matlab program into a standalone program, you need to set the environment variable <code>MCR_CACHE_ROOT</code> in the scratch directory with: <code>export MCR_CACHE_ROOT=$SCRATCH</code> before starting the execution. This line should be added to the job script before the line where you specify the task you want to run. This setting will override the default setting by MATLAB Compiler Runtime. By default, a directory for temporary data cache used by the MATLAB Compiler Runtime is created at user's home directory <code>$HOME</code>. Without this setting, users may run into the situation that the program running on scratch space\u00a0frequently accesses its cache space in the home space, which will greatly slow down the execution of the code, and may potentially slow down the whole system or even cause system instability. \u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-interactively","title":"Running MATLAB Interactively","text":"<p>In this document, we refer to an interactive session as one that involves a user typing commands into the MATLAB command windows.</p> <p>The simplest way to run MATLAB as a graphical application is to use an OnDemand session. However, if you only wish to use the command line or use the graphics through <code>ssh</code>, see the instructions below.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#short-sessions-two-hours","title":"Short Sessions (&lt; two hours)","text":"<ul> <li> <p><code>ssh</code> to one of the dev nodes and run Matlab:</p> <pre><code>hpc@gateway:~&gt; ssh dev-intel16\nLast login: Mon Dec  4 12:54:44 2017 from gateway\n===\nThis front-end node is not meant for running big or long-running jobs.  Jobs\nthat need to run longer than a few minutes should be submitted to the queue.\nLong-running jobs on front-end nodes will be killed without warning.\n===\nhpc@dev-intel16:~&gt; matlab -nodisplay\n</code></pre> <p>More information about running jobs interactively on compute nodes can be reviewed at\u00a0Running Programs Interactively</p> </li> <li> <p>If you require graphics, please ensure that you have an Xserver     running. For Linux and MAC users,</p> <pre><code>ssh -X username@hpcc.msu.edu\n</code></pre> <p>If you want graphics, but don't want the desktop, type</p> <pre><code>hpc@dev-intel16:~&gt; matlab -nodesktop\n</code></pre> <p>This will let you run code from the command line instead of from the IDE interface, but will still allow you to use graphics (e.g., make plots).</p> </li> </ul> <p>Interactive MATLAB jobs running on development nodes are limited to a two hour wall time limit, and will be killed automatically after two hours.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#long-sessions-two-hours","title":"Long Sessions (&gt; two hours)","text":"<p>Longer interactive sessions are possible, but are not recommended. Modify the following commands to suit your requirements.</p> <p>If graphics are not required,</p> <pre><code>hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 \n\n\nsalloc: Granted job allocation 310982\n\nsalloc: Waiting for resource configuration\n\nsalloc: Nodes lac-376 are ready for job\n\nhpc@dev-intel18:-&gt; matlab -nodisplay \n</code></pre> <p>If graphics are required, add the option <code>\u2013x11</code> to the <code>salloc</code> command:</p> <pre><code>hpc@dev-intel18:-&gt; salloc --nodes=1 --ntasks=1 --cpus-per-task=1 --mem=2gb --time=04:00:00 --x11\n</code></pre> <p>Warning</p> <p>The above commands submit a job to the cluster. If the resources are not immediately available, you will have to wait till the requested resources are available. Requesting a job for four hours or less will typically be scheduled relatively quickly. Users may need to adjust the resources accordingly with the usage of the MATLAB program.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-non-interactively","title":"Running MATLAB Non-Interactively","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#short-jobs-two-hours","title":"Short Jobs (&lt; two hours)","text":"<p>A short job could be run on a development node without opening the matlab command window. From a development node, type</p> <pre><code>hpc@dev-intel16:~&gt; matlab -nodisplay -r \"myMatlab\" &amp;\n</code></pre> <p><code>myMatlab.m</code> will start running in the background.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#long-jobs-two-hours","title":"Long Jobs (&gt; two hours)","text":"","tags":["tutorial","MATLAB"]},{"location":"Matlab/#single-matlab-job","title":"Single MATLAB Job","text":"<p>To submit jobs to the cluster, a job script needs to be written. And submitted to the queue. The following sample job script file can be modified to suit your needs:</p> <p><code>myJob.sbatch</code></p> <pre><code>#!/bin/bash\n\n### Specify the Resources Needed\n\n#SBATCH --time=02:05:00 \n#SBATCH --nodes=1 \n#SBATCH --ntasks=1 \n#SBATCH --cpus-per-task=1 \n#SBATCH --mem-per-cpu=1G        \n\n### Set SLURM Admin parameters\n\n#SBATCH --job-name=myJobName\n#SBATCH --output=%x-%j.SLURMout\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=myNetID@msu.edu\n\n### Navigate to your data directory and run Matlab\n\ncd $HOME/my                           \nmatlab -nodisplay -r \"myMatlab\"\n</code></pre> <ul> <li><code>--time=02:05:00</code> is the number of <code>hours:minutes:seconds</code> your job needs to     run. If it runs longer than this, it will be killed. If you request     more time than you need, your job may be delayed while the scheduler     finds a time to run it. If you don't know how long your job needs,     you will have to make a guess and use the real running time to     improve this number on future runs. The maximum walltime that can be     requested is <code>168:00:00</code>.</li> <li><code>--nodes=1</code> because you are running one matlab client in the job. If     you want put multiple matlab run in a single job script, you may     request more nodes for the job.</li> <li><code>--cpus-per-task=1</code> here because the matlab script <code>MyMatlab.m</code> does     not use multiple threads. If you are using multiple threads, you may     need to request more cpus.</li> <li><code>--mem-per-cpu=1G</code> reserves 1 gigabytes per CPU of memory for the job. We     recommend user to serve at least 1GB for each session plus the     total size of data variables used in the computation. User could     click here to see the recommendation of the matlab     requirement by Mathworks.</li> <li><code>myJobName</code> is a string to make your job easier to identify it when     managing or monitoring your jobs.</li> <li><code>--output=%x-%j.SLURMout</code> sets the SLURM output filename to <code>myJobName-IDnumber.SLURMout</code></li> <li><code>myMatlab</code> is the name of your matlab script without the .m extension</li> <li><code>--mail-type=ALL</code> tells SLURM to email you with <code>ALL</code> job scheduling events e.g., start, complete, exit-code</li> <li><code>--mail-user=myNetID@msu.edu</code> sets the email address SLURM uses; change <code>myNetID@msu.edu</code> to your prefered email address</li> </ul> <p>Submit your job with:</p> <pre><code>hpc@dev-intel16:~&gt; sbatch myJob.sbatch\n</code></pre> <p><code>myJob.sbatch</code> is the name of your job script as shown above.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-parallel-computing-toolbox","title":"Using the MATLAB Parallel Computing Toolbox","text":"<p>The MATLAB Parallel Computing Toolbox (PCT) provides users several parallel computing features.\u00a0</p> <ul> <li> <p>Parallel for-loops (parfor) . \u00a0(User could run <code>module load     powertools; getexample MATLAB_parfor</code> to download a directory     \"MATLAB_parfor\" which contains an example of using parpool and     parfor with the \"local\" profile)</p> </li> <li> <p>Support for GPU computing</p> </li> <li> <p>Offload computing from your laptop to HPCC cluster (with MATLAB     Distributed Computing Server)\u00a0</p> </li> <li> <p>Distributed arrays and <code>spmd</code> (single-program-multiple-data) for large     dataset handling and data-parallel algorithms</p> <ol> <li>One GPU card will be used for each worker. In order to use multiple GPUs, user need to use <code>spmd</code> capability that each instance of the program will use one card and multiple instances of the program take multiple cards.</li> <li>If you use GPU capability, you need to have matlab run on a node with GPU. dev-intel16-k80 and dev-amd20-v100 are the development nodes with GPUs. To request GPUs, use <code>\u2013-gpus=&lt;type&gt;:&lt;number&gt;</code> to request a number and type of GPU. <code>#SBATCH ---gpus=k80:1</code> is an example to request one k20 GPU. Valid GPU type are k80 and v100. Note that type is optional, but the number of GPU is necessary.</li> </ol> </li> </ul>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-parallel-server","title":"Using the MATLAB Parallel Server","text":"<p>The MATLAB Parallel Server lets users solve computationally and data-intensive problems by executing MATLAB and Simulink based applications on the HPCC cluster and clouds. (see the document for more information). HPCC cluster has this product installed.</p> <p>We recommend that users prototype their applications using the Parallel Computing Toolbox, and then scale up to a cluster using MATLAB Parallel Server. To scale up to cluster, user does not need to recode the program. User only need to change the profile of the cluster.\u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#setup-and-validate-your-cluster-profile","title":"Setup and validate your cluster profile","text":"<p>In this step you define a cluster profile to be used in subsequent steps.</p> <ol> <li> <p>Start the Cluster Profile Manager from the MATLAB desktop by     selecting on the\u00a0Home\u00a0tab in the\u00a0Environment\u00a0area\u00a0Parallel\u00a0&gt; Create     and\u00a0Manage Clusters.\u00a0</p> </li> <li> <p>Create a new profile in the Cluster Profile Manager by selecting Add     Cluster Profile\u00a0&gt;\u00a0Slurm.</p> </li> <li> <p>With the new profile selected in the list, click\u00a0Rename\u00a0to edit the     profile name,\u00a0Press\u00a0Enter.</p> </li> <li> <p>Select a profile in the list, click Edit to edit the profile     accordingly. After finishing editing, click Done to save the     profile.</p> </li> <li> <p>Click validation to validate the profile. The profile could be used     when it pass all the validation tests.</p> </li> </ol> <p>Starting from version 2018a, MATLAB supports the SLURM scheduler. Please refer to Mathworks' document for how to configure the cluster with the SLURM scheduler here. Previous versions of MATLAB are not recommended for use with the HPCC scheduler.</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-the-matlab-thread-based-worker-pool","title":"Using the MATLAB thread-based worker pool","text":"<p>Started from Matlab version R2020a, thread-based worker pool is introduced. Please refer to this document\u00a0for more details. On HPCC, we provide our users an example showing how to use thread-base pool with <code>parfor</code>, as well as the comparison between process-based and thread-based pools. To obtain the example, module load <code>powertools</code> and <code>MATLAB/2020a</code>, then run <code>getexample MATLAB_threadPool</code>.\u00a0</p>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-the-matlab2020a-on-amd-nodes","title":"Running the MATLAB/2020a on AMD nodes","text":"<p>There is a bug in MATLAB/2020a that will lead to a \"segmentation fault\" on AMD node associated with the Java virtual machine. The patch may be introduced in the next release. If you find that the code works on other version but crashes in 2020a version, you may try the workaround that launch the matlab session without java virtual machine as the following:</p> <pre><code>[hpc@eval-epyc19 ~]$ matlab -nodisplay -nojvm -r \"myExample\"\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#running-matlab-on-intel14-nodes","title":"Running MATLAB on intel14 nodes","text":"<p>There is an existing problem of running some functions on Intel14 nodes.\u00a0If you run into 'Illegal instruction detected' error, please use a node of other type.\u00a0Please add constrain to exclude intel14 type of compute nodes when submit to SLURM. For example:</p> <pre><code>#SBATCH --constraint=[intel16|intel18|amd20]\n</code></pre>","tags":["tutorial","MATLAB"]},{"location":"Matlab/#using-matlab-with-python","title":"Using MATLAB with Python","text":"<p>Here is the link to the cheat sheets provided by the Mathworks for users' reference.</p>","tags":["tutorial","MATLAB"]},{"location":"Migrating_Conda_Environments/","title":"Migrating Conda Environments","text":"<p>As part of ICER's OS upgrade, all software has changed. While Conda does a good job at isolating software in environments and installing most things from scratch, it is possible that you may need to reinstall your environments on the new system.</p> <p>Optional</p> <p>In limited testing, we have seen that Conda environments created on the current system appear to work on the new system without intervention. However, if you notice issues or your environments become nonfunctional, please follow the steps below.</p>","tags":["how-to guide","OS upgrade","Python","Conda"]},{"location":"Migrating_Conda_Environments/#save-your-current-environments","title":"Save your current environments","text":"<p>First, start a backwards compatibility container and get a list of your environments.</p> <pre><code>ssh user@hpcc.msu.edu\nssh dev-amd20\nold_os\nmodule load Conda/3\nconda env list\n</code></pre> <p>Select the environments you would like to transfer over. In this guide, we will use an example named <code>project</code>. Use <code>conda env export</code> to get the list of packages in the environment.</p> <pre><code>conda env export -n project &gt; project_environment.yml\n</code></pre>","tags":["how-to guide","OS upgrade","Python","Conda"]},{"location":"Migrating_Conda_Environments/#reinstall-conda-on-the-new-operating-system","title":"Reinstall Conda on the new operating system","text":"<p>Optional</p> <p>In limited testing, ICER has not seen a need to reinstall Conda on the new operating system. However, if you are having issues, this would be a good troubleshooting step to take.</p> <p>Log into a development node running the new operating system, and follow our instructions for Installing Conda.</p> <p>In the step where you are asked to install Miniforge3 in your home directory, make sure to choose a different location if you'd like to maintain backwards compatibility, e.g., <code>/mnt/home/&lt;user&gt;/miniforge3_ubuntu</code>. You can add the following to the <code>.bashrc</code> file in your home directory to make sure that the new version of Conda is only used on Ubuntu nodes.</p> <pre><code>if [[ $(source /etc/os-release &amp;&amp; echo $ID) == \"ubuntu\" ]]; then\n    export CONDA3PATH=/mnt/home/&lt;user&gt;/miniforge3_ubuntu\nfi\n</code></pre>","tags":["how-to guide","OS upgrade","Python","Conda"]},{"location":"Migrating_Conda_Environments/#create-a-new-virtual-environment","title":"Create a new virtual environment","text":"<p>Exit the container back into a development node running the new operating system, and reinstall your environments. To avoid confusion with the old environment, you can use a new name.</p> <pre><code># Exit backwards compatibility container\nexit \n\n# Load Conda module\nmodule purge\nmodule load Conda/3\n\n# Reinstall environment (may take a while)\nconda env create -f project_environment.yml -n project_ubuntu\n</code></pre> <p>The new environment is ready to use.</p>","tags":["how-to guide","OS upgrade","Python","Conda"]},{"location":"Migrating_Conda_Environments/#update-old-batch-scripts","title":"Update old batch scripts","text":"<p>Assuming this Conda environment was used in a batch script, you now need to update it for the new system. Suppose we have the following script, <code>my_script.sb</code>.</p> my_script.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\nmodule purge\nmodule load Conda/3\n\nconda activate project\npython do_some_work.py\n</code></pre> <p>Copy the script for the new system.</p> <pre><code>cp my_script.sb my_script_ubuntu.sb\n</code></pre> <p>Then, change the environment name to match the new Conda environment.</p> my_script_ubuntu.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\nmodule purge\nmodule load Conda/3\n\nconda activate project_ubuntu\npython do_some_work.py\n</code></pre> <p>To test, make sure you log into an Ubuntu development node, and submit the job using</p> <pre><code>sbatch my_script_ubuntu.sb\n</code></pre>","tags":["how-to guide","OS upgrade","Python","Conda"]},{"location":"Migrating_Python_Virtual_Environments/","title":"Migrating Python Virtual Environments","text":"<p>As part of ICER's OS upgrade, the available versions and locations of Python on the system have changed. Since Python virtual environments link to Python on the system, virtual environments will need to be reinstallled to work properly on the new system.</p> <p>To migrate virtual environments, please follow the steps below.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#save-your-current-virtual-environment","title":"Save your current virtual environment","text":"<p>First, activate your virtual environment in a backwards compatibility container. In this guide, we will use an example named <code>project</code>:</p> <pre><code>ssh user@hpcc.msu.edu\nssh dev-amd20\nold_os\n# In directory where \"project\" virtual env exists\nsource project/bin/activate\n</code></pre> <p>Then, save the packages installed via <code>pip</code> to a <code>requirements.txt</code> file.</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#create-a-new-virtual-environment","title":"Create a new virtual environment","text":"<p>Now, exit the container back into a development node running the new operating system, and, if you have specific version requirements, select the version of Python you'd like to use from the module system. If you have the default modules loaded, this will be <code>Python/3.11.3</code>.</p> <pre><code># Exit backwards compatibility container\nexit\n# Load newer version of Python than default (optional)\nmodule purge\nmodule load Python/3.11.5-GCCcore-13.2.0\n</code></pre> <p>Then use this python to create a new virtual environment. To avoid confusion with the old environment and to maintain backwards compatibility if necessary, use a new name.</p> <pre><code>python -m venv project_ubuntu\n</code></pre>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#install-the-packages-in-your-old-virtual-environment","title":"Install the packages in your old virtual environment","text":"<p>Activate the new virtual environment, and use <code>pip</code> to reinstall the packages from the old environment.</p> <pre><code>source project_ubuntu/bin/activate\n# The next step may take a while\npip install -r requirements.txt\n</code></pre> <p>The new environment is ready to use. If the reinstallation fails, see the Troubleshooting section below.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#update-old-batch-scripts","title":"Update old batch scripts","text":"<p>Assuming this virtual environment was used in a batch script, you now need to update it for the new system. Suppose we have the following script, <code>my_script.sb</code>.</p> my_script.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\nsource project/bin/activate\npython do_some_work.py\n</code></pre> <p>Copy the script for the new system.</p> <pre><code>cp my_script.sb my_script_ubuntu.sb\n</code></pre> <p>Then, change the way the virtual environment is activated to match our new virtual environment.</p> my_script_ubuntu.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\n# Make sure we have the right version of Python loaded\nmodule purge\nmodule load Python/3.11.5-GCCcore-13.2.0\n\n# Use the new virtual environment\nsource project_ubuntu/bin/activate\npython do_some_work.py\n</code></pre> <p>To test, make sure you log into an Ubuntu development node, and submit the job using</p> <pre><code>sbatch my_script_ubuntu.sb\n</code></pre>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#troubleshooting","title":"Troubleshooting","text":"","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#using-an-older-version","title":"Using an older version","text":"<p>Moving from older versions of Python to newer versions can cause problems when reinstalling packages. If you run into issues, try an older version of Python on the new operating system, e.g., when creating the new virtual environment, use</p> <pre><code>module purge\nmodule load Python/3.10.4-GCCcore-11.3.0\npython -m venv project_ubuntu\nsource project_ubuntu/bin/activate\npip install -r requirements.txt\n</code></pre> <p>However, even the older versions on the new operating system are still relatively new (we don't recommend using anything older than <code>Python/3.10.4</code>, as the older versions are deprecated). If you need an even older version, we recommend you create a Conda environment. See our instructions for Installing Conda.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#using-conda","title":"Using Conda","text":"<p>Not for reinstalling Conda environments</p> <p>This section explains how to reinstall a Python virtual environment into a Conda environment. For instructions to migrate Conda environments, please see our Conda specific documentation.</p> <p>After installing Conda, you can create an environment with the same version of Python from your virtual environment, and install the packages via <code>pip</code>.</p> <pre><code>module purge\nmodule load Conda/3\n# Use version of Python from old venv, can get using \"python --version\"\nconda create -n project_ubuntu python=3.6.3 pip\nconda activate project_ubuntu\npip install -r requirements.txt\n</code></pre> <p>To use this Conda environment in the future, use</p> <pre><code>module purge\nmodule load Conda/3\nconda activate project_ubuntu\n</code></pre> <p>Note that when using Conda, we recommend using the Conda package manager to install packages via <code>conda install</code> instead of using <code>pip</code>. However, for migrating virtual environments like this, using <code>pip</code> is fine, so long as you do not use <code>conda install</code> to install more packages into the same environment afterward.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#using-pre-installed-packages","title":"Using pre-installed packages","text":"<p>Also note that the module system already includes a few Python \"bundles\" that have popular packages pre-installed. These include <code>Python-bundle-PyPI</code> and <code>Scipy-bundle</code>. Use the <code>module spider</code> command to see if these can meet your needs.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_Python_Virtual_Environments/#use-backwards-compatibility-containers","title":"Use backwards compatibility containers","text":"<p>If all else fails you can use our backwards compatibility scripts (which have limitations). For more information, please see our backwards compatibility documentation.</p>","tags":["how-to guide","OS upgrade","Python","virtual environment"]},{"location":"Migrating_R_Packages/","title":"Migrating R Packages","text":"<p>As part of ICER's OS upgrade, the system libraries and available versions of R have changed. This will may require R packages to be reinstalled. To do so, please follow the steps below.</p> <p>Optional</p> <p>In limited testing, some R packages installed on the current system appear to work on the new system without intervention. However, packages that have external dependencies will need to account for new module names, and will likely need to be reinstalled. Additionally, packages installed with an older version of R than is available in the new module system will need to be reinstalled.</p>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#save-your-currently-installed-packages","title":"Save your currently installed packages","text":"<p>First, start a backwards compatibility container, and start the version of R you are used to using on the HPCC. For the purposes, of this example, we will use <code>R/4.2.1</code>.</p> <pre><code>ssh user@hpcc.msu.edu\nssh dev-amd20\nold_os\nmodule purge\nmodule load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1\nR\n</code></pre> <p>Within R, save your installed packages to a file and exit.</p> <pre><code>packages &lt;- installed.packages()[,\"Package\"]\nsave(packages, file = \"Rpackages\")\nq()\n</code></pre>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#backup-your-previous-library","title":"Backup your previous library","text":"<p>To avoid conflicts between packages installed on different operating systems, move your old packages to a different location. Usually, your packages are stored in subdirectories of <code>~/R/x86_64-pc-linux-gnu-library</code> labeled by the version of R they were installed with, but if you would like to double check, you can run <code>R</code> and look at the first entry of</p> <pre><code>.libPaths()\n</code></pre> <p>Move this library to a backup location so you can continue to use the CentOS operating system if necessary. Run this command from the command line.</p> <pre><code>mv ~/R/x86_64-pc-linux-gnu-library ~/R/x86_64-pc-linux-gnu-library.centos_backup\n</code></pre>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#reinstall-packages-with-a-version-of-r-on-the-new-operating-system","title":"Reinstall packages with a version of R on the new operating system","text":"<p>Now, exit the container back into a development node running the new operating system. Load the version of R you would like to use. ICER highly recommends using <code>R-bundle-CRAN/2023.12-foss-2023a</code>, as this already contains a large number packages users often need, and will reduce the number you need to install manually.</p> <pre><code># Exit backwards compatibility container\nexit\n# Load desired version of R\nmodule purge\nmodule load R-bundle-CRAN/2023.12-foss-2023a\nR\n</code></pre> <p>Then reinstall the packages listed in your <code>Rpackages</code> file that are not already installed.</p> <pre><code>load(\"Rpackages\")\nnew_packages &lt;- setdiff(packages, installed.packages()[,\"Package\"])\nfor(p in new_packages)\ninstall.packages(p)\n</code></pre> <p>If you would like to check what packages will be installed before running the <code>install.packages()</code> for loop, you can look at <code>new_packages</code>.</p> <p>Your packages will now be ready to use with this new version of R.</p>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#update-old-batch-scripts","title":"Update old batch scripts","text":"<p>Assuming these packages were used in a batch script, you now need to update it for the new system. Suppose we have the following script, <code>my_script.sb</code>.</p> my_script.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\nmodule load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1\nRscript do_some_work.R\n</code></pre> <p>Copy the script for the new system.</p> <pre><code>cp my_script.sb my_script_ubuntu.sb\n</code></pre> <p>Then, change to using the new version of R. </p> my_script_ubuntu.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\n# Make sure we have the right version of R loaded\nmodule purge\nmodule load R-bundle-CRAN/2023.12-foss-2023a\n\nRscript do_some_work.R\n</code></pre> <p>To test, make sure you log into an Ubuntu development node, and submit the job using</p> <pre><code>sbatch my_script_ubuntu.sb\n</code></pre>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#optional-going-back-to-the-old-system","title":"Optional: Going back to the old system","text":"<p>In the event that you need to rerun your R code in a CentOS backwards compatibility container, R will not find your old libraries that you have moved to <code>~/R/x86_64-pc-linux-gnu-library.centos_backup</code>. You have two options:</p>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#tell-r-where-your-packages-are-using-an-environment-variable","title":"Tell R where your packages are using an environment variable","text":"<p>This option works for any R code run after the <code>R_LIBS_USER</code> environment variable has been set.</p> my_script.sb<pre><code>#/bin/bash --login\n\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=500MB\n\nmodule load GCC/11.3.0 OpenMPI/4.1.4 R/4.2.1\nexport R_LIBS_USER=~/R/x86_64-pc-linux-gnu-library.centos_backup\nRscript do_some_work.R\n</code></pre> <p>Note that even though the <code>R_LIBS_USER</code> variable is set in the script, you can also set it from the command line before starting <code>R</code> interactively.</p>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_R_Packages/#tell-r-where-your-packages-are-in-your-r-code","title":"Tell R where your packages are in your R code","text":"<p>This option requires modification to your R code, but is more flexible. Before you run any <code>library()</code> commands in an R script or interactively, run the R command</p> <pre><code>.libpaths(\"~/R/x86_64-pc-linux-gnu-library.centos_backup\")\n</code></pre>","tags":["how-to guide","OS upgrade","R"]},{"location":"Migrating_to_the_New_Operating_System/","title":"Migrating to the New Operating System","text":"<p>Check back for updates!</p> <p>This page will continue to be updated with new information. Check back regularly for updates.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_New_Operating_System/#recommendations-to-users","title":"Recommendations to users","text":"<p>In order to transition to the upgraded system, please follow these steps:</p> <ol> <li>If you SSH to the HPCC through VS Code or proxy jumps, follow the instructions below.</li> <li>Use development nodes that have been upgraded to Ubuntu.</li> <li>Change the module names and versions of the software you are using to match the new system.<ul> <li>Note: The module names will have changed in the upgraded system, and not all versions will be carried over from the current system</li> <li>Review the Available Software page or use <code>module spider &lt;module_name&gt;</code> or <code>module avail</code> to find a module you need</li> </ul> </li> <li>Reinstall any Python virtual environments.</li> <li>Try running a small test of your existing code on the development nodes, replacing the module names with those available on the upgraded system.</li> <li>Try running a small test of your existing code with a SLURM job.</li> <li>If your code no longer runs as expected it is likely incompatible with the new operating system.<ol> <li>If you compile your own code or have built/installed packages in R or Python, rebuild using the modules available on the upgraded system. We recommend doing this in a separate location so your existing code will continue to work.</li> <li>If you use R, consider reinstalling your R packages.</li> <li>If you use Conda, consider reinstalling Conda and your environments.</li> </ol> </li> <li>Test your existing codes and scripts with our backwards compatibility helper commands as a backup.</li> </ol> <p>If you run into problems at any of these steps, please contact us using the subject \"OS Upgrade 2024\" with a description of the problem and any files necessary to reproduce it.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_New_Operating_System/#unable-to-login-via-vs-code-or-receiving-warning-remote-host-identification-has-changed-message","title":"Unable to login via VS Code or receiving \"WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\" message","text":"<p>Applies to VS code</p> <p>The errors below will stop most users from using VS Code to SSH into the HPCC, but will be hidden in VS Code's SSH output. Please try following the instructions below and try logging in again.</p> <p>When logging in to upgraded nodes using SSH keys and proxy jumps, you may receive a message like:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nPlease contact your system administrator.\nAdd correct host key in ~/.ssh/known_hosts to get rid of this message.\nOffending RSA key in ~/.ssh/known_hosts:6\nYou can use following command to remove the offending key:\nssh-keygen -R dev-amd20 -f ~/.ssh/known_hosts\nHost key for dev-amd20 has changed and you have requested strict checking.\nHost key verification failed.\n</code></pre> <p>or:</p> <pre><code>The authenticity of host 'dev-amd20 (&lt;no hostip for proxy command&gt;)' can't be established.\nRSA key fingerprint is SHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>These messages are expected as the keys used to verify the \"identity\" of the development nodes have changed from what your computer has used before. To fix the first error, run the command</p> <pre><code>ssh-keygen -R &lt;dev-node-name&gt; -f ~/.ssh/known_hosts\n</code></pre> <p>on the command line on your local computer, not the HPCC. Alternatively, you can also delete the file <code>~/.ssh/known_hosts</code> to reset all host keys. This will result in receiving the second message anytime you SSH to any other computer (even ones outside of ICER, like GitHub) until you've accepted the connection again.</p> <p>To accept the new connection and fix the second error, enter \"yes\" and hit enter.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/","title":"Migrating to the upgraded module system","text":"<p>The OS upgrade requires reinstalling all software on the HPCC. ICER has also used this as an opportunity to realign our module system closer to the tools we use to install software.</p> <p>As such, users have to take steps to transition their workflows to work successfully with the new module system. Please follow the steps below to make the necessary changes.</p> <p>The key points are to:</p> <ol> <li>Look up the new names/versions for software</li> <li>Make sure that when loading multiple pieces of software at once, they have compatible \"toolchains\"</li> </ol>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#find-the-new-names-of-software","title":"Find the new names of software","text":"<p>The names of all nearly all software have changed. To search for the new software names, please review the Available Software page or use the <code>module spider</code> command.</p> <p>For example, you can load <code>R</code> in the previous module system using</p> <pre><code>module load GCC/11.2.0 OpenMPI/4.1.1 R/4.2.2\n</code></pre> <p>On the upgraded module system, search for <code>R</code> and ignore the \"gateway\" modules that are dependencies. Additionally, most of the versions of software supported have changed, so you may need to search without the version.</p> input<pre><code>module spider R\n</code></pre> output<pre><code>-------------------------------------------------------------------\n  R:\n-------------------------------------------------------------------\n    Description:\n      R is a free software environment for statistical computing\n      and graphics.\n\n     Versions:\n        R/3.6.3-foss-2022b\n        R/4.2.2-foss-2022b\n        R/4.3.2-gfbf-2023a\n        R/4.3.3-gfbf-2023b\n    ...\n</code></pre> <p><code>R/4.2.2</code> is available under the name <code>R/4.2.2-foss-2022b</code>.</p> <p>The line</p> <pre><code>module load GCC/11.2.0 OpenMPI/4.1.1 R/4.2.2\n</code></pre> <p>in a script or on the command line can then be replaced with</p> <pre><code>module load R/4.2.2-foss-2022b\n</code></pre> <p>On the new operating system. When there is only one version of the software with that version, you can use only the version and leave off the toolchain identifier, e.g., </p> <pre><code>module load R/4.2.2\n</code></pre> <p>will also work.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#load-modules-from-compatible-toolchains","title":"Load modules from compatible toolchains","text":"<p>In the new module system, the structure of most module names follows the same format:</p> <pre><code>&lt;software_name&gt;/&lt;software_version&gt;-&lt;toolchain_name&gt;-&lt;toolchain_version&gt;\n</code></pre> <p>The \"toolchain\" is the set of programs used to compile and install that software, and needs to be loaded first before the software itself can be loaded. On the old system, this was done manually, e.g., by first loading <code>GCCcore</code> and <code>OpenMPI</code> (see the above example). On the new system this is done automatically.</p> <p>However, this allows for the potential of loading modules with incompatible toolchains at the same time. Doing so will produce an error message, e.g.,</p> input<pre><code>module load R/4.2.2-foss-2022b JAGS/4.3.2-foss-2023a\n</code></pre> output<pre><code>Lmod has detected the following error:  The previous module command attempted to load \"foss/2023a\" while \"foss/2022b\" was already\nloaded. This is likely due to loading a module with incompatible dependencies from the one currently loaded. Please use\n\n    $ module spider &lt;original_module_name&gt;\n\nTo search for other versions that may have compatible dependencies. If there are no such versions, please unload your current modules with\n\n    $ module purge\n\nbefore attempting to load this module. For additional assistance, please contact ICER staff at https://contact.icer.msu.edu.\n</code></pre> <p>Instead, you will need to search for versions of the software that have compatible toolchains using the <code>module spider</code> command (again, see the above example).</p> <p>Another alternative (if it fits your workflow) is to load these modules in separate stages. First, load the first module, and do your work with that software. Then run <code>module purge</code>, load the second module, and do your work with that software.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#loading-modules-from-subtoolchains-is-also-allowed","title":"Loading modules from subtoolchains is also allowed","text":"<p>You are also allowed to load modules from \"subtoolchains\" that are compatible with a toolchain that is already loaded, that is, software that has been installed with a subset of the pieces in a full toolchain. For example, the <code>foss/2023a</code> toolchain includes <code>GCCcore/12.3.0</code> which is a subtoolchain. The two primary toolchains are <code>foss</code> and <code>intel</code>. For a full listing of their subtoolchains, see EasyBuild's documentation, but common examples include <code>gfbf</code>, <code>gompi</code>, <code>iimpi</code>, and <code>iimkl</code>.</p> <p>Most subtoolchains share the same versioning scheme, labeled by a year and a letter. On the HPCC the three primary versions of all toolchains are <code>2022b</code>, <code>2023a</code>, and <code>2023b</code>. So for example, you can load modules with the <code>foss-2023a</code> and <code>gfbf-2023a</code> toolchains at the same time. The exception to this scheme is <code>GCCcore</code>. It is always named using the version of the GCC software, and corresponds to the year/letter scheme as follows:</p> <ul> <li><code>GCCcore/12.2.0</code> is in all <code>2022a</code> toolchains</li> <li><code>GCCcore/12.3.0</code> is in all <code>2023a</code> toolchains</li> <li><code>GCCcore/13.2.0</code> is in all <code>2023b</code> toolchains</li> </ul> <p>This can be verified by loading a year/letter toolchain and running <code>module list</code>, e.g.,</p> input<pre><code>module load foss/2023a\nmodule list\n</code></pre> output<pre><code>Currently Loaded Modules:\n  1) GCCcore/12.3.0                     9) hwloc/2.9.1-GCCcore-12.3.0       17) OpenBLAS/0.3.23-GCC-12.3.0\n  2) zlib/1.2.13-GCCcore-12.3.0        10) OpenSSL/1.1                      18) FlexiBLAS/3.3.1-GCC-12.3.0\n  3) binutils/2.40-GCCcore-12.3.0      11) libevent/2.1.12-GCCcore-12.3.0   19) FFTW/3.3.10-GCC-12.3.0\n  4) GCC/12.3.0                        12) UCX/1.14.1-GCCcore-12.3.0        20) gompi/2023a\n  5) numactl/2.0.16-GCCcore-12.3.0     13) libfabric/1.18.0-GCCcore-12.3.0  21) FFTW.MPI/3.3.10-gompi-2023a\n  6) XZ/5.4.2-GCCcore-12.3.0           14) PMIx/4.2.4-GCCcore-12.3.0        22) ScaLAPACK/2.2.0-gompi-2023a-fb\n  7) libxml2/2.11.4-GCCcore-12.3.0     15) UCC/1.2.0-GCCcore-12.3.0         23) foss/2023a\n  8) libpciaccess/0.17-GCCcore-12.3.0  16) OpenMPI/4.1.5-GCC-12.3.0\n</code></pre> <p>Note that software without a toolchain (e.g., <code>MATLAB</code>) can be loaded with any other module.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#use-module-purge-to-load-software-incompatible-with-default-modules","title":"Use <code>module purge</code> to load software incompatible with default modules","text":"<p>All default modules are installed under <code>2023a</code> toolchains. If you try to load a module with any other toolchain, you must first run</p> <pre><code>module purge\n</code></pre> <p>to avoid the errors discussed above.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#use-the-exact-name-of-software-when-loading","title":"Use the exact name of software when loading","text":"<p>At the moment, the module system is case sensitive. For example,</p> <pre><code>module load Python\n</code></pre> <p>works, while</p> <pre><code>module load python\n</code></pre> <p>does not. ICER is working to resolve this, but for now, please make sure to use the name exactly as specified in the <code>module spider</code> results (note that searching with <code>module spider</code> is not case sensitive).</p>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#search-for-and-load-extensions","title":"Search for and load extensions","text":"<p>Some modules include \"extensions\" to software. For example, the <code>R-bundle-CRAN</code> includes hundreds of <code>R</code> packages, and <code>SciPy-bundle</code> includes many commonly used Python packages for data analysis. When using <code>module spider</code>, you can also search for these extensions:</p> input<pre><code>module spider numpy\n</code></pre> output<pre><code>----------------------------------------------------------------------------\n  numpy:\n----------------------------------------------------------------------------\n     Versions:\n        numpy/1.19.4 (E)\n        numpy/1.22.3 (E)\n        numpy/1.24.2 (E)\n        numpy/1.25.1 (E)\n        numpy/1.26.2 (E)\n\nNames marked by a trailing (E) are extensions provided by another module.\n\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"numpy\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider numpy/1.26.2\n----------------------------------------------------------------------------\n</code></pre> <p>As the message suggests, extensions are marked with an <code>(E)</code>. To figure out what module loads them, run <code>module spider</code> with the specific version:</p> input<pre><code>module spider numpy/1.26.2\n</code></pre> output<pre><code>----------------------------------------------------------------------------\n  numpy: numpy/1.26.2 (E)\n----------------------------------------------------------------------------\n    This extension is provided by the following modules. To access the extension you must load one of the following modules. Note that any module names in parentheses show the module location in the software hierarchy.\n\n\n       SciPy-bundle/2023.11-gfbf-2023b\n\n\nNames marked by a trailing (E) are extensions provided by another module.\n</code></pre>","tags":["how-to guide","OS upgrade"]},{"location":"Migrating_to_the_Upgraded_Module_System/#request-missing-modules","title":"Request missing modules","text":"<p>If a software module you need was not carried over from the previous system, please request it by submitting a ticket with the subject \"OS Upgrade 2024\". </p> <p>If you need access to a piece of software immediately and cannot wait for it to be reinstalled, please look into using our backwards compatibility options.</p> <p>Version availability</p> <p>ICER may not be able to install older versions of software especially if they require older compilers that do not work well on the new operating system. Please try using versions of the software that are already installed before reaching out to request older versions. When requesting an older version, please provide a short justification.</p>","tags":["how-to guide","OS upgrade"]},{"location":"Mothur/","title":"Mothur","text":"","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#loading-mothur","title":"Loading Mothur","text":"<p>Mothur version 1.40.3 on the HPCC has two running modes, with and without MPI functionality. You can load either one by:</p> <pre><code>module purge\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-nonMPI-Python-2.7.13        # Mothur non-MPI version\nmodule load icc/2017.1.132-GCC-6.3.0-2.27 impi/2017.1.132 Mothur/1.40.3-Python-2.7.13               # Mothur MPI version\n</code></pre> <ul> <li> <p>As of Feb 7, 2019, the highest version is\u00a0Mothur/1.41.3 (in MPI mode only).</p> </li> <li> <p>As reported by some Mothur users, when using Mothur and vsearch together, the only compatible version of vsearch is 1.8. So after loading Mothur, you would add a line of \"<code>module load vsearch/1.8.0</code>\".</p> </li> </ul>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#running-mothur","title":"Running Mothur","text":"<p>Take a look at this example code <code>batch.m</code>:</p> <p><code>/mnt/research/common-data/Examples/mothur/batch.m</code>:</p> <pre><code>set.current(fasta=ex.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.subsample.fasta, count=ex.trim.contigs.good.unique.good.filter.unique.precluster.uchime.pick.pick.subsample.count_table,  processors=1)\ndist.seqs(fasta=current, cutoff=0.2, processors=8)\n</code></pre> <p>where we specified <code>processors=8</code> in line 2.\u00a0To be able to actually utilize 8 processors, you need to launch Mothur using either of the following commands, depending on whether MPI is enabled or not.</p> <pre><code>MPI: mpirun -np 8 mothur batch.m\nnon-MPI: mothur batch.m\n</code></pre>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"Mothur/#differences-between-mpi-and-non-mpi-runs","title":"Differences between MPI and non-MPI runs","text":"<p>MPI jobs can run across multiple nodes at the cost of overhead. This can lead to increased memory usage and decreased performance. The additional processor advantages offered by MPI may be cancelled out by I/O waits to disk. If you request many more processes than can be provided by a single node, use MPI mode. If you choose the MPI type, specify number of processes in the SLURM script by <code>--ntask=8</code> for the example above. SLURM will determine how many nodes and tasks per node are needed. Also,\u00a0memory request in this case should be made on a per CPU basis (by defining <code>--mem-per-cpu</code>).</p> <p>Non-MPI jobs run on a single node with multiple threads/processes. For above Mothur command, you should set up something like</p> <p><code>#SBATCH --nodes=1</code></p> <p><code>#SBATCH --ntasks-per-node=1</code></p> <p><code>#SBATCH --cpus-per-task=8</code></p> <p>in your job submission script. If most of the nodes in the cluster are highly occupied, the job scheduler may have a hard time finding the nodes with availability of your desired number of threads.</p>","tags":["tutorial","Mothur","bioinformatics"]},{"location":"MySQL_configuration/","title":"MySQL configuration for OrthoMCL","text":""},{"location":"MySQL_configuration/#overview","title":"Overview","text":"<p>OrthoMCL is a program that aids in the identification of orthologs. \u00a0The OrthoMCL tool uses NCBI BLAST and the MCL application in conjunction with a relational database (MySQL). \u00a0OrthoMCL version 2.0.2 is available on the HPCC and can be loaded as a module.</p> <p>However, because of the relational database requirement, the HPCC must be contacted in advance to setup a database for your runs. This tutorial briefly describes how to obtain access to the program, and how to use configuration files provided by the HPCC.</p>"},{"location":"MySQL_configuration/#database-access","title":"Database Access","text":"<p>Before beginning your HPCC runs, you will need to complete an\u00a0help request ticket request to request a database and MySQL account for your use of OrthoMCL.\u00a0\u00a0 This information will be provided to you in the form of a configuration file, and you save this to your directory.\u00a0 You can use the filename as a command-line argument to relevant scripts comprising the OrthoMCL application, which tell OrthMCL how to connect to your database.\u00a0</p>"},{"location":"MySQL_configuration/#configuration-file","title":"Configuration File","text":"<p>The following is an example of the configuration file you will receive:</p> <pre><code>dbVendor=mysql\ndbConnectString=dbi:mysql:someUserdb:db-01:3306\ndbLogin=someUser\ndbPassword=somePassword\n# DO NOT CHANGE ANYTHING ABOVE THIS LINE UNLESS YOU KNOW WHAT YOU'RE DOING\nsimilarSequencesTable=SimilarSequences\northologTable=Ortholog\ninParalogTable=InParalog\ncoOrthologTable=CoOrtholog\ninterTaxonMatchView=InterTaxonMatch\npercentMatchCutoff=50\nevalueExponentCutoff=-5\n</code></pre>"},{"location":"MySQL_configuration/#command-example","title":"Command Example","text":"<p>A typical run that would require the database configuration file might look something like the following:</p> <pre><code>orthomclPairs orthomcl.config log_file cleanup=[yes|no|only|all] &lt;startAfter=TAG&gt;\n</code></pre> <p>In the example above, the file \"orthomcl.config\" is the name of the configuration/connection file (provided by the HPCC) that you want to use for your run.</p>"},{"location":"MySQL_configuration/#purging-the-database-between-runs","title":"Purging the Database Between Runs","text":"<p>To facilitate multiple concurrent, or faster consecutive runs, many users ask for more than one database at setup time. \u00a0HPCC will typically be able to provide you with up to four (4) such databases. \u00a0Please specify this in your request.</p> <p>Once your run is completed, you will need to purge the database of its contents prior to beginning new runs using the same database. \u00a0To have this performed, please contact ICER\u00a0 via https://contact.icer.msu.edu, select \"other\" in the form and let us know you need your database purge and the name of the database\u00a0 if you have multiple databases (or just provide the connection string like <code>dbConnectString=dbi:mysql:someUserdb:db-01:3306</code> ).</p> <p>When your work with OrthoMCL is complete, please notify the staff via the ICER contact form so we can purge your databases from the system.</p>"},{"location":"MySQL_configuration/#modifying-the-configuration-file","title":"Modifying the Configuration File","text":"<p>Most users will not need to modify the configuration file provided by the HPCC. \u00a0The most common modification needed will be to change the name of the database to be accessed in those cases where users are provided with access to more than one database.</p> <p>The relevant line to be modified is shown below:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb:db-01:3306 \n</code></pre> <p>In the example above, the database name is \"someUserdb\". \u00a0</p> <p>Let's assume (for example), a user had been issued 4 databases named: someUserdb, someUserdb2, someUserdb3, someUserdb4. \u00a0To perform a run using one of these other databases, we would need to make a copy of the configuration file and change the name in that file, for example:</p> <pre><code>dbConnectString=dbi:mysql:someUserdb2:db-01:3306\n</code></pre> <p>You may then structure the command for each OrthoMCL run to use the configuration file (and related database) desired.</p>"},{"location":"MySQL_configuration/#more-information","title":"More Information","text":"<p>Refer to the OrthoMCL User Manual.</p>"},{"location":"NCBI_Entrez_Direct_tools/","title":"NCBI Entrez Direct tools","text":"<p>Entrez Direct (eDirect) provides access to the NCBI's suite of interconnected databases (publication, sequence, structure, gene, variation, expression, etc.) using Linux command lines. People who need to retrieve sequence and other type of data from NCBI regularly should use the eDirect toolset. To use edirect commands on HPCC, run:</p> <pre><code>ssh dev-intel18\nmodule load Perl/5.28.1\nmodule load edirect\n</code></pre> <p>A full documentation of eDirect is available from https://www.ncbi.nlm.nih.gov/books/NBK179288/</p> <p>A few useful examples can be found in this blog http://bioinformatics.cvr.ac.uk/blog/ncbi-entrez-direct-unix-e-utilities/</p> <p>eDirect cookbook:\u00a0https://github.com/NCBI-Hackathons/EDirectCookbook/blob/master/README.md</p>"},{"location":"OS_Upgrade/","title":"Operating System Upgrade","text":"<p>ICER has upgraded the operating system on all development and compute nodes from CentOS 7 to Ubuntu 22.04. The primary transition for general usage compute nodes occurred on June 17th, 2024.</p> <p>As part of this process, all software must be rebuilt for the new operating system. This includes all user compiled software, and all software provided by ICER via the module system. ICER has used this as an opportunity to reevaluate software offerings and organization, and as such, the structure of the module system has changed.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#summary","title":"Summary","text":"<ul> <li>ICER upgraded the operating system on all nodes from CentOS 7 to Ubuntu 22.04</li> <li>Users need to change <code>module load</code> commands, possibly updating to different software versions</li> <li>Users likely need to reinstall/recompile software that is not loaded from the module system (including packages from R, Python, etc) (but should test first)</li> <li>Users need to reinstall Python virtual environments</li> <li>Users may need to reinstall Conda and recreate Conda environments (but should test first)</li> <li>Backwards compatibility scripts are available</li> <li>Help is available! Contact us with any problems or questions you have using the subject line \"OS Upgrade 2024\"!</li> </ul> <p>For specific tasks users need to complete to use the new operating system, please see our page on Migrating to the New OS.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#timeline","title":"Timeline","text":"On June 17 <p>Most nodes are transitioned to the new operating system. </p> <p>On this date:</p> <ul> <li>All development nodes have been moved from CentOS to Ubuntu.<ul> <li>They will be accessible either with or without the <code>-ubuntu</code> suffix. See our Testing nodes page for more information.</li> <li>Two CentOS gateway nodes are temporarily available under a different name</li> </ul> </li> <li>The scheduler (SLURM) will briefly be paused.<ul> <li>All jobs currently running will continue to run.</li> <li>All jobs in the queue will remain in the queue and be eligible for scheduling when the scheduler is available.</li> </ul> </li> <li>All jobs are now submitted to nodes running the new OS by default either with or without the line <code>#SBATCH --reservation=ubuntu_compute</code>.<ul> <li>Buy-ins and a small group of nodes will temporarily remain on the old operating system.</li> </ul> </li> <li>All OnDemand apps have been replaced by alternatives that run on the new operating system.<ul> <li>Apps running on CentOS will still be temporarily available and will have \"(Legacy)\" in the title.</li> </ul> </li> </ul> On July 17 Buy-in nodes have been transitioned (unless requested earlier) On August 6 All remaining CentOS nodes and legacy OnDemand apps have been removed","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#why-is-this-happening","title":"Why is this happening?","text":"","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#operating-system-upgrade_1","title":"Operating system upgrade","text":"<p>The HPCC requires operating system updates for similar reasons to a personal computer requiring an operating system upgrade. The current operating system, CentOS 7, was released ten years ago, and is nearing the end of its support. In order to ensure that the HPCC stays secure, operational, and stable, a new, supported operating system will take its place.</p> <p>ICER has chosen to change Linux distributions from CentOS (or any other Red Hat Enterprise Linux derivative) to Ubuntu. This decision involves many factors, but primary amongst them was the ability to continue using ICER's older nodes, increasing the supply of available compute to our users. From a practical perspective, Ubuntu and CentOS will feel very similar to users, and the majority of commands you may be used to using will still continue to work.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade/#software-upgrade","title":"Software upgrade","text":"<p>Behind the scenes, changing a computer's operating system also changes the libraries and programs that the software used on a regular basis relies on. This requires reinstallation of nearly every piece of software to ensure that it is compatible with the new system.</p> <p>Historically, ICER has a mix of software from a variety of sources, but primarily uses a tool called EasyBuild to install and manage software. To better manage future installations and upgrades, we have used this as an opportunity to reassess which software we can most effectively support. The end result is a software stack better aligned with EasyBuild (which stops supporting older pieces of software) that includes with newer versions that can be tracked and updated more easily.</p>","tags":["explanation","OS upgrade"]},{"location":"OS_Upgrade_Changelog_and_Known_Issues/","title":"Changelog and Known Issues for OS Upgrade","text":""},{"location":"OS_Upgrade_Changelog_and_Known_Issues/#changelog","title":"Changelog","text":""},{"location":"OS_Upgrade_Changelog_and_Known_Issues/#module-changes","title":"Module changes","text":"<p>As part of the operating system upgrade, the module naming scheme has been revised so it is no longer necessary to load a \"gateway\" set of modules before accessing other software. However, this means that nearly every module name has changed to show the dependencies it uses.</p> <p>To handle this transition, search the Available Software page or use the <code>module spider</code> command to find the new name of modules you are currently using, e.g.,</p> <pre><code>module spider Python\n</code></pre> <p>However, there are some specific module changes that are not clear from the output of <code>module spider</code>:</p> <ul> <li>For <code>Python</code> and <code>R</code>, consider using bundles instead of the plain module names. These include many helpful pre-installed packages (use the <code>module spider &lt;insert-bundle-name&gt;</code> command for specifics):<ul> <li>For <code>Python</code>, we recommend either <code>Scipy-Bundle</code> and/or <code>Python-bundle-PyPI</code></li> <li>For <code>R</code>, we recommend either <code>R-bundle-CRAN</code> and/or <code>R-bundle-Bioconductor</code></li> </ul> </li> <li>The <code>PGI</code> module has been replaced by <code>NVHPC</code> (installation still in progress). This is due to NVIDIA's rebranding of PGI.</li> <li>The <code>LiftOver</code> software is provided by the <code>Kent_tools</code> module.</li> </ul>"},{"location":"OS_Upgrade_Changelog_and_Known_Issues/#known-issues","title":"Known issues","text":"<p>Below are issues that ICER is aware of and is taking steps to resolve. If you find an issue that is not on this page, please contact us with the subject line \"OS Upgrade 2024\"</p> <ul> <li> <p>When starting up RStudio Server, you may only see a gray blank screen. To fix, this open a command line and run</p> <pre><code>mv .local/share/rstudio .local/share/rstudio.backup\n</code></pre> <p>This will move your RStudio configuration files to a backup. Note that this will likely reset your RStudio session, so you may need to reopen previous projects and files, and could lose any unsaved work.</p> </li> <li> <p>Job scripts that start with <code>#!/bin/sh</code> will result in errors like <code>/var/lib/slurmd/jobXXXXXXXX/slurm_script: XX: module: not found</code>. Please change this line to <code>#!/bin/bash</code>.</p> </li> <li> <p>When logging in to upgraded nodes using SSH keys and proxy jumps, you may receive a message like:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the RSA key sent by the remote host is\nSHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nPlease contact your system administrator.\nAdd correct host key in &lt;location&gt;/.ssh/known_hosts to get rid of this message.\nOffending RSA key in &lt;location&gt;/.ssh/known_hosts:6\nYou can use following command to remove the offending key:\nssh-keygen -R dev-amd20 -f &lt;location&gt;/.ssh/known_hosts\nHost key for dev-amd20 has changed and you have requested strict checking.\nHost key verification failed.\n</code></pre> <p>or:</p> <pre><code>The authenticity of host 'dev-amd20 (&lt;no hostip for proxy command&gt;)' can't be established.\nRSA key fingerprint is SHA256:rydhZ58BeXsXgWQisWlbH6E0IFG+2+LSxC9a7OfZBro.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>These messages are expected as the keys used to verify the \"identity\" of the development nodes have changed from what your computer has used before. To fix the first error, run the command</p> <pre><code>ssh-keygen -R &lt;dev-node-name&gt; -f ~/.ssh/known_hosts\n</code></pre> <p>on your local computer, not the HPCC. Alternatively, you can also delete the file <code>~/.ssh/known_hosts</code> to reset all host keys. This will result in receiving the second message anytime you SSH to any other computer (even ones outside of ICER, like GitHub) until you've accepted the connection again.</p> <p>To accept the new connection and fix the second error, enter \"yes\" and hit enter.</p> </li> <li> <p>The <code>module load</code> command is case sensitive, e.g., <code>module load Python</code> works, but <code>module load python</code> does not. Please use <code>module spider</code> to find the correct name, including capitalization, of the modules you would like to load. We are working on bringing case insensitivity back to the module system.</p> </li> <li>When using the OnDemand Desktop, you may receive an error dialog stating \"The panel encountered a problem while loading 'IndicatorAppletCompleteFactory::IndicatorAppletComplete'. Do you want to delete the applet from your configuration?\" Click the \"Delete\" option, and this error will not return in the future.</li> <li>Please note that all jobs need to be submitted from a node running on the same operating system as the target node (i.e., only submit a job that can run on Ubuntu using a development node running Ubuntu).</li> <li>We are noticing problems with the <code>GROMACS</code> MPI tests on older nodes. For this reason, <code>GROMACS</code> is currently available on every cluster except for <code>intel14</code> and <code>intel16</code>.</li> <li> <p>The R package <code>Matrix</code> (which is a dependency for many other packages including <code>ggplot2</code>) is incompatible with versions of R earlier than 4.4.0 (i.e., all of the versions installed on the new operating system. We recommend using the <code>R-bundle-CRAN</code> module instead of <code>R</code> which includes a pre-installed version of <code>Matrix</code>. If you need to install it yourself using <code>install.packages</code>, use the command</p> <pre><code>install.packages(https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz, repos=NULL, type=\"source\")\n</code></pre> </li> <li> <p>The <code>powertools</code> module is available, but some tools still require modification to work on the new system. If one of the tools you need is not working, please request that we update it using the contact form to help us prioritize our updates.</p> </li> <li> <p>Firefox can't open from an Interactive Desktop on OnDemand. The fix is to run <code>mv ~/.mozilla ~/.mozilla_backup</code>.</p> </li> <li>The OnDemand Job Composer cannot submit jobs until the OnDemand server is moved to Ubuntu, which is expected to occur mid July.</li> <li>Can't find <code>module</code> command in VS Code? Try <code>source /etc/profile</code>.</li> </ul>"},{"location":"Open_OnDemand/","title":"Open OnDemand","text":"<p>Open OnDemand helps researchers efficiently  utilize the HPCC by providing easy web access with graphical user interfaces from any device. The features include, though are not limited to:</p> <ul> <li>Plugin-Free web experience</li> <li>Easy file management</li> <li>Easy job submission, management, and monitoring </li> <li>Graphical desktop environments and desktop applications;     Jupyter Notebook, RStudio, ...</li> <li>One-click app icons on a desktop to launch your favorite GUI applications </li> <li>Command-line shell access</li> </ul>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#connect-to-hpcc-ondemand","title":"Connect to HPCC OnDemand","text":"<p>To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu/.  It will first redirect to the CILogon website for authentication.  </p> <p></p> <p>From here, select \"Michigan State University\" as the identity provider and click \"Log On\". This will redirect you to a page where you can log in with your MSU credentials.  </p> <p>Note</p> <p>A valid MSU email address with NetID and password are required to access HPCC OnDemand. This may prohibits the access of external collaborators. Please take this into consideration when you choose tools provided on HPCC OnDemand for your group projects involving external collaborators. </p> <p>After sign in, you will reach the HPCC OnDemand web portal with the following menu options:    </p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#files","title":"Files","text":"<p>All user's files in the home, research, scratch, and software file spaces can be accessed.   </p> <p></p> <p>Select a directory then use the <code>File Explorer</code> to download, upload,  view, edit, and move files. You can learn more about the explorer at https://www.osc.edu/resources/online_portals/ondemand/file_transfer_and_management.</p> <p></p> <p>Note</p> <p>The OnDemand portal is best for transferring files less than ~1 GB in size.  For transferring larger files to and from the HPCC,  see Large file transfer (Globus)</p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#jobs","title":"Jobs","text":"<p>Active Jobs: List jobs in the queue; monitor or manage those jobs</p> <p></p> <p>Job Composer: Submit a job script with resource requests and  command lines; create new scripts from a job template, copy a previous job submission,  or select an existing job script from a specified directory. You can learn more about the job composer at https://www.osc.edu/resources/online_portals/ondemand/job_management.</p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#interactive-apps","title":"Interactive Apps","text":"<p>Desktops: Request an <code>Interactive Desktop</code>; once it starts, a graphical user interface (GUI) is provided for running GUI based applications</p> <p></p> <p>Servers: Launch an HPCC served application</p> <ul> <li>Jupyter: includes both Notebook and Lab. Use custom environments with the \"Conda Environment\" and \"Singularity Overlay\" options in the \"Jupyter Location\" menu.</li> <li>RStudio Server: Versions of R from 4.2.2-4.3.3, including Bioconductor and CRAN packages pre-installed.</li> <li>TensorBoard: Monitor your machine learning processes.</li> <li>Code Server: Run VS Code in your browser on the HPCC.</li> </ul> <p>GUIs: Launch an HPCC provided, GUI based application directly</p> <ul> <li>MATLAB: 2023b</li> <li>Stata: 18SE, 18MP are available</li> <li>ParaView: Visualize your data.</li> </ul> <p>Note</p> <p>Please make sure you request enough memory. Otherwise, your interactive  session won't start or your running processes will be terminated prior to completion. </p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#development-nodes","title":"Development Nodes","text":"<p>Start a bash session a specified development node and input commands using the terminal's command line interface. </p> <p></p> <p>Note</p> <p>Currently, there is no remote graphical display capability while in the terminal i.e., no X11 forwarding. For GUI based applications, please use the Interactive Apps feature. </p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#my-interactive-sessions","title":"My Interactive Sessions","text":"<p>All of a user's interactive jobs are displayed, along with a description of the resources currently allocated to each job. User's may then manage aspects of those jobs using the associated action buttons. The Delete button will end the interactive session.</p> <p></p>","tags":["reference","OnDemand"]},{"location":"Open_OnDemand/#debugging-information","title":"Debugging Information","text":"<p>OnDemand jobs produce log files that can help ICER staff fix issues with apps. If you submit a ticket, it helps to provide the <code>output.log</code> file for the OnDemand session with a problem.</p> <p>To access this file, click the Session ID in the Interactive Session card of the session experiencing the issue (red circle below).</p> <p></p> <p>In the file browser that appears, click the checkbox next to <code>output.log</code> (red circle below) in addition to any other files you may be asked for, then click the Download button at the top of the file browser (purple arrow below).</p> <p></p> <p>Please attach the <code>output.log</code> file (and any other files you are asked for) to a ticket to assist ICER staff.</p> <p>We are currently working to add more functionality to the OnDemand interface. Please feel free to contact us if you have any questions or suggestions.</p>","tags":["reference","OnDemand"]},{"location":"Powertools/","title":"Powertools overview","text":"<p>Powertools are useful commands that provide additional functionality or simpler ways to access existing functionality on the HPCC. To view a list of all  available powertools with their documentation, run <code>powertools</code> on a development node. </p> <p>By default, powertools is loaded when users log into a development node.  Users may also run the <code>module list</code> command to check if it is loaded.  If it is not, please use the command:</p> <p><code>$ module load powertools</code></p> <p>to load the module before running powertools commands. Powertools can also be  used in job submission scripts the same as any other commands, if the powertools  module has been loaded.</p> <p>A selection of widely useful powertools are listed below as links to their description pages.</p>"},{"location":"Powertools/#qs","title":"qs","text":"<p>Display job list.</p>"},{"location":"Powertools/#node_status","title":"node_status","text":"<p>Display a list of compute nodes and their properties.</p>"},{"location":"Powertools/#bi","title":"bi","text":"<p>Information of Buy-In Account</p>"},{"location":"Powertools/#js","title":"js","text":"<p>Display job steps and their resource usages.</p>"},{"location":"Powertools/#getexample","title":"getexample","text":"<p>Download user examples.</p>"},{"location":"Powertools_longjob_by_DMTCP/","title":"Powertools longjob by DMTCP","text":"<p>The following are instructions for trying out <code>longjob</code> powertool on HPCC system. First, you start with a basic submission script. For example, consider the following simple submission script:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=168:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\n\nsrcdir=${SLURM_SUBMIT_DIR}/bin/\nWORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\nmkdir -p ${WORK}\n\n# Copy files to work directory\ncp -r $srcdir/* $WORK/\n\n#Move to the working directory\ncd $WORK\n\n#Run my program\n./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nscontrol show job ${SLURM_JOBID}\n\nexit $ret\n</code></pre> <p>To get <code>longjob</code> to work, the following modifications might need to be made:</p> <ol> <li>Change walltime to be less than 4 hours if you would like to have     more available nodes to your job.</li> <li>Wrap all setup-code that only needs to be run once in an if     statement that checks for the file \"Files_Copied\". This will ensure     that the setup-code only runs the first time the script is run     because in the first time there should be no file with the name     \"Files_Copied\".</li> <li>Add the <code>longjob</code> command before the command in the submission     script that you want to checkpoint.</li> <li> <p>Load the powertools module and turn on aliases. i.e. add the     following lines of code to the script:</p> <pre><code>shopt -s expand_aliases\nmodule load powertools\n</code></pre> </li> <li> <p>Set the following environment variables as appropriate for your job:</p> <ul> <li>JobScript\u00a0 \u2013\u00a0 Name of the job script file which will get     resubmitted. The default is the first submitted job script name.</li> <li>DMTCP_Checkpoint_Time\u00a0 \u2013\u00a0\u00a0Time (in seconds) which DMTCP     needs to work on checkpointing. The default is 5 minutes.</li> <li>DMTCP_CHECKPOINT_INTERVAL\u00a0 \u2013\u00a0 Time (in seconds) between     automatic checkpoints. The default is 4-8 hours. For walltime     less than 4 hours, the default will do checkpointing once at     ${DMTCP_Checkpoint_Time} + 1 minute before the end of     walltime.\u00a0</li> <li>DMTCP_CHECKPOINT_DIR\u00a0\u00a0\u2013\u00a0 Name of the directory to save     checkpoint image and log flies. The default is     ckpt_${SLURM_JOB_NAME}. For job array, the default is     ckpt_${SLURM_JOB_NAME}_${SLURM_ARRAY_TASK_ID}. If two     different jobs use the the same directory to run with the     <code>longjob</code> command, please make sure the environment variables     (or SLURM_JOB_NAME) are set different so their image files are     not saved in the same directory.</li> </ul> </li> </ol> <p>The following is a modified example script with the changes:</p> <pre><code>#!/bin/bash -login\n#SBATCH --nodes=1 --ntasks=1 --cpus-per-task=1\n#SBATCH --time=04:00:00\n#SBATCH --mem=2gb\n#SBATCH --constraint=intel16\n#SBATCH --job-name=MyJob0\n#SBATCH --mail-type=FAIL,END\n\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1\nmodule load powertools\n\n# Change checkpointing environment variables if necessary:\n# export DMTCP_Checkpoint_Time=60                     -- change checkpointing time\n# export DMTCP_CHECKPOINT_INTERVAL=7200               -- change time interval between checkpoints\n# export DMTCP_CHECKPOINT_DIR=ckpt_${SLURM_JOB_NAME}  -- change where to save checkpointing files\n\n# Change to a directory other than ${SLURM_SUBMIT_DIR} if necessary:\n# cd /mnt/scratch/${USER}/WorkPlace\n\nif [ ! -f Files_Copied ]\nthen\n    srcdir=${SLURM_SUBMIT_DIR}/bin/\n    WORK=/mnt/scratch/${USER}/KineticSN/${SLURM_JOBID}\n    mkdir -p ${WORK}\n\n    # Copy files to work directory\n    cp -r $srcdir/* $WORK/\n\n    #Run main simulation program\n    cd $WORK\n    touch Files_Copied \n\nfi\nlongjob ./SimulationTest -scattering_flag 0 -weak_reaction_flag 0 -outputVisData 100\nret=$?\n\nexit $ret\n</code></pre> <p>If everything works as expected, you should be able to submit the above job script and it will resubmit itself until the job completes. Note, this is rough code, not completely tested and does work in all cases. For example, one case that could propose a problem is if the main program gets caught in a loop and never exits. In this case, the code will keep submitting itself indefinitely. \u00a0Note that in addition you can't include output redirection as you'd expect, that is a command like <code>myprogram.py &gt; myoutput.txt</code> \u00a0and <code>longjob myprogram.py &gt; myoutput.py</code> is not the same (the redirection here applies to <code>longjob</code>, not your program). \u00a0  </p> <p>If you have difficulty, please contact us.</p>","tags":["tutorial","checkpointing","powertools"]},{"location":"Python/","title":"Python packages","text":"<p>It's difficult for the HPCC to host a vast volume of Python packages, and to resolve conflicts between Python versions. Users are encouraged to install and manage packages on their own, through conda or virtual environments.</p>"},{"location":"Python/#using-conda","title":"Using conda","text":"<p>Users install their own version of Python through Conda in the home or research space. This gives them full control on their preferred versions of Python and packages. Isolated virtual environments can be created and controlled by conda environments.</p>"},{"location":"Python/#using-virtualenv","title":"Using virtualenv","text":"<p>By default, Python version 3.6.4 (compiled by GCC/6.4.0) is loaded and ready to use after logging into HPCC dev-nodes. Other versions can be found by running <code>module spider Python</code>. Usually, pre-installed packages are available for each Python version. If users need to install new ones, they can install them in the home or research space through virtual environments.</p>"},{"location":"Python_on_HPC/","title":"Python on HPC","text":"<p>Python is popular because it makes a great first impression; i) clean, clear syntax, ii) multi-paradigm, iii) interpreted, iv) duck typing, garbage collection, and most of all, v) instant productivity. It keeps up with users' needs. It has i) flexible, full-featured data structures ii) extensive standard libraries iii) reusable open-source package iv) package management tools v) good unit testing frameworks. </p> <p>In exchange for user-friendliness and ease of use, Python becomes one of the slowest computer languages, primarily because it is an interpreted language, and allows a single thread to run in the interpreter's memory space at once. Python is typically 30 to 300 times slower than C or Fortran. However, Python has a powerful and enthusiastic open-source community which continuously improves the capability of Python.\u00a0</p> <p>On this page, we want to</p> <ul> <li>explain what the MSU HPCC is doing to support Python users.</li> <li>provide guidance to help users improve Python performance on the     HPCC.</li> <li>point out tools that support developers of Python on the HPCC.</li> </ul> <p>We assume that</p> <ul> <li>you know and use Python, or</li> <li>you know and use the HPCC and are curious about using Python in your own     HPCC work.</li> </ul>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#how-to-use-python-on-the-hpcc","title":"How to use Python on the HPCC?","text":"<p>Python applications usually use packages and modules that require specific version of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue,</p> <ul> <li>users can create an isolated virtual environment\u00a0with a particular     version of Python on our system in a self-contained directory of     their home or research space.</li> <li>users can\u00a0install their own version of Python through     Conda in their home or research     space. This gives users full control on their preferred versions of     python and packages.</li> </ul> <p>To get started using Python on the HPCC, you have to load a Python module. A few helpful module commands would be <code>module avail Python</code>, <code>module spider Python</code>, and <code>module load Python</code>. More information on our module system can be found here.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#python-with-virtual-environments","title":"Python with virtual environments","text":"<p>More details of how to use virtual environments can be found at this page.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#python-with-conda","title":"Python with Conda","text":"<p>More details of how to use Conda can be found at this page.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>For Jupyter notebook users, we have the Open OnDemand platform  To connect to HPCC OnDemand, visit\u00a0https://ondemand.hpcc.msu.edu.\u00a0After logging in, choose interactive apps, select Jupyter Notebook, request resources as you need. Your Jupyter Notebook will start when the requested resources are ready.</p> <p></p> <p></p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#can-my-python-code-be-faster","title":"Can my Python code be faster?","text":"<p>Now, you are ready to use Python on the HPCC. Now, let's learn a few tips to make your Python codes faster.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#vectorization","title":"Vectorization","text":"<p>Vectorization speeds up Python code without using loops. Instead of loops, NumPy can help by minimizing the running time of code efficiently. NumPy offers various operations to be performed over vectors such as the dot product, cross product, and matrix multiplication. See the Numpy array documentation for more information.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#numba","title":"Numba","text":"<p>Numba\u00a0compiles Python codes just in time with a few decorators, without much modification of code.\u00a0In addition, Numba offers automatic parallelization which is very easy to use. You just need to add the one line decorator,\u00a0<code>@njit(parallel=True)</code>. More information can be found here. Numba also supports NVIDIA CUDA. It is easy to use (at least much easier to use than other programming languages). </p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#use-threaded-libraries","title":"Use Threaded Libraries","text":"<p>Packages like NumPy and SciPy are already built with MPI and multithread support via BLAS/LAPACK, and MKL. In general, it is a plausible guess that most solvers have already been implemented in pure Python. In addition, many major threaded libraries and packages already have binds such as PyTrilinos, Petsc4py, Elemental, and SLEPc. So, don't try to reinvent the wheel. If it is not new, it is probably already implemented for high performance.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#mpi","title":"MPI","text":"<p>Python has a package for MPI, mpi4py.</p> <p>It is</p> <ul> <li>a pythonic wrapping of the system's native MPI.</li> <li>a provider of almost all MPI-1, 2 and common MPI-3 features.</li> <li>very well maintained.</li> <li>distributed with major Python distributions.</li> <li>portable and scalable.</li> <li>dependent only on NumPy, Cython (build only), and MPI libraries.</li> </ul> <p>More information can be found here.</p>","tags":["explanation","Python","Jupyter"]},{"location":"Python_on_HPC/#other-python-resources","title":"Other Python Resources","text":"<p>The following are a few Python resource links.</p> <ul> <li>https://www.python.org/about/gettingstarted/</li> <li>https://wiki.python.org/moin/BeginnersGuide/</li> <li>https://www.codecademy.com/learn/python/</li> <li>https://www.coursera.org/specializations/python/</li> <li>https://software-carpentry.org/lessons/</li> <li>https://pymotw.com/</li> <li>HPCC wiki Python page</li> <li>Python video on youtube</li> <li>https://www.py4e.com/ </li> </ul>","tags":["explanation","Python","Jupyter"]},{"location":"QIIME_2/","title":"QIIME 2","text":"<p>QIIME (Quantitative Insights Into Microbial Ecology) is an open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. Since QIIME 1 is no longer supported officially (see announcement at http://qiime.org), it's not installed on the Ubuntu system of HPCC. The way QIIME 2 is installed and run on the HPCC Ubuntu is through conda https://docs.qiime2.org/2018.2/install/native/.</p> <p>You may follow our instructions to install Conda in your home directory.</p> <p>Below is how to install QIIME 2 (version 2018.2) in your home directory via conda:</p>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"QIIME_2/#install-qiime-2","title":"Install QIIME 2","text":"<pre><code>export PATH=$PATH:$HOME/miniforge3/bin\nwget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\nconda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\nrm qiime2-2018.2-py35-linux-conda.yml\n\nsource activate qiime2-2018.2\nqiime --help # test if installation is successful\n# all your QIIME commands go here\nsource deactivate\n</code></pre>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"QIIME_2/#example-of-analysis","title":"Example of analysis","text":"<p>The example below is from a previous version of the current tutorial.</p> <pre><code>mkdir qiime2-moving-pictures-tutorial\ncd qiime2-moving-pictures-tutorial\nwget -O \"sample-metadata.tsv\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/sample_metadata.tsv\"\n\nmkdir emp-single-end-sequences\nwget -O \"emp-single-end-sequences/barcodes.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/barcodes.fastq.gz\"\nwget -O \"emp-single-end-sequences/sequences.fastq.gz\" \"https://data.qiime2.org/2018.2/tutorials/moving-pictures/emp-single-end-sequences/sequences.fastq.gz\"\n\nqiime tools import \\\n  --type EMPSingleEndSequences \\\n  --input-path emp-single-end-sequences \\\n  --output-path emp-single-end-sequences.qza\n\nqiime demux emp-single \\\n  --i-seqs emp-single-end-sequences.qza \\\n  --m-barcodes-file sample-metadata.tsv \\\n  --m-barcodes-column BarcodeSequence \\\n  --o-per-sample-sequences demux.qza\n\nqiime demux summarize \\\n  --i-data demux.qza \\\n  --o-visualization demux.qzv\n\nqiime tools view demux.qzv\n</code></pre> <p>A full list of tutorials for QIIME2 can be found here and there is also list of plugins for QIIME2 for handling tasks such as trimming adaptors, demultiplexing, and denoising.</p>","tags":["tutorial","QIIME 2","bioinformatics"]},{"location":"RCentOS7generalinfo/","title":"General information","text":"<p>Warning</p> <p>Since HPCC OS upgrade to Ubuntu in summer 2024, R modules have changed. See our general migration notes and R migration notes for updates. We will revamp R-related tutorials over time.</p>","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#a-quick-start","title":"A quick start","text":"<p>R 4.0.2 has the largest number of libraries installed and is the recommended one to use. To load it, you can run:</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4</code> <code>module load R/4.0.2</code></p> <p>Some users may have a local R package directory specified in <code>~/.Renviron</code>; this may create a problem if you load your local packages which were built with an older version of R. Unless you have updated them all, we recommend that you launch R by <code>R --no-environ</code> which suppresses the search of local packages.</p> <p>For other versions of R, run <code>module spider R</code> to get a complete list. </p>","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#be-sure-to-specify-version-number","title":"Be sure to specify version number","text":"<p>While it is valid to load R simply by <code>module load R</code>, it may point to a different R version than your desired one. Please be specific about the version, as with any software module.</p>","tags":["tutorial","R"]},{"location":"RCentOS7generalinfo/#rstudio","title":"Rstudio","text":"<p>The best way to launch an Rstudio session is to log into our OnDemand server, which is dedicated to running GUI applications. Here is a short video showing how to request an Rstudio \"job\" from the HPCC cluster after you've logged into OnDemand.</p>","tags":["tutorial","R"]},{"location":"RNA_seq_example/","title":"RNA-seq example","text":"<p>While RNA-seq analysis pipeline is changing, here is an example for demonstration purpose. The pipeline involves <code>tophat</code>, <code>cufflinks</code> and so on. We will provide updates to this page as needed.</p>"},{"location":"R_others/","title":"Some packages and other information","text":"","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#rstan","title":"rstan","text":"<p>The example here follows that in\u00a0RStan Getting Started.\u00a0To test rstan on the HPCC, first load R 3.6.2:</p> <pre><code>module purge  \nmodule load GCC/8.3.0 OpenMPI/3.1.4\u00a0R/3.6.2-X11-20180604\n</code></pre> <p>As of February 2020, the rstan version is 2.19.2.</p> <p>The stan model file \"8schools.stan\" contains:</p> <pre><code>data {\n  int&lt;lower=0&gt; J;         // number of schools \n  real y[J];              // estimated treatment effects\n  real&lt;lower=0&gt; sigma[J]; // standard error of effect estimates \n}\nparameters {\n  real mu;                // population treatment effect\n  real&lt;lower=0&gt; tau;      // standard deviation in treatment effects\n  vector[J] eta;          // unscaled deviation from mu by school\n}\ntransformed parameters {\n  vector[J] theta = mu + tau * eta;        // school treatment effects\n}\nmodel {\n  target += normal_lpdf(eta | 0, 1);       // prior log-density\n  target += normal_lpdf(y | theta, sigma); // log-likelihood\n}\n</code></pre> <p>The R code (\"run.R\") to run stan model contains:</p> <pre><code>library(\"rstan\")\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nschools_dat &lt;- list(J = 8,\ny = c(28, 8, -3, 7, -1, 1, 18, 12),\nsigma = c(15, 10, 16, 11, 9, 11, 10, 18))\nfit &lt;- stan(file = '8schools.stan', data = schools_dat)\nprint(fit)\npairs(fit, pars = c(\"mu\", \"tau\", \"lp__\"))\nla &lt;- extract(fit, permuted = TRUE) # return a list of arrays\nmu &lt;- la$mu\n</code></pre> <p>To run the model from the command line:</p> <p><code>Rscript run.R</code></p> <p>In addition to the results printed to the stdout, there is an R object file named 8schools.rds generated. This is due to that we've set auto_write to TRUE in run.R. More about the auto_write option:</p> <p>Logical, defaulting to the value of rstan_options(\"auto_write\"), indicating whether to write the object to the hard disk using saveRDS. Although this argument is FALSE by default, we recommend calling rstan_options(\"auto_write\" = TRUE) in order to avoid unnecessary recompilations. If file is supplied and its dirname is writable, then the object will be written to that same directory, substituting a .rds extension for the .stan extension. Otherwise, the object will be written to the tempdir.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#rjags","title":"rjags","text":"<p>To use {rjags}, first load R and JAGS\u00a0from a dev-node (e.g. dev-amd20) as follows:</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-and-jags","title":"Loading R and JAGS","text":"<pre><code>module purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\nmodule load JAGS/4.3.0\n</code></pre> <p>Next, we will run a short example of data analysis using rjags. This example comes from\u00a0this tutorial which presents many Bayesian models using this package.</p> <p>To invoke R from the command line: <code>R --vanilla</code></p> <p>Then, in the R console, you can run the following codes (for detailed explanation refer to the tutorial mentioned above):</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#sample-r-code-using-rjags-commands","title":"Sample R code using {rjags} commands","text":"<pre><code>library(rjags)\ndata &lt;- read.csv(\"data1.csv\")\nN &lt;- length(data$y)\ndat &lt;- list(\"N\" = N, \"y\" = data$y, \"V\" = data$V)\ninits &lt;- list( d = 0.0 )\n\njags.m &lt;- jags.model(file = \"aspirinFE.txt\", data=dat, inits=inits, n.chains=1, n.adapt=500)\nparams &lt;- c(\"d\", \"OR\")\nsamps &lt;- coda.samples(jags.m, params, n.iter=10000)\nsummary(window(samps, start=5001))\nplot(samps)\n</code></pre> <p>where the two input files, <code>data1.csv</code> and <code>aspirinFE.txt</code>, need to be located in the working directory. The content of the two files is below.</p> data1.csv<pre><code>N,y,V\n1,0.3289011,0.0388957\n2,0.3845458,0.0411673\n3,0.2195622,0.0204915\n4,0.2222206,0.0647646\n5,0.2254672,0.0351996\n6,-0.1246363,0.0096167\n7,0.1109658,0.0015062\n</code></pre> aspirinFE.txt<pre><code>model {\n\n    for ( i in 1:N ) {\n\n        P[i] &lt;- 1/V[i]\n        y[i] ~ dnorm(d, P[i])\n    }\n\n    ### Define the priors\n    d ~ dnorm(0, 0.00001)\n\n    ### Transform the ln(OR) to OR\n    OR &lt;- exp(d)\n}\n</code></pre> <p>A screen shot of the entire run including the output figures is attached here. </p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r2openbugs","title":"R2OpenBUGS","text":"<p>R package R2OpenBUGS is available on the HPCC.</p> <p>OpenBUGS is a software package that performs Bayesian inference Using Gibbs Sampling. The latest version of OpenBUGS on the HPCC is 3.2.3.\u00a0R2OpenBUGS is an R package that allows users to run OpenBUGS from R.</p> <p>To load R and OpenBUGS, run:</p> <pre><code>module purge  \nmodule load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2  \nmodule load OpenBUGS\n</code></pre> <p>Then, in your R session, use <code>library(R2OpenBUGS)</code> to load this package. You can execute the following testing R code to see if it works for you.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#testing-r2openbugs","title":"Testing R2OpenBUGS","text":"<pre><code>library(R2OpenBUGS)\n\n# Define the model \nBUGSmodel &lt;- function() {\n\n    for (j in 1:N) {\n        Y[j] ~ dnorm(mu[j], tau)\n        mu[j] &lt;- alpha + beta * (x[j] - xbar)\n    }\n\n    # Priors\n    alpha ~ dnorm(0, 0.001)\n    beta ~ dnorm(0, 0.001)\n    tau ~ dgamma(0.001, 0.001)\n    sigma &lt;- 1/sqrt(tau)\n}\n\n# Data\nData &lt;- list(Y = c(1, 3, 3, 3, 5), x = c(1, 2, 3, 4, 5), N = 5, xbar = 3)\n\n# Initial values for the MCMC\nInits &lt;- function() {\n    list(alpha = 1, beta = 1, tau = 1)\n}\n\n# Run BUGS \nout &lt;- bugs(data = Data, inits = Inits, parameters.to.save = c(\"alpha\", \"beta\", \"sigma\"), model.file = BUGSmodel, n.chains = 1, n.iter = 10000)\n\n# Examine the BUGS output\nout\n\n# Inference for Bugs model at \"/tmp/RtmphywZxh/model77555ea534aa.txt\",\n# Current: 1 chains, each with 10000 iterations (first 5000 discarded)\n# Cumulative: n.sims = 5000 iterations saved\n#          mean  sd 2.5%  25%  50%  75% 97.5%\n# alpha     3.0 0.6  1.9  2.7  3.0  3.3   4.1\n# beta      0.8 0.4  0.1  0.6  0.8  1.0   1.6\n# sigma     1.0 0.7  0.4  0.6  0.8  1.2   2.8\n# deviance 13.0 3.7  8.8 10.2 12.0 14.6  22.9\n#\n# DIC info (using the rule, pD = Dbar-Dhat)\n# pD = 3.9 and DIC = 16.9\n# DIC is an estimate of expected predictive error (lower deviance is better).\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r-interface-to-tensorflowkeras","title":"R interface to TensorFlow/Keras","text":"<p>After you've installed TF in your conda environment, load R 4.1.2 and set up a few environment variables:</p> <pre><code>module purge; module load GCC/11.2.0  OpenMPI/4.1.1 R/4.1.2\nexport CONDA_PREFIX=/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/:$CONDA_PREFIX/lib/python3.9/site-packages/tensorrt:$LD_LIBRARY_PATH\nexport XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib # to fix \"Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice.\"\n</code></pre> <p>Above, you need to change <code>CONDA_PREFIX</code> to your own conda env path (that is, the directory where you have installed Anaconda).</p> <p>Then, run the following R code to test if R/TF works as expected.</p> <pre><code>library(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n\nuse_python(\"/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023/bin\")\nuse_condaenv(\"/mnt/home/user123/anaconda3/envs/tf_gpu_Feb2023\")\n\n# simple test\ntf$config$list_physical_devices(\"GPU\")\n\n# model training\n\n# loading the keras inbuilt mnist dataset\ndata&lt;-dataset_mnist()\n\n#separating train and test file\ntrain_x&lt;-data$train$x\ntrain_y&lt;-data$train$y\ntest_x&lt;-data$test$x\ntest_y&lt;-data$test$y\n\nrm(data)\n\n# converting a 2D array into a 1D array for feeding into the MLP and normalising the matrix\ntrain_x &lt;- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255\ntest_x &lt;- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255\n\n# converting the target variable to once hot encoded vectors using keras inbuilt function\ntrain_y&lt;-to_categorical(train_y,10)\ntest_y&lt;-to_categorical(test_y,10)\n\n# defining a keras sequential model\nmodel &lt;- keras_model_sequential()\n\n# defining the model with 1 input layer[784 neurons], 1 hidden layer[784 neurons] with dropout rate 0.4 and 1 output layer[10 neurons]\n# i.e number of digits from 0 to 9\n\nmodel %&gt;% \nlayer_dense(units = 784, input_shape = 784) %&gt;% \nlayer_dropout(rate=0.4)%&gt;%\nlayer_activation(activation = 'relu') %&gt;% \nlayer_dense(units = 10) %&gt;% \nlayer_activation(activation = 'softmax')\n\n# compiling the defined model with metric = accuracy and optimiser as adam.\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n\n# fitting the model on the training dataset\nmodel %&gt;% fit(train_x, train_y, epochs = 20, batch_size = 128)\n\n# evaluating model on the cross validation dataset\nloss_and_metrics &lt;- model %&gt;% evaluate(test_x, test_y, batch_size = 128)\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#r-351-with-intel-mkl","title":"R 3.5.1 with Intel MKL","text":"<p>Intel MKL can accelerate R's speed in linear algebra calculations (such as cross-product, matrix decomposition, inverse computation, linear regression and etc.) by providing BLAS with higher performance.  On the HPCC, only 3.5.1 has been built by linking to Intel MKL.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r","title":"Loading R","text":"","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-351-built-w-openblas","title":"Loading R 3.5.1 built w/ OpenBLAS","text":"<pre><code>module purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#loading-r-351-built-w-mkl","title":"Loading R 3.5.1 built w/ MKL","text":"<pre><code>module purge\nmodule load R-Core/3.5.1-intel-mkl\n</code></pre> <p>You could double check the BLAS/LAPACK libraries linked by running <code>sessionInfo()</code> in R.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking","title":"Benchmarking","text":"<p>We have a simple R code,\u00a0<code>crossprod.R</code>,\u00a0for testing the computation time.\u00a0The code is below, where the function <code>crossprod</code>\u00a0can run in a multi-threaded mode implemented by OpenMP.</p> crossprod.R<pre><code>set.seed(1)\nm &lt;- 5000\nn &lt;- 20000\nA &lt;- matrix(runif (m*n),m,n)\nsystem.time(B &lt;- crossprod(A))\n</code></pre> <p>Now, open an interactive SLURM job session by requesting 4 cores:</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking-openblas-vs-mkl-multi-threads","title":"Benchmarking OpenBLAS vs. MKL (multi-threads)","text":"<pre><code>salloc --time=2:00:00 --cpus-per-task=4 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 50.036   1.574   14.156\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 28.484   1.664   8.737\n</code></pre> <p>Above, the boost in speed is clear from using MKL as compared with OpenBLAS.</p> <p>Even if we use a single thread (by requesting one core), MKL still shows some advantage.</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#benchmarking-openblas-vs-mkl-single-thread","title":"Benchmarking OpenBLAS vs. MKL (single-thread)","text":"<pre><code>salloc --time=2:00:00 --cpus-per-task=1 --mem=50G --constraint=intel18\n\n# After you are allocated a compute node, do the following.\n\n# 1) Run R/OpenBLAS\nmodule purge\nmodule load GCC/7.3.0-2.30 OpenMPI/3.1.1 R/3.5.1-X11-20180604\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 47.763   0.598   49.287\n\n\n# 2) Run R/MKL\nmodule purge\nmodule load R-Core/3.5.1-intel-mkl\nRscript --vanilla crossprod.R &amp;\n# Time output:\n# user     system  elapsed \n# 25.846   0.641   27.006\n</code></pre>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_others/#notes","title":"Notes","text":"<p>When loading R, the OpenMP environment variable\u00a0<code>OMP_NUM_THREADS</code> is left unset.\u00a0This means that when running R code directly on a dev-node, all CPUs on that node will be used by the internal multithreading library compiled into R. This is discouraged since the node will be overloaded and your job may even fail. Therefore, please set <code>OMP_NUM_THREADS</code> to a proper value before running the R code. For example,</p> <p><code>$ OMP_NUM_THREADS=4</code></p> <p><code>$ Rscript --vanilla crossprod.R</code></p> <p>On the other hand, when the code is run on a compute node allocated by SLURM, you don\u2019t need to set OMP_NUM_THREADS as R would automatically detect CPUs available for use (which should have been requested in your salloc command or sbatch script).</p>","tags":["tutorial","R","JAGS","OpenBUGS","TensorFlow"]},{"location":"R_workshop_tutorial/","title":"R workshop tutorial","text":""},{"location":"R_workshop_tutorial/#preparation","title":"Preparation","text":"<ol> <li>Basic knowledge of R\u00a0language, Linux, and the HPCC environment.</li> <li>Login<ol> <li><code>ssh -XY YourAccount@hpcc.msu.edu</code></li> <li><code>ssh\u00a0dev-amd20</code></li> </ol> </li> <li>We will be using R 4.0.2. To load     it:\u00a0<code>module purge; module load\u00a0GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2</code></li> <li>Copy files that we are using in this workshop to your home     directory: <code>cp -r /mnt/research/common-data/workshops/R-workshop .</code></li> </ol>"},{"location":"R_workshop_tutorial/#r-startup","title":"R startup","text":"<p>When an R session starts, it looks for and loads two hidden configuration files, <code>.Renviron</code> and <code>.Rprofile</code>.</p> <ul> <li><code>.Renviron</code>: contains environment variables to be set in R sessions</li> <li><code>.Rprofile</code>: contains R commands to be run in R sessions</li> </ul> <p>The following search order is applied: your current working directory, your home directory, and the system-wide <code>R_HOME/etc/</code> (you can use R command <code>R.home()</code> to check path of <code>R_HOME</code>). Below are examples of the two files which have been placed in our R workshop directory. You need to use <code>ls -a</code> to list them since they are \"hidden\" files.</p> <p><code>.Rprofile</code> (an example)</p> <pre><code>cat(\"Sourcing .Rprofile from the R-workshop directory.\\n\")\n\n# To avoid setting the CRAN mirror each time you run install.packages\nlocal({\n  options(repos = \"https://repo.miserver.it.umich.edu/cran/\")\n})\n\noptions(width=100) # max number of columns when printing vectors/matrices/arrays\n\n.First &lt;- function() cat(\"##### R session begins for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"), \"You are currently in\", getwd(), \"#####\\n\\n\")\n.Last &lt;- function()  cat(\"\\n##### R session ends for\", paste(Sys.getenv(\"USER\"),\".\", sep=\"\"),  \"You are currently in\", getwd(), \"#####\\n\\n\")\n</code></pre> <p><code>.Renviron</code> (an example)</p> <pre><code>R_DEFAULT_PACKAGES=\"datasets,utils,grDevices,graphics,stats,methods\"\nR_LIBS_USER=/mnt/home/longnany/Rlibs\n</code></pre> <p>Let's run a short Rscript command to see what we get:</p> <pre><code>$ Rscript -e 'date()'\n</code></pre> <p>Notes:</p> <ul> <li>Personalizing these two files can reduce code portability.</li> <li> <p>If you don't want R or Rscript to read any <code>.Renviron</code> or <code>.Rprofile</code>     files when starting an R session, use option <code>--vanilla</code>.  </p> <p>A caveat:\u00a0if you explicitly export an R environment variable, such as\u00a0 <code>export R_LIBS_USER=~/Rlibs</code>, then adding\u00a0<code>--vanilla</code>\u00a0will not ignore its value. See below the result.</p> <pre><code>$ Rscript --vanilla -e '.libPaths()' # .libPaths() is used to see the directories where R searches for libraries\n[1] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n$ export R_LIBS_USER=~/Rlibs\n$ Rscript --vanilla -e '.libPaths()'\n[1] \"/mnt/home/longnany/Rlibs\"\n[2] \"/opt/software/R/4.0.2-foss-2019b/lib64/R/library\"\n</code></pre> </li> </ul>"},{"location":"R_workshop_tutorial/#rscript-one-liner","title":"Rscript one-liner","text":"<p>The general format of <code>Rscript</code>:</p> <pre><code>Rscript [options] [-e expression1 -e expression2 ... | source file] [args]\n</code></pre> <ul> <li> <p><code>options</code>: can be multiple; all beginning with <code>--</code>     (e.g.,\u00a0<code>--vanilla</code> as mentioned above). To learn about all the     options, run <code>Rscript --help</code> on the command line.</p> </li> <li> <p><code>expression1, expression2 ...</code>: can be one or multiple. They are R     commands.</p> </li> <li> <p><code>source file</code>: R source code.</p> </li> <li> <p><code>args</code>: arguments to be passed.</p> </li> </ul> <p>You may have both expressions and source file present in your Rscript line. Here are a few one-liner examples:</p> <p>Rscript one-liner examples</p> <pre><code># Ex1: simple loop\n$ Rscript -e 'for (i in 1:5) print(paste(\"g\", i))'\n\n\n# Ex2: print time\n$ Rscript -e 'date()'\n\n\n# Ex3: quick math (calculating quotient and remainder)\n$ Rscript -e '128 %/% 11' -e '128 %% 11'\n\n\n# Ex4: get help for command \"paste\"\n$ Rscript -e 'help(paste)'\n\n\n# Ex5: used in conjunction with pipe.\n#   Generate three sets of random Normal variables with different means (sd all being one); means are given in file test.dat.\n$ cat &gt; test.dat # ctrl+D to go back when done typing\n1\n10\n20\n$ cat test.dat | Rscript -e 'input_con  &lt;- file(\"stdin\"); open(input_con); while (length(oneLine &lt;- readLines(con = input_con, n = 1, warn = FALSE)) &gt; 0) {print(rnorm(5,mean=as.numeric(oneLine)))};close(input_con)'\n</code></pre>"},{"location":"R_workshop_tutorial/#using-rscript-with-source-code","title":"Using Rscript with source code","text":"<p>a. simple usage:</p> <p>Instead of using <code>'-e your_commands</code>', we now put R commands in a source file and run it with <code>Rscript</code>. Below is an R script file and we can run <code>Rscript multiplots.R</code> to get a PDF file <code>multiplots.pdf</code>. To view it, run <code>evince multiplots.pdf</code>. Or, you can go to our OnDemand File Browser to open the file in your web browser.</p> <p><code>multiplots.R</code>: a very simple example of making 4 plots on the same page</p> <pre><code>pdf(\"multiplots.pdf\")\npar(mfrow=c(2,2))\nfor (i in 1:4) plot(1:10, 1:10, type=\"b\", xlab=bquote(X[.(i)]), ylab=bquote(Y[.(i)]), main=bquote(\"Multiple plots: #\" ~ .(i)))\ndev.off()\n</code></pre> <p>b. passing command line arguments:</p> <p>We can also pass arguments to our R script, as shown in the example below.</p> <p><code>args-1.R</code></p> <pre><code>args &lt;- commandArgs(trailingOnly = TRUE)\n\nnargs &lt;- length(args)\nfor (i in 1:nargs) {\n    cat(\"Arg\", i, \":\", args[i], \"\\n\")\n}\n\n\ncat(\"Generating\",as.numeric(args[nargs-1]), \"normal variables with mean =\", as.numeric(args[nargs]), \"and sd = 1 \\n\")\nrnorm(as.numeric(args[nargs-1]), mean=as.numeric(args[nargs]))\n</code></pre> <p>Running script <code>args-1.R</code>:</p> <pre><code>$ Rscript args-1.R 5 3\nSourcing .Rprofile from the 11092017-R-Workshop directory.\n##### R session begins for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n\nArg 1 : 5\nArg 2 : 3\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 2.707162 3.677923 3.192272 2.531973 3.699060\n\n##### R session ends for longnany. You are currently in /mnt/home/longnany/Documents/11092017-R-Workshop #####\n</code></pre> <p>c. processing command line arguments with {<code>getopt</code>}:</p> <p><code>args-2.R</code></p> <pre><code>require(\"getopt\", quietly=TRUE)\n\nspec = matrix(c(\n    \"Number\", \"n\", 1, \"integer\",\n    \"Mean\", \"m\", 1, \"double\"\n), byrow=TRUE, ncol=4) # cf. https://cran.r-project.org/web/packages/getopt/getopt.pdf\n\nopt = getopt(spec);\n\nif (is.null(opt$Number)) {\n    n &lt;- 5\n} else {\n    n &lt;- opt$Number\n}\n\nif (is.null(opt$Mean)) {\n    m &lt;- 3\n} else {\n    m &lt;- opt$Mean\n} \n\ncat(\"Generating\", n, \"normal variables with mean =\", m, \"and sd = 1 \\n\")\nrnorm(n=n, mean=m)\n</code></pre> <p>Running the script <code>args-2.R</code>:</p> <pre><code># Use long flag names\n$ Rscript --vanilla args-2.R --Number 10 --Mean -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -0.4776278 -1.7759145 -0.9977682 -2.6452126 -3.4050587 -2.2358362\n [7] -1.2696362 -1.6213633 -2.7013074 -1.9271954\n\n# Use short flag names\n$ Rscript --vanilla args-2.R -n 10 -m -2\nGenerating 10 normal variables with mean = -2 and sd = 1\n [1] -2.2241837 -1.6704711  0.1481244  0.2072124 -1.0385386 -1.5194874\n [7] -2.6744478 -2.4683039 -0.7962113 -1.1901021\n\n# No arguments provided so defaults are used\n$ Rscript --vanilla args-2.R\nGenerating 5 normal variables with mean = 3 and sd = 1\n[1] 3.951492 4.255879 4.485044 2.727223 3.039532\n</code></pre>"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-doparallel-single-node-multiple-cores","title":"Submitting parallel jobs to the cluster using <code>doParallel</code>: single node, multiple cores","text":"<p>To submit a single-node job, we recommend the package <code>doParallel</code>.  The <code>doParallel</code> package is a \"parallel backend\" for the <code>foreach</code> package, making it possible to execute foreach loops in parallel.</p> <p>Note</p> <p><code>doParallel</code> supports two functionalities: multicore and snow. The most important difference between them is that multicore can only run tasks on a single node (computer) whereas snow can execute tasks on different nodes in a cluster. This leads to different commands for registering the parallel backend. In our example here, we are interested in using the multicore-like parallelism, since we are trying to use the many cores on a single compute node in the cluster. To learn more about their differences, you can refer to this discussion.</p> <p>Example R code <code>R_doParallel_singlenode.R</code>: run 10,000 bootstrap iterations of fitting a logistic regression model</p> <pre><code>library(doParallel)\n\n# Registering a parallel backend, using the \"multicore\" functionality\nregisterDoParallel(cores=as.numeric(Sys.getenv(\"SLURM_CPUS_ON_NODE\")[1]))\n\nx &lt;- iris[which(iris[,5] != \"setosa\"), c(1,5)]\ntrials &lt;- 10000\nptime &lt;- system.time({\n    r &lt;- foreach(icount(trials), .combine=cbind) %dopar% {\n        ind &lt;- sample(100, 100, replace=TRUE)\n        result1 &lt;- glm(x[ind,2]~x[ind,1], family=binomial(logit))\n        coefficients(result1)\n    }\n})[3]\n\ncat(\"Time elapsed:\", ptime, \"\\n\")\ncat(\"Currently registered backend:\", getDoParName(), \"\\n\")\ncat(\"Number of workers used:\", getDoParWorkers(), \"\\n\")\nprint(str(r)) # column-binded result\n</code></pre> <p>Now, submit the job to the HPCC through the following SLURM script <code>submit-doParallel.sbatch</code>:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=doParallel_test\n#\n# Number of nodes needed:\n#SBATCH --nodes=1\n#\n# Tasks per node:\n#SBATCH --ntasks-per-node=1\n#\n# Processors per task:\n#SBATCH --cpus-per-task=4\n#\n# Memory per node:\n#SBATCH --mem=500M\n#\n# Wall time (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=3:00:00\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\nRscript --vanilla R_doParallel_singlenode.R &gt; R_doParallel_singlenode.Rout\n</code></pre> <p>Submission command: <code>sbatch --constraint=\"[intel16|intel18|amd20|amd22]\" submit-doParallel.sbatch</code></p> <p>The output file <code>R_doParallel_singlenode.Rout</code> \u00a0should look like:</p> <pre><code>Time elapsed: 8.946\nCurrently registered backend: doParallelMC\nNumber of workers used: 4\n num [1:2, 1:10000] -14.6 2.26 -11.86 1.91 -7.75 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"(Intercept)\" \"x[ind, 1]\"\n  ..$ : chr [1:10000] \"result.1\" \"result.2\" \"result.3\" \"result.4\" ...\nNULL\n</code></pre>"},{"location":"R_workshop_tutorial/#submitting-parallel-jobs-to-the-cluster-using-pbdmpi-multiple-nodes","title":"Submitting parallel jobs to the cluster using\u00a0<code>pbdMPI</code>: multiple nodes","text":"<p>MPI stands for Message Passing Interface, and is the standard for managing multi-node communication. The R package <code>Rmpi</code> is an implementation of it for R. Because of its complicated usage, here we will use another R package, <code>pbdMPI</code>,  which greatly simplifies the use of MPI from R.</p> <p><code>pbdMPI</code> is a more recent R MPI package developed under the pbdR project, which simplifies MPI interaction and thus reduces the traffics of node-to-node communication. It works in Single Program/Multiple Data (SPMD) mode, which is an important distinction as compared with <code>Rmpi</code>.</p> <p>As an illustration, we consider the problem of computing the log likelihood of data following a 2-dimensional Multi-Variate Normal (MVN) distribution. The example below applies Cholesky decomposition on the 2-by-2 covariance matrix (code line 25 below), solves a system of linear equations (line 30), followed by some matrix/vector operation (line 31). Line 30 and 31 are where MPI plays a vital role via distributed computing.</p> <p>Below is a graphic illustration of solving a system of linear equations by part so as to be able to distribute the task. To learn more about the underlying mechanism, you can take a look at this note.</p> <p></p> <p><code>MVN.R</code>: MPI in SPMD mode</p> <pre><code># Load pbdMPI and initialize the communicator\nlibrary(pbdMPI, quiet = TRUE)\ninit()\n\n# Check processes\ncomm.cat(\"All processes start...\\n\\n\")\nmsg &lt;- sprintf(\"I am rank %d on host %s of %d processes\\n\", comm.rank(), Sys.info()[\"nodename\"], comm.size())\ncomm.cat(msg, all.rank=TRUE, quiet=TRUE) # quiet=T tells each rank not to \"announce\" itself when it's printing\n\n\nset.seed(1234)\nN &lt;- 100\np &lt;- 2\nX &lt;- matrix(rnorm(N * p), ncol = p)\nmu &lt;- c(0.1, 0.2)\nSigma &lt;- matrix(c(0.9, 0.1, 0.1, 0.9), ncol = p)\n\n# Load data partially by processors\nid.get &lt;- get.jid(N)\nX.spmd &lt;- matrix(X[id.get, ], ncol = p)\ncomm.cat(\"\\nPrint out the matrix on each process/rank:\\n\\n\", quiet=TRUE)\ncomm.print(X.spmd, all.rank=TRUE, quiet=TRUE)\n\n# Cholesky decomposition\nU &lt;- chol(Sigma) # U'U = Sigma\nlogdet &lt;- sum(log(abs(diag(U))))\n\n# Call R's backsolve function for each chunk of the data matrix X (i.e. B.spmd)\nB.spmd &lt;- t(X.spmd) - mu\nA.spmd &lt;- backsolve(U, B.spmd, upper.tri = TRUE, transpose = TRUE) # U'A = B\ndistval.spmd &lt;- colSums(A.spmd * A.spmd)\n\n# Use sum as the reduction operation\nsum.distval &lt;- allreduce(sum(distval.spmd), op = \"sum\")\ntotal.logL &lt;- -0.5 * (N * (p * log(2 * pi) + logdet * 2) + sum.distval)\n\n# Output\ncomm.cat(\"\\nFinal log-likelihood:\\n\\n\", quiet=TRUE)\ncomm.print(total.logL, quiet=TRUE)\n\nfinalize()\n</code></pre> <p>SLURM submission script: <code>submit-pbdMPI.sbatch</code>:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=pbdMPI_test\n#\n# Number of MPI tasks:\n#SBATCH --ntasks=20\n#\n# Processors per task:\n#SBATCH --cpus-per-task=1\n#\n# Memory:\n#SBATCH --mem-per-cpu=800M\n#\n# Wall clock limit (e.g. \"minutes\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\"):\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\necho \"SLURM_NTASKS: $SLURM_NTASKS\"\n\nmodule purge\nmodule load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n\n# Suppress warnings about forks and missing CUDA libraries\nexport OMPI_MCA_mpi_warn_on_fork=0\nexport OMPI_MCA_mpi_cuda_support=0\n\nmpirun -n $SLURM_NTASKS Rscript --vanilla MVN.R &gt; MVN.Rout\n</code></pre> <p>Now, submit the job by\u00a0<code>sbatch --constraint=\"[intel16|intel18|amd20|amd22]\" submit-pbdMPI.sbatch</code>. When finished, <code>MVN.Rout</code> should contain the following information:</p> <pre><code>COMM.RANK = 0\nAll processes start...\n\nI am rank 0 on host lac-419 of 20 processes\nI am rank 1 on host lac-420 of 20 processes\n... ...\nI am rank 18 on host lac-421 of 20 processes\nI am rank 19 on host lac-421 of 20 processes\n\nPrint out the matrix on each process/rank:\n\n           [,1]       [,2]\n[1,] -0.1850210 -0.2771503\n[2,] -0.8596753 -0.1081122\n[3,] -1.0927853  0.8690750\n[4,] -0.5831948  0.8032846\n[5,]  1.0796870  0.2354514\n            [,1]       [,2]\n[1,]  0.05274001 -0.8410549\n[2,] -0.92568393  0.9461704\n[3,]  1.01632804  0.6875080\n[4,]  0.34271061  0.4905065\n[5,]  0.84192956 -1.6685933\n... ...\n            [,1]       [,2]\n[1,] -0.55673659 -0.1229023\n[2,] -0.03156318 -0.8501178\n[3,] -1.28832627 -1.3115801\n[4,] -0.47546826 -0.4856559\n[5,]  0.81134183  0.7499220\n            [,1]       [,2]\n[1,] -0.95620195  0.6560605\n[2,]  0.04671396  0.6093924\n[3,]  0.18986742  0.2077641\n[4,]  0.73281327 -1.0452661\n[5,]  2.27968859  1.0611428\n\nFinal log-likelihood:\n\n[1] -283.2159\n</code></pre> <p>You can download more R pbdMPI examples from here.</p>"},{"location":"Rclone_-_rsync_for_cloud_storage/","title":"Rclone - rsync for cloud storage","text":"<p>Rclone can be used to copy files from/to their Microsoft OneDrive or Google Drive cloud storage to/from HPCC disk space. This tool can also be used to mount a user's cloud storage to their HPCC disk so that the storage on cloud could be used as extended disk space.</p> <p>Rclone is installed on HPCC system wide. To use it, users should first\u00a0 load the software module into their environment using command: </p> <pre><code>module load Rclone\u00a0\n</code></pre> <p>For more details of using rclone, users can visit Rclone web site at\u00a0https://rclone.org/.</p> <p>To start using Rclone, users need to run the following command to configure it:</p> <pre><code>rclone config\n</code></pre> <p>The instructions for this command could be found at https://rclone.org/commands/rclone_config/.</p> <p>Specifically, to configure for Google Drive, see\u00a0https://rclone.org/drive/, and to configure for Microsoft Onedrive, see\u00a0https://rclone.org/onedrive/ for instructions. The specific details of how to start using this software on HPCC could be found in the document\u00a0Rclone.pdf</p> <p>After successfully configuring the software, users should be able to use \"rclone\" command to copy or mount the cloud storage to HPCC. There are many rclone sub-commands that can be used to handle file transfers and manage files on HPCC and cloud storage. To get help, use \"rclone --help\" as shown below:</p> <pre><code>[hpc@dev-intel16-k80 ~]$ module load Rclone\n[hpc@dev-intel16-k80 ~]$ rclone --help\n\nRclone syncs files to and from cloud storage providers as well as\nmounting them, listing them in lots of different ways.\n\nSee the home page (https://rclone.org/) for installation, usage,\ndocumentation, changelog and configuration walkthroughs.\n\nUsage:\n  rclone [flags]\n  rclone [command]\n\nAvailable Commands:\n  about           Get quota information from the remote.\n  authorize       Remote authorization.\n  cachestats      Print cache stats for a remote\n  cat             Concatenates any files and sends them to stdout.\n  check           Checks the files in the source and destination match.\n  cleanup         Clean up the remote if possible\n  config          Enter an interactive configuration session.\n  copy            Copy files from source to dest, skipping already copied\n  copyto          Copy files from source to dest, skipping already copied\n  copyurl         Copy url content to dest.\n  cryptcheck      Cryptcheck checks the integrity of a crypted remote.\n  cryptdecode     Cryptdecode returns unencrypted file names.\n  dbhashsum       Produces a Dropbox hash file for all the objects in the path.\n  dedupe          Interactively find duplicate files and delete/rename them.\n  delete          Remove the contents of path.\n  deletefile      Remove a single file from remote.\n  genautocomplete Output completion script for a given shell.\n  gendocs         Output markdown docs for rclone to the directory supplied.\n  hashsum         Produces an hashsum file for all the objects in the path.\n  help            Show help for rclone commands, flags and backends.\n  link            Generate public link to file/folder.\n  listremotes     List all the remotes in the config file.\n  ls              List the objects in the path with size and path.\n  lsd             List all directories/containers/buckets in the path.\n  lsf             List directories and objects in remote:path formatted for parsing\n  lsjson          List directories and objects in the path in JSON format.\n  lsl             List the objects in path with modification time, size and path.\n  md5sum          Produces an md5sum file for all the objects in the path.\n  mkdir           Make the path if it does not already exist.\n  mount           Mount the remote as file system on a mountpoint.\n  move            Move files from source to dest.\n  moveto          Move file or directory from source to dest.\n  ncdu            Explore a remote with a text based user interface.\n  obscure         Obscure password for use in the rclone.conf\n  purge           Remove the path and all of its contents.\n  rc              Run a command against a running rclone.\n  rcat            Copies standard input to file on remote.\n  rcd             Run rclone listening to remote control commands only.\n  rmdir           Remove the path if empty.\n  rmdirs          Remove empty directories under the path.\n  serve           Serve a remote over a protocol.\n  settier         Changes storage class/tier of objects in remote.\n  sha1sum         Produces an sha1sum file for all the objects in the path.\n  size            Prints the total size and number of objects in remote:path.\n  sync            Make source and dest identical, modifying destination only.\n  touch           Create new file or change file modification time.\n  tree            List the contents of the remote in a tree like fashion.\n  version         Show the version number.\n\nUse \"rclone [command] --help\" for more information about a command.\nUse \"rclone help flags\" for to see the global flags.\nUse \"rclone help backends\" for a list of supported services.\n[hpc@dev-intel16-k80 ~]$ \n</code></pre> <p>The tool \"cloudSync\" was developed to help user to synchronize the files between their cloud storages. It is accessible through \"powertools\" which should automatically loaded upon logging into HPCC, but can be manually loaded with 'ml load powertools' if need be. Users are welcome to try it  and report any problems to us via contact form\u00a0here.</p> <p>Following are a few examples of running rclone commands after successfully having configured the cloud storage. Assume that the cloud storage is configured as\u00a0 the name \"MyOneDrive\".\u00a0</p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#1-see-current-remote-storage","title":"(1) See current remote storage","text":"<p>We can check the current configuration of rclone using 'rclone config'. As is shown below, we can see that there are currently two remote cloud storage configured:\u00a0  \"MyOneDrive\" and \"googledoc\"\u00a0</p> <pre><code>[user@dev-intel18 ~]$ rclone config\nCurrent remotes:\n\nName                 Type\n====                 ====\nMyOneDrive           onedrive\ngoogledoc            drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n\n[user@dev-intel18 ~]$\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#2-check-the-remote-storage-information","title":"(2) Check the remote storage information","text":"<p>We can see the remote storage usage and quota using \"rclone about\" command.</p> <pre><code>[user@dev-intel16-k80 ~]$ rclone about MyOneDrive:\nTotal:   5T\nUsed:    450.999M\nFree:    4.998T\nTrashed: 404.576k\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#3-list-the-contents-of-the-cloud-storage","title":"(3) List the contents of the cloud storage","text":"<pre><code>[user@dev-intel18 ~]$ rclone lsd MyOneDrive:\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMAGES\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#4-copy-files-on-hpcc-to-remote-cloud","title":"(4) Copy files on HPCC to remote cloud:","text":"<pre><code>[user@dev-intel18 ~]$ rclone copy Project MyOneDrive:Project   # copy the content of directory \"Project\" to remote cloud storage\n\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:               # view the contents of cloud storage to confirm the copy\n          -1 2018-02-02 08:57:54         0 Attachments\n          -1 2019-08-27 15:43:33         1 IMPACT\n          -1 2019-08-22 15:50:10        42 Matlab\n          -1 2019-02-26 17:12:01        16 Microsoft Teams Chat Files\n          -1 2018-08-24 08:56:32         1 Notebooks\n          -1 2020-04-27 15:43:25         2 Project\n[user@dev-intel18 ~]$ rclone lsd MyOneDrive:Project\n          -1 2020-04-27 15:44:39         1 GPAW\n          -1 2020-04-27 15:43:26         3 MATLAB\n</code></pre>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#5-copy-files-on-cloud-storage-to-hpcc","title":"(5) Copy files on cloud storage to HPCC:","text":"<pre><code>[user@dev-intel18 Project]$ ls                                 # current content of Project directory before copy\nGPAW  MATLAB\n[user@dev-intel18 Project]$ rclone copy MyOneDrive:IMPACT ./   # copy the content of IMPACT in cloud to current directory         \n[user@dev-intel18 Project]$ ls                                 # confirm that the copy is done\nGPAW  impact_run  MATLAB\n</code></pre> <p>Note</p> <p>Although \"rclone copy\" is similar as unix commands rsync and cp, when using it, users should be aware of the differences and know the details of its behavior.\u00a0</p> <p>(1) \"rclone copy\" does not\u00a0transfer unchanged files, testing by size and modification time or MD5SUM. In this sense, it is similar as linux command rsync;</p> <p>(2) When running \"<code>rclone copy source:sourcepath dest:destpath</code>\", if\u00a0<code>source:sourcepath</code>\u00a0is a directory, <code>dest:destpath</code> should also be a directory. \u00a0It does not copy the directory <code>source:sourcepath</code>, instead, it will copy the content of the directory <code>source:sourcepath</code>\u00a0to the destination <code>dest:destpath</code>. If\u00a0<code>dest:destpath</code>\u00a0does not exist, it will be created and the content of <code>source:sourcepath</code>\u00a0will be stored in it.</p> <p>(3) \"rclone copyto\" is a very similar rclone command to \"rclone copy\". The only difference is that it can\u00a0be used to upload single files to files other than their current name. When running \"<code>rclone copyto source:sourcepath dest:destpath</code>\", if\u00a0<code>source:sourcepath</code>\u00a0is a file, <code>dest:destpath</code> could be a new file name. If\u00a0<code>source:sourcepath</code>\u00a0is a directory, it would be the same as using \"<code>rclone copy\".</code></p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Rclone_-_rsync_for_cloud_storage/#6-checks-the-files-in-the-source-and-destination-match","title":"(6)\u00a0Checks the files in the source and destination match.","text":"<pre><code>[user@dev-intel18 Project]$ rclone check impact_run MyOneDrive:IMPACT/impact_run   # check if it is matched both sides\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 0 differences found\n2020/04/27 16:19:01 NOTICE: One drive root 'IMPACT/impact_run': 21 matching files\n</code></pre> <p>Note</p> <p>For archiving your files to your cloud storage, if the connection between HPCC and your cloud storage is not stable, we would NOT recommend using \"rclone move\" because it may loss the data during the transfer. Instead, we recommend using \"rclone copy\" to successfully copy the files over and run \"rclone check\" to check if files are identical. After that, it is safe to delete local copy of the files.</p> <p>Note</p> <p>When using \"rclone mount\" command to mount your cloud storage to HPCC, there are two things users should be careful:</p> <p>(1) When running rclone mount, the process runs NOT as the user, instead, it runs as a \"root\" of the cloud storage. Therefore, user may see the error message like \"mount helper error: fusermount: failed to open mountpoint for reading: Permission denied\". User could use /tmp space for mount point because that space is accessible for all users. Users should be very careful to open the permission to others for the purpose of using rclone mount.\u00a0</p> <p>(2) The \"rclone mount\" users should unmount it after use using \"fusermount -u \\&lt;endpoint_dir&gt;\". Note that sometimes the endpoint is not unmounted from some nodes due to timeout or some reason, you may see the message like \"Transport endpoint is not connected\" when accessing the endpoint directory on the node. Just manually unmount it again should resolve the issue.\u00a0</p> <p>Note</p> <p>When using \"rclone config\" command to configure your cloud storage on HPCC, the command will\u00a0guide you through an interactive setup process. At the step of auto config, after you chose \"y\", it will start authentication. You will see something like:</p> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\n\nLog in and authorize rclone for access\nWaiting for code...\n</code></pre> <p>At this time, a Firefox browser should be opened. If you did not get the browser window, check if you used -X option to allow X11 forwarding when you run ssh. \u00a0You may follow the instructions at Connect to HPCC System\u00a0to get the display right.</p> <p>It will take a few minutes to get the browser open and connected. Please be patient. If the browser window is open but does not open the authentication page, you could manually input the link provided by the \"rclone config\" command to the firefox browser's url address box to connect to the site. DO NOT use the link on your personal computer's browser. The authentication have to use the browser on HPCC development node. \u00a0</p>","tags":["how-to guide","rsync","cloud","rclone"]},{"location":"Regular_Expressions/","title":"Regular Expressions","text":"<p>A regular expression is a powerful tool to match patterns. With this tool, you can validate text input, search/replace text within a file, batch rename files, test for patterns within strings etc.</p> <p>There are two types of regular expressions: the basic regular expressions (BRE), and the extended regular expressions (ERE). Most utilities (including <code>vi</code>, <code>sed</code>, and <code>grep</code>)\u00a0use the basic regular expression. <code>awk</code> and <code>egrep</code> use the extended expression.</p> <p>There are three parts to a regular expression: anchors, character sets, and modifiers. Anchors\u00a0are used to specify the position of the pattern in relation to a line of text. Character sets match one or more characters in a single position. Modifiers specify how many times the previous character set is repeated.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#anchor-characters-and","title":"Anchor characters <code>^</code> and <code>$</code>","text":"<p>The character <code>^</code> is the starting anchor, and the character <code>$</code> is the ending anchor. The regular expression <code>^A</code> will match all lines that start with a capital A. The expression <code>A$</code> will match all lines that end with the capital A.</p> <p>The anchor characters works only if they are located in a proper location. Otherwise, \u00a0they no longer act as anchors. For example, <code>^</code> is only an anchor if it is the first character in a regular expression and <code>$</code> is only an anchor if it is the last character. The expression <code>$1</code> and <code>1^</code> do not have an anchor. If you want to match a <code>^</code> at the beginning of the line, or a <code>$</code> at the end of a line, you must escape the special characters with a backslash.</p> pattern Matches <code>^A</code> A at the beginning of a line <code>A$</code> A at the end of a line <code>A^</code> A^ anywhere on a line <code>$A</code> $A anywhere on a line <code>^^</code> ^ at the begining of a line <code>$$</code> $ at the end of a line","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-a-character-with-character-sets","title":"Matching a character with character sets","text":"<p>The regular expression <code>the</code> has three characters: <code>t</code>, <code>h</code>, and <code>e</code>. It will match any line with the string \"the\" inside it. However, it will also match the word \"there\" or \"them\". To prevent this, put spaces before and after the pattern as <code>the</code>. You can combine the string with an anchor such as <code>^HPCC</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#specifying-a-range-of-characters-with","title":"Specifying a range of characters with <code>[ ]</code>","text":"<p>If you want to match specific characters, you can use the square brackets to identify the exact characters you are searching for. The pattern that will match any line of text that contains exactly one number is <code>^[0123456789]$</code></p> <p>You can use the hyphen between two characters to specify a range. For example <code>^[0-9]$</code> is identical to <code>^[0123456789]$</code>.</p> <p>You can intermix explicit characters with character ranges. This pattern will match a single character that is a letter, number, or underscore: <code>[A-Za-z0-9_]</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#specifying-exceptions-in-character-sets-with","title":"Specifying exceptions in character sets with <code>[^ ]</code>","text":"<p><code>[^]</code> matches a single character that is not contained within the brackets.</p> <p>For example, <code>[^abc]</code> matches any character other than \"a\", \"b\", or \"c\". <code>[^a-z]</code> matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed.\u00a0To match all characters except vowels use <code>[^aeiou]</code>.</p> Regular Expression Matches <code>[]</code> The characters \"[]\" <code>[0]</code> The character \"0\" <code>[0-9]</code> Any number <code>[^0-9]</code> Any character other than a number <code>[-0-9]</code> Any number or a \"-\" <code>[0-9-]</code> Any number or a \"-\" <code>[^-0-9]</code> Any character except a number or a \"-\" <code>[]0-9]</code> Any number or a \"]\" <code>[0-9]]</code> Any number followed by a \"]\" <code>[0-9-z]</code> Any number,\u00a0or any character between \"9\" and \"z\". <code>[0-9\\-a]]</code> Any number, a \"-\", a \"a\", or a \"]\"","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-anything-with-the-wildcard-character","title":"Matching anything with the wildcard character <code>.</code>","text":"<p>A dot <code>.</code> is a special meta-character. It will match any character, except the end-of-line character.</p> <p>For example, the pattern that will match a line with a single characters is <code>^.$</code>, and a line with two characters is <code>^..$</code>.\u00a0You can use <code>...\\.</code> to match three (wildcard) characters, and escape the final wildcard meta-character to match the period instead.\u00a0</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#repeating-character-sets-with","title":"Repeating character sets with <code>*</code>","text":"<p>The <code>*</code> character matches the preceding element zero or more times.</p> <p>For example, <code>ab*c</code> matches \"ac\", \"abc\", \"abbbc\", etc. <code>[xyz]*</code> matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. </p> <p>Using parentheses will create an element that can be repeated with <code>*</code>. For example, <code>(ab)*</code> matches \"\", \"ab\", \"abab\", \"ababab\", and so on.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#matching-a-specific-number-of-sets-with-and","title":"Matching a specific number of sets with <code>\\{</code> and <code>\\}</code>","text":"<p>You cannot specify a maximum number of sets with the <code>*</code> modifier. There is a special pattern you can use to specify the minimum and maximum number of repeats. This is done by putting those two numbers between <code>\\{</code> and <code>\\}</code>.</p> <p><code>\\{m, n\\}</code> matches the preceding element at least m and not more than n times. </p> <p>For example, <code>a\\{3,5\\}</code> matches only \"aaa\", \"aaaa\", and \"aaaaa\". Another example is\u00a0<code>[a-z]\\{4,8\\}</code> which\u00a0matches 4, 5, 6, 7 or 8 lower case letters.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#more-examples","title":"More examples","text":"Regular expression Matches <code>.og</code> any three-character string ending with \"og\", including \"dog\", \"fog\", and \"hog\". <code>[df]og</code> \"dog\" and \"fog\". <code>[^d]og</code> all strings matched by <code>.og</code> except \"dog\". <code>[^df]og</code> all strings matched by <code>.og</code> other than \"dog\" and \"fog\". <code>^[df]og</code> \"dog\" and \"fog\", but only at the beginning of the string or line. <code>[df]og$</code> \"dog\" and \"fog\", but only at the end of the string or line. <code>\\[.\\]</code> any single character surrounded by \"[\" and \"]\" since the brackets are escaped, for example: \"[a]\" and \"[b]\". <code>b.*</code> b followed by zero or more characters, for example: \"b\" and \"boy\" and \"bowl\".","tags":["reference","command line"]},{"location":"Regular_Expressions/#extended-regular-expressions","title":"Extended regular expressions","text":"<p>The command line utilities <code>egrep</code> and <code>awk</code> use the extended regular expressions. In extended extensions, the backslash before some special characters is no longer required.</p> <p>For example, <code>\\{...\\}</code> becomes <code>{...}</code> and <code>\\(...\\)</code> becomes <code>(...)</code>.</p> <p>Examples:</p> <ul> <li><code>[hc]+at</code> matches with \"hat\", \"cat\", \"hhat\", \"chat\", \"hcat\", \"ccchat\" etc</li> <li><code>[hc]?at</code> matches \"hat\", \"cat\" and \"at\"</li> <li><code>([cC]at)|([dD]og)</code> matches \"cat\", \"Cat\", \"dog\" and \"Dog\"</li> <li>The characters <code>(</code>, <code>)</code>, <code>[</code>, <code>]</code>, <code>.</code>, <code>*</code>, <code>?</code>, <code>+</code>, <code>|</code>, <code>^</code>, and <code>$</code> are   special symbols and have to be escaped with a backslash symbol in order to be   treated as literal characters. For example:</li> <li><code>a\\.(\\(|\\))</code> matches the string \"a.)\" or \"a.(\"</li> </ul> <p>Modern regular expression tools allow a quantifier to be specified as non-greedy (i.e., match the fewest number of times), by putting a question mark after the quantifier. For example, in the string \"[a] [bb]\", <code>\\[.*?\\]</code> will match \"[a]\" since it matches the wildcard the fewest number of times.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#comparison","title":"Comparison","text":"BRE ERE Matches <code>\\( \\)</code> <code>( )</code> a marked subexpression. The string matched within the parentheses can be recalled later. <code>\\+</code> <code>+</code> the preceding element one or more times. <code>\\?</code> <code>?</code> the preceding element one or zero times. <code>\\|</code> <code>|</code> the preceding element or the following element. <code>\\{m, n\\}</code> <code>{m, n}</code> the preceding element at least m and not more than n times. <code>\\{m\\}</code> <code>{m}</code> the preceding element exactly m times. <code>\\{m,\\}</code> <code>{m,}</code> the preceding element at least m times. <code>\\{,n\\}</code> <code>{,n}</code> the preceding element not more than n times.","tags":["reference","command line"]},{"location":"Regular_Expressions/#examples","title":"Examples","text":"BRE ERE Matched results <code>\\(ab\\)*</code> <code>(ab)*</code> \"\", \"ab\", \"abab\", \"ababab\" etc. <code>ab\\+c</code> <code>ab+c</code> \"abc\", \"abbbc\", etc, but not \"ac\". <code>[xyz]|+</code> <code>xyz+</code> \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", etc. <code>\\(ab\\)</code> <code>(ab)+</code> \"ab\", \"abab\", \"ababab\" etc. <code>ab\\?c</code> <code>ab?c</code> \"ac\" or \"abc\". <code>\\(ab\\)\\?</code> <code>(ab)?</code> \"\" or \"ab\". <code>abc\\|def</code> <code>abc|def</code> \"abc\" or \"def\". <code>a\\{3,5\\}</code> <code>a{3,5}</code> \"aaa\", \"aaaa\", and \"aaaaa\". <code>ba\\{,2\\}b</code> <code>ba{,2}b</code> \"bb\", \"bab\", \"baab\".","tags":["reference","command line"]},{"location":"Regular_Expressions/#posix-character-sets","title":"POSIX character sets","text":"<p>POSIX has added newer and more convenient ways to search for character sets. For example, you can use\u00a0<code>[:upper:]</code> instead of\u00a0<code>[A-Z]</code>. In fact, <code>[A-Z]</code> can be different on different systems based on the <code>LC_COLLATE</code> value. For further discussion, check here. On the HPCC at MSU, the default of\u00a0<code>[A-Z]</code> is\u00a0\u00a0a, A, b, B, c, C, ....y, Y, z, Z, which is standard collations (en_US).\u00a0</p> <p>You can use\u00a0<code>[[:upper:]]</code> instead of <code>[:upper:]</code>, and you can mix the old style and POSIX styles, such as\u00a0<code>[1-9[:upper:]]</code>.</p>","tags":["reference","command line"]},{"location":"Regular_Expressions/#listing","title":"Listing","text":"Expression matches <code>[:alnum:]</code> Alphanumeric <code>[:alpha:]</code> Alphabetic <code>[:blank:]</code> Whitespace, tabs, etc <code>[:cntrl:]</code> Control character <code>[:digit:]</code> digit <code>[:graph:]</code> Printable and visible characters <code>[:lower:]</code> Lower case character <code>[:print:]</code> Printable character <code>[:punct:]</code> Punctuation <code>[:space:]</code> Whitespace <code>[:upper:]</code> Upper case character <code>[:xdigit:]</code> Extended digit","tags":["reference","command line"]},{"location":"Replacing_conda_install/","title":"Replacing an existing Conda installation","text":"<p>Due to changes in software and licensing, you may wish to replace your current installation of conda with a different version such as Miniforge. There are three different ways to maintain your environments during this change: using the conda \"clone\" feature, build a new identical environment, or by modifying <code>.condarc</code>.</p> <p>This guide assumes you have installed a new conda version into a different directory than your existing installation, e.g. your current installation is in the directory <code>~/miniconda3</code> and you install Miniforge into <code>~/miniforge3</code>.</p> <p>Warning</p> <p>As you go through this guide, regularly use the command <code>which conda</code> to make sure you are using the correct installation for each command.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Replacing_conda_install/#using-conda-clone","title":"Using conda --clone","text":"<p>First, check that you are using your new conda version by running</p> input<pre><code>which conda\n</code></pre> <p>in your terminal, logged on to an HPCC dev node.</p> <p>Then, you can run the clone command:</p> input<pre><code>conda create --name myclone --clone /path/to/myenv\n</code></pre> <p>where <code>myclone</code> is the name of your copy, and <code>/path/to/myenv</code> is the full path to the environment you want to copy e.g. <code>/mnt/home/MyMSUID/miniconda3/myenv</code>.</p> <p>You can repeat this for all of your existing environments. Once you have completed the process, you can delete your previous conda version entirely. </p> <p>Warning</p> <p>Check that you have enough file space and file count available to make full copies of your old environments using the <code>quota</code> command.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Replacing_conda_install/#using-conda-list","title":"Using conda list","text":"<p>To build an identical environment with the same packages as the original, activate your original conda version. Check that you are using your original conda version by running</p> input<pre><code>which conda\n</code></pre> <p>in your terminal, logged on to an HPCC dev node.</p> <p>Then activate the environment you want to export. Then run</p> input<pre><code>conda env export --no-builds &gt; spec-file.yml\n</code></pre> <p>where <code>spec-file.yml</code> is a file containing the precise packages used to build your environment. </p> <p>Deactivate your original conda version.</p> <p>Then, activate your new conda version and run the command</p> input<pre><code>conda create --name myenv --file spec-file.yml\n</code></pre> <p>which will create a new environment named <code>myenv</code> with the same package list. You can repeat this for all of your existing environments. Once you have completed the process, you can delete your previous conda version entirely. </p> <p>Note</p> <p>If you are low on space, you can create the specification files, then delete the old conda installation before creating the new environments. Make sure you try out the specification file process to ensure it works for your new conda version.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Replacing_conda_install/#modifying-condarc","title":"Modifying .condarc","text":"<p>Note</p> <p>This also works for conda environments in research spaces!</p> <p>To avoid duplicating environments or downloading packages, you can point your new conda version at your old environments by using the <code>.condarc</code> settings file. </p> <p>First, check that you are using your new conda version by running</p> input<pre><code>which conda\n</code></pre> <p>in your terminal, logged on to an HPCC dev node.</p> <p>Then, run the command</p> input<pre><code>conda config --append envs_dirs /path/to/old/envs\n</code></pre> <p>where <code>/path/to/old/envs</code> is the path to your original environments e.g. <code>/mnt/home/MyMSUID/miniconda3/envs</code>. You can find this path by running <code>conda env list</code> using your old conda version, which will produce output like this:</p> output<pre><code># conda environments:\n#\nbase                     /mnt/home/MyMSUID/miniconda3\nmyenv                   /mnt/home/MyMSUID/miniconda3/envs/myenv\n</code></pre> <p>You can check that the path in <code>.condarc</code> is correct by running <code>more .condarc</code> in your home directory. The output should look like:</p> output<pre><code>envs_dirs:\n  - /mnt/home/MyMSUID/miniconda3/envs\n</code></pre> <p>and then run <code>conda env list</code> using your new conda version, which should produce output like this:</p> output<pre><code># conda environments:\n#\nbase                     /mnt/home/MyMSUID/miniforge3\nmyenv                   /mnt/home/MyMSUID/miniconda3/envs/myenv\n</code></pre> <p>Note how <code>base</code> is in the new conda version, while <code>myenv</code> is the old path. You can then activate the old environment using the new version with <code>conda activate myenv</code> as usual.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Requesting_GPUs/","title":"Requesting GPUs","text":"<p>Jobs that will utilize ICER's GPU resources must request these devices through SLURM. This page will cover the nuances of GPU resource requests and how those requests relate to CPU and memory requests.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#basics-of-gpu-requests","title":"Basics of GPU Requests","text":"<p>GPUs may be requested using the <code>--gpus</code> (or <code>-G</code>) option within a batch script or on the command line when using <code>sbatch</code> or <code>salloc</code>.  At minimum, users must specify the number of GPUs they want to request. Due to the various generations of GPUs available on the HPCC, ICER strongly suggests that users also specify the type of GPU. This is because your software may be compiled for a specific GPU architecture. </p> <p>Software that can run on GPUs will still need a CPU to manage the GPU. Unless you know that your software can take advantage of multiple CPUs while also using one or more GPUs, you should only request one CPU for this purpose. Additionally, be sure your software can utilize multiple GPUs at the same time before requesting more than one GPU.</p> <p>For example, the following requests one V100 GPU that will be managed by a single CPU: <pre><code>#SBATCH --ntasks=1\n#SBATCH --gpus=v100:1\n</code></pre> This GPUs will be hosted on the same node as the single CPU. Notice the colon <code>:</code> that separates the type and number of GPUs. For more on the available types of GPUs, see our page on GPU resources.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#gpus-in-ondemand","title":"GPUs in OnDemand","text":"<p>GPUs can also be selected for Open OnDemand apps. Look for the \"Advanced Options\" checkbox. Once selected, you should see options for \"Node type\" and \"Number of GPUs\". You must set the node type if you wish to select a particular GPU model; for example, \"amd20\" or \"intel18\" for the V100 GPUs. Reference the GPU resources page to determine what type of node you should request for each type of GPU. SLURM will be unable to schedule your job if you request more GPUs than are present on that type of node.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#memory-gpus","title":"Memory &amp; GPUs","text":"<p>GPU's have their own memory (sometimes referred to as Video Random Access Memory or VRAM) that is separate from the RAM used by the CPU. The name VRAM follows from the origin of GPUs as dedicated graphical rendering hardware and does not imply any restrictions on the type of data a GPU can process.</p> <p>Each of the GPUs available on the HPCC have varying amounts of VRAM as seen in our hardware resource tables. Requesting a GPU through SLURM will automatically give you access to all of its VRAM. This is in contrast to CPU RAM, which you must explicitly request through SLURM.</p> <p>For example, the SLURM option <code>--mem-per-gpu</code> will request a certain amount of CPU memory per GPU requested. The following settings will request a total of 6GB of CPU RAM, 3GB for each of the two GPUs requested: <pre><code>#SBATCH --ntasks=1       # number of CPUs\n#SBATCH --gpus=v100:2    # number of GPUs\n#SBATCH --mem-per-gpu=3G # memory for CPUs\n</code></pre> Please note that <code>--mem</code> (total memory) and <code>--mem-per-cpu</code> are mutually exclusive with <code>--mem-per-gpu</code>.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#utilizing-multiple-gpus","title":"Utilizing Multiple GPUs","text":"<p>Some software is able to utilize multiple GPUs. These GPUs may be managed by one or more CPUs depending on how the software was written. Make sure you understand your software's capabilities and limits before requesting multiple GPUs and/or CPUs at the same time; otherwise, these resources will remain idle but still count against your yearly usage limits.</p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#one-cpu-for-multiple-gpus","title":"One CPU for Multiple GPUs","text":"<p>Most software that can utilize multiple GPUs will still use a single CPU to manage them. This means that the total number of GPUs your software can utilize is restricted by the number of GPUs on one node. See our table on GPU Resources for a reference.</p> <p>This arrangement can be visualized with the following diagram. The user has requested a single CPU and four GPUs, shown in blue. The single CPU communicates instructions and data with all of the allocated GPUs. Since the hypothetical node in this diagram only has four GPUs, the user can only use a maximum of four GPUs at a time on this type of node.</p> <p></p> <p>Requesting this kind of configuration is straightforward: <pre><code>#SBATCH --ntasks=1\n#SBATCH --gpus=v100:4\n</code></pre></p>","tags":["reference","GPU"]},{"location":"Requesting_GPUs/#multiple-cpus-for-multiple-gpus","title":"Multiple CPUs for Multiple GPUs","text":"<p>It's possible for multiple CPUs to manage a set of GPUs when using multiple processes. The most common way to achieve this is by using MPI. Each MPI process can manage one or more GPUs while also communicating with other processes. With the addition of MPI, the software is no longer restricted by the number of GPUs on a node. </p> <p>One possible configuration for this is represented in the diagram below. In this diagram, each node has one CPU managing two GPUs. The two CPUs are able to communicate with each other via MPI (gray arrows).</p> <p></p> <p>The best way to request this kind of configuration is to use the <code>--gpus-per-node</code> option: <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --gpus-per-node=v100:2\n</code></pre></p> <p>Another option is to have each MPI process manage only one GPU, as diagrammed below. Each of the four CPUs manages a GPU of the same color. These CPUs are also able to communicate with each other via MPI (gray arrows).</p> <p></p> <p>The optimal way to request this configuration is to use the <code>--gpus-per-task</code> option: <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --gpus-per-task=v100:1\n</code></pre> This will automatically bind one GPU to each of the four total tasks (CPUs) requested.</p>","tags":["reference","GPU"]},{"location":"Research_Space/","title":"Research Space","text":"<p>A research group's central directory, or research space, is a shared directory established by a MSU\u00a0Principal Investigator (PI) for use by the members of the PI's research group.</p> <p>To create a research space the PI must submit a Research Request form. The initial limit on storage is 50GB and the initial limit on the number of files contained in a research space is 1,000,000 files. A PI may request an increase in storage of up to 1TB of space at no cost by submitting a Quota Increase Request form or an increase beyond 1TB or 1 million files for an annual fee by submitting a Large Quota Increase Request form. Use the <code>quota</code> command to check a research group's current space and file quotas.</p> <pre><code>$ quota\nResearch Groups: Space    Space   Space     Space     Files     Files     Files     Files        \n                 Quota    Used    Available % Used    Quota     Used      Available % Used\n-----------------------------------------------------------------------------------------------\n&lt;groupname&gt;      4096G    3733G   363G      91%       2097152   432525    1664627   21%\n</code></pre> <p>The group's research space is associated with an assigned group name and is located at <code>/mnt/research/&lt;groupname&gt;</code> by default. It is accessible to all users who have been added to the group by the PI. Members of the group must set their user file mode creation mask and file permissions correctly such that the other group members have the appropriate access to the shared files in the research space. See the section Using a research space for more details.  </p> <p>All research space files are periodically, automatically copied to an offsite disaster recovery servers, except those files that a group has opted to store in a specially requested <code>nodr</code> space.\u00a0To request data from this system, please submit a help ticket containing the file paths and the period, i.e. the time frame, from which the files should be restored.  </p>","tags":["explanation","quota","files"]},{"location":"Research_Space/#using-a-research-space","title":"Using a research space","text":"<p>To configure a research space such that all group members have the appropriate access to the files and directories contained within, it is important to read and follow the instructions below.</p> <p>1. Ensure that all directories created in a research space have the group ownership set to the <code>&lt;groupname&gt;</code>  and the set-group-ID (<code>setgid</code>) bit enabled. </p> <p>By default, the group's research space <code>/mnt/research/&lt;groupname&gt;</code> is set with the correct group ownership and <code>setgid</code> bit. This setting makes newly created sub-directories and files within inherit the group ownership of the parent directory rather than the primary group of the individual user. To check the group ownership and <code>setgid</code> bit of a directory within the research space use the <code>ls -ld &lt;path/to/directory&gt;</code> command:</p> <pre><code>$ ls -ld /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\ndrwxrws--- 9 &lt;username&gt; &lt;groupowner&gt; 8192 Jul 22 08:38 /mnt/research/&lt;groupname&gt;/&lt;subdirectory&gt;\n</code></pre> <p>The <code>setgid</code> bit is indicated by the <code>s</code> in place of the group's executable permissions. See the page File Permissions on HPCC for more details. </p> <p>If the access permissions of a sub-directory are not set correctly, the group may encounter a <code>Disk quota exceeded</code> error when creating or copying files. See the section Quotas on a research space for more details. Use the following commands to ensure all directories and files in the group's research space have the proper settings:</p> <pre><code>$ find /mnt/research/&lt;groupname&gt;/ -not -group &lt;groupowner&gt; -print0 | xargs -0 chgrp &lt;groupowner&gt;\n$ find /mnt/research/&lt;groupname&gt;/ -type d -print0 | xargs -0 chmod g+s\n</code></pre> <p>2. Do not use the <code>mv</code> or <code>cp -p</code> commands to transfer files into the group's research space directories. </p> <p>Both <code>mv &lt;filename&gt;</code> and <code>cp -p &lt;filename&gt;</code> may preserve an undesired group ownership attribute even when transferred into a research space directory with ownership and permissions configured correctly. You should use <code>cp &lt;filename&gt;</code> without the <code>-p</code> option.</p> <p>3. Use the <code>rsync --chmod=Dg+s</code> command to transfer files from a local machine to the HPCC research space.</p> <p>For example, use the command</p> <pre><code>$ rsync -avz testdir --chmod=Dg+s &lt;username&gt;@rsync.hpcc.msu.edu:/mnt/research/&lt;groupname&gt;/\n</code></pre> <p>to transfer a directory <code>testdir</code> from your local machine to the HPCC research space  <code>/mnt/research/&lt;groupname&gt;</code>. This will automatically configure the transferred directory with the <code>setgid</code> bit.</p> <p>4. During a bash session set the user file mode creation mask to <code>0007</code> or any lower value. </p> <p>Use the  <code>umask</code> command to set the file mode creation mask, e.g. </p> <pre><code>$ umask 0007\n</code></pre> <p>For the duration of the session, files and directories created by the user are now readable, writable and executable for all members of the research group. Alternatively, a user may run the following Powertools  command one time to add the line\u00a0<code>umask 0002</code> to the\u00a0user's\u00a0<code>.bashrc</code>\u00a0file:</p> <pre><code>$ module load powertools\n$ umask_in_bash                \n</code></pre> <p>5. Ensure that each group member's currently active group ID is set to the research group of interest.</p> <p>Each time a group member logs in to the HPCC that user's group ID is set to the primary group ID assigned by default when a user's account was created. However, a user may be added to other research groups at the request of that group's PI. After a login, the user may then toggle between group memberships using the  <code>newgrp &lt;groupname&gt;</code>  command when needed. For more information, refer to the  Change Primary Group  page.\u00a0A user may request that the primary group ID be changed from the default setting by submitting a request via a  help ticket.</p>","tags":["explanation","quota","files"]},{"location":"Research_Space/#quotas-on-a-research-space","title":"Quotas on a research space","text":"<p>The space and file quotas on an research space are calculated by matching the group ownership settings of files stored on the HPCC to that of the research space group name. Hence, a user may not create files larger than 8 MB in a specified research space with a group ownership attribute different from that of the research space. Attempting to do so will likely result in a  <code>Disk quota exceeded</code>  error even though use of the  <code>quota</code>  command indicates that the reseach space quotas have not been exceeded. To resolve the <code>Disk quota exceeded</code> error in the absence of actual quota violations, users should follow the instructions in the section Using a research space  to ensure that:</p> <ul> <li> <p>The directory into which the files will be transfered has the same group ownership as the research space and the set-group-ID bit</p> </li> <li> <p>If the file already exists, its group ownership has been changed to that of the research space</p> </li> <li> <p>If the file is to be created, the primary group of the user creating the file is set to that of the research space</p> </li> </ul>","tags":["explanation","quota","files"]},{"location":"Running-multiple-jobs-sequentially_34963786.html/","title":"Running multiple jobs sequentially","text":"<p>Create a python script (python_script.py)with the following code.</p> <p>python_script.py </p> <pre><code>print(\"Hello, World!\")\n</code></pre> <p>Create an R script (r_script.R)with the following code.</p> <p>r_script.R </p> <pre><code>z=rnorm(10000,mean=10,sd=2)\nmean(z)\nsd(z)\npdf(file=\"r_histogram.pdf\")\nhist(z,freq=FALSE,nclass=100)\n</code></pre> <p>Make a copy of hello.sb and name it multi_seq.sb</p> <p>Edit multi_seq.sb to run the tasks, python_script.py and r_script.r :</p> <ul> <li>Request resources</li> <li>Load the required modules,\u00a0</li> <li>sequentially run the two scripts</li> </ul> <p>Submit job to compute node</p> <p>Answer </p> <pre><code>#Make a copy of hello.sb and name it multi_seq.sb\ncp hello.sb multi_seq.sb\n\n#Using your favorite editor(nano, vi, emacs, gedit, etc.), \n#make the following edits to the multi_seq.sb script to run the tasks, python_script.py and r_script.r simultaneously:\n\ngedit multi_seq.sb\n\n#Request resources: Typically you'll need to request the largest number of nodes needed for each task. Since each task above only uses one node and one core there is no change to the number of nodes or cores.\n\n#Load the required modules \nmodule load R/3.5.0-X11-20180131\nmodule load python\n\n#sequentially run the two scripts\npython3 python_script.py\nRscript r_script.R\n\n#Submit job to compute node: Execute the following at the command line\nsbatch multi_seq.sb\n</code></pre>"},{"location":"Running_Gaussian_by_Command_Lines/","title":"Running Gaussian by Command Lines","text":"<p>Here we are going to do an geometry optimization calculation on a simple molecule Formamide (HCONH<sub>2</sub>). Users can use the Gaussian input file <code>g16.com</code> :</p> <p>g16.com</p> <pre><code>%NProcShared=4\n%Mem=5GB\n%NoSave\n%chk=g16.chk\n# opt freq b3lyp/cc-pvdz\n\nTitle Card: Single Molecule Formamide\n\n0 1\n C                  3.89594917   -4.10509404   -0.06119675\n O                  5.13960552   -4.14687675    0.12626969\n H                  3.41099598   -3.16562892   -0.22589552\n N                  3.10941607   -5.34695260   -0.05391726\n H                  2.79423275   -5.53644967    0.87600227\n H                  2.31994463   -5.24505745   -0.65918762                                                      \n</code></pre> <p>In the file, the <code>%</code> lines (Link 0 section) specify the system resources. <code>%NprocShared</code> gives how many CPUs to use in a node and <code>%Mem</code> indicates how much memory to use. If any file specified before the <code>%NoSave</code> line, it will not be saved once Gaussian finishes the calculation normally.\u00a0 <code>%chk</code> specify a check point file name to save and <code>#</code> line (Route section) specify the methods of Gaussian calculations. You can give this Gaussian input a title name in Title Card section. After that, use\u00a0Molecule Specification section to assign the coordinates of the atoms with the charge and spin multiplicity of the system in the first line. Please make sure there is at least one empty line in the end of file.</p> <p>By logging into HPCC and ssh to a dev node, users can find Gaussian program installed in HPCC with the module commands:</p> <pre><code>$ module spider Gaussian\n\n----------------------------\n  Gaussian:\n----------------------------\n Versions:\n        Gaussian/g16_AVX\n        Gaussian/g16-AVX2\n        Gaussian/g16\n\n----------------------------\n  For detailed information about a specific \"Gaussian\" module (including how to load the modules) use the module's full name.\n  For example:\n\n     $ module spider Gaussian/g16\n----------------------------\n</code></pre> <p>Please load Gaussian module with g16 version:</p> <pre><code>$ module load Gaussian/g16\n$ which g16\n/opt/software/Gaussian/g16-AVX/g16/g16\n</code></pre> <p>and the Gaussian command \"g16\" can be used. Simply run the command by giving the input and output file names:</p> <pre><code>$ g16 &lt; g16.com &gt; g16.log\n</code></pre> <p>and it starts to calculate the system on the node. It will take about 2 minutes to complete the calculation. After it is finished, you can check the output file <code>g16.log</code>. All dev nodes have 2-hour CPU limit. Please restrict your resource usage on them.</p>"},{"location":"Running_Gaussian_by_Command_Lines/#gaussview","title":"GaussView","text":"<p>Users can also use <code>GaussView</code> to create their molecular system and do calculations. To use GaussView, just run the command <code>gview.sh</code> after loading the Gaussian module.</p>"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/","title":"SLURM Check, Modify and Cancel a Job using the scontrol &amp; scancel commands","text":""},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scontrol-command","title":"scontrol command","text":"<p>Besides the brief listing of every job using the <code>squeue</code> command, a user can also see the detailed information of each job. Run the SLURM command <code>scontrol show</code> with a job ID:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=nobody(804293) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=404 Nice=0 Account=classres QOS=normal\n   JobState=PENDING Reason=Resources Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:00 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=Unknown EndTime=Unknown Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T12:38:48\n   Partition=general-short-14,general-short-16,general-short-18,general-long-14,general-long-16,general-long-18,classres-14,classres-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=(null)\n   NumNodes=80-80 NumCPUs=160 NumTasks=80 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=40,mem=80G,node=40,gres/gpu=40\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel14 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>You can check if the information is right for the job. If the job has not started to run and you would like change any specification, you can hold the job first using the <code>scontrol hold</code> command:</p> <pre><code>$ scontrol hold 8929\n$ squeue -l -u $USER\nFri Aug  3 12:26:57 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  PENDING       0:00      1:00     80 (JobHeldUser)\n</code></pre> <p>where you can see from the results of the <code>squeue</code> command, the job is pending due to the user's hold. You can choose the information you want to change in <code>scontrol show</code> results. Put them in the <code>scontrol update</code> command and modify the information after the <code>=</code> symbol. For example, the command line</p> <pre><code>$ scontrol update job 8929  NumNodes=2-2 NumTasks=2 Features=intel16\n</code></pre> <p>will change the resource request of the job 8929 from 80 nodes and 80 tasks with intel14 nodes to 2 nodes and 2 tasks with intel16 nodes. After the update, you can use the <code>scontrol show</code> command again to verify the job setting. Once you are done with the update work, you can release the job hold by command <code>scontrol release</code>:</p> <pre><code>$ scontrol release 8929\n$ squeue -l -u $USER\nFri Aug  3 13:18:10 2018\n             JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n              8929 general-s     test   nobody  RUNNING       0:07      1:00      2 lac-[386-387]\n</code></pre> <p>The job is now running due to the change of the resource request by the command <code>scontrol update</code>. Again, we can check the running job using the command <code>scontrol show</code>:</p> <pre><code>$ scontrol show job 8929\nJobId=8929 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=379 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:08 TimeLimit=00:01:00 TimeMin=N/A\n   SubmitTime=2018-08-01T14:33:04 EligibleTime=2018-08-01T14:33:04\n   StartTime=2018-08-03T13:18:03 EndTime=2018-08-03T13:18:11 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-03T13:18:03\n   Partition=general-long-16 AllocNode:Sid=dev-intel18:4996\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[386-387]\n   BatchHost=lac-386\n   NumNodes=2 NumCPUs=4 NumTasks=2 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=4,mem=4G,node=2,billing=4\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=2G MinTmpDiskNode=0\n   Features=intel16 DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-8929.out\n   Power=\n</code></pre> <p>For complete usage information about the <code>scontrol</code> command, please refer to https://slurm.schedmd.com/scontrol.html at the SLURM web site.</p>"},{"location":"SLURM_Check_Modify_and_Cancel_a_Job_by_scontrol_scancel_Command/#scancel-command","title":"scancel command","text":"<p>If at any moment before the job complete, you would like to remove the job, you can use the <code>scancel</code> command to cancel a job. For example, the command</p> <pre><code>$ scancel 8929\n</code></pre> <p>will cancel job 8929. For a complete usage information about the <code>scancel</code> command, please refer to https://slurm.schedmd.com/scancel.html at the SLURM web site.</p>"},{"location":"SLURM_Queueing_and_Partitions/","title":"SLURM Queueing and Partitions","text":"","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#partitions","title":"Partitions","text":"<p>SLURM partitions are separate queues for submitted jobs. Each partition has a different set of constraints that controls which jobs should be in that queue.</p> <p>In most cases, you will not need to specify a partition when submitting a job. Given a specified (or default) account, SLURM's job submit plugin control will automatically send your job to the correct partition. For information regarding changing your account when submitting and ensuring access to buy-in nodes, please see the page on Buy-In and Account Management</p> <p>The jobs in each partition are evaluated by the scheduler to determine the order in which they are run. See How jobs are scheduled for more information.</p>","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#types-of-partitions","title":"Types of Partitions","text":"Name Purpose general-short This partition includes all nodes--buy-in and non-buy-in--and runs jobs that request a wall time of four hours or less. Jobs in this partition are considered for scheduling after jobs in buy-in partitions and the general-long partition. To prevent these jobs from being continuously bumped by general-long/buy-in jobs, they are also submitted to general-long. general-long This partition includes non-buy-in nodes and allows jobs to run for up to seven days. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-bigmem This partition includes non-buy-in nodes with more memory and CPU cores than most nodes and allows jobs to run for up to seven days. Jobs requesting more than 256GB or 40 CPUs per node are automatically submitted to this partition. This partition ensures large jobs get priority access to large nodes over jobs that can run elsewhere.\u00a0Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. general-long-gpu This partition contains non-buy-in nodes with GPUs and allows jobs to run for up to seven days. Jobs requesting GPUs are automatically submitted to the partition. This partition ensures jobs requesting GPUs get priority access to nodes with GPUs over jobs not requesting GPUs. Jobs with a requested wall time of four hours or less are also submitted to the general-short partition. Buyin Partition (names vary) A partition is created for each buy-in account. Each buy-in partition includes all non-buy-in nodes, allowing buy-in jobs to span buy-in and non-buy-in nodes. These jobs get equal consideration for scheduling on non-buy-in nodes as jobs in general-long. When jobs submitted to these partitions request a wall time of four hours or less, they are also submitted to the general-short, enabling them to use other available buy-in nodes and ensuring they are scheduled as fast as possible.","tags":["reference","buyin","partitions"]},{"location":"SLURM_Queueing_and_Partitions/#how-jobs-are-assigned-to-queues","title":"How jobs are assigned to queues","text":"<p>Depending on whether you are using the <code>general</code> account or a buy-in specific account, your job will be submitted to each of the following queues depending on the wall time, memory, and GPUs requested.</p> Default Account Wall Time&lt;=4 Hours Wall Time&lt;=4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&lt;=4 Hours and\u00a0GPUs Reqested Wall Time&gt;4 Hours\u00a0 Wall Time&gt;4 Hours and\u00a0&gt;=256GB or 40CPU Wall Time&gt;4 Hours and\u00a0GPUs general general-long, general-short general-long-bigmem, general-short general-long-gpu, general-short general-long general-long-bigmem general-long-gpu <code>&lt;buyin&gt;</code> <code>&lt;buyin&gt;</code>, general-long, general-short <code>&lt;buyin&gt;</code>, general-long-bigmem, general-short <code>&lt;buyin&gt;</code>, general-long-gpu, general-short <code>&lt;buyin&gt;</code>, general-long <code>&lt;buyin&gt;</code>, general-long-bigmem <code>&lt;buyin&gt;</code>, general-long-gpu <p>where <code>&lt;buyin&gt;</code> is the name of buyin account.</p>","tags":["reference","buyin","partitions"]},{"location":"SLURM_commands/","title":"SLURM Commands","text":"<p>SLURM uses command-line commands to control jobs and clusters as well as show detailed information about jobs. The table below presents the most frequently used commands on HPCC. A complete list can be found at the SLURM documentation page. Please also see this SLURM cheatsheet.</p> Command Example Usage Description srun <code>srun my_program --arg foo</code> Run parallel jobs; often used within job scripts. salloc <code>salloc -c 2 --time=1:00:00</code> Request an interactive job on a compute node. sbatch <code>sbatch my_job.sb</code> Used to submit batch jobs to the SLURM scheduler. squeue <code>squeue -u user123</code> View information about jobs - pending or running - in the queue. scancel <code>scancel 123456789</code> Used to cancel jobs or job steps that are under the control of SLURM. sacct <code>sacct -o \"JobID,AllocCPUS,State\"</code> Display accounting data for all jobs (and job steps) - running, pending, or ended - stored in the SLURM database. sprio <code>sprio -j 123456789</code> View the factors considered for a job's scheduling priority. sinfo <code>sinfo -p general-long-gpu</code> View status information SLURM nodes and partitions.","tags":["reference","slurm"]},{"location":"SLURM_overview/","title":"What is SLURM?","text":"<p>Most of the HPCC's resources are in the form of compute nodes. These compute nodes are accessed by running batch jobs.</p> <p>The HPCC uses SLURM (Simple Linux Utility for Resource Management) to manage compute node resources.  SLURM is an open-source, fault-tolerant, and highly scalable scheduling system. It has been employed by a large number of national and international computing centers. </p> <p>Users can submit batch jobs to the SLURM scheduler from the command line. Job requests include a specification of desired resources as well as the commands necessary for your program(s) to run. These job requests can either be in a job script or entered on the command line.</p>","tags":["explanation","slurm"]},{"location":"SLURM_resource_request_guide/","title":"SLURM resource request guide","text":"<p>This guide will help you identify ways you can improve your SLURM job submission  scripts to request the appropriate resources for your jobs. This will help your jobs queue efficiently.</p> <p>Note</p> <p>Walltime is the time taken measured by a \"clock on the wall\", i.e. it is the time taken for your code to run, and the time you will request in a SLURM script. CPU time is the amount of time used per CPU, so if 5 CPUs are used for the entire walltime, the CPU time will be 5 times the walltime.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#measure-your-resource-requirements","title":"Measure your resource requirements","text":"<p>The first step to obtain the correct resource requirements is to understand your  code's resource usage. There are many ways to do this, so this guide will not be exhaustive. </p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#code-documentation","title":"Code documentation","text":"<p>If your code has documentation, this should be the first place to go. The code documentation or descriptive journal article may have guidelines for resource  requirements such as memory usage, walltime, and number of CPU cores. If the  code is currently maintained or you have a support agreement with the software  producer, you can contact the developers to ask them about their experience with HPCC resource requirements.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#local-testing","title":"Local testing","text":"<p>If your code can be run on a local computer such as your laptop, you can easily  estimate the required walltime by timing a run of the code. You may also be able  to estimate the required CPU cores and memory using your computer's resource  monitor (Task Manager for Windows, Activity Monitor on Mac). For timing on Linux systems,  you can use the <code>time</code> command. This is run as <code>time &lt;your process name&gt;</code> and will return 3 time meaurements: <code>real</code>, <code>user</code> and <code>sys</code>. <code>real</code> is the equivalent of walltime, while <code>user</code> and <code>sys</code> are CPU time measurements. See this Wikipedia article for more information: https://en.wikipedia.org/wiki/Time_%28Unix%29.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#development-node-testing","title":"Development node testing","text":"<p>Note</p> <p>Development nodes have a maximum CPU time of 2 hours for processes. They are also used by other users which will affect CPU and GPU usage estimates.</p> <p>Our development nodes are a potentially useful place to investigate your code's  resource requirements for short jobs (&lt; 2 CPU hours). Note that each additional  CPU you use reduces your total allowed process time. Testing is best done when the dev node reports low usage.</p> <p>You can use the Linux tool <code>top</code> to measure memory and CPU usage. Some  development nodes (<code>dev-intel16-k80</code> and <code>dev-amd20-v100</code>) have  access to GPUs to help you determine GPU resource requirements.  As mentioned in Local testing you can use the <code>time</code> command  to measure walltime and CPU time, though this may be inaccurate in some cases.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#basic-slurm-run","title":"Basic SLURM run","text":"<p>Ideally you will have been able to estimate your resource requirements using  documentation or a local computer before this step. Then you  can use these estimates for your SLURM run. If not, you will need to use a  permissive resource request with a large amount of memory and walltime so that  you can measure your code's needs. You should expect the queue time for this test job to be long. For faster queuing of test jobs, request walltimes of less than 4 hours.</p> <p>After the run completes (or all the walltime is used), you can determine the  approximate resource requirements of your code by inspecting the amount of time taken. For more in-depth statistics, see  <code>seff</code> and <code>reportseff</code> below.</p> <p>For jobs that you expect to take longer than 4 hours, you will need to  understand your code's scaling. Scaling is how your code changes its run  time as more CPUs or GPUs are used to run the code. To measure scaling, you can run your code a few times with increasing resource requests each time and  measure how long it takes. Then you can fit a simple linear or exponential  function to these points and approximate intermediate requests, or extrapolate  larger resource requests. </p>","tags":["how-to guide","slurm","queue"]},{"location":"SLURM_resource_request_guide/#seff-and-reportseff","title":"<code>seff</code> and <code>reportseff</code>","text":"<p><code>seff</code> and <code>reportseff</code> are useful tools for investigating your resource request efficiency. They will provide statistics for individual jobs (<code>seff &lt;job id&gt;</code>)  or a report of multiple jobs (<code>reportseff -u &lt;user name&gt;</code>). <code>seff</code> statistics  list the used and requested resources as well as a percentage efficiency.  <code>reportseff</code> statistics include:</p> <ul> <li>the time efficiency of the job (TimeEff), which is the percentage use of the requested walltime;</li> <li>the CPU efficiency of the job (CPUEff), which is the percentage use of the  requested CPU cores;</li> <li>the memory efficiency of the job (MemEff), which is the percentage use of the requested memory.</li> </ul> <p>You can use these tools to get a quick measurement of your resource request usage.</p> <p>Note</p> <p><code>reportseff</code> can sometimes generate malformed output if the number of lines overflow the terminal window, in such cases pipe the ouput of 'reportseff' through more: <code>reportseff &lt;options&gt; | more</code>.</p> <p>The <code>reportseff</code> developers can be reached at  https://github.com/troycomi/reportseff.</p>","tags":["how-to guide","slurm","queue"]},{"location":"SSH_Key-Based_Authentication/","title":"SSH key-based authentication","text":"<p>While the most common way of login to the HPCC is by using the username/password pair, a more secure authentication method is the use of SSH keys. Although setting up your keys is a little more complex, it is a one-time investment. The HPCC provides key-based authentication as an option, in addition to the usual password-based login. </p> <p>Note</p> <p>Starting in October 2022, login to our rsync gateway (<code>rsync.hpcc.msu.edu</code>)  will accept SSH keys as the ONLY authentication method. Username/password won't work.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#what-are-ssh-keypairs","title":"What are SSH keypairs?","text":"<p>An SSH keypair consists of a private key and a public key. Your private key is a secret key just like your password which you should not share with anyone. On the other hand, your public key can be made publicly available in the same way that your name can be made public. The public key is stored on the server you attempt to log into (that is, the HPCC), while the private key is stored on your own computer. When a user attempts to log in, an encryption process starts on the HPCC side, using the public key. With your private key, your computer will be able to decrypt the encrypted message sent from the HPCC. When everything matches up, your login is approved.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#via-command-line-tools-linux-macos-mobaxterm-terminal-on-windows","title":"Via command line tools (Linux, MacOS, MobaXterm terminal on Windows)","text":"<p>SSH tool suites usually provide a utility for generating these keypairs. On a Mac and Linux, you can run the command <code>ssh-keygen</code> using the built in terminal Terminal. </p> <p>On a Windows computer, you will first need to download MobaXterm to generate SSH key pairs. See the Intro to MobaXterm for more on getting started. Once you have installed MobaXerm, run the program and choose the \"Start local terminal\" option.</p> <p></p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs-on-command-line","title":"Generating SSH keypairs on command line","text":"<p>To generate a keypair from command line (e.g., after opening a terminal on Mac, Linux or through MobaXterm on Windows), run</p> <p><code>ssh-keygen -t rsa -b 4096</code></p> <p>You will be given an option for protecting your private key with a passphrase. Please do this, as it will prevent your private key from being used by a malicious individual if it is ever stolen.</p> <p>After you have set a passphrase and the keys has generated, you will find the key's files in the <code>.ssh</code> directory under your home directory. By default, <code>id_rsa</code> is the private key file and <code>id_rsa.pub</code> the public key file.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#uploading-your-public-key-to-the-hpcc","title":"Uploading your public key to the HPCC","text":"<p>In order to login to HPCC with key-based authentication from your local computer, you will need to add your public key to the <code>~/.ssh/authorized_keys</code> file.  The following steps will guide you through this process:</p> <ol> <li> <p>Log on to HPCC gateway <code>gateway.hpcc.msu.edu</code> or start an interactive ondemand session to edit within a linux GUI environment</p> </li> <li> <p>On the HPCC, make sure you have a directory named <code>.ssh</code> under your home directory. If not, create one by running <code>mkdir ~/.ssh</code> in the terminal</p> </li> <li> <p>Upload your public key <code>id_rsa.pub</code> from your computer to <code>gateway.hpcc.msu.edu</code>. There are multiple ways to do so, as given here.</p> </li> <li> <p>Append the public key file to another file <code>~/.ssh/authorized_keys</code>. In order to do so, assuming that the pub key file has been copied to your home directory from Step  3, you can run the following command</p> <p><code>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p> </li> <li> <p>Set correct permissions by running </p> <p><code>chmod 700 ~/.ssh</code> </p> <p><code>chmod 600 ~/.ssh/authorized_keys</code></p> </li> </ol> <p>Warning</p> <p>Only copy key files with the <code>.pub</code> extension. Key files without this extension are private keys, which should never be shared!</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#via-gui-tools-in-mobaxterm","title":"Via GUI Tools in MobaXTerm","text":"<p>We recommend Windows users use MobaXTerm to generate and manage their key pairs. If you are Windows user who is not comfortable with the command line, the following instructions will guide you through using Graphical User Interface (GUI) tools built into MobaXterm which should work similar to other Windows applicaitons. Please follows these instructions carefully as each step is important to ensure your keys are created using the proper format for use with HPCC.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#generating-ssh-keypairs-using-mobakeygen","title":"Generating SSH keypairs using MobaKeyGen","text":"<p>In MobaXTerm, click Tools -&gt; MobaKeyGen (SSH Key Generator) in the menu bar.  You will see the following window:</p> <p></p> <p>Click Generate and follow the commands on the screen. The Parameters should be set to RSA, Number of bits 4096. The <code>Key comment</code> field can be used to enter a description of the key e.g. \"My home PC\". As above, we recommend adding a key passphrase.</p> <p></p> <p>Save the private key to your computer by going to the Conversions menu at the top of the screen, and selecting \"Export OpenSSH key\". Save the resulting file in <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code> where <code>&lt;Account_Name&gt;</code> is your Windows account name. Create the <code>.ssh</code> directory if it doesn't already exist.</p>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#uploading-your-public-key-to-the-hpcc_1","title":"Uploading your public key to the HPCC:","text":"<ol> <li>Copy the Public key from the MobaKeyGen window. Make sure you copy all of the characters. </li> </ol> <p>Note</p> <p>If you use \"Save public key\", the file will not be in the correct format. </p> <p></p> <ol> <li> <p>Log on to the HPCC gateway <code>gateway.hpcc.msu.edu</code> or start an interactive Open OnDemand session</p> </li> <li> <p>On the HPCC, make sure you have a directory named <code>.ssh</code> under your home directory. If not, create one by running <code>mkdir ~/.ssh</code> on the HPCC or using the GUI file browser in the applications menu of an interactive OnDemand session.</p> </li> <li> <p>Using an editor on the HPCC, such as <code>nano</code> or the editors in the OnDemand GUI session, open or create the <code>~/.ssh/authorized_keys</code> file and paste the copied public key. Save the file.</p> </li> <li> <p>Set correct permissions by running these commands in the HPCC terminal or using the GUI file browser of an OnDemand session</p> <p><code>chmod 700 ~/.ssh</code> </p> <p><code>chmod 600 ~/.ssh/authorized_keys</code></p> </li> </ol>","tags":["how-to guide","ssh"]},{"location":"SSH_Key-Based_Authentication/#adding-your-private-key-to-mobaxterms-ssh-connection","title":"Adding your private key to MobaXterm's SSH connection","text":"<p>In the MobaXTerm window, in the User sessions pane, right click the link you use to connect to the hpcc (usually something like <code>hpcc.msu.edu (&lt;netid&gt;)</code>), and select \"Edit session\". In the \"Advanced SSH settings\" pane, click \"Use private key\", and add the path to the key file you saved in <code>C:\\Users\\&lt;Account_Name&gt;\\.ssh\\id_rsa</code>. Click \"OK\" to save.</p> <p>When you use this session in the future, you should no longer be prompted for a password.</p> <p></p> <p>Note</p> <p>If you use MobaXTerm for access, make sure you keep MobaXTerm up to date.  We recommend using MobaXTerm to generate keys for use with MobaXTerm on Windows.</p> <p>Note</p> <p>If you are having trouble connecting with an SFTP session in MobaXterm to rsync.hpcc.msu.edu, make sure you  check the \"Use private key\" box and point towards the private key (id_rsa) file which should be in either: - C:\\Users\\\\AppData\\Roaming\\MobaXterm\\home.ssh if you used command line or - C:\\Users\\.ssh if you use the the GUI tool <p></p>","tags":["how-to guide","ssh"]},{"location":"Scavenger_Queue/","title":"Scavenger Queue","text":"<p>The scavenger queue allows users to run preemptible jobs on idle cores. Jobs in the scavenger queue may be interrupted if resources are required for other non-scavenger jobs.</p> <p>With few exceptions, each researcher using the HPCC is limited in the number of jobs or cores they can run at one time. Annually, non-buyin users are limited in the total number of CPU and GPU hours they can use. These limits do not apply to jobs submitted to the scavenger queue.</p> <p>Jobs in the scavenger queue can start on resources that would otherwise be left idle, improving research throughput. Similar to jobs submitted to the general-long queue, these jobs can request up to a 7-day wall time. The default behavior for interrupted jobs is to be re-queued, but users can opt for cancellation if it is more conducive to their workflow.</p> <p>Note</p> <p>We recommend that only users who can checkpoint and restart or have a workflow implemented that can manage jobs being canceled or requeued use this queue.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Scavenger_Queue/#usage","title":"Usage","text":"<p>To use the scavenger queue, add the following line to your job script: <pre><code>#SBATCH --qos=scavenger\n</code></pre></p> <p>To prevent your job from requeuing automatically if interrupted, add the following line to your job script: <pre><code>#SBATCH --no-requeue\n</code></pre></p> <p>The scavenger queue is not affected by the amount of wall time requested in your job script, e.g. 24 hours wall time is treated with the same scavenger queue priority as 4 hours wall time.</p> <p>Scavenger queue jobs will be automatically assigned to the <code>scavenger</code> account, regardless of the <code>-A</code> setting in your job script.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Scavenger_Queue/#scheduling","title":"Scheduling","text":"<p>The scavenger queue runs using the backfill scheduler (see How Jobs are Scheduled). Job scheduling may take on the order of minutes to occur, depending on the current load on the HPCC. Scavenger queue jobs run for a minimum of 1 minute before they can be preempted, but typically scavenger queue jobs run for approximately 1 hour before preemption.</p>","tags":["how-to guide","slurm","partitions"]},{"location":"Science_DMZ/","title":"High Speed Research Network","text":"<p>MSU's High Speed Research Network (formerly the Science Demilitarized\u00a0Zone) is a portion of the network designed to optimize high-performance for research applications. This Research Network enables researchers to disseminate terabytes or even petabytes of specialized data more easily\u00a0and\u00a0at speeds of 10 to 100 gigabits per second to other institutions and cloud providers. This ability to share data immeasurably increases its value, as the insights extrapolated from it by additional researchers have the potential to change society in significant and meaningful ways.\u00a0</p> <p>The\u00a0High Speed Research Network offers increased network speeds and reliability, broadly enhancing MSU\u2019s research and education cyberinfrastructure. All campus network users will benefit from the high-speed network connections that will be used for sharing data already stored on ICER's HPCC and on the NSF-funded OSIRIS storage infrastructure.\u00a0</p> <p>The High Speed Research Network also eliminates obstacles for better access to valuable data. By sharing resources and working together, researchers are better positioned to collaboratively find solutions to our biggest problems. This project also lays the foundation for a new relationship between MSU IT and the Office of Research and Innovation, strengthening collaboration and strategic planning as MSU develops cyberinfrastructure capabilities to enhance scientific research support.\u00a0\u00a0</p>"},{"location":"Science_DMZ/#using-the-high-speed-research-network","title":"Using the High Speed Research Network","text":"<p>Globus is the recommended method to transfer big data files as it automatically leverages the High Speed Research Network. For a general overview of Globus and information on setting up a Globus account, see Transferring data with Globus For walk-through training on using Globus, please self-enroll in ICER's DMZ Globus Training D2L course.</p>"},{"location":"Scratch_File_Systems/","title":"Scratch Space","text":"<p>Each user is provided with a working directory know as scratch space. This space is intended for intensive input/output (I/O) operations i.e., heavy reading and writing of data, involving very large files and/or a very large number of small files. Research groups may also request a scratch space. Unlike the home space and research\u00a0space, the scratch space is not intended for long-term storage and cannot be accessed from a gateway node, with the exception of the rsync gateway used for file transfer. </p> <p>Data stored in a user's scratch space is not backed-up, and files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users.  The limit on storage is 50TB and the initial limit on the number of files contained in a scratch space is 1,000,000 files. </p> <p>A user's scratch space is available at <code>/mnt/scratch/$USER</code>, or use the bash environmental variable <code>$SCRATCH</code>. Use the <code>quota</code> command to check a user's current space and file quotas.</p> <pre><code>$ quota\n\nTemporary Filesystems:\n---------------------------------------------------------------------------------------------------------------------------------------\n\n/mnt/scratch (/mnt/gs21)        Space Quota  Space Used   Space Free   Space % Used Filess Quota Files Used   Files Free   Files % Used\n                                51200G       0G           51200G       0%           1048576      1            1048575      0%       \n</code></pre>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#using-a-scratch-space","title":"Using a Scratch Space","text":"<p>Scratch Space can sustain high data transfer rates and is a good  choice for data files used in running parallel on multiple nodes with intensive I/O requirements. Jobs of this type will run much faster with  data accessed from a scratch space and users should follow the procedure  below for best practice:    </p> <ol> <li>Configure the job script and/or the main program for scratch space I/O using the path <code>/mnt/scratch/$USER</code> or the variable <code>$SCRATCH</code></li> <li>Copy input data from the home space or research space to scratch space; to maintain data integrity keep the original data files in the home space or research space**</li> <li>Schedule the job and confirm successful completion of the I/O operations</li> <li>Move the resulting output data back to either the home space or research space</li> <li>Delete that data from the scratch space</li> </ol>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#time-limits-on-scratch-space","title":"Time Limits on Scratch Space","text":"<p>Files in a scratch space with no I/O operations for 45 days will be automatically deleted to ensure the space is available to all users. To find files in a scratch space approaching the 45 day limit run the following command:</p> <p><code>find $SCRATCH -type f -ctime +40</code></p> <p>Here the <code>+40</code> argument specifies files with no I/O for more than 40 days. Users may set this argument to any number of days desired, up to the 45 day limit. This time is measured based on the \"change time\" of the file, so files extracted from archives will not be deleted until the 45 day limit is reached, after they have been extracted.</p>","tags":["explanation","quota","files"]},{"location":"Scratch_File_Systems/#multiple-scratch-spaces","title":"Multiple Scratch Spaces","text":"<p>You may have multiple directories in scratch, such as one associated with your home directory (/mnt/scratch/$USER) and one or more associatedithed with research spaces (/mnt/scratch/). Research scratch directory, like regular research directories are shared with other memebers of your reserearch group and intended to facilitate collaboration with groups. However, like the scratch space under your username, research scratch spaces is not backed-up, and files will be automatically deleted after 45 days. <p>Importantly, the limits on your scratch space useage applies to all scratch spaces. In other words, you have a maximum of 50TB and  1,000,000 files in scratch, regardless of which scratch directory those files are in.</p>","tags":["explanation","quota","files"]},{"location":"Sensitive_Data_on_the_HPCC/","title":"Sensitive Data on the HPCC","text":"<p>Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with the HPCC to ensure data security.  </p> <p>Examples of sensitive data include:</p> <ul> <li>Personally Identifiable Information (PII)</li> <li>Protected Health Information (PHI)</li> <li>Controlled Unclassified Information (CUI)</li> <li>Data covered under the Heath Insurance Portability and     Accountability Act (HIPAA)</li> <li>Data covered under the Federal Information Security Management Act     (FISMA)</li> <li>Data covered under the Federal Education Rights and Privacy Act     (FERPA)</li> <li>Data with security requirements set by the MSU Institutional Review     Board (IRB)</li> <li>Controlled access data from the NIH Database of Genotypes and     Phenotypes (dbGaP)</li> </ul> <p>Information about the HPCC's Sensitive Data Policy is available to users with MSU credentials.</p> <p>MSU users may also visit https://data-storage-finder.tech.msu.edu/ for more information about data storage choices on campus (NOTE: This page is only accessible from campus or using Campus VPN).</p>","tags":["reference","files","sensitive data"]},{"location":"Show_Job_Steps_by_sacct_and_srun_Commands/","title":"Showing job steps","text":"<p>SLURM provides commands to show the execution information of each command line in a job script. This can be helpful for debugging and testing. In order to get  such information, the wrapper command <code>srun</code> needs to be used. Let's take a look at the following job script:</p> <pre><code>#!/bin/bash\n\n#SBATCH -N 4 -n 4 -c 2\n#SBATCH --time=00:05:00\n#SBATCH --mem=1G\n\nmodule purge; module load GCC/6.4.0-2.28 OpenMPI/2.1.2\nmodule list\n\nmpicc mpi-hello.c -o hello.exe\n\necho; echo \"====== mpirun hello.exe ======\"\nmpirun hello.exe                                            #0 Step\n\necho; echo \"====== srun hello.exe ======\"\nsrun hello.exe                                              #1 Step\n\necho; echo \"====== srun -n 8 -c 1 hello.exe ======\"\nsrun -n 8 -c 1 hello.exe                                    #2 Step\n\necho; echo \"====== srun  ======\"\nsrun NoSuchCommand                                          #3 Step\n\necho; echo \"====== mpirun  ======\"\nmpirun NoSuchCommand                                        #4 Step\n\necho; echo \"====== scontrol show job $SLURM_JOB_ID ======\"\nsrun -N 1 -n 1 -c 1 scontrol show job $SLURM_JOB_ID         #5 Step\n</code></pre> <p>Although there are many command lines, only 6 of them are executed with either mpirun or srun wrapper and marked with the comments (from step 0 to 5) in the end. SLURM can record each of the 6 executions as a job step. Once the job is submitted by sbatch command and starts running, you can use sacct command to check the steps:</p> <pre><code>$ sacct -j 10732\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n10732              test general-l+   classres          8  COMPLETED      0:0\n10732.batch       batch              classres          2  COMPLETED      0:0\n10732.extern     extern              classres          8  COMPLETED      0:0\n10732.0           orted              classres          6  COMPLETED      0:0\n10732.1       hello.exe              classres          8  COMPLETED      0:0\n10732.2       hello.exe              classres          8  COMPLETED      0:0\n10732.3      NoSuchCom+              classres          8     FAILED      2:0\n10732.4           orted              classres          6  COMPLETED      0:0\n10732.5        scontrol              classres          1  COMPLETED      0:0\n</code></pre> <p>where the Job has ID 10732 and the 6 steps are shown from JobID 10732.0 to 10732.5.</p> <p>We can also use a powertools command js to see more detailed information (such as memory usage and a list of used nodes) about the steps:</p> <pre><code>$ js -j 10732 -C5\n\nSLURM Job ID: 10732\n===============================================================================================================================\n          JobID |               10732 |         10732.batch |        10732.extern |             10732.0 |             10732.1 |\n        JobName |                test |               batch |              extern |               orted |           hello.exe |\n           User |            changc81 |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |             lac-380 |       lac-[380-383] |       lac-[381-383] |       lac-[380-383] |\n         NNodes |                   4 |                   1 |                   4 |                   3 |                   4 |\n         NTasks |                     |                   1 |                   4 |                   3 |                   4 |\n          NCPUS |                   8 |                   2 |                   8 |                   6 |                   8 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |            00:05:00 |                     |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:02 |            00:00:01 |\n      SystemCPU |           00:03.283 |           00:00.562 |           00:00.001 |           00:00.646 |           00:00.572 |\n        UserCPU |           00:02.119 |           00:00.753 |           00:00.003 |           00:00.396 |           00:00.281 |\n       TotalCPU |           00:05.403 |           00:01.316 |           00:00.005 |           00:01.042 |           00:00.853 |\n     AveCPULoad |            0.337687 |             0.08225 |           0.0003125 |               0.521 |               0.853 |\n         MaxRSS |                     |              10409K |                120K |                861K |                863K |\n      MaxVMSize |                     |             652100K |             173968K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:44 | 2018-08-06T13:22:54 | 2018-08-06T13:22:57 |\n            End | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 | 2018-08-06T13:22:56 | 2018-08-06T13:22:58 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n===============================================================================================================================\n          JobID |             10732.2 |             10732.3 |             10732.4 |             10732.5 |\n        JobName |           hello.exe |       NoSuchCommand |               orted |            scontrol |\n           User |                     |                     |                     |                     |\n       NodeList |       lac-[380-383] |       lac-[380-383] |       lac-[381-383] |             lac-380 |\n         NNodes |                   4 |                   4 |                   3 |                   1 |\n         NTasks |                   8 |                   4 |                   3 |                   1 |\n          NCPUS |                   8 |                   8 |                   6 |                   1 |\n         ReqMem |                 1Gn |                 1Gn |                 1Gn |                 1Gn |\n      Timelimit |                     |                     |                     |                     |\n        Elapsed |            00:00:01 |            00:00:00 |            00:00:01 |            00:00:00 |\n      SystemCPU |           00:01.141 |           00:00.051 |           00:00.289 |           00:00.017 |\n        UserCPU |           00:00.521 |           00:00.031 |           00:00.096 |           00:00.035 |\n       TotalCPU |           00:01.663 |           00:00.083 |           00:00.385 |           00:00.053 |\n     AveCPULoad |               1.663 |                     |               0.385 |                     |\n         MaxRSS |              34812K |                865K |                865K |                840K |\n      MaxVMSize |             324436K |             324436K |             324440K |             324436K |\n          Start | 2018-08-06T13:22:58 | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 |\n            End | 2018-08-06T13:22:59 | 2018-08-06T13:22:59 | 2018-08-06T13:23:00 | 2018-08-06T13:23:00 |\n       ExitCode |                 0:0 |                 2:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |              FAILED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>From the results above, we can see the executions by mpirun are different from srun. First of all, for mpirun, the JobName only show \"orted\" no matter what commands are used in the steps 10732.0 and 10732.4. However, srun shows the correct commands in all of the steps (10732.1, 10732.2, 10732.3 and 10732.5). Secondly, mpirun results show only 3 tasks with 6 CPUs are used but srun results correctly show 4 tasks with 8 CPUs in step 10732.1, 8 tasks with 8 CPUs in step 10732.2 and 1 task with 1 CPU in 10732.5 step. Finally, both steps 10732.3 and 10732.4 ran the same command NoSuchCommand where there is no such file or directory and should cause an error execution. However, mpirun wrapper still consider it is complete without error. Only srun wrapper get the FAIL state with an exit code 2.</p> <p>From the job output in the following results, we see no difference between the outputs of the step 10732.0 (mpirun hello.exe) and the step 10732.1\u00a0(srun hello.exe). SLURM seems to get a good sacct information with srun but not with mpirun. If you wish to use the step information, do not forget to put srun in the command lines.</p> <pre><code>Currently Loaded Modules:\n  1) GCCcore/6.4.0   2) binutils/2.28   3) GCC/6.4.0-2.28   4) OpenMPI/2.1.1\n\n\n====== mpirun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 4\nHello From: lac-381              I am processor 2 of 4\nHello From: lac-382              I am processor 3 of 4\nHello From: lac-383              I am processor 4 of 4\n\n====== srun -n 8 -c 1 hello.exe ======\nHello From: lac-380 I am the recieving processor 1 of 8\nHello From: lac-380              I am processor 2 of 8\nHello From: lac-381              I am processor 3 of 8\nHello From: lac-381              I am processor 4 of 8\nHello From: lac-382              I am processor 5 of 8\nHello From: lac-382              I am processor 6 of 8\nHello From: lac-383              I am processor 7 of 8\nHello From: lac-383              I am processor 8 of 8\n\n====== srun  ======\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-381: task 1: Exited with exit code 2\nsrun: error: lac-383: task 3: Exited with exit code 2\nsrun: error: lac-382: task 2: Exited with exit code 2\nslurmstepd: error: execve(): NoSuchCommand: No such file or directory\nsrun: error: lac-380: task 0: Exited with exit code 2\n\n====== mpirun  ======\n--------------------------------------------------------------------------\nmpirun was unable to find the specified executable file, and therefore\ndid not launch the job.  This error was first reported for process\nrank 0; it may have occurred for other processes as well.\n\nNOTE: A common cause for this error is misspelling a mpirun command\n      line parameter option (remember that mpirun interprets the first\n      unrecognized command line token as the executable).\n\nNode:       lac-380\nExecutable: NoSuchCommand\n--------------------------------------------------------------------------\n\n====== scontrol show job 10732 ======\nJobId=10732 JobName=test\n   UserId=changc81(804793) GroupId=helpdesk(2103) MCS_label=N/A\n   Priority=103 Nice=0 Account=classres QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   RunTime=00:00:18 TimeLimit=00:05:00 TimeMin=N/A\n   SubmitTime=2018-08-06T13:22:43 EligibleTime=2018-08-06T13:22:43\n   StartTime=2018-08-06T13:22:44 EndTime=2018-08-06T13:27:44 Deadline=N/A\n   PreemptTime=None SuspendTime=None SecsPreSuspend=0\n   LastSchedEval=2018-08-06T13:22:44\n   Partition=general-long-16 AllocNode:Sid=lac-249:5133\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=lac-[380-383]\n   BatchHost=lac-380\n   NumNodes=4 NumCPUs=8 NumTasks=4 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=8,mem=4G,node=4,billing=8\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=2 MinMemoryNode=1G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Gres=(null) Reservation=(null)\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/mnt/home/changc81/GetExample/helloMPI/test\n   WorkDir=/mnt/home/changc81/GetExample/helloMPI\n   Comment=stdout=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdErr=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   StdIn=/dev/null\n   StdOut=/mnt/home/changc81/GetExample/helloMPI/slurm-10732.out\n   Power=\n</code></pre> <p>For a complete instruction of sacct command, please refer to the SLURM web site.</p>","tags":["tutorial","slurm","srun"]},{"location":"Singularity_Advanced_Topics/","title":"Singularity Advanced Topics","text":"","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-containers","title":"Building containers","text":"","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-writable-containers","title":"Building writable containers","text":"<p>By default, the containers you build in Singularity are read-only. Any changes you make are not saved. This usually is not a problem if the container you use has everything you need, since you can save files in your working directory or home directory and they will persist outside of the containers.</p> <p>However, if your container is missing something that doesn't make sense to include in your home directory that you would like to persist between runs of the container (like another piece of software), you can build a container in a writable directory, called a sandbox.</p> <p>Tip</p> <p>An alternative way to have a writable (portion of a) filesystem in Singularity is an overlay. Overlays are files that act like a storage drive you can \"plug in\" to your container rather than encompassing the entire root filesystem. Since overlays are a viewed as a single file, they are great for \"tricking\" the HPCC into allowing you to use more files than your quota allows. For more information, including powertools to help you get started and examples installing conda, see the Lab Notebook on Singularity Overlays.</p> <p>You can create a sandbox using the <code>singularity build</code> command with the <code>--sandbox</code> option. As arguments, use a directory name for the location of the sandbox and an image you want to start with (which can either be a URI or a file):</p> <pre><code>singularity build --sandbox alpine/  docker://alpine\n</code></pre> <p>If you look inside the directory, it looks like the full file system for the container</p> <pre><code>$ ls alpine\nbin  environment  home  media  opt   root  sbin         srv  tmp  var\ndev  etc          lib   mnt    proc  run   singularity  sys  usr\n</code></pre> <p>To run this image with any Singularity command, pass the directory as the image name:</p> <pre><code>singularity shell alpine/\n</code></pre> <p>However in order to make changes that will persist, you need to use the <code>--writable</code> option. Let's try to install some software:</p> <pre><code>$ singularity shell --writable alpine/\nSingularity&gt; python\n/bin/sh: python: command not found\nSingularity&gt; apk add --update py3-pip\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.17/main/x86_64/APKINDEX.tar.gz\nfetch https://dl-cdn.alpinelinux.org/alpine/v3.17/community/x86_64/APKINDEX.tar.gz\n(1/19) Installing libbz2 (1.0.8-r4)\n...\n(19/19) Installing py3-pip (22.3.1-r1)\nSingularity&gt; python\nPython 3.10.11 (main, Apr  6 2023, 01:16:54) [GCC 12.2.1 20220924] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>If we use the <code>alpine/</code> sandbox again, we'll still have access to Python! The sandbox can be packaged back up into an image file by again using the <code>singularity build</code>:</p> <pre><code>$ singularity build alpine_with_python.sif alpine/\nINFO:    Starting build...\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_with_python.sif\n$ singularity exec alpine_with_python.sif python --version\nPython 3.10.11\n</code></pre> <p>However, this method for building new containers does not leave a record of how you changed the base image. For better reproducibility, you should use a Singularity definition file as described in the next section.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#building-containers-from-scratch-with-singularity-definition-files","title":"Building containers from scratch with Singularity definition files","text":"<p>Warning</p> <p>Building Singularity files from definition files requires super user permissions. You will need to install Singularity on your local computer to run these steps.</p> <p>Alternatively, you might prefer building a Docker container and using it in Singularity as discussed below.</p> <p>To build containers, Singularity uses a Singularity definition file which is similar to a Dockerfile in Docker. We will walk through building a Singularity definition file that creates a comparable image to the one in our Docker tutorial.</p> <p>We will set up our working directory the same way:</p> <pre><code>cd ~\nmkdir my_first_Singularity_image\ncd my_first_Singularity_image\n</code></pre> <p>and create the python script <code>hello.py</code>:</p> hello.py<pre><code>print(\"Hello world!\")\nprint(\"This is my 1st Singularity image!\")\n</code></pre> <p>Now, create the file <code>Singularity</code> with the content below:</p> Singularity<pre><code>Bootstrap: docker\nFrom: alpine\n\n# copy files required to run\n%files \n    hello.py /usr/src/my_app/\n\n# install python and pip\n%post\n    apk add --update py3-pip\n\n# run the application\n%runscript\n    python3 /usr/src/my_app/hello.py\n</code></pre> <p>Now, let's learn the meaning of each section.</p> <p>The first section means that we will use Alpine Linux as a base image. In fact, the <code>Bootstrap</code> line tells Singularity that we are using the Docker alpine image hosted on Docker Hub. Other options for the <code>Bootstrap</code> line include <code>library</code> for images in Sylab's container library and <code>shub</code> for images on the (archived) Singularity hub.</p> <pre><code>Bootstrap: docker\nFrom: alpine\n</code></pre> <p>The <code>%files</code> section tells Singularity which files in our local directory we want to copy to the container and where we should move them. In this case, we are copying our Python script into <code>/usr/src/my_app/</code> in the container.</p> <pre><code>%files \n    hello.py /usr/src/my_app/\n</code></pre> <p>The <code>%post</code> section is used to install <code>pip</code> using the Alpine Package Keeper (<code>apk</code>).</p> <pre><code>%post\n    apk add --update py3-pip\n</code></pre> <p>Finally, the <code>%runscript</code> section tells the container what command to run when the container is invoked through <code>singularity run</code>.</p> <pre><code>%runscript\n    python3 /usr/src/my_app/hello.py\n</code></pre> <p>We can now build the image using the <code>singularity build</code> command. Don't forget that you'll need super user permission!</p> <pre><code>sudo singularity build my_first_image.sif Singularity\n</code></pre> <p>This will create the <code>my_first_image.sif</code> file that you can now run.</p> <pre><code>$ singularity run my_first_image.sif\nHello world!\nThis is my 1st Singularity image!\n</code></pre> <p>You can now use this singularity image file anywhere you like, including the HPCC.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#migrating-from-docker-to-singularity","title":"Migrating from Docker to Singularity","text":"<p>For more information regarding Docker support in Singularity, please see the official documentation.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#direct-comparision","title":"Direct comparision","text":"Topic Docker Singularity Installation Local computer only HPCC and local computer Privileges Requires super user privileges Only requires super user privileges for building images Compatibility Docker images Docker and Singularity images Images Cached and managed by Docker Available as <code>.sif</code> files (can also be cached and managed by Singularity) File sharing Manually specifying bind mounts (e.g., <code>-v</code> option) Automatically binds useful directories (<code>$HOME</code>, <code>$PWD</code>, etc.); others can be specified via <code>--bind</code> option and through overlay files Build file Dockerfile Singularity definition file Downloading images <code>docker pull &lt;container&gt;</code> <code>singularity pull &lt;uri-prefix&gt;://&lt;container&gt;</code> Running <code>docker run &lt;container&gt;</code> <code>singularity run &lt;container&gt;.sif</code> Running command <code>docker run &lt;container&gt; &lt;command&gt;</code> <code>singularity exec &lt;container&gt;.sif &lt;command&gt;</code> Interactive shell <code>docker -it &lt;container&gt; sh</code> <code>singularity shell &lt;container&gt;.sif</code>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#converting-from-docker-images-to-singularity-images","title":"Converting from Docker images to Singularity images","text":"<p>There are a few ways to use Docker images with Singularity. If the image is publicly available on Docker Hub, it is as easy as using the <code>singularity pull</code> command with a Docker URI. See the example in the Singularity introduction. If you are installing from a private repository on Docker Hub, use the <code>--docker-login</code> flag with <code>singularity pull</code> to authenticate with Docker.</p> <p>If the Docker image is only available locally (e.g., you are testing local builds and don't want to push to a repository), you have two options. First, you can build a Singularity image directly from a cached Docker image:</p> <pre><code>sudo singularity build &lt;singularity-image-filename&gt;.sif docker-daemon://&lt;docker-image-name&gt;\n</code></pre> <p>Note that this requires Singularity and Docker to be installed on the same system, and requires super user permissions.</p> <p>The second option is to first archive the Docker image into a <code>tar</code> file, then use this to build the Singularity image:</p> <pre><code>docker save &lt;docker-image-name&gt; -o docker_image.tar\nsingularity build &lt;singularity-image-filename&gt;.sif docker-archive://docker_image.tar\n</code></pre> <p>Here you could perform the <code>docker save</code> on your local machine, move the <code>docker_image.tar</code> file to the HPCC, and then run the <code>singularity build</code> step on the HPCC since it does not require super user privileges.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#a-note-on-permissions","title":"A note on permissions","text":"<p>Singularity automatically mounts many system directories in your container, including <code>$HOME</code> and <code>$PWD</code>. When you enter a shell in a Singularity container, you will be in the same directory you started from. You are also logged in as the same user inside the Singularity container as you are on the host when you start the container.</p> <p>In contrast, a Docker shell usually starts in <code>/</code> as the <code>root</code> user (or some other user).  Thus, you may have different permissions in a Docker container that is run in Singularity. This can cause problems if a Docker container expects you to be able to write to directories that your HPCC user will not have access to (like <code>/root</code>).</p> <p>In these cases, you may have to modify the Dockerfile used to create the Docker image so that anything you need to access is stored in a location accessible to your user.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#using-singularity-with-mpi-and-gpus","title":"Using Singularity with MPI and GPUs","text":"<p>If you are running a container that uses MPI, you must use <code>srun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK</code> before the <code>singularity</code> command to make the command aware of all resources allotted. See a template script below.</p> singularity_mpi.sbatch<pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=singularity-test\n#\n# Number of MPI tasks needed for use case:\n#SBATCH --ntasks=18\n#\n# Number of nodes to split the tasks across:\n#SBATCH --nodes=2\n#\n# Processors per task:\n#SBATCH --cpus-per-task=4\n#\n# Memory per CPU\n#SBATCH --mem-per-cpu=1G\n#\n# Wall clock limit:\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\ncd &lt;directory containing the singularity image file (.sif)&gt;\nsrun -n $SLURM_NTASKS -c $SLURM_CPUS_PER_TASK singularity exec &lt;singularity-file&gt;.sif &lt;commands&gt;\n</code></pre> <p>To run a container that takes advantage of GPU resources, you can use the <code>--nv</code> flag on any <code>run</code>, <code>exec</code>, or <code>shell</code> singularity commands. Otherwise, use the standard <code>sbatch</code> setup for running any GPU job. An example script that pulls a TensorFlow container and displays the available GPUs is shown below.</p> singularity_gpu.sbatch<pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=singularity-test\n#\n# Request GPU:\n#SBATCH --gpus=v100:1\n#\n# Memory per CPU\n#SBATCH --mem-per-cpu=20G\n#\n# Wall clock limit:\n#SBATCH --time=30\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\nsingularity pull docker://tensorflow/tensorflow:latest-gpu\nsingularity exec --nv tensorflow_latest-gpu.sif python -c \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"\n</code></pre>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Advanced_Topics/#cached-images","title":"Cached images","text":"<p>When you use Docker image without pulling it first, it appears that no Singularity image file was created:</p> <pre><code>$ mkdir new_dir\n$ cd new_dir\n$ singularity exec docker://ubuntu uname -a\nGetting image source signatures\nCopying blob a1d0c7532777 done  \n...\nLinux dev-amd20 3.10.0-1160.80.1.el7.x86_64 #1 SMP Tue Nov 8 15:48:59 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n$ ls\n$\n</code></pre> <p>In fact, Singularity stores these files and the files used to create them in a cache:</p> <pre><code>$ singularity cache list\nThere are 9 container file(s) using 4.06 GiB and 68 oci blob file(s) using 4.18 GiB of space\nTotal space used: 8.24 GiB\n</code></pre> <p>As you can see, the files stored here can build up quickly. You can clean this cache using</p> <pre><code>singularity cache clean\n</code></pre> <p>Everything in the cache can be safely removed, and will just be redownloaded if needed again.</p> <p>By default, Singularity uses <code>~/.singularity/cache</code> to store these files. If you want to use another directory (e.g., your scratch space), you can use the <code>SINGULARITY_CACHEDIR</code> environment variable. Singularity also uses a temporary directory (<code>/tmp</code> by default) that you might also want to change using the <code>SINGULARITY_TEMPDIR</code> environment variable. For example:</p> <pre><code>mkdir -p $SCRATCH/singularity_tmp\nmkdir -p $SCRATCH/singularity_scratch\nSINGULARITY_CACHEDIR=$SCRATCH/singularity_scratch SINGULARITY_TMPDIR=$SCRATCH/singularity_tmp singularity --debug pull --name ubuntu-tmpdir.sif docker://ubuntu\n</code></pre> <p>Using the <code>--debug</code> flag shows a lot of information, but at the end we see these lines:</p> <pre><code>VERBOSE [U=919141,P=3605]  Full()    Build complete: /mnt/gs21/scratch/&lt;user&gt;/singularity_scratch/cache/oci-tmp/tmp_011111517\nDEBUG   [U=919141,P=3605]  cleanUp() Cleaning up \"/mnt/gs21/scratch/&lt;user&gt;/singularity_tmp/build-temp-030629912/rootfs\" and \"/mnt/gs21/scratch/&lt;user&gt;/singularity_tmp/bundle-temp-545905815\"\n</code></pre> <p>verifying that the scratch directories were used.</p>","tags":["explanation","containers","Singularity","Docker"]},{"location":"Singularity_Introduction/","title":"Singularity Introduction","text":"<p>Note</p> <p>This tutorial is adapted from the Container Camp Tutorial produced and copyrighted by CyVerse.</p> <p>Singularity allows users to run software inside of containers. Another popular container system is Docker, which is interoperable with Singularity. Singularity provides many features that make it well suited for HPC applications, and therefore, it is the container system that is installed on our HPCC.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#installation","title":"Installation","text":"<p>To install Singularity on your local machine, please see the installation instructions in the documentation. Singularity can only be run natively on Linux, but if you need to run it locally on a Windows or Mac computer, you can use a virtual machine provided by the creators of Singularity. See these alternative instructions for details.</p> <p>Note</p> <p>As of May 2023, the version of Singularity on the MSU HPCC is currently 3.11. For any questions that this tutorial or the Advanced Topics page do not answer, please consult the official documentation for this version. All Singularity commands are built into the system such as <code>singularity shell</code> and <code>singularity exec</code>, which means you can invoke these commands directly from the command line.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#check-installation","title":"Check installation","text":"<p>If you would like to check your installation of Singularity on your local machine or the installation on the HPCC, you can run</p> <pre><code>$ singularity pull shub://vsoch/hello-world\nINFO:    Downloading shub image\n59.75 MiB / 59.75 MiB      [========================================================================================] 100.00% 10.46 MiB/s 5s\n\n$ singularity run hello-world_latest.sif\nRaawwWWWWWRRRR!! Avocado!\n</code></pre> <p>In the above example, we used the Singularity Hub \"unique resource identifier,\" or URI, \"shub://\" which tells the software to run an image from Singularity Hub. To get help, you can use the <code>--help</code> flag which gives a general overview of Singularity options and subcommands as follows:</p> <pre><code>$ singularity --help\n\nLinux container platform optimized for High Performance Computing (HPC) and\nEnterprise Performance Computing (EPC)\n\nUsage:\n  singularity [global options...]\n\nDescription:\n  Singularity containers provide an application virtualization layer enabling\n  mobility of compute via both application and environment portability. With\n  Singularity one is capable of building a root file system that runs on any\n  other Linux system where Singularity is installed.\n\nOptions:\n  -d, --debug     print debugging information (highest verbosity)\n  -h, --help      help for singularity\n  --nocolor   print without color output (default False)\n  -q, --quiet     suppress normal output\n  -s, --silent    only print errors\n  -v, --verbose   print additional information\n      --version   version for singularity\n ...\n</code></pre> <p>You can use the <code>help</code> command if you want to see the information about subcommands. For example, to see the <code>pull</code> command help,</p> <pre><code>$ singularity help pull\n\nPull an image from a URI\n\nUsage:\nsingularity pull [pull options...] [output file] &lt;URI&gt;\n\nDescription:\nThe 'pull' command allows you to download or build a container from a given\nURI. Supported URIs include:\n\nlibrary: Pull an image from the currently configured library\n    library://user/collection/container[:tag]\n\ndocker: Pull an image from Docker Hub\n    docker://user/image:tag\n\nshub: Pull an image from Singularity Hub\n    shub://user/image:tag\n\noras: Pull a SIF image from a supporting OCI registry\n    oras://registry/namespace/image:tag\n\nhttp, https: Pull an image using the http(s?) protocol\n    https://library.sylabs.io/v1/imagefile/library/default/alpine:latest\n...\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#downloading-pre-built-images","title":"Downloading pre-built images","text":"<p>We already downloaded a pre-built image \"hello-world\" from shub, one of the  registries, using pull command. This is the easiest way to use Singularity.</p> <p>You can use the <code>pull</code> command to download pre-built images from a number of Container Registries. Here we\u2019ll be focusing on the Singularity Hub or Docker Hub. The following are some of container registries.</p> <ul> <li>library - images hosted on Sylabs Cloud</li> <li>shub - images hosted on the (archived) Singularity Hub</li> <li>docker - images hosted on Docker Hub</li> </ul>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#pulling-an-images-from-sylabs-cloud-library","title":"Pulling an images from Sylabs cloud library","text":"<p>In this example, I will pull a base Alpine container from Sylabs cloud:</p> <pre><code>$ singularity pull library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 4.74 MiB/s 0s\n</code></pre> <p>You can rename the container using the <code>--name</code> flag:</p> <pre><code>$ singularity pull --name my_alpine.sif library://sylabsed/linux/alpine\nINFO:    Downloading library image\n 2.08 MiB / 2.08 MiB [===========================================================================================] 100.00% 9.65 MiB/s 0s\n</code></pre> <p>The above example will save the image in the current directory as <code>my_alpine.sif</code></p>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#pulling-an-image-from-docker-hub","title":"Pulling an image from Docker hub","text":"<p>Many programs are available as Docker containers pre-built, and many of those are available on Docker Hub. For more details on using Docker containers with Singularity, see our section on Migrating from Docker to Singularity.</p> <p>Here is a quick example of pulling an Alpine Docker container for use with Singularity.</p> <pre><code>$ singularity pull docker://alpine\nINFO:    Converting OCI blobs to SIF format\nINFO:    Starting build...\nGetting image source signatures\nCopying blob df20fa9351a1 done\nCopying config 0f5f445df8 done\nWriting manifest to image destination\nStoring signatures\n2020/08/20 15:53:52  info unpack layer: sha256:df20fa9351a15782c64e6dddb2d4a6f50bf6d3688060a34c4014b0d9a752eb4c\nINFO:    Creating SIF file...\nINFO:    Build complete: alpine_latest.sif\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#interacting-with-images","title":"Interacting with images","text":"<p>You can interact with images via the <code>shell</code>, <code>exec</code>, and <code>run</code> commands. To learn how to interact with images, let's first pull an image <code>lolcow_latest.sif</code> from the library.</p> <pre><code>singularity pull library://sylabsed/examples/lolcow\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#shell","title":"<code>shell</code>","text":"<p>The <code>shell</code> command allows you to spawn a new shell within your container and interact with it as if it is a virtual machine.</p> <pre><code>$ singularity shell lolcow_latest.sif\nSingularity&gt;    \n</code></pre> <p>The change in prompt indicates that you have entered the container. Once inside of a container, you are the same user as you are on the host system.</p> <pre><code>Singularity&gt; whoami\nchoiyj\n</code></pre> <p>You will also have access to a few directories that you can access outside the container, most notably, the directory you ran the container from and your home directory. Try running <code>pwd</code> and <code>ls ~</code> inside the container to verify this.</p> <p>Anything you write to these directories will stay around after you are done with the container. This is a significant difference from Docker, where the default is to close off the files in the container and you need to use \"bind mounts\" to manually connect file spaces.</p> <p>To exit from a container, type <code>exit</code>.</p> <pre><code>Singularity&gt; exit\n$ \n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#exec","title":"<code>exec</code>","text":"<p>The <code>exec</code> command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the <code>cowsay</code> program within the <code>lolcow_latest.sif</code> container:</p> <pre><code>$ singularity exec lolcow_latest.sif cowsay container camp rocks\n______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre> <p>You can also use <code>shell</code> command to run the program in the container.</p> <pre><code>$ singularity shell lolcow_latest.sif\nSingularity&gt; cowsay container camp rocks\n ______________________\n&lt; container camp rocks &gt;\n ----------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#run","title":"<code>run</code>","text":"<p>Singularity containers contain runscripts. These are predefined scripts which define the actions of a container when user runs it. The runscript can be performed with the <code>run</code> command, or simply by calling the container as though it were an executable.</p> <pre><code>$ singularity run lolcow_latest.sif\n _________________________________________\n/ You're ugly and your mother dresses you \\\n\\ funny.                                  /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>","tags":["tutorial","containers","Singularity"]},{"location":"Singularity_Introduction/#submitting-a-singularity-job","title":"Submitting a Singularity job","text":"<p>In general, running Singularity commands is the same as running any kind of program when you prepare your SLURM script.</p> <p>In this case, you put your Singularity commands (<code>singularity exec &lt;image&gt;.sif &lt;command&gt;</code>) right after the <code>sbatch</code> directive lines you use to specify your job resources. If the program needs to use multiple threads/cores on a node, say 8, you would request 8 cores using <code>#SBATCH --cpus-per-task=8</code> as you would do with any other <code>sbatch</code> script.</p> <p>For situations where you are using MPI within the container (e.g., you would like to split your code over multiple nodes) or would like to use GPU resources, please see Using Singularity with MPI and GPUs on the Advanced Topics page.</p>","tags":["tutorial","containers","Singularity"]},{"location":"Slurm_Environment_Variables/","title":"SLURM Environment Variables","text":"<p>The SLURM controller will set variables in the environment of the batch script. Below is a list of SLURM variables that you may use within your job script. Some variables are only defined if their corresponding options were invoked, such as those pertaining to job arrays or task and CPU configurations. See the curated list of job specifications for more on these options.</p> SLURM Variables Description SLURM_ARRAY_TASK_COUNT Total number of tasks in a job array SLURM_ARRAY_TASK_ID Job array ID (index) number SLURM_ARRAY_TASK_MAX Job array's maximum ID (index) number SLURM_ARRAY_TASK_MIN Job array's minimum ID (index) number SLURM_ARRAY_TASK_STEP Job array's index step size SLURM_ARRAY_JOB_ID Job array's master job ID number SLURM_CLUSTER_NAME Name of the cluster on which the job is executing SLURM_CPUS_ON_NODE Number of CPUS on the allocated node SLURM_CPUS_PER_TASK Number of cpus requested per task. Only set if the <code>--cpus-per-task</code> option is specified. SLURM_JOB_ACCOUNT Account name associated of the job allocation SLURM_JOBID, SLURM_JOB_ID The ID of the job allocation SLURM_JOB_CPUS_PER_NODE Count of processors available to the job on this node. SLURM_JOB_DEPENDENCY Set to value of the --dependency option SLURM_JOB_NAME Name of the job SLURM_NODELIST, SLURM_JOB_NODELIST List of nodes allocated to the job SLURM_NNODES, SLURM_JOB_NUM_NODES Total number of different nodes in the job's resource allocation SLURM_MEM_PER_NODE Takes the value of <code>--mem</code> if this option was specified. SLURM_MEM_PER_CPU Takes the value of <code>--mem-per-cpu</code> if this option was specified. SLURM_NTASKS, SLURM_NPROCS Same as <code>-n</code> or <code>--ntasks</code> if either of these options was specified. SLURM_NTASKS_PER_NODE Number of tasks requested per node. Only set if the <code>--ntasks-per-node</code> option is specified. SLURM_NTASKS_PER_SOCKET Number of tasks requested per socket. Only set if the <code>--ntasks-per-socket</code> option is specified. SLURM_SUBMIT_DIR The directory from which sbatch was invoked SLURM_SUBMIT_HOST The hostname of the computer from which <code>sbatch</code> was invoked SLURM_TASK_PID The process ID of the task being started SLURMD_NODENAME Name of the node running the job script SLURM_JOB_GPUS GPU IDs allocated to the job (if any).","tags":["reference","slurm","job script"]},{"location":"Slurm_Environment_Variables/#setting-environment-variables-in-slurm-jobs","title":"Setting Environment Variables in SLURM Jobs","text":"<p>You may also set your own variables for use in your SLURM jobs.    One way is to set them inside the script itself, but that requires modifying the script. </p> <p>It is possible to pass variables into a SLURM job when you submit the job using the <code>--export</code> flag.  For example to pass the value of the variables <code>REPS</code> and <code>X</code> into the job script named <code>jobs.sb</code> you can use:</p> <pre><code>sbatch --export=REPS=500,X='test' jobs.sb\n</code></pre> <p>These are then available in your jobs as <code>$REPS</code> and <code>$X</code></p>","tags":["reference","slurm","job script"]},{"location":"Slurm_Environment_Variables/#using-variables-to-set-slurm-job-name-and-output-files","title":"Using variables to set SLURM job name and output files","text":"<p>SLURM does not support using arbitrary variables in the <code>#SBATCH</code> lines within a job script; for example, <code>#SBATCH -N=$REPS</code> will not replace <code>$REPS</code> with the variable's value.</p> <p>For specifying filenames (such as the SLURM log/output file), a limited number of pre-defined variables are available. These include <code>%j</code>, which references the job ID. For example, you can use <code>#SBATCH --output=analysis-%j.out</code> to set a custom output filename that includes the job number. The full list of these variables is available in the <code>sbatch</code> documentation</p> <p>Job options specified from the command line have precedence over values defined in the job script and you can set certain SLURM variables in the command line.  For example, you could set the job name and output/error files from the <code>sbatch</code> command line:</p> <pre><code>RUNTYPE='test'\nRUNNUMBER=5\nsbatch --job-name=$RUNTYPE.$RUNNUMBER.run \\\n--output=$RUNTYPE.$RUNUMBER.txt \\\n--export=A=$A,b=$b jobscript.sbatch\n</code></pre> <p>However note in this example, the output file doesn't have the job ID, which is not available from the command line. The job ID is only defined inside the environment created when running the batch script.</p>","tags":["reference","slurm","job script"]},{"location":"Stata/","title":"Stata","text":"<p>Many versions of Stata are installed on the ICER HPC. When you log in, Stata is not available by default, but it may be load easily using this command\u00a0 (note you must type Stata with a capital 'S'</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n</code></pre> <p>This loads Stata SE version 15.\u00a0 This is equivalent to using the command</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.SE\n</code></pre> <p>Stata has a command line version and a GUI (windowed) version.\u00a0To use the command line, type stata at the prompt.\u00a0You will see this :</p> <p>Using Stata command line</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ stata\n\n  ___  ____  ____  ____  ____ (R)\n /__    /   ____/   /   ____/\n___/   /   /___/   /   /___/   15.0   Copyright 1985-2017 StataCorp LLC\n  Statistics/Data Analysis            StataCorp\n                                      4905 Lakeway Drive\n                                      College Station, Texas 77845 USA\n                                      800-STATA-PC        http://www.stata.com\n                                      979-696-4600        stata@stata.com\n                                      979-696-4601 (fax)\n\n15-user Stata network perpetual license:\n       Serial number:  401506213245\n         Licensed to:  iCER / HPCC at Michigan State University\n                       East Lansing, MI\n\nNotes:\n      1.  Unicode is supported; see help unicode_advice.\n\n.\n</code></pre> <p>In which you may type Stata commands.\u00a0Type 'exit' to quit this version.\u00a0</p> <p>To run a Stata do file from the command line in 'batch', you use the syntax</p> <pre><code>[hpc@dev-intel18 ~]$ stata -b do dofilename.do\n</code></pre>"},{"location":"Stata/#versions","title":"Versions","text":"<p>Stata comes in several versions: IC, SE, and MP; see https://www.stata.com/products/which-stata-is-right-for-me/ for details for the differences.\u00a0Stata/IC has limitations on the numbers of variables that affect most users but has no licensing restrictions (see below). While the default version of Stata available when you load the module is Stata/SE, you currently have to use the command 'stata-se' to start the 'SE' version</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.SE\n[hpc@dev-intel18 ~]$ stata-se  # to run the interactive version\n[hpc@dev-intel18 ~]$ stata-se -b do dofilename.do\u00a0 # to run a do file \n</code></pre> <p>However, to use the \"MP\" Version, you must load it explicitly.\u00a0</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata/15.0.MP\n[hpc@dev-intel18 ~]$ stata-mp\n</code></pre> <p>Note that even if you load Stata/MP or SE module, as above, if you just use the command <code>stata</code> it will load the IC version.\u00a0To see which version of Stata you are current in, use the <code>about</code> command at the dot prompt.\u00a0To use these special versions to run a bach do file, use stata-se and stata-mp instead of plain stata</p> <p>For Stata/SE There are 15 licenses available, so 15 users may use it, for MP there are 5 user licenses of 8-cores each. Please exit the program when you are finished with it.\u00a0\u00a0</p>"},{"location":"Stata/#gui-version","title":"GUI version","text":"<p>To use the GUI version, you must first be connected to HPCC with X11 forwarding ( MobaXterm for Windows, XQuartz for Mac - see\u00a0 instructions on installing an SSH client )\u00a0 or using a web-base connection to HPCC (see\u00a0 instructions on Connecting via web site ). Once an X11 or remote desktop client is connected, you can run xstata on a dev node:</p> <pre><code>[hpc@dev-intel18 ~]$ module load Stata\n[hpc@dev-intel18 ~]$ xstata\n</code></pre> <p></p>"},{"location":"Stata/#variables-limits","title":"Variables Limits","text":"<p>Even if you load the MP or SE versions, Stata limits the number of variables to 5000 unless you tell it otherwise.\u00a0\u00a0\u00a0 For information use the help set_maxvar command at the dot prompt.\u00a0\u00a0 You can set the maxvar for your session or in your do file with (for example to 6000)</p> <pre><code>. set maxvar 6000\n</code></pre> <p>There are other settings related to memory usage which are important as Stata attempts to be very conservative.\u00a0 For more information use the Stata \"memory\" command</p>"},{"location":"Stata/#running-jobs","title":"Running Jobs","text":"<p>Note you must use the command line version inside a sb script when running jobs.\u00a0 To copy a working example of Stata job file into your\u00a0 home directory, you can use our getexample tool</p> <p>Getting Stata Example</p> <pre><code>module load powertools\ncd ~\ngetexample STATA_example\ncd STATA_example\n\n# look into the README file in this folder for details\ncat README\n</code></pre>"},{"location":"Stata/#more-help","title":"More help","text":"<p>For questions requiring deeper knowledge of statistics, \u00a0users could contact\u00a0CSTAT services at\u00a0 https://cstat.msu.edu/cstat-services\u00a0and use\u00a0the \"schedule a meeting\" link to submit an intake form.</p>"},{"location":"Submitting-multiple-jobs-simultaneously_40337501.html/","title":"Submitting Multiple Jobs Simultaneously","text":"<ul> <li>Make a copy of the job script multi_seq.sb and name it     multi_sim.sb</li> <li>Edit multi_sim.sb to simultaneously run python_script.py and     r_script.R Be sure to update the resources required to run     multi_sim.sb if necessary.</li> <li> <p>Submit job to compute node</p> <p>Answer </p> </li> </ul> <pre><code>    #Make a copy of the job script multi_seq.sb and name it multi-sim.sb\n    cp multi_seq.sb multi_sim.sb\n\n    #Edit multi_sim.sb to simultaneously run python_script.py and r_script.R\n    gedit multi_sim.sb\n    python3 python_script.py&amp;\n    Rscript r_script.R&amp;\n    wait\n\n    #Note: Be sure to use \u201c&amp;\u201d (otherwise run in sequential) and \u201cwait\u201d (otherwise job exit immediately)\n    #Be sure to update the resources required to run multi-sim.sb if necessary. Note, the number of nodes/cores requested should be the sum of the nodes/cores needed to run each job. In this case since each job uses one node and one core. Requesting one node is sufficient.\n\n    #Submit job to compute node. Type the following at the command line:\n    sbatch multi_sim.sb\n</code></pre>"},{"location":"Submitting_a_TensorFlow_job/","title":"Submitting a TensorFlow job","text":"<p>After you've installed TF in your conda environment, we can submit a TF job to the cluster. In order to make use of GPU computing, we'll need to request for a GPU node in the job script through the <code>--gpu</code> directive.</p> <p>As an example, the TF python code, <code>matmul.tf2.py</code>, is shown below:</p> <pre><code>import tensorflow as tf\ntf.debugging.set_log_device_placement(True)\ntf.config.set_soft_device_placement(True)\n\nwith tf.device('/device:GPU:2'):\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\nprint(c)\n</code></pre> <p>Because <code>tf.config.set_soft_device_placement</code> is turned on, even if this code is assigned a CPU-only node, it will still run. The multiplication step will be carried out using the CPU.</p> <p>Now, let's write our SLURM job script, <code>testTF.sbatch</code>, which contains the following:</p> <pre><code>#!/bin/bash\n\n# Job name:\n#SBATCH --job-name=test_matmul\n#\n# Request GPU:\n#SBATCH --gpus=v100:1\n#\n# Memory:\n#SBATCH --mem-per-cpu=20G\n#\n# Wall clock limit (minutes or hours:minutes or days-hours):\n#SBATCH --time=20\n#\n# Standard out and error:\n#SBATCH --output=%x-%j.SLURMout\n\n\n# [Insert command to load your Conda] -- see https://docs.icer.msu.edu/Using_conda/\n\nconda activate tf_Jul2024\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib:/mnt/home/user123/miniforge3/envs/tf_Jul2024/lib/python3.10/site-packages/tensorrt\n\npython matmul.tf2.py\n\nconda deactivate\n</code></pre> <p>To submit it,  run <code>sbatch testTF.sbatch</code> from the command line.  The final result will be written to the file <code>test_matmul-&lt;jobid&gt;.SLURMout</code>.</p>"},{"location":"TF-GPU/","title":"TF GPU usage","text":"<p>Authorship</p> <p>This guide was written by Siddak Marwaha (ICER student intern from MSU Astrophysics and Data Science, Spring 2023).</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#tensorflow-gpu-usage","title":"TensorFlow GPU Usage","text":"","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#introduction","title":"Introduction","text":"<p>HPCC provides GPU resources for machine learning tasks. GPUs can accelerate the training and inference of deep learning models, allowing for faster experimentation and better performance. TensorFlow is a popular open-source machine learning framework that supports GPU acceleration. This guide will walk you through the steps of utilizing GPU resources on HPCC using TensorFlow.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#setup","title":"Setup","text":"<p>Ensure you have the latest TensorFlow GPU release installed.</p> <pre><code>import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n</code></pre> <p>TensorFlow can perform computations on different types of devices, including CPUs and GPUs. These devices are identified by specific names, such as <code>/device:CPU:0</code> for the CPU and <code>/GPU:0</code> for the first visible GPU, <code>CPU:1</code> and <code>GPU:1</code> for the second and so on.</p> <p>When running TensorFlow operations that have both CPU and GPU implementations, the GPU device is prioritized by default. For example, if you have both a CPU and a GPU available, the GPU will be used to run the operation, unless you specifically request to use the CPU instead. However, if an operation doesn't have a corresponding GPU implementation, then it will fall back to the CPU device. For example, if you have a CPU and a GPU, and you're running an operation that only has a CPU implementation, the operation will run on the CPU even if you requested to run it on the GPU.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#logging-device-placement","title":"Logging device placement","text":"<p>The following code sets a TensorFlow option to log the device used to run each operation, then creates two matrices (<code>a</code> and <code>b</code>) and multiplies them using TensorFlow's built-in matrix multiplication function (<code>tf.matmul</code>). The result of the multiplication is printed to the console. By setting the log option, we can see which device (CPU or GPU) is used to perform the computation:</p> <pre><code>tf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)\n</code></pre> <p>The expected result for GPU used is:</p> <pre><code>Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\ntf.Tensor(\n[[22. 28.]\n [49. 64.]], shape=(2, 2), dtype=float32)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#manual-device-placement","title":"Manual device placement","text":"<p>If you want to choose a specific device for an operation instead of letting TensorFlow automatically select it for you, you can use the tf.device function. This creates a context where all operations inside it will run on the same device you choose. However, by default TensorFlow will use a GPU if it is available and configured properly.</p> <p>Again, the following code sets <code>log_device_placement</code> to True, which will cause TensorFlow to print the assigned device for each operation. Then, it places two constant tensors 'a' and 'b' on the CPU using the <code>with tf.device('/CPU:0')</code> block. Finally, it multiplies 'a' and 'b'. This demonstrates how to explicitly place tensors on specific devices and how TensorFlow prioritizes GPU over CPU when both are available:</p> <pre><code>tf.debugging.set_log_device_placement(True)\n\n# Place tensors on the CPU\nwith tf.device('/CPU:0'):\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Run on the GPU\nc = tf.matmul(a, b)\nprint(c)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#limiting-tf-to-certain-gpus","title":"Limiting TF to certain GPUs","text":"<p>If you want to use only certain GPUs, you can use the <code>tf.config.set_visible_devices</code> method to limit TensorFlow to those GPUs. This can help avoid memory fragmentation and ensure that the specific GPUs you want to use are available. </p> <p>The following code checks if there are any GPUs available on the system by listing the physical devices. If there are GPUs available, it restricts TensorFlow to only use the first GPU by setting it as the visible device. It also lists the logical devices to confirm the GPU usage. If there are any errors, such as the visible devices being set after the GPUs have already been initialized, it will catch the error and print it. The purpose of this code is to manage the available GPUs and ensure that TensorFlow uses them efficiently:</p> <pre><code>gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n</code></pre> <p>Example output:</p> <pre><code>1 Physical GPUs, 1 Logical GPU\n</code></pre> <p>Note</p> <p>Physical vs Logical devices: physical devices refer to the actual hardware components such as a GPU or CPU that are present in the system. On the other hand, logical devices refer to the virtual representations of these physical devices that are exposed to TensorFlow for computation. When TensorFlow is initialized on a machine with GPUs, it detects the available physical devices and creates a logical device for each physical device. Each logical device can have multiple components, such as a GPU with multiple cores, and it may also have a subset of the memory of the physical device. Logical devices are used by TensorFlow to distribute and manage the computation across the available physical devices in an efficient manner.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-a-single-gpu-on-a-multi-gpu-system","title":"Using a single GPU on a multi-GPU system","text":"<p>If you have multiple GPUs, TensorFlow will use the one with the lowest ID number by default. If you want to use a different GPU, you need to tell TensorFlow which one to use specifically. The following code attempts to perform a matrix multiplication operation between two TensorFlow constant tensors using a non-existent GPU device <code>/device:GPU:4</code>. Since this device does not exist, it should raise a RuntimeError exception. The code also sets <code>tf.debugging.set_log_device_placement(True)</code> to log the placement of operations on devices. So, if the program runs successfully, it will log which device the operation ran on. If you try to run operations on a specific GPU device that does not exist, you will get a runtime error. </p> <pre><code>tf.debugging.set_log_device_placement(True)\n\ntry:\n  # Specify an invalid GPU device\n  with tf.device('/device:GPU:4'):\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    c = tf.matmul(a, b)\nexcept RuntimeError as e:\n  print(e)\n</code></pre> <p>You can use <code>tf.config.set_soft_device_placement(True)</code> to instruct TensorFlow to automatically choose a supported device to run the operations in case the specified device is not available. This can help make your code more flexible and robust in case the availability of GPU devices changes over time.</p> <p>Note</p> <p>Eager vs Graph execution modes: since TensorFlow 2.0, the eager execution is the default and soft device placement is enabled by default when running in eager mode. Therefore, with the above code snippet running in the eager mode, you won't get an error even without having <code>tf.config.set_soft_device_placement(True)</code>. However, for complex model training, graph execution has the advantages of being faster, more flexible, and robust. If you opt to use it, you will need to enable soft device placement.</p> <p>The first two lines of the following code enable TensorFlow to choose a device to run operations on, and then log where each operation is executed:</p> <pre><code>tf.config.set_soft_device_placement(True)\ntf.debugging.set_log_device_placement(True)\n\n# Creates some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\nprint(c)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-multiple-gpus","title":"Using multiple GPUs","text":"<p>Developing machine learning models to work with multiple GPUs allows the model to use additional resources and potentially scale better. However, if you only have a single GPU available, you can still simulate multiple GPUs using virtual devices. This makes it easier to test and develop for multi-GPU setups without needing additional physical GPUs.</p> <p>The following code is creating two virtual GPUs with 1GB memory each. It first lists the physical GPUs available on the system using <code>tf.config.list_physical_devices('GPU')</code>. If there are GPUs available, it uses <code>tf.config.set_logical_device_configuration()</code> to create two logical devices with a memory limit of 1024 MB (1GB) each on the first physical GPU. The code then lists the logical GPUs created with <code>tf.config.list_logical_devices('GPU')</code> and prints the number of physical and logical GPUs. If there is an error setting up the virtual devices, the error message is printed:</p> <pre><code>gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Create 2 virtual GPUs with 1GB memory each\n  try:\n    tf.config.set_logical_device_configuration(\n        gpus[0],\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n</code></pre> <p>Output:</p> <pre><code>1 Physical GPU, 2 Logical GPUs\n</code></pre> <p>This output indicates that there is one physical GPU available on the system, and two logical GPUs have been created on that physical GPU using virtual devices. Each logical GPU has a memory limit of 1GB. The code successfully created the virtual devices without any errors. </p> <p>Once there are multiple logical GPUs available to the runtime, you can utilize the multiple GPUs with <code>tf.distribute.Strategy</code> or with manual placement.</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#using-tfdistributestrategy","title":"Using tf.distribute.Strategy","text":"<p>The best practice for using multiple GPUs is to use <code>tf.distribute.Strategy</code>. The next code sets up a mirrored strategy for training a neural network model on multiple GPUs. It first enables device placement logging, then lists the logical GPUs available to the runtime. It creates a <code>MirroredStrategy</code> object, which distributes the training across multiple GPUs. The <code>with strategy.scope()</code> block defines the model architecture and compiles it with a mean squared error loss and stochastic gradient descent optimizer:</p> <pre><code>tf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy(gpus)\nwith strategy.scope():\n  inputs = tf.keras.layers.Input(shape=(1,))\n  predictions = tf.keras.layers.Dense(1)(inputs)\n  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n  model.compile(loss='mse',\n                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n</code></pre> <p>The program is using multiple GPUs to process the data faster by splitting the input data between the GPUs and running a copy of the model on each GPU. This approach is called \"data parallelism\".</p>","tags":["TensorFlow","GPU"]},{"location":"TF-GPU/#manual-placement","title":"Manual placement","text":"<p><code>tf.distribute.Strategy</code> is a tool that allows you to replicate your model on multiple devices, which can improve performance. You can also achieve the same thing manually by building your model on each device. The following program demonstrates how to manually replicate computation across multiple GPUs. It creates copies of a matrix multiplication operation on each available GPU and then adds the results of those computations on the CPU to obtain the final result. It also uses <code>tf.debugging.set_log_device_placement(True)</code> to print the placement of each operation to the console for debugging purposes:</p> <pre><code>tf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nif gpus:\n  # Replicate your computation on multiple GPUs\n  c = []\n  for gpu in gpus:\n    with tf.device(gpu.name):\n      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      c.append(tf.matmul(a, b))\n  with tf.device('/CPU:0'):\n    matmul_sum = tf.add_n(c)\n  print(matmul_sum)\n</code></pre> <p>Example Output:</p> <pre><code>Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AddN in device /job:localhost/replica:0/task:0/device:CPU:0\ntf.Tensor(\n[[ 88. 112.]\n [196. 256.]], shape=(2, 2), dtype=float32)\n</code></pre>","tags":["TensorFlow","GPU"]},{"location":"TFcode/","title":"TensorFlow model training code for testing","text":"<p>This page assumes that you've followed the instructions to install TensorFlow using conda and successfully installed TF in your conda environment. Below we provide more TF model training code for you to fully test your installation. Remember to log onto <code>dev-amd20-v100</code>.</p> <p>All the code should be typed in (or copy-paste) to an interactive python interpreter, after running the first four lines below from your terminal. After you are done and have quit the python session, remember to deactivate your conda environment via <code>conda deactivate</code>.</p> <pre><code># [Insert command to load your Conda] -- see https://docs.icer.msu.edu/Using_conda/\n\nconda activate tf_Jul2024\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib:/mnt/home/user123/miniforge3/envs/tf_Jul2024/lib/python3.10/site-packages/tensorrt\n\npython\n\n# [Insert your python code after &gt;&gt;&gt; the prompt of the interactive Python interpreter]\n\nconda deactivate\n</code></pre> <p>Code 1</p> <pre><code>import tensorflow as tf\nimport numpy as np\n\n# Generate random data\nx = np.random.rand(100, 10)\ny = np.random.randint(0, 2, size=(100,))\n\n# Define the model architecture\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x, y, epochs=10, validation_split=0.2)\n\n# Evaluate the model on test data\ntest_loss, test_acc = model.evaluate(x, y, verbose=2)\nprint(f'Test accuracy: {test_acc}')\n</code></pre> <p>Code 2</p> <pre><code>import tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)\n</code></pre> <p>Code 3</p> <pre><code>import tensorflow as tf\nfrom tensorflow.keras.datasets import boston_housing\n\n(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu', input_shape=(13,)),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\nhistory = model.fit(x_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n\ntest_loss, test_mae = model.evaluate(x_test, y_test)\n\nprint(f'Test MAE: {test_mae:.2f}')\n</code></pre> <p>Special thanks to Siddak Marwaha who provided the above code.</p>","tags":["how-to guide","TensorFlow"]},{"location":"Targeting_Cluster_Architectures/","title":"Targeting Cluster Architectures","text":"<p>While all HPCC nodes are the x86_64 architecture, some newer processors have features that are not supported by older processors. A program compiled on a newer processor may not run on an older processor and may result in an 'Illegal Instruction' error. This can be corrected by specifying compilers parameters that control which processor instruction are used.</p> <p>Use the following options for your compiler to ensure your programs will run on all HPCC nodes.</p> Compiler Type Min Version Max Version Arguments GCC 6.4 N/A -march=core-avx-i -mtune=skylake-avx512 GCC 4.9 \\&lt; 6.4 -march=core-avx-i -mtune=silvermont GCC 4.8 \\&lt; 4.9 -march=core-avx-i -mtune=core-avx2 GCC 4.6 \\&lt; 4.8 -march=core-avx-i GCC 4.3 \\&lt; 4.6 -march=core2 GCC 3.3 \\&lt; 4.3 -march=nocona Intel 2015.1 N/A -mAVX -axCORE-AVX-I,CORE-AVX2,CORE-AVX512 <p>The new amd20 cluster does not support AVX-512.</p>"},{"location":"Torque_vs._SLURM/","title":"Migrating to SLURM from TORQUE","text":"<p>While the HPCC uses the SLURM resource manager, users may be familiar with the TORQUE resource manager from  running jobs on other systems. </p> <p>SLURM handles resource requests slightly differently than TORQUE. TORQUE frames requests in terms of CPUs; for instance, <code>#PBS -l ppn=&lt;count&gt;</code> specifies the desired number of CPUs (processors) per node. On the other hand, SLURM frames requests in terms of tasks; e.g., <code>#SLURM --ntasks-per-node=&lt;count&gt;</code>. A task may then have multiple CPUs assigned to it for the purposes of threading: <code>#SBATCH --cpus-per-task=&lt;count&gt;</code>.</p> <p>We recommend reviewing our pages on writing and submitting job scripts, the list of SLURM resource specifications, and our example SLURM scripts to help with the transition to SLURM. Users may also benefit from this side by side comparison between TORQUE and SLURM options, environment variables, and commands.</p>","tags":["explanation","torque","slurm"]},{"location":"Transferring_Data_with_Google_using_Globus/","title":"Transferring Data with Google using Globus","text":""},{"location":"Transferring_Data_with_Google_using_Globus/#initial-setup","title":"Initial setup","text":"<p>Go to the Globus File Manager page and log in with your MSU credentials if necessary. For this example, we will transfer files between the ICER's HPCC and MSU Google endpoints. First, connect to your drive space on ICER's HPCC. Then, in the search box (black arrow), enter msu#google.</p> <p></p> <p>Select the endpoint msu#google.</p> <p></p> <p>At this point you may be asked for permission to allow Globus access to your Google drive.</p> <p></p> <p>Click Continue (orange oval):</p> <p></p> <p>Click the wrench under status heading (orange oval).</p> <p></p> <p>Click Allow (orange oval).</p> <p></p> <p>You will see your status changed (orange oval). You now have access to your Google Drive. Click File Manager in the upper left corner (black oval) to return to the File Manager main page.</p> <p></p> <p>You now can transfer files between your ICER HPCC drive space and your Google drive space, and you will see their respective contents in a split screen. You can transfer folders and files using the two methods, 1)\u00a0Drag and drop (not shown here), or 2)\u00a0Transfer &amp; Sync Options (orange oval and black arrow).</p> <p></p> <p>Please note that Google enforces a transfer limit per person per minute. If you see warnings that connection limit has been reached the transfer will still work but be slower. Also, by default, Globus does not allow access to your Google files, as seen in the message received if you attempt to share (purple arrow above) a file or folder from your Google Drive.</p> <p></p> <p>While it is possible to allow other users to access your collection from Google Drive if you change the permissions, it is not recommended for data security reasons.</p>"},{"location":"Transferring_Data_with_Google_using_Globus/#transferring-from-google-shared-filesdrives","title":"Transferring from Google shared files/drives.","text":"<p>It is also possible to transfer files/folders that are in the \"Shared with me\" section of Google Drive, or from \"Shared Drives\".   Globus by default shows your own Google drive files, but you can access those shared files/drives by clicking the \"up a level\" button in the file browser circled in the following screen shot.  The \"Path\" box should be just a single slash \"/\" and you will see \"My Drive\", \"Shared With Me\" and \"Team Drives.\"   \"Team Drives\" is the same as \"Shared Drives\" (Google's previous name for Shared Drives).</p> <p></p>"},{"location":"Transferring_data_with_Globus/","title":"Transferring data with Globus","text":"","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#what-is-globus","title":"What is\u00a0Globus?","text":"<p>Globus is a\u00a0free service to the MSU community\u00a0for secure, reliable research data management. Globus\u00a0gives\u00a0users\u00a0the ability to\u00a0move\u00a0and\u00a0share data\u00a0regardless of\u00a0user or\u00a0file location\u00a0through a single web browser-based interface.\u00a0Users\u00a0can manage data from any device (e.g.,\u202fsupercomputer, tape archive, lab cluster or equipment, public cloud, or personal computer/laptop) from anywhere in the world\u00a0using\u00a0their\u00a0existing\u00a0institutional\u00a0identities.\u00a0Use of Globus\u00a0removes data management roadblocks by providing unified access to all storage locations, making\u00a0it easy\u00a0to work with data while ensuring reliability\u00a0and security.\u00a0For more information on\u00a0Globus, check out  the Globus website here.</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#why-should-i-use-globus","title":"Why\u00a0should\u00a0I use\u00a0Globus?","text":"<p>Globus is ideal for moving large files and data transfers between ICER\u2019s HPCC or external research collaborators\u00a0because of its truly fire-and-forget method of transferring data.\u00a0After you\u00a0initiate\u00a0a file transfer,\u00a0Globus will work on your behalf to optimize transfer performance, monitor for transfer completion and correctness, and recover from network errors, credential expiration, and collection downtime without restarting the transfer. This allows you to\u00a0navigate away from File Manager, close the browser window, and even logout.\u00a0</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#transferring-data-with-globus_1","title":"Transferring data with Globus","text":"<p>The HPCC has a Globus data transfer endpoint, <code>msuhpcc</code>. This can be used to do large data transfers to/from your personal computer, to/from collaborators or to/from external HPC sites. \u00a0You can also use it to share data.</p> <p>With Globus, you can create a transfer (to or from) between your computer and a folder on the HPCC which you have access to. This could be a folder in your home, research or scratch spaces.</p> <p>You can also create Globus \"shares\" to your external colleagues. They can use your created link to access the HPCC directory. **You can not create a globus share on any folder in your home directory.\u00a0** You can only share folders in scratch space or any research space you have access to.\u00a0\u00a0 \u00a0</p> <p>To use Globus Online, please perform the following steps:</p> <ol> <li>If you do not have a globus account, create one     at\u00a0https://www.globus.org</li> <li>Log into the MSU Globus Online portal,     https://globus.msu.edu\u00a0and set up\u00a0a free Globus     Online account.</li> <li>If you wish to use Globus to transfer data to/from your local     computer, install the Globus Connect Personal tool.</li> <li>On the Globus\u00a0Start Transfer page,      enter <code>msuhpcc</code>\u00a0as the end point on one side. This will pull up an authentication window. Use your MSU NetID for the     username, and your MSU NetID password for the passphrase. This will     set up an authenticated session that will last as long as specified     in the Credential Lifetime field, up to a limit of 2 weeks.</li> <li>Select and authenticate with the other endpoint for your transfer,     and initiate your transfer. An example can be seen from     How To Transfer Files with Globus.</li> <li>To share data in a HPCC folder accessible to you with other     persons,\u00a0 please check     How To Share Data Using Globus.</li> </ol> <p>More information about using Globus can be found on\u00a0Data transfer and sharing using Globus or Globus support page.</p> <p>For further training on the Science DMZ and how to use Globus, please self-enroll in ICER's DMZ Globus Training\u00a0D2L course.</p>","tags":["how-to guide","globus"]},{"location":"Transferring_data_with_Globus/#icers-dmz-globus-training-globus-walkthrough-documents-optional","title":"ICER's DMZ Globus Training - Globus Walkthrough Documents (optional)","text":"<ol> <li> <p>Find and Connect to HPCC using Globus</p> </li> <li> <p>Transfer from PC to HPCC using Globus</p> </li> <li> <p>Transferring Data between Endpoints Using Globus</p> </li> <li> <p>Transferring Data with Google using Globus</p> </li> <li> <p>Sharing Data using Globus</p> </li> </ol>","tags":["how-to guide","globus"]},{"location":"Trimmomatic/","title":"Trimmomatic","text":"<p>Trimmomatic is a tool for trimming Illumina FASTQ data and removing adapters. \u00a0When data is sequenced on Illumina, adapters are added for the fragments to attach to the beads. \u00a0If these adapters are not removed they can result in false assembly or other issues. \u00a0Additionally, the quality of the sequences varies across the length of the read, and poorer quality regions can be trimmed using Trimmomatic. \u00a0Running Trimmomatic is a good first step in quality filtering your Illumina data.\u00a0</p> <p>To run it on the HPCC (for example trimming paired-end reads):</p> <pre><code>java -jar /opt/software/Trimmomatic/0.36-Java-1.8.0_92/trimmomatic-0.36.jar PE [-threads &lt;threads] [-phred33 | -phred64] [-trimlog &lt;logFile&gt;] &lt;input 1&gt; &lt;input 2&gt; &lt;paired output 1&gt; &lt;unpaired output 1&gt; &lt;paired output 2&gt; &lt;unpaired output 2&gt; &lt;step 1&gt; ...\n</code></pre> <p>Read\u00a0the manual\u00a0for how to use it. Note that\u00a0the adapter sequence files are in\u00a0<code>/opt/software/Trimmomatic/0.36-Java-1.8.0_92/adapters/</code>. If you couldn't find the adapters you need in that directory, you will need to obtain them from elsewhere (for example asking the person who ran the library prep for you).</p>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/","title":"Trinity for RNA-seq de novo assembly","text":""},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#loading-module","title":"Loading module","text":"<p>Take loading Trinity 2.6.6 as an example, we run:</p> <pre><code>module purge\nmodule load icc/2017.4.196-GCC-6.4.0-2.28 impi/2017.3.196 Trinity/2.6.6\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#most-basic-run-transcript-assembly","title":"Most basic run (transcript assembly)","text":"<p>A typical Trinity command for assembling strand-specific paired-end RNA-seq data would look like:</p> <p>A typical run of Trinity</p> <pre><code>Trinity \\\n  --seqType fq \\\n  --max_memory 2G \\\n  --left reads.left.fq \\\n  --right reads.right.fq \\\n  --SS_lib_type RF \\\n  --CPU 10\n</code></pre> <p>This will generate output files in a new directory <code>trinity_out_dir</code> in the working directory. Among them, the assembled transcripts file is \"<code>Trinity.fasta</code>\". For more detail, check out\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki.</p> <p>When you submit the above command as a job to the cluster, you need to request 10 CPUs in the sbatch script with the following lines (in addition to your other sbatch directives):</p> <p>sbatch code snippet</p> <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=10\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#transcript-quantification","title":"Transcript quantification","text":"<p>Trinity provides abundant utility scripts for post-assembly analysis, such as quality assessment, transcript quantification and differential expression tests. For some of them, external software tools need to be installed separately (that is, they are not bundled with Trinity). For example, for the transcript quantification step, we will need one of RSEM, eXpress, kalllisto and salmon (cf.\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification). We have made all these four available on the HPCC. As instructed by Trinity, \"the tools should be available via your PATH setting\". So, in the next example where we choose to use RSEM to align reads to the assembled transcript and then quantify transcript abundance, we first set the\u00a0<code>PATH</code>\u00a0variable so that RSEM can be automatically searched for by trinity.</p> <p>Using RSEM for transcript quantification</p> <pre><code># Assuming\n#    1) you've loaded Trinity module already and\n#    2) your current working directory is trinity_out_dir generated from the previous assembly step. \n\nexport PATH=/opt/software/RSEM/1.3.1-GCCcore-6.4.0/usr/local/bin:$PATH\n\n/opt/software/Trinity/2.6.6/util/align_and_estimate_abundance.pl --seqType fq --transcripts Trinity.fasta \\\n    --est_method RSEM \\\n    --left ../reads.left.fq \\\n    --right ../reads.right.fq \\\n    --SS_lib_type RF \\\n    --aln_method bowtie \\\n    --trinity_mode \\\n    --prep_reference \\\n    --thread_count 10 \\\n    --output_dir RSEM_out\n</code></pre> <p>The RSEM computation generates two primary output files containing estimated abundances in the subdirectory\u00a0<code>RSEM_out</code>\u00a0as specified in the command above:\u00a0<code>RSEM.isoforms.results</code>\u00a0(transcript level) and\u00a0<code>RSEM.genes.results</code>\u00a0(gene level).</p>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#more-utilities","title":"More utilities","text":"<p>Please consult\u00a0https://github.com/trinityrnaseq/trinityrnaseq/wiki for detail.</p> <p>Note that a few R packages are needed for differential expression analysis (https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Differential-Expression). These have been installed in\u00a0<code>R/4.0.2</code>\u00a0which can be loaded by</p> <pre><code>module purge; module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2\n</code></pre>"},{"location":"Trinity_for_RNA-seq_de_novo_assembly/#version-note","title":"Version note","text":"<p>The latest version is 2.91. After loading it, you may load R 4.0.2 for DE analysis.</p> <p><code>module purge</code> <code>module load GCC/8.3.0 OpenMPI/3.1.4 R/4.0.2 Trinity/2.9.1</code> <code>module load R/4.0.2</code></p>"},{"location":"User_Created_Modules/","title":"User Created Modules","text":"<p>Info</p> <p>This is for advanced usage, since creating your own module files for software access is usually not necessary.</p> <p>If you develop or install your own software, you might consider writing a modulefile to help manage your environment variables. HPCC presently uses the LMOD module package, developed at TACC. The following is a typical module file with comments. Name your files with a <code>.lua</code> extension.</p> <pre><code>-- -*- lua -*- \n help( \n [[ \n Describe your software here. \n ]]) \n\n -- comments are prefaced with two dashes \n\n whatis(\"Description: Name of software\") \n whatis(\"URL:  www.ucc.org \") \n\n local install_path = \"/mnt/home/ongbw/opt/mysoftware\" \n\n -- set an environment variable \n setenv(\"MYSOFTWARE_HOME\",install_path) \n\n -- add to PATH variable \n prepend_path('PATH', pathJoin(install_path,\"bin\")) \n\n -- add Library Paths \n prepend_path('LD_LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n prepend_path('LIBRARY_PATH',pathJoin(install_path,\"lib\")) \n\n -- add include paths \n prepend_path('INCLUDE',pathJoin(install_path,\"include\"))\n</code></pre> <p>For more information, see the Lmod documentation</p>","tags":["reference","modules"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/","title":"Using BLAS and FlexiBLAS to speed up linear algebra operations","text":"","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#what-is-blas","title":"What is BLAS?","text":"","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#overview","title":"Overview","text":"<p>BLAS (or Basic Linear Algebra Subprograms) is used to perform linear algebra operations (like vector addition, matrix multiplication, etc) on computers. Since these operations are the basic building blocks of most algorithms, how they work can dramatically impact the speed of most programs.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#implementations","title":"Implementations","text":"<p>BLAS is actually a \"specification\", meaning it is only list of possible operations and a common syntax for calling them. There are then different \"implementations\" of BLAS that determine how the operations are done and are generally highly-optimized. Commonly used implementations include:</p> <ul> <li>OpenBLAS</li> <li>The Intel Math Kernel Library</li> <li>BLIS</li> <li>ATLAS</li> </ul> <p>Each of these implementations approaches optimizing linear algebra operations differently, and can obtain different performance results in different situations. There is also a \"reference implementation\" of BLAS provided by Netlib, the organization which creates the BLAS specification. This reference implementation is meant to demonstrate how a BLAS implementation should work rather than being performant and is not recommended in practice.</p> <p>BLAS implementations are usually installed as a static (<code>.a</code> file) or dynamic (or <code>.so</code> file) library that other programs can then \"link to\" when they are compiled.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#how-blas-is-used-in-practice","title":"How BLAS is used in practice","text":"<p>Code can be written that interfaces with BLAS directly, but it's usually more common to use a piece of code that calls the BLAS for you. For example, the <code>numpy</code> package in Python performs its linear algebra operations with a BLAS implementation when available. Similarly, most linear algebra operations in R and MATLAB call a BLAS implementation behind the scenes.</p> <p>Note that even if you're not doing linear algebra operations directly, there's a very good chance that code you're using does somewhere along the way!</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#what-is-flexiblas","title":"What is FlexiBLAS?","text":"<p>FlexiBLAS is not a BLAS implementation, but is an easy way to switch between implementations.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#compiling-with-blas","title":"Compiling with BLAS","text":"<p>When a piece of code uses BLAS, it usually needs to link to an implementation. This example would link <code>mycode.c</code> on the HPCC to the OpenBLAS dynamic library:</p> <pre><code>module load GCC OpenBLAS\ngcc -lopenblas mycode.c -o mycode\n</code></pre> <p>You could also compile the same code with the Intel Math Kernel Library (or IMKL) (note the extra options which are provided by Intel):</p> <pre><code>module load GCC imkl\ngcc -m64  -lmkl_rt -Wl,--no-as-needed -lpthread -lm -ldl mycode.c -o mycode\n</code></pre> <p>The compiled code is then fixed to use that implementation of BLAS.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#compiling-with-flexiblas","title":"Compiling with FlexiBLAS","text":"<p>FlexiBLAS lets you swap between multiple implementations. This lets you compile the code once with FlexiBLAS as the \"implementation\", which is really just a layer in the middle that points to the real implementation in the background:</p> <pre><code>module load GCC FlexiBLAS\ngcc -lflexiblas mycode.c -o mycode\n</code></pre>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#using-flexiblas","title":"Using FlexiBLAS","text":"<p>When code is compiled with FlexiBLAS, you can swap BLAS implementations (or as FlexiBLAS calls them, \"backends\") out at runtime. For more information about using FlexiBLAS not covered here, try running <code>flexiblas help</code> or see the FlexiBLAS documentation.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#listing-available-blas-backends","title":"Listing available BLAS backends","text":"<p>When ICER installs FlexiBLAS and different BLAS implementations, we tell FlexiBLAS what backends are available. These can be seen with</p> input<pre><code>flexiblas list\n</code></pre> output<pre><code>System-wide:\nSystem-wide (config directory):\n BLIS\n   library = libflexiblas_blis.so\n   comment = \n IMKL\n   library = libflexiblas_imkl_gnu_thread.so\n   comment = \n IMKL_SEQ\n   library = libflexiblas_imkl_sequential.so\n   comment = \n NETLIB\n   library = libflexiblas_netlib.so\n   comment = \n OPENBLAS\n   library = libflexiblas_openblas.so\n   comment = \nUser config:\nHost config:\nEnviroment config:\n</code></pre> <p>The default backend is set by ICER to be <code>OPENBLAS</code>.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#swapping-blas-backends-at-runtime","title":"Swapping BLAS backends at runtime","text":"<p>Assuming the <code>mycode</code> executable has been compiled with FlexiBLAS, running it as is will use the default backend (OpenBLAS):</p> <pre><code>./mycode\n</code></pre> <p>To use a different backend, set the environment variable <code>FLEXIBLAS</code> to that backend name:</p> <pre><code>FLEXIBLAS=IMKL ./mycode\n</code></pre> <p>Make sure the module containing that implementation is loaded</p> <p>FlexiBLAS can't tell whether an backend is actually installed and working until it tries to run. In the above example using <code>FLEXIBLAS=IMKL</code>, you would need to manually load the <code>imkl</code> module that provides the correct libraries beforehand with</p> <pre><code>module load imkl\n</code></pre> <p>If the BLAS backend you request is not loaded, FlexiBLAS will fall back to the (very slow) reference implementation.</p> <p>You can also pass in the location of a BLAS implementation dynamic library file:</p> <pre><code>FLEXIBLAS=myblas.so ./mycode\n</code></pre>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#setting-a-default-flexiblas-backend","title":"Setting a default FlexiBLAS backend","text":"<p>If you prefer to use a different backend of BLAS than OpenBLAS at all times, you can set your local configuration with:</p> <pre><code>flexiblas default &lt;IMPLEMENTATION&gt;\n</code></pre> <p>For example, using</p> <pre><code>flexiblas default IMKL\n</code></pre> <p>will always use the Intel Math Kernel Library. Note that you still need to load the <code>imkl</code> module so that FlexiBLAS can find the library (see the note above)!</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#benchmarking-blas-implementations-with-flexiblas","title":"Benchmarking BLAS Implementations with FlexiBLAS","text":"","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#using-python","title":"Using Python","text":"<p>As mentioned above, <code>numpy</code>'s linear algebra operations are performed using BLAS. ICER has installed <code>numpy</code> with FlexiBLAS, so you can choose your BLAS backend.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#benchmark-script","title":"Benchmark script","text":"<p>The results below are run on <code>dev-intel14-k20</code> and computed using the following script:</p> <pre><code>#!/bin/bash\n\nmodule purge\nmodule load SciPy-bundle imkl BLIS\n\nBACKENDS=\"OPENBLAS BLIS IMKL IMKL_SEQ NETLIB\"\n\nfor BACKEND in $BACKENDS; do\n    echo $BACKEND\n    export FLEXIBLAS=$BACKEND \n    time python -c \"import numpy as np; x = np.random.randn(5000, 5000); x @ x.T\"\ndone\n</code></pre> <p>This script computes the cross product (the matrix times its transpose) of a 5000-by-5000 matrix of normally distributed random values. Note that we need to load the <code>imkl</code> and <code>BLIS</code> modules in addition to <code>SciPy-bundle</code> to use them with FlexiBLAS. <code>FlexiBLAS</code> and its default backend <code>OpenBLAS</code> are loaded as dependencies of <code>SciPy-bundle</code> and do not need to be specified.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#results","title":"Results","text":"<p>The output from one run of the script is copied below</p> <pre><code>OPENBLAS\n\nreal    0m6.410s\nuser    0m6.159s\nsys 0m0.170s\n\nBLIS\n\nreal    0m6.733s\nuser    0m6.587s\nsys 0m0.132s\n\nIMKL\n\nreal    0m1.606s\nuser    0m7.572s\nsys 0m0.338s\n\nIMKL_SEQ\n\nreal    0m6.633s\nuser    0m6.475s\nsys 0m0.116s\n\nNETLIB\n\nreal    1m2.583s\nuser    1m2.442s\nsys 0m0.120s\n</code></pre> <p>The <code>real</code> time is the time that you would wait for the code to run, whereas the <code>user</code> time is the time that that the process was running on the CPUs. Note that the <code>real</code> time can be less than the <code>user</code> time when code is run in parallel (as in the case of <code>IMKL</code>).</p> <p>The <code>elapsed</code> time shows that for this specific cross-product example, <code>IMKL</code> is fastest, <code>OPENBLAS</code>, <code>BLIS</code>, and <code>IMKL_SEQ</code> (the non-parallel version of <code>IMKL</code>) are comparable, and <code>NETLIB</code> is much slower.</p> <p>Note that this is only one small example with no extra configuration and results will vary by the type of operation and hardware used. We encourage you to explore more about the benefits of each backend and how they may be configured for your situation and experiment with different options.</p> <p>For example, <code>BLIS</code> allows for parallelization similar to <code>IMKL</code> with an extra command line argument that produces a similar result:</p> input<pre><code>export BLIS_NUM_THREADS=8 &amp;&amp; export FLEXIBLAS=BLIS &amp;&amp; time python -c \"import numpy as np; x = np.random.randn(5000, 5000); x @ x.T\"\n</code></pre> output<pre><code>real    0m2.010s\nuser    0m7.302s\nsys 0m0.202s\n</code></pre>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#using-r","title":"Using R","text":"<p>As mentioned above, R's linear algebra operations are performed using BLAS. ICER has installed R with FlexiBLAS, so you can choose your BLAS backend.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#benchmark-script_1","title":"Benchmark script","text":"<p>The results below are run on <code>dev-intel14-k20</code> and computed using the following script:</p> <pre><code>#!/bin/bash\n\nmodule purge\nmodule load R imkl BLIS\n\nBACKENDS=\"OPENBLAS BLIS IMKL IMKL_SEQ NETLIB\"\n\nfor BACKEND in $BACKENDS; do\n    echo $BACKEND\n    FLEXIBLAS=$BACKEND Rscript -e \"system.time({x &lt;- replicate(5e3, rnorm(5e3)); tcrossprod(x) })\"\ndone\n</code></pre> <p>This script computes the cross product (the matrix times its transpose) of a 5000-by-5000 matrix of normally distributed random values. Note that we need to load the <code>imkl</code> and <code>BLIS</code> modules in addition to <code>R</code> to use them with FlexiBLAS. <code>FlexiBLAS</code> and its default backend <code>OpenBLAS</code> are loaded as dependencies of R and do not need to be specified.</p>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_BLAS_and_FlexiBLAS_to_speed_up_linear_algebra_operations/#results_1","title":"Results","text":"<p>The output from one run of the script is copied below</p> <pre><code>OPENBLAS\n   user  system elapsed\n  7.512   0.497   8.009\nBLIS\n   user  system elapsed\n  7.728   0.307   8.035\nIMKL\n   user  system elapsed\n  9.158   0.451   3.119\nIMKL_SEQ\n   user  system elapsed\n  7.765   0.312   8.078\nNETLIB\n   user  system elapsed\n 50.242   0.481  50.732\n</code></pre> <p>The <code>elapsed</code> time is the time that you would wait for the code to run, whereas the <code>user</code> time is the time that that the process was running on the CPUs. Note that the <code>elapsed</code> time can be less than the <code>user</code> time when code is run in parallel (as in the case of <code>IMKL</code>).</p> <p>The <code>elapsed</code> time shows that for this specific cross-product example, <code>IMKL</code> is fastest, <code>OPENBLAS</code>, <code>BLIS</code>, and <code>IMKL_SEQ</code> (the non-parallel version of <code>IMKL</code>) are comparable, and <code>NETLIB</code> is much slower.</p> <p>Note that this is only one small example with no extra configuration and results will vary by the type of operation and hardware used. We encourage you to explore more about the benefits of each backend and how they may be configured for your situation and experiment with different options.</p> <p>For example, <code>BLIS</code> allows for parallelization similar to <code>IMKL</code> with an extra command line argument that produces a similar result:</p> input<pre><code>BLIS_NUM_THREADS=8 FLEXIBLAS=BLIS Rscript -e \"system.time({x &lt;- replicate(5e3, rnorm(5e3)); tcrossprod(x) })\"\n</code></pre> output<pre><code>   user  system elapsed \n  8.769   0.369   3.381 \n</code></pre>","tags":["explanation","BLAS","LAPACK","FlexiBLAS","linear algebra","Python","R"]},{"location":"Using_Python_in_HPCC_with_virtualenv/","title":"Using Python in HPCC with virtualenv","text":"<p>Python applications usually use packages and modules that require specific versions of libraries. This means one installed application may conflict with another application due to using the same library but with different versions. It is difficult to meet the requirements of every application by one global Python installation. To resolve this issue, users can create an isolated virtual environment with a particular version of Python in a self-contained directory of their home or research space. Any package and the dependent libraries installed inside the directory can be available only through the virtual environment. Different applications can then use different virtual environments to avoid any conflict.</p>","tags":["tutorial","Python","virtual environment"]},{"location":"Using_Python_in_HPCC_with_virtualenv/#create-and-use-virtual-environments","title":"Create and use virtual environments","text":"<p>To create python virtual environments, please make sure your preferred version of Python is loaded. It is also a good idea to create a directory of the python version to store different environments and their applications:</p> <pre><code>[UserName@dev-intel18 ~]$ module list Python\n\nCurrently Loaded Modules Matching: Python\n  1) Python/3.6.4\n\n[UserName@dev-intel18 ~]$ which python\n/opt/software/Python/3.6.4-foss-2018a/bin/python\n\n[UserName@dev-intel18 ~]$ mkdir Python3.6.4\n[UserName@dev-intel18 ~]$ cd Python3.6.4\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>Currently, two common tools can be used to create Python virtual environments. Please use only one of them:</p> <ol> <li> <p>venv is available for Python 3.3 and later by default. The application <code>pip</code> and <code>setuptools</code> should be ready to use in HPCC system. A virtual environment can be created by running \"<code>python3 -m venv &lt;DIR&gt;</code>\", where <code>&lt;DIR&gt;</code> is the directory of the created environment. Following is an example of the command, and the directory <code>tutorial</code> is created for the virtual environment.</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ python3 -m venv tutorial  \n[UserName@dev-intel18 Python3.6.4]$ ls tutorial  \nbin  include  lib  lib64  pyvenv.cfg\n</code></pre> </li> <li> <p>virtualenv supports all Python versions. By default, HPCC system has     <code>pip</code>, <code>setuptools</code> and <code>wheel</code> installed and available. Similarly     to venv, a virtual environment can be created by executing     <code>virtualenv &lt;DIR&gt;</code>, where applications of\u00a0the virtual environment are installed in <code>tutorial</code> directory. <pre><code>    [UserName@dev-intel18 Python3.6.4]$ virtualenv tutorial\n    Using base prefix '/opt/software/Python/3.6.4-foss-2018a'\n    New python executable in /mnt/home/UserName/Python3.6.4/tutorial/bin/python\n    Installing setuptools, pip, wheel...done.\n    [UserName@dev-intel18 Python3.6.4]$ ls tutorial\n    bin  include  lib  lib64  pip-selfcheck.json\n</code></pre></p> </li> </ol> <p>Please use one of them to create the virtual environment.</p> <p>The created <code>tutorial</code> environment can now be used by sourcing the script file <code>activate</code> under the <code>bin</code> directory:</p> <pre><code>[UserName@dev-intel18 Python3.6.4]$ source tutorial/bin/activate\n(tutorial) [UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>where the name inside the\u00a0parentheses <code>(tutorial)</code> in front of the prompt line shows the current Python environment. To leave the environment, just run \"deactivate\":</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ deactivate\n[UserName@dev-intel18 Python3.6.4]$\n</code></pre> <p>and the parentheses disappear. Any time you want to use the <code>tutorial</code> environment. Simply source the file again: <code>source ~/Python3.6.4/tutorial/bin/activate</code> and the environment is back. More information can be found about venv or virtualenv.</p>","tags":["tutorial","Python","virtual environment"]},{"location":"Using_Python_in_HPCC_with_virtualenv/#install-packages-from-pypi-using-pip","title":"Install packages from PyPI using pip","text":"<p>The most common usage of pip is to install python packages from the Python Package Index with a requirement specifier. Users can also check other usages with pip. Below, some of the common usage scenarios are introduced.</p> <p>To install the latest version of a python package, users can run <code>pip install &lt;Package Name&gt;</code>, for example, using <code>sympy</code> as <code>&lt;Package Name&gt;</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"sympy\"\nCollecting sympy\nUsing cached https://files.pythonhosted.org/packages/21/21/f4105795ca7f35c541d82c5b06be684dd2f5cb4f508fb487cd7aea4de776/sympy-1.4-py2.py3-none-any.whl\nCollecting mpmath&gt;=0.19 (from sympy)\nInstalling collected packages: mpmath, sympy\nSuccessfully installed mpmath-1.1.0 sympy-1.4\n</code></pre> <p>To install a specific version of a python package, please run <code>pip install &lt;Package Name&gt;==&lt;Version Number&gt;</code>. For example, install <code>numpy</code> with <code>&lt;version number&gt;</code> as <code>1.16.2</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install \"numpy==1.16.2\"\nCollecting numpy==1.16.2\nDownloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.3MB 10.5MB/s\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.16.2\n</code></pre> <p>To upgrade an already installed package to the latest from PyPI, users can run <code>pip install --upgrade</code>. For example, upgrade the installed package <code>numpy</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip install --upgrade numpy\nCollecting numpy\nUsing cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nFound existing installation: numpy 1.16.2\n    Uninstalling numpy-1.16.2:\n    Successfully uninstalled numpy-1.16.2\nSuccessfully installed numpy-1.17.2\n</code></pre> <p>With pip, you can also list all installed packages and their versions with the command <code>pip freeze</code>:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nmpmath==1.1.0\nnumpy==1.17.2\nsympy==1.4\n</code></pre> <p>For more detail, see the pip docs, which includes a complete Reference Guide.</p>","tags":["tutorial","Python","virtual environment"]},{"location":"Using_Python_in_HPCC_with_virtualenv/#pythonpath-environment-variable","title":"PYTHONPATH environment variable","text":"<p>You can use the environment variable <code>PYTHONPATH</code> to include the packages already installed by the same python version in other directories. By adding the <code>site-packages</code> paths to <code>PYTHONPATH</code> environment variable and separating them with the \"<code>:</code>\" symbol:</p> <pre><code>(tutorial) [UserName@dev-intel18 Python3.6.4]$ export PYTHONPATH=~/Python3.6.4/tutorial/lib/python3.6/site-packages:/opt/software/Python/3.6.4-foss-2018a/lib/python3.6/site-packages\n(tutorial) [UserName@dev-intel18 Python3.6.4]$ pip freeze\nabsl-py==0.5.0\nalabaster==0.7.12\nappdirs==1.4.3\nartemis==0.1.4\n...\nnose==1.3.7\nnumpy==1.17.2\nnumpydoc==0.8.0\n...\nsuspenders==0.2.6\nsympy==1.4\ntensorboard==1.10.0\ntensorflow==1.10.1\n...\nvirtualenv==15.1.0\nwcwidth==0.1.7\nWerkzeug==0.14.1\nxopen==0.3.5\n</code></pre> <p>all packages inside the paths are now ready to use. Please make sure the <code>site-packages</code> path of the current environment is set the first in\u00a0<code>PYTHONPATH</code> variable. If a package is installed in more than one path (possibly with different versions), the package of the first path showing in the variable (<code>PYTHONPATH</code>) will be used.</p>","tags":["tutorial","Python","virtual environment"]},{"location":"Using_conda/","title":"Using Conda","text":"<p>Attention</p> <p>This wiki serves as a very limited introduction. ICER strongly  recommendeds reading the conda user guide  specific to the version of interest before you start using conda  on the HPCC. </p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#introduction","title":"Introduction","text":"<p>For most Python users, installing packages is done simply with the command <code>pip install AwesomePackage</code>  on the terminal, or similar in Jupyter Notebooks.</p> <p>However for complex research software, it's sometimes not that simple.  There may be other software and libraries that must be installed or part of the operating system for a python package to work that need to be installed on the computer, so-called \"dependencies,\" and the versions must all be compatible. This is not just true of Python -- most systems have a \"package management\" system that works with recipes for installing the correct versions of many programs. The Anaconda company contributed the great open source Conda package manager for Python and scientific software and it's been very successful.</p> <p>Specifically, Conda is an open-source package management system that installs and updates packages and their dependencies. Conda also easily creates, saves, loads, and switches between environments on the HPCC. It was created for Python programs but it can package and distribute software for any language as a collection of 1,000+ open-source packages with free community support.</p> <p>Miniforge is a minimal installer that provides access to Conda and a large set of community-contributed packages via conda-forge. Miniforge and Conda are both available with a permissive license.</p> <p>You may also be familiar with Anaconda, which also distributes Conda and performs a similar role as Miniforge. However Anaconda uses a different installer and provides a curated list of packages. Anaconda is not available under a permissive license.</p> <p>ICER recommends using Miniforge. However the two distributions act similarly and will give access to the same <code>conda</code> program.</p> <p>There are two methods for using Conda on the HPCC. The first is using the <code>Miniforge3</code> module. The second is to install Miniforge manually, and is detailed in the appendix.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#using-the-miniforge3-module-quick-start-option","title":"Using the Miniforge3 Module (Quick Start Option)","text":"<p>To access the <code>conda</code> command, first unload other modules to ensure you do not conflict with other versions of Python. Then load the Miniforge3 module:</p> <pre><code>module purge\nmodule load Miniforge3\n</code></pre> <p>When you create Conda environments and install packages, they will be installed in your home directory under the path <code>/mnt/home/$USER/.conda/envs/myenv</code>.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-environments","title":"Managing Environments","text":"<p>To create a conda environment, use the command  </p> <p><code>conda create --name &lt;environment_name&gt;</code> </p> <p>where the text <code>&lt;environment_name&gt;</code> is to be replaced with the  name you choose. For example, to create an environment named \"myenv\" use the command    </p> <p><code>conda create --name myenv</code> </p> <p>The output will display    </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /mnt/home/$USER/.conda/envs/myenv\n\n\n\nProceed ([y]/n)?    \n</code></pre> <p>Type <code>y</code> to create the environment. No packages have been installed in  this environment yet. </p> <p>To create a conda environment with pre-specified packages and/or versions  of Python, use the command above with additional arguments. Here are a  few examples that demonstrate the syntax:     </p> <p>Create an environment named \"bioenv\" with the \"biopython\" package  </p> <pre><code>conda create --name bioenv biopython\n</code></pre> <p>Create an environment named \"scienv\" with Python version 3.9 and version 1.6.0  of the \"scipy\" packages  </p> <pre><code>conda create --name scienv python=3.9 scipy=1.6.0\n</code></pre> <p>Create an environment named \"astroenv\" with version 1.6.0  of the \"scipy\" packages, the current \"asteroid\" package, and the current  \"bable\" packages  </p> <pre><code>conda create --name astroenv scipy=1.6.0 asteroid bable\n</code></pre> <p>To copy an existing environment, use the command    </p> <p><code>conda create --name &lt;new environment name&gt; --clone &lt;existing environment name&gt;</code> </p> <p>For example, the command    </p> <p><code>conda create --name newenv --clone myenv</code> </p> <p>will create a new environment named \"newenv\" that contains the same packages as the existing environment \"myenv\".    </p> <p>To display a list of all conda environments, use the command  </p> <p><code>conda info --envs</code> </p> <p>The output will display    </p> output<pre><code># conda environments:\n#\nastroenv                 /mnt/home/$USER/.conda/envs/astroenv\nbase                  *  /opt/software-current/2023.06/x86_64/generic/software/Miniforge3/24.3.0-0\nbioenv                   /mnt/home/$USER/.conda/envs/bioenv\nmyenv                    /mnt/home/$USER/.conda/envs/myenv\nnewenv                   /mnt/home/$USER/.conda/envs/newenv\nscienv                   /mnt/home/$USER/.conda/envs/scienv\n</code></pre> <p>where an active environment is denoted with the <code>*</code> symbol.  </p> <p>To activate a conda environment, use the command     </p> <p><code>conda activate &lt;environment_name&gt;</code> </p> <p>The current environment should be in (parentheses) in front of the command prompt. For example, the command    </p> <p><code>conda activate astroenv</code> </p> <p>will result in the new command prompt <code>(astroenv) $</code>.    </p> <p>To switch to another environment, just use the <code>conda activate</code> command with the new environment name.    </p> <p>To deactivate the current conda environment, use the command  </p> <p><code>conda deactivate</code> </p> <p>To remove an environment, use the command</p> <p><code>conda remove --name &lt;name of environment&gt; --all</code></p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-packages","title":"Managing Packages","text":"<p>To list all packages currently installed in an environment, first activate the environment then use the command     </p> <p><code>conda list</code> </p> <p>For example, after running the commands    </p> <p><code>conda activate scienv</code> <code>conda list</code> </p> <p>the output will display    </p> output<pre><code># packages in environment at /mnt/home/$USER/.conda/envs/scienv:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             5.1                       1_gnu  \nblas                      1.0                         mkl  \nca-certificates           2022.07.19           h06a4308_0  \ncertifi                   2022.6.15        py39h06a4308_0  \n... ...\n... ...\nzlib                      1.2.12               h7f8727e_2\n</code></pre> <p>To search for a package in the conda-forge repository that you would like to install, use the command    </p> <p><code>conda search &lt;package name&gt;</code>  where the <code>&lt;package name&gt;</code> is to be replaced with the name of package to search for. For example, the command    </p> <p><code>conda search beautifulsoup4</code> </p> <p>results in the output display    </p> output<pre><code>Loading channels: done\n# Name                       Version           Build  Channel             \nbeautifulsoup4                 4.6.0          py27_1  pkgs/main           \nbeautifulsoup4                 4.6.0  py27h3f86ba9_1  pkgs/main           \n... ...\n... ...          \nbeautifulsoup4                4.11.1  py38h06a4308_0  pkgs/main           \nbeautifulsoup4                4.11.1  py39h06a4308_0  pkgs/main\n</code></pre> <p>To install a package in the active environment, use the command    </p> <p><code>conda install &lt;package name&gt;</code> </p> <p>where <code>&lt;package name&gt;</code> is to be replaced with the name of package to install. For example, the command    </p> <p><code>(scienv) $ conda install beautifulsoup4</code> </p> <p>will output the display    </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /mnt/home/$USER/.conda/envs/scienv\n\n  added / updated specs:\n    - beautifulsoup4\n\n\nThe following NEW packages will be INSTALLED:\n\n  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py39h06a4308_0\n  soupsieve          pkgs/main/noarch::soupsieve-2.3.1-pyhd3eb1b0_0\n\n\nProceed ([y]/n)?\n</code></pre> <p>Type \u2018y\u2019 to install the package.</p> <p>To install a package in another environment, use the command    </p> <p><code>conda install --name &lt;environment name&gt; &lt;package name&gt;</code> </p> <p>where <code>&lt;environment name&gt;</code> is to be replaced with the name of the target environment and <code>&lt;package name&gt;</code> is to be replaced with the name of package to install. For example, the command    </p> <p><code>(scienv) $ conda install --name myenv beautifulsoup4</code> </p> <p>will install the package \"beautifulsoup4\" in the inactive environment \"myenv\".</p> <p>Note</p> <p>It is best to install all packages at once, so that all dependencies are installed at the same time.</p> <p>**Not all packages can be installed with the simple command ** <code>conda install</code>. Some packages may reside in other repositories or \"channels\". For example, Bioconda is a popular repository for tools useful in biomedical research. To install a packages using a different channel, use the <code>-c</code> option like</p> <p><code>conda install -c bioconda samtools</code></p> <p>where the <code>-c</code> flag designates the channel.</p> <p>Bioconda specific setup</p> <p>If you plan to use Bioconda often, they suggest that you run the following one-time setup to ensure that you can install compatible packages into your environment:</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <p>See the Bioconda homepage for details.</p> <p>If packages you are interested in are not available from conda or another conda repository that you have access to use the 'pip' package manger within a conda environment via the command</p> <p><code>pip install &lt;package name&gt;</code></p> <p>where <code>&lt;package name&gt;</code> is to be replaced by the name of the desired package. For example, the commands   <pre><code>    $ conda activate scienv\n   (scienv) $ pip install see\n</code></pre></p> <p>will install the package \"see\" in the active environment \"scienv\".</p> <p>Avoid mixing <code>conda</code> and <code>pip</code> installed packages</p> <p>When creating an environment Conda, it is highly recommended to install all packages with <code>conda</code> only and do not <code>pip</code>. If you need to install packages with both tools, install all <code>conda</code> packages first, then install <code>pip</code> packages. This ensures that the <code>conda</code> installed packages have the correct dependencies and setup.</p> <p>To update a package use the command</p> <p><code>conda update &lt;package name&gt;</code>.</p> <p>To remove a package from the active environment, use the command</p> <p><code>conda remove &lt;package name&gt;</code></p> <p>where <code>&lt;package name&gt;</code> is to be replaced by the name of the package to be removed.</p> <p>To remove a package from another environment, use the command    </p> <p><code>conda remove --name &lt;environment name&gt; &lt;package name&gt;</code> </p> <p>where <code>&lt;environment name&gt;</code> is to be replaced with the name of the target environment and <code>&lt;package name&gt;</code> is to be replaced with the name of package to be removed. For example, the command    </p> <p><code>(scienv) $ conda remove --name myenv beautifulsoup4</code> </p> <p>will remove the package \"beautifulsoup4\" in the inactive environment \"myenv\".</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#using-conda-with-slurm","title":"Using conda with SLURM","text":"<p>You can activate a conda environment from within a SLURM Job Script. Include the <code>conda activate &lt;environment name&gt;</code> and <code>conda deactivate</code> commands in the 'bash command' portion of the SLURM job script. Ensure to first navigate into the directory where Miniforge is installed e.g., <code>cd $HOME</code>.</p> <pre><code>#!/bin/bash\n#SBATCH -t 60 # Runtime in minutes\n.\n.\n.\n#SBATCH --output=conda_%j.out # Standard out goes to this file\n\n# Load Conda module\nmodule load Conda/3\n# Activate conda environment and run job commands\ncd $HOME\nconda activate &lt;environment name&gt;\n&lt;bash command&gt;\n&lt;bash command&gt;\n.\n.\n.\n&lt;bash command&gt;\nconda deactivate\n</code></pre>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#appendix-installing-miniforge-manually-advanced-option","title":"Appendix: Installing Miniforge Manually (Advanced Option)","text":"<p>You may wish to install Miniforge yourself rather than using the version on the HPCC (e.g., to use a newer version).</p> <p>Doing so requires both a default component installed and maintained by the HPCC administrators via the Software module system and a user installed and maintained component. This dual component configuration is required to ensure system wide compliance for the user's customized Conda environments. Hence, if you are not using the <code>Miniforge3</code> module, users must install Miniforge in the home or research space to have full control of their Conda managed environments and packages; and users must also load the HPCC administrated Conda/3 module to allow Conda managed environments and packages to run smoothly on the HPCC.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#miniforge-installation-script","title":"Miniforge Installation Script","text":"<p>To install the user managed component of Miniforge on the HPCC visit  https://conda-forge.org/miniforge/  and follow the instructions below:</p> <ol> <li> <p>Find the link for the 64-bit (x86_64) Installer for Minforge3 for Linux and copy the URL associated with this link (right-click on the download link and select \"copy link address\" or \"copy link location\"); For example, https://github.com/conda-forge/miniforge/releases/download/24.3.0-0/Miniforge3-24.3.0-0-Linux-x86_64.sh.</p> </li> <li> <p>Login to the HPCC, login to a development node, navigate to the desired installation directory if it is somewhere other than your homespace,\u00a0then run the command <code>wget &lt;copied link address&gt;</code>     where <code>&lt;copied link address&gt;</code> above is to be replaced with the URL obtained in step 1.</p> </li> <li> <p>Once the Miniforge file is downloaded, run the command <code>bash &lt;MiniforgeFileName.sh&gt;</code>      where <code>&lt;MiniforgeFileName.sh&gt;</code> above is replaced with the name of the Miniforge     file downloaded with the <code>curl</code> command. For example, <code>Miniforge3-24.3.0-0-Linux-x86_64.sh</code>.  </p> <p>During Installation you will need to:  </p> <ul> <li>Accept the license terms; the output will display <code>Do you accept the license terms [yes|no]?</code>  Read the license, and if you agree, type <code>yes</code> to accept.</li> <li>Choose the installation location; the output will display <code>Miniforge3 will now be installed into this location: /mnt/home/$USER/miniforge3</code> <code>- Press ENTER to confirm the location</code> <code>- Press CTRL-C to abort the installation</code> <code>- Or specify a different location below</code>  (note: it may take a long while to complete this step)</li> <li>Choose to initialize conda; the output will display <code>Do you wish the installer to initialize conda?</code> <code>by running conda init? [yes|no]</code> <code>[no] &gt;&gt;&gt;</code> Please type <code>no</code>, a more careful initialization is described after the installation instructions.</li> </ul> </li> </ol> <p>Note</p> <p>Please remember the directory where Miniforge was installed. This installation path will be used next. </p> <p>Upon successful installation, the output will display   </p> output<pre><code>You have chosen to not have conda modify your shell scripts at all.\nTo activate conda's base environment in your current shell session:\n\neval \"$(/mnt/home/$USER/minforge3/bin/conda shell.YOUR_SHELL_NAME hook)\" \n\nTo install conda's shell functions for easier access, first activate, then:\n\nconda init\n\nThank you for installing Miniforge3!\n</code></pre> <p>Note</p> <p>Please disregard this output. Users must manually configure Miniforge as described in the next section.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#editing-bashrc","title":"Editing <code>.bashrc</code>","text":"<p>To avoid conflicts between the user-installed Miniforge distribution and system-installed Python distributions, a modification of the <code>$HOME/.bashrc</code> file is necessary. The <code>.bashrc</code> file in the user's home space can be modified to set a specified environment every time you login to an HPCC node. You can modify the <code>.bashrc</code> file with an editor such as <code>vim</code> or <code>nano</code>.  To modify the <code>.bashrc</code> file for Miniforge installations please follow these steps:</p> <ol> <li> <p>Navigate to your home space and open the <code>.bashrc</code> file with an editor e.g., run  <code>cd $HOME</code> followed by <code>vim .bashrc</code>. Once in the vim editor, press the [i] key to enter \"--insert--\" mode</p> </li> <li> <p>Set the variable \"<code>CONDA3PATH</code>\" to the Miniforge installation directory by adding the command line  <code>export CONDA3PATH=&lt;Miniforge3 installation path&gt;</code>. Make sure to include the final <code>/</code> at the end of the path.</p> </li> <li> <p>Save the modified <code>.bashrc</code> file by pressing the [esc] key to exit     \"--insert--\"     mode; followed by <code>:wq</code> to save and quit vim.</p> </li> </ol> <p>Warning</p> <p>If there was a block of code in your <code>.bashrc</code> file that begins with <code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;</code> and ends with <code># &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</code> and the lines between are not commented out (i.e., they did not begin with <code>#</code>s), please run <code>conda init --reverse</code> to remove these lines. Alternatively, you may comment them out yourself.</p> <p>Run <code>logout</code> to exit your SSH session on the development node, then reconnect with SSH to enable your changes.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#loading-the-conda-module","title":"Loading the Conda Module","text":"<p>Warning</p> <p>Loading the Conda/3 module requires that you first unload any Python modules with <code>module unload Python</code> or <code>module purge</code>.</p> <p>As stated above users must load the HPCC administered Conda module to ensure system-wide compliance when using and managing Conda environments and packages. To load the Conda/3 module login to a dev node and run the command</p> <pre><code>module load Conda/3\n</code></pre> <p>If you would like to automatically load the Conda/3 module upon login, add the command    <code>module load Conda/3 2&gt; /dev/null</code>  to the\u00a0 <code>.bashrc</code> file after the  <code>export CONDA3PATH=...</code> command  </p> <p>Difference between <code>Conda/3</code> and <code>Miniforge3</code> modules</p> <p>If you have installed conda manually, use <code>module load Conda/3</code>. If you are using the one installed on the HPCC, use <code>module load Miniforge3</code>. As you read through ICER's documentation, you may see either module being used. Substitute for the version that you have chosen.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_conda/#managing-user-installed-conda","title":"Managing User-Installed Conda","text":"<p>To ensure conda is properly installed and determine the  installed version, use the command <code>$ conda --version</code>  If properly installed, the conda version will be output to the display. For example,   <code>$ conda 22.11.0</code>.</p> <p>Conflict with locally installed packages</p> <p>If you have used the <code>pip install</code> command to install packages before using Conda, you may find conflicts between the packages Conda uses and the packages locally installed into the <code>$HOME/.local/lib/pythonX.Y</code> directory.</p> <p>For example, after running a <code>conda</code> command for the first time, you may see an error like</p> <pre><code>Traceback (most recent call last):\n    File \"/mnt/home/$USER/miniforge3/lib/python3.10/site-packages/conda/exception_handler.py\", line 17, in __call__\n    return func(*args, **kwargs)\n    ...\nImportError: cannot import name 'JSONDecodeError' from 'requests.exceptions' (/mnt/home/$USER/.local/lib/python3.10/site-packages/requests/exceptions.py)\n</code></pre> <p>You will need to move your locally installed packages to a separate location. The final line of the errors shows where the user installed packages are. You can move them to a backup location if you need them later using a command like:</p> <pre><code>mv $HOME/.local/lib/python3.10 $HOME/.local/lib/python3.10.backup\n</code></pre> <p>Make sure to replace <code>python3.10</code> above with the version that shows up in your error message. After this, you will no longer be able to use your locally installed packages. For this reason ICER recommends not mixing Conda installed packages and locally installed packages.</p> <p>To update conda to the most recent version, use the command</p> <p><code>$ conda update conda</code> </p> <p>The output will display text similar to     </p> output<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /mnt/home/$USER/miniforge3\n\n  added / updated specs:\n    - conda\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-22.11.0              |   py39h06a4308_0         872 KB\n    ruamel.yaml-0.16.12        |   py39h27cfd23_1         184 KB\n    ruamel.yaml.clib-0.2.6     |   py39h7f8727e_0         137 KB\n    ------------------------------------------------------------\n                                           Total:         1.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n  ruamel.yaml        pkgs/main/linux-64::ruamel.yaml-0.16.12-py39h27cfd23_1\n  ruamel.yaml.clib   pkgs/main/linux-64::ruamel.yaml.clib-0.2.6-py39h7f8727e_0\n\nThe following packages will be UPDATED:\n\n  conda                               4.13.0-py39h06a4308_0 --&gt; 22.11.0-py39h06a4308_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\nruamel.yaml-0.16.12  | 184 KB    | ######################################################## | 100% \nruamel.yaml.clib-0.2 | 137 KB    | ######################################################## | 100% \nconda-22.11.0        | 872 KB    | ######################################################## | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n</code></pre> <p>Type <code>y</code> to continue with the update.</p>","tags":["Conda","Python","how-to guide","tutorial"]},{"location":"Using_the_Data_Machine/","title":"Using the Data Machine","text":"<p>This page acts as a reference for using some of the features of the Data Machine. For more general information on what the Data Machine can offer, please see the Data Machine overview.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#table-of-data-machine-resources","title":"Table of Data Machine resources","text":"Node CPUs Memory Local NVME storage GPU GPU memory acm-048 acm-049 acm-070 acm-071 128 2 TB 32 TB nal-004 nal-005 128 512 GB 32 TB 4 NVIDIA A100 GPUs each 80 GB (per GPU) nal-006 nal-007 128 512 GB 32 TB 4 NVIDIA A100 GPUs eachsplit into 7 allocatable units 10 GB (per unit) <p>Two GPU nodes have four full A100 GPUs and two GPU nodes have four GPUs each split into seven units that can be requested. Each of these split units has 10 GB of memory. These units are requested similarly to normal GPUs. See the examples below.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#running-code-on-the-data-machine","title":"Running code on the Data Machine","text":"<p>Though the Data Machine is not a buy-in node, the same procedures are used behind the scenes to run on Data Machine nodes. Therefore, users must be added to the <code>data-machine</code> buy-in account to run jobs on the Data Machine. To be added to this account please submit a request.</p> <p>Note</p> <p>The <code>data-machine</code> account is limited to Data Machine nodes only.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#ondemand-data-machine-access","title":"OnDemand Data Machine access","text":"<p>Each OnDemand app has an \"Advanced Options\" checkbox. This opens additional form entries. To use the Data Machine nodes, enter <code>data-machine</code> in the SLURM Account  text box. Your job will queue onto a Data Machine node. Other resources (time,  CPU and memory) can be requested as usual by filling out the \"Number of hours\",  \"Number of cores per task\" and \"Amount of memory\" boxes.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#gpu-access","title":"GPU access","text":"<p>To use a single GPU unit with your OnDemand session, in the \"Advanced Options\" section and enter <code>a100_1g.10gb</code> under \"Number of GPUs\". If you would like multiple units, use <code>a100_1g.10gb:n</code> where <code>n</code> is the number of units you would like on a single node.</p> <p>To request full GPUs, use <code>a100</code> instead of <code>a100_1g.10gb</code>.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#slurm-scripting-data-machine-access","title":"SLURM scripting Data Machine access","text":"<p>Below are some examples of SLURM resource requests for the Data Machine.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#partial-data-machine-node","title":"Partial Data Machine node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#full-data-machine-node-with-no-gpu-with-large-memory","title":"Full Data Machine node with no GPU with large memory","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=2TB  # Uses all memory on a large memory node\n#SBATCH --cpus-per-task=128  # Uses all CPUs on a node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#one-gpu-unit-on-a-single-node","title":"One GPU unit on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100_1g.10gb  # Request one GPU unit on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#two-gpu-units-on-a-single-node","title":"Two GPU units on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100_1g.10gb:2  # Request two GPU units on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#one-full-gpu-on-a-single-node","title":"One full GPU on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100  # Request one GPU on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#two-full-gpus-on-a-single-node","title":"Two full GPUs on a single node","text":"<pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --mem=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=32  # Set to your desired number of CPUs\n#SBATCH --gpus=a100:2  # Request two GPUs on the reserved node\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#using-the-fast-nvme-storage","title":"Using the fast NVME storage","text":"<p>You can preload your data into local NVME storage using \"burst buffers\". SLURM will move the data you want to use into NVME storage before your job starts.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#requesting-a-node","title":"Requesting a node","text":"<p>At the moment, burst buffers work best when requesting one specific node in the data machine. This ensures that the time SLURM takes to move your data does not count against the time you reserve the node for.</p> <p>However, be careful which node you pick. If this node is busy, SLURM will wait until it is available to assign it to you. You can use the </p> <pre><code>buyin_status --account data-machine\n</code></pre> <p>to see the current usage of the Data Machine nodes.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#example-burst-buffer-resource-specification","title":"Example burst buffer resource specification","text":"<p>In this example, we'll assume that we don't need a GPU and choose <code>acm-048</code>.</p> <pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodelist=acm-048  # Restrict to a specific data machine node\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --memory=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=128  # Set to your desired number of CPUs\n#BB source=/mnt/home/&lt;username&gt;/important/data/here\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#using-the-local-data","title":"Using the local data","text":"<p>SLURM sets an environment variable <code>BB_DATA</code> with the location of your data on the local NVME storage. Use this directory to access your data with less latency than the home, research, or scratch space where it originally came from.</p>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#saving-data-written-to-local-storage","title":"Saving data written to local storage","text":"<p>Usually, if you edit data on local storage, your changes will be lost after the job ends. However, if you add the specification <code>resync=true</code> to the <code>#BB</code> line in your submission script, that data will be copied to the location it was originally taken from after the job ends.</p> <p>Example:</p> <pre><code>#!/bin/bash\n#SBATCH --account=data-machine  # Run under the data machine buy-in\n#SBATCH --nodelist=acm-048  # Restrict to a specific data machine node\n#SBATCH --nodes=1  # Reserve only one node\n#SBATCH --time=4:00:00  # Reserve for four hours (or your desired amount of time)\n#SBATCH --memory=256GB  # Set to your desired amount of memory\n#SBATCH --cpus-per-task=128  # Set to your desired number of CPUs\n#BB source=/mnt/home/&lt;username&gt;/important/data/here resync=true\n</code></pre>","tags":["reference","data machine"]},{"location":"Using_the_Data_Machine/#debugging-burst-buffer-issues","title":"Debugging burst buffer issues","text":"<p>To check on the status of your submitted jobs, use the command</p> <pre><code>squeue --me\n</code></pre> <p>The <code>NODELIST(REASON)</code> column of the output may give information relevant to burst buffer steps, e.g.,</p> <pre><code>   JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMI  NODES NODELIST(REASON)\n24411390 data-mach data_mac grosscra  PENDING       0:00   1:00:00      1 (BurstBufferResources)\n24411197 data-mach data_mac grosscra  PENDING       0:00   1:00:00      1 (burst_buffer/lua: slurm_bb_data_in: )\n</code></pre> <p>A job with the <code>BurstBufferResources</code> reason is waiting for a node to run on and begin transferring resources. In the example above, a job is running the <code>slurm_bb_data_in</code>, i.e., it is transferring the data to the node.</p> <p>For more information or if there are problems with the burst buffer specification, use the command</p> <pre><code>scontrol show job &lt;jobid&gt;\n</code></pre> <p>For example, running <code>scontrol show job 24411197</code> while the job above was transferring data, the output ends with</p> <pre><code>...\nBurstBuffer=#BB source=/mnt/home/grosscra/scripts\nBurstBufferState=staging-in\n...\n</code></pre> <p>Often the <code>Comment</code> field in the <code>scontrol show job &lt;jobid&gt;</code> output can give helpful burst buffer information.</p>","tags":["reference","data machine"]},{"location":"Variables_I/","title":"Variables - Part I","text":"<p>The shell is a program that takes commands from the input device (usually, a keyboard) and gives them to the operating system to perform.\u00a0On most Linux system including HPC at MSU, <code>bash</code> works as the shell. Besides <code>bash</code>, other shells are available, but here, we will focus on <code>bash</code>.</p> <p>This tutorial assumes you have:</p> <ul> <li>minimal programming knowledge</li> <li>minimal Linux shell knowledge (see Linux Comand Line for Beginners I for an introduction)</li> </ul>","tags":["tutorial","command line"]},{"location":"Variables_I/#writing-a-script","title":"Writing a script","text":"<p>Let's create a file <code>first.sh</code> on the terminal using your favorite editor. If you rarely use any editor on Linux, this is a good chance to start using one of them (Linux text editors). A popular one for which is already installed on the HPCC is <code>nano</code>.</p> first.sh<pre><code>#!/bin/bash\n# This is a comment!\necho Hello World         # This is a comment, too!\n</code></pre> <p>The first line tells Linux that the file is to be executed by <code>/bin/bash</code>. <code>#!</code> will be explained later. The second line begins with <code>#</code>. This special character makes the line as a comment, and it is ignored by the shell. The only exception is when the first\u00a0line of the file starts with <code>#!</code>.</p> <p>The third line runs a command <code>echo</code>, with two parameters/arguments 'Hello' and 'World'. The symbol <code>#</code> on line 3 makes the rest of the line a comment.</p> <p>Now, after exiting the text editor, run <code>chmod u+x first.sh</code> on the command line to make the text file executable, then run <code>./first.sh</code>.</p> input<pre><code>chmod u+x first.sh\n./first.sh\n</code></pre> output<pre><code>Hello World\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#using-variables","title":"Using variables","text":"<p>Next, let's expand on <code>first.sh</code> by using variables.\u00a0Create a new script called <code>var1.sh</code> with the following content:</p> var1.sh<pre><code>#!/bin/bash\nMY_MESSAGE=\"Hello World\"\necho $MY_MESSAGE\n</code></pre> <p>This assigns the string <code>Hello World</code> to the variable <code>MY_MESSAGE</code> then <code>echo</code> command prints the value of the variable. Note that you need the quotes around the string.</p> <p>To use variables, <code>$</code> is required in front of variables. If you use <code>echo MY_MESSAGE</code> in the above, it will \u00a0print <code>MY_MESSAGE</code> instead of <code>Hello World</code>. The scope of the variable <code>MY_MESSAGE</code> is only inside of the script, and when the script finished the variable is empty (don't forget to use <code>chmod u+x var1.sh</code> to make a script executable).</p> input<pre><code>./var1.sh\n</code></pre> output<pre><code>Hello World\n</code></pre> input<pre><code>echo $MY_MESSAGE\n</code></pre> output<pre><code>\n</code></pre> <p>In addition, if you use a variable without declaration, it returns empty string. There is no warning or error message. </p>","tags":["tutorial","command line"]},{"location":"Variables_I/#exploring-variable-scope","title":"Exploring variable scope","text":"<p>Let's create a shell script <code>var2.sh</code>.</p> var2.sh<pre><code>#!/bin/bash \necho \"MYVAR is: $MYVAR\"\nMYVAR=\"hi there\"\necho \"MYVAR is: $MYVAR\"\n</code></pre> <p>Then run the script. You can use <code>chmod u+x</code> to make <code>var2.sh</code> executable and run it as the previous examples or use the <code>sh</code> command:</p> input<pre><code>bash var2.sh\n</code></pre> output<pre><code>MYVAR is:\nMYVAR is: hi there\n</code></pre> <p>The first <code>MYVAR</code> is empty because it is not declared. The second <code>MYVAR</code> has the value we expected. The scope of the variables in a script is only inside the script. For example, <code>MYVAR</code> is only valid inside <code>var2.sh</code> and when the script finishes, <code>MYVAR</code> is empty again.</p> input<pre><code>./var2.sh\n</code></pre> output<pre><code>MYVAR is:\nMYVAR is: hi there\n</code></pre> input<pre><code>echo $MYVAR\n</code></pre> output<pre><code>\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#saving-variables","title":"Saving variables","text":"<p>You can declare variables with <code>export</code> command in a shell. Check the scope of variables.</p> input<pre><code>MYVAR=\"hello there\"\nexport MYVAR\n./var2.sh\n</code></pre> output<pre><code>MYVAR is: hello there\nMYVAR is: hi there\n</code></pre> input<pre><code>$ echo $MYVAR\nhello there\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_I/#extended-example","title":"Extended example","text":"<p>You can use variables in many ways. Here is one example.</p> var3.sh<pre><code>#!/bin/bash\necho \"What is your name?\"\nread USER_NAME\necho \"Hello $USER_NAME\"\necho \"I will create a file called ${USER_NAME}_file\"\ntouch ${USER_NAME}_file\n</code></pre> <p>Let's run the script.</p> input<pre><code>chmod u+x var3.sh\n./var3.sh\n</code></pre> output<pre><code>What is your name?\nICER\nHello ICER\nI will create a file called ICER_file\n$ls -l ICER_file\n-rw-r--r--  1 choiyj  staff  0 Jan  5 14:08 ICER_file\n</code></pre> <p>Notice that we use curly braces for a file name. If you use <code>$USER_NAME_file</code> instead of <code>${USER_NAME}_file</code>, the shell returns the empty string because there is no variable called <code>USER_NAME_file</code> in the script.</p>","tags":["tutorial","command line"]},{"location":"Variables_II/","title":"Variables - Part II","text":"<p>Linux offers a set of pre-defined variables. These pre-defined variables contain useful information.</p> <p>The first set of variables are <code>$0</code>, <code>$1</code>, ..., <code>$9</code>, and <code>$#</code>.</p> <p>The variable <code>$0</code> is the name of the program as it was called. For example, if you run <code>example.sh</code> which uses the variable <code>$0</code>, it will return <code>example.sh</code>. <code>$1</code>, ..., <code>$9</code> are the first 9 additional parameters the script was called with. The total number of parameters that the script is called with is stored in <code>$#</code>.</p> <p>To access all parameters at once, we can use <code>$@</code> and <code>$*</code>. <code>$@</code> is a special variable takes the entire list of parameters and separates them into a list of parameters. Thus, the variable <code>$@</code> is all parameters <code>$1</code>, ..., <code>$any_number</code>. \u00a0The variables <code>$*</code> is similar but does not preserve any whitespace or quoting, so \"File with spaces\" becomes \"File\" \"with\" \"spaces\". This is similar to the echo command.</p> <p>Let's do a hands on example.</p> var4.sh<pre><code>#!/bin/bash\necho \"Number of parameters from input: $# parameters\"\necho \"My name is $0\"\necho \"My first parameter is $1\"\necho \"My second parameter is $2\"\necho \"All parameters are $@\"\n</code></pre> <p>Here is a sample run for the above script.</p> input<pre><code>bash ./var4.sh\n</code></pre> output<pre><code>Number of parameters from input: 0 parameters\nMy name is ./var4.sh\nMy first parameter is\nMy second parameter is\nAll parameters are\n\n``` bash title=\"input\"\nbash ./var4.sh My lazy fox\n</code></pre> output<pre><code>Number of parameters from input: 3 parameters\nMy name is ./var4.sh\nMy first parameter is My\nMy second parameter is lazy\nAll parameters are My lazy fox\n</code></pre> <p><code>$#</code> and <code>$1</code>, ..., <code>$9</code> are set by the shell. We can take more than 9 parameters by using the <code>shift</code> command. Look at the next example.</p> test.sh<pre><code>#!/bin/bash\nwhile [ \"$#\" -gt \"0\" ]\ndo\n  echo \"\\$1 is $1\"\n  shift\ndone    \n</code></pre> <p>The backslash character <code>\\</code> is used to \"escape\" <code>$</code> so that it is not interpreted by the shell. This script uses <code>shift</code> to move all parameters down one slot (removing <code>$1</code>) until <code>$#</code> is down to zero.</p> <p>Here is a sample run for the above script.</p> input<pre><code>bash ./test.sh The quick brown fox jumps over the lazy dog.\n</code></pre> output<pre><code>$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n</code></pre> <p>We can write a script using <code>$*</code> to get the same result.</p> <pre><code>#!/bin/bash\n\nfor TOKEN in $*\ndo\n    echo \\$1 is $TOKEN\ndone\n</code></pre> <p>In the previous scripts, <code>while</code>, <code>for</code>, and <code>do ... done</code>\u00a0are loop commands which will be covered later.</p> <p>The\u00a0<code>$?</code>\u00a0variable represents the exit status of the previous command.\u00a0Exit status is a numerical value returned by every command upon its completion. Most commands return 0 if they were successful, and 1 if they were unsuccessful.</p> test.sh<pre><code>#!/bin/bash\n\nfor TOKEN in $*\ndo\n    echo \\$1 is $TOKEN\ndone\necho $?\n</code></pre> <p>Here is the result of the sample run.</p> input<pre><code>bash ./test.sh The quick brown fox jumps over the lazy dog.\n</code></pre> output<pre><code>$1 is The\n$1 is quick\n$1 is brown\n$1 is fox\n$1 is jumps\n$1 is over\n$1 is the\n$1 is lazy\n$1 is dog.\n0\n</code></pre>","tags":["tutorial","command line"]},{"location":"Variables_II/#see-also","title":"See also","text":"<ul> <li>How To Read and Set Environmental and Shell Variables on Linux by Justin Ellingwood</li> </ul>","tags":["tutorial","command line"]},{"location":"Virtual_Terminals/","title":"Virtual Terminals","text":""},{"location":"Virtual_Terminals/#gnu-screen","title":"GNU Screen","text":"<p>GNU Screen is a program that allows you to create a virtual terminal session inside a single terminal window. It is useful for dealing with multiple programs from a command line interface and for separating programs from the Unix shell that started the program.</p> <p>To start screen, simply type</p> <pre><code>screen\n</code></pre> <p>at the command prompt. Even if it looks like nothing has happened, you are now in a new window within screen.\u00a0\u00a0 Describing the details of how screen works is beyond the scope of this entry, but it allows you to leave a session running even after you've logged out (or disconnected because of network issues).\u00a0\u00a0</p> <p>One challenge with the 'screen' command is that by default you can't load any modules or other system commands.\u00a0 That's because 'screen' does not run the shell profile commands (in /etc/profile and /etc/profile.d).\u00a0 HPCC configures the modules system and several other system variables/settings in these profile scripts. Without running them, the module system won't work.</p> <p>However, you can ask screen to run this profile by including the line shell=-$SHELL in the screen config file \".screenrc\" in your home directory.\u00a0 To make a screen config file, try this (provided you don't already have a .screenrc file)</p> <p>create default .screenrc</p> <pre><code>echo 'shell -$SHELL' &gt;&gt; ~/.screenrc\n</code></pre> <p>You'll have to exit and restart screen to see the changes.</p>"},{"location":"Virtual_Terminals/#screen-commands","title":"Screen Commands","text":"<p>To send commands to screen (instead of the window you're working in), you preface them with Ctrl-a, i.e., you type the Ctrl key and \"a\" together, release both keys, then type the next letter to invoke the command.</p> <ul> <li> <p>Ctrl-A then:</p> ? Display available screen commands \" List running screen windows N Display number of current window c Open a new window [number] Switch to window [number] [ Copy (so you can paste to another window) ] Paste k Kill the current window Ctrl-\\ Quit and close all screen windows A Label the screen window M Monitor for activity d Detach the current screen session </li> <li> <p>Startup commands:</p> <code>screen -ls</code> List detached screen sessions on the server screen -S (name for new screen) Create a screen with specified name <code>screen --r [id]</code> Reattach the specified screen </li> </ul>"},{"location":"Virtual_Terminals/#detachingreattaching-screen-sessions","title":"Detaching/Reattaching Screen Sessions","text":"<p>To detach a running screen session, type <code>Ctrl-A d</code> This will detach the screen session with all of its windows but leave all of the related processes running. You can start and detach as many screen sessions as you wish, each with its own windows and processes. You can even logout of the server, and your screen sessions will continue running while you're gone.</p> <p>To reattach a screen session, find the id of the one you want with <code>screen -ls</code> then reattach it with <code>screen -r id</code></p> <p>If you don't remember which screen sessions you have opened on our cluster, you can type:</p> <pre><code>module load powertools\nuserinfo &lt;uid&gt;\n</code></pre> <p>If you are running a licensed software (such as MATLAB) within a screen session, please remember to kill it when you're done developing for the day. This will help us better manage licenses for the MSU research community.</p>"},{"location":"Virtual_Terminals/#screen-customizations","title":"Screen customizations","text":"<p>The attached file (click here) includes customizations to screen that places an information bar at the bottom of the terminal. Many people find it very usefu.\u00a0\u00a0 View the file and copy selected or all lines to\u00a0 your \".screenrc\" file.\u00a0</p>"},{"location":"Virtual_Terminals/#tmux","title":"Tmux","text":"<p>tmux is intended to be the modern, BSD-licensed alternative to screen. Both programs are available on HPCC. It allows splitting a window horizontally and vertically, and copying and pasting between multiple buffers. More information is available at http://tmux.sourceforge.net.</p>"},{"location":"accessHPCC_overview/","title":"Overview of HPCC access","text":"<p>Accessing the HPCC resources requires a user account and a proper login method. In this section, you will find out information about</p> <p>1)  How to apply for an HPCC account</p> <p>2)  How to use SSH to connect to the HPCC</p> <p>3)  How to use your web browser to connect to the HPCC</p> <p>4)  Information for users from other universities in Michigan</p> <p>For details, please click on the relevant links in the navigation menu under the current section.</p>"},{"location":"bi/","title":"bi","text":"<p>bi is a powertools program that provides innformation about user buy-in accounts on SLURM. For more information on buy-in accounts, see here. Running bi on its own will display your user name, any SLURM account your belong to and you default account:</p> <pre><code>$ bi\n\nUser:     panchyni\nAccounts: bioinformaticscore   [ MaxJobs=520, MaxSubmit=1000 ]\n          scavenger            [ MaxJobs=520, MaxSubmit=1000 ]\n          general              [ MaxJobs=520, MaxSubmit=1000 ]\nDefault:  general\n</code></pre> <p>A full description of arguments for bi can be found by acccessing the help for the program by runnig \"bi -h\" on the command line (this information is also reproduced below)</p> <pre><code>$ bi -h\n\n  Usage:\n    bi\n    bi -h\n    bi [ -a &lt;account&gt; | -u &lt;user&gt; ]\n    bi -d [ -u &lt;user&gt; ]\n    bi -l [ -u &lt;user&gt; ]\n\n    -h | --help               Display this help message\n    -a | --account &lt;account&gt;  Display users and nodes of a buyin account\n    -u | --user &lt;user&gt;        Display buyin accounts for this user\n    -d | --default            Display only the user's default buyin account\n    -l | --list               List jobs and usage status of buyin nodes\n</code></pre>","tags":["reference"]},{"location":"development_nodes/","title":"Development nodes","text":"<p>Warning</p> <p>Any long running (total CPU run time <code>&gt; 2hrs</code>) jobs on dev nodes will be killed automatically without advance notice.</p> <p>The HPCC has several development nodes that are available for users to compile their code and do short runs (less than 2 hours) to estimate run-time and memory usage.\u00a0</p> <p>These development nodes run the latest operating system and have similar configurations and environment setups as the compute nodes of the same clusters.\u00a0 Please use these development nodes to compile your program and test the work flow of your job script. For running long-time or large-resource computations, please submit jobs to use compute nodes.</p> <p>Warning</p> <p>Code compiled on older development nodes (dev-intel14 and dev-intel14-k20) may have errors when running on the latest clusters due to an outdated instruction set. To resolve this, compile your code on a newer development node.</p> <p>Users may <code>ssh</code> to the development nodes after connecting to the gateway via SSH. To access a certain development node, for example dev-amd20, please run\u00a0<code>ssh dev-amd20</code> from the gateway.  Users may also directly connect to development nodes by setting up SSH Tunneling Alternatively, they may be accessed through the \"Development Nodes\" tab on OnDemand.</p> <p>Nodes with  -k80 or -v100 suffixes have GPU cards required by GPU-enabled software, but may be used for any software.\u00a0Note there is not a development node containing the AMD20 A100 GPUs.\u00a0</p> Node Hostname Cores Memory Notes dev-amd20 128 960GB AMD EPYC 7H12 64-Core Processor @ 2.6GHz dev-amd20-v100 48 187GB Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz and 4 Tesla V100S dev-intel18 40 377GB Two 2.4Ghz 20-core Intel Xeon Gold 6148 CPU (40 cores total) dev-intel16 28 128GB Two 2.4Ghz 14-core Intel Xeon E5-2680v4 (28 cores total) dev-intel16-k80 28 256GB Intel16 node with 4 Nvidia Tesla K80 GPUs <p>Once your program is compiled and job script is tested, users can submit it to the SLURM queue by specifying various constraints such as\u00a0job duration, memory usage, number of CPUs, software license reservations and so on.</p>","tags":["reference"]},{"location":"filesystem_overview/","title":"Overview of HPCC file systems","text":"<p>The HPCC is comprised of several different file systems:</p> <ul> <li>The Home Space</li> <li>Research Space</li> <li>Scratch Space</li> <li>Node-local storage space</li> </ul> <p>Home, Research and Scratch are network file systems, meaning that each node in the cluster must go through the network switch to access these spaces. This can potentially slow down file I/O for jobs. Alternatively, there are several options for local file storage which either uses hard drives connected to each development and compute node or the node's RAM. See each respective page above for more information. We also have Guidelines for Choosing File Storage and I/O.</p> <p>Data that are subject to external security requirements are considered sensitive data. These requirements may be imposed by regulations or contractual obligations. In either case, users that intend to store sensitive data on HPCC systems must work with us to ensure data security. For more information, check out our Sensitive data hosting wiki page.</p>"},{"location":"getexample/","title":"getexample","text":"<p>getexample is powertools program for downloading user examples for various programs installed on HPCC. Using getexample requires specifying an examplename a list of which can be found by running getexample without any arguments (see below):</p> <pre><code>$ getexample\nDownload an HPC example:\nusage:\n   getexample &lt;examplename&gt;\n\nWhere &lt;examplename&gt; is the name of the example you want to\ndownload.  This will create a directory named examplename which\nyou can cd into and read the README file for details (if one is\navailable) or read .qsub or .sb file for how the example is run.\nYou may submit the example with 'qsub *.qsub' or 'sbatch *.sb'.\n\nFor Example:\n  getexample helloworld\n\nPossible example names:\nabaqus_example        cuda_hybrid     Intro2Linux_Oct_2019    MATLAB_threadPool  PETSc_example\nABySS             DDT_examples    Job_dependency      MKL_benchmark      Python_MPI\nADMB_example          dmtcp_longjob   LAPACK_example      MKL_c_eigenvalues  Python_numpy\naffinity          espresso_benchmark  magma_example       MKL_Example        QuantumESPRESSO\nAmber_example         FFTW        makefile_example    MKL_FFTW       R_parallel_examples\nbasic_array_job       FI491       maker_MPI       MKL_parallel       SAS_example\nblast             fluentMPI       MATLAB_basic        mothur_example     ScaLAPACK\nblender_farm          fortran_openmp      MATLAB_compiler     MPI_OpenMP_GPU     simpleMatlab\nBOOST_example         FreeSurfer      MATLAB_GPU          MPI_pi         STATA_array\nbowtie            gmp_mpfr        MATLAB_many_jobs    multi_variable     STATA_example\nbrother_test          helloHPCC       MATLAB_parallel     NAMD_example       tbb_example\nClang_example         helloMPI        MATLAB_parameter_sweep  OpenACC_example    TotalView_MPI_example\nCMakePackageExamples  helloworld      MATLAB_parfor       openmp_exercise    VASP_example\ncuda              intro2hpcc      MATLAB_patternsearch    PC2HPC         XSEDE_MPI_WORKSHOP\n</code></pre>","tags":["reference"]},{"location":"install_ssh_client/","title":"Install SSH Client","text":"<p>To use the HPCC via a command line interface (as opposed to the webinterface provided by OnDemand), you will need an SSH client.  To launch interactive GUI programs over the command line, you will additionally need an X Windows (aka X11) server. Please see the following recommendations for your operating system.</p> WindowsMac OSLinux <p>We recommend installing\u00a0MobaXterm Home Edition. This program provides both an SSH client and an X Windows server. Follow this tutorial to set up MobaXterm.</p> <p>Warning</p> <p>Make sure you keep MobaXTerm up to date. If you use a new version of PuTTYgen to create SSH keys  (see here) an older version of MobaXTerm may not be able to read your generated keys.</p> <p>You can use Terminal program that comes installed with your operating system as your SSH Client. However, for running\u00a0graphical user interface (GUI) programs on HPCC, you will need to install the X Windows server program XQuartz. See https://www.xquartz.org/ for download instructions.</p> <p>Similar to Mac OS, you can use the built-in Terminal as your SSH client. There is no need to download additional X Windows server software.</p>","tags":["reference","ssh"]},{"location":"job_policies/","title":"Job Policies","text":"<p>The following limits apply generally to all MSU users of the HPCC. Those at affiliate institutions may be working under slightly different policies. The limits are in place to help our large user community share the HPCC. However, if these policies are an impediment to completing your research, please contact us.</p>","tags":["reference","slurm"]},{"location":"job_policies/#cpu-and-gpu-usage-limits","title":"CPU and GPU usage limits","text":"<ul> <li> <p>HPCC users who do not have a buy-in account are given a 'general'     SLURM account. The general account is limited to\u00a0500,000 CPU hours     (30,000,000 minutes) and 10,000\u00a0GPU hours (600,000 minutes)\u00a0every     year (from January 1st to December 31st) starting from 2021. </p> <ul> <li>A CPU hour is the walltime of your job multiplied by the number of CPUs used.   The same applies for GPU hours.</li> </ul> </li> <li> <p>There is no yearly usage limit on CPU or GPU time with a buy-in     account. If you have a buy-in account, your jobs will be run under that     account by     default, unless     the manager of the buy-in account has chosen to opt-in (requiring jobs to     be submitted with the <code>-A</code>     flag) instead of     opt-out.</p> </li> <li> <p>Users with general accounts can use the powertools command <code>SLURMUsage</code>     to check their used CPU and GPU time (in minutes) and remaining CPU and     GPU time (in hours):</p> <p><code>$ ml powertools # run this command if powertools not loaded</code></p> <p><code>$ SLURMUsage</code></p> </li> <li> <p>If users without a buy-in account need more CPU or GPU time due to     reaching the limits, they can request additional CPU/GPU hours     by filling out the CPU/GPU Increase Request online form.</p> </li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#limits-on-job-resource-requests","title":"Limits on job resource requests","text":"<ul> <li>Time: Users can schedule jobs and run for at most 7 days (168     hours)\u00a0 ( <code>--time=168:00:00</code>)</li> <li>CPU: Users can utilize up to a total of 1040 cores and have at most 520 jobs     running at any one time.\u00a0The core usage value is reflected in the SLURM     variable <code>QOSMaxCpuPerUserLimit</code>(Buyin groups who have purchased more than     1040 cores can exceed this limit)</li> <li>Queue: The maximum number of jobs that can be queued or running per user is     1000 jobs.</li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#buy-in-program","title":"Buy-in program","text":"<p>Faculty can purchase nodes via our buy-in program. The program guarantees jobs submitted with a buy-in group will start running on their buy-in nodes in 4 hours. However, due to contention between buy-in group jobs, the guarantee might not be fulfilled if requested resources are occupied or reserved by other jobs of the buy-in group.</p>","tags":["reference","slurm"]},{"location":"job_policies/#policy-summary","title":"Policy summary","text":"<ul> <li>Jobs that run under 4 hours are able to run on the largest set of     nodes (the combination of community + specialized hardware + buy-in     nodes.\u00a0 See below for details)\u00a0</li> <li>Jobs that request more resources (processors or RAM) have priorities     over smaller jobs because these jobs are more difficult to schedule.</li> <li>Jobs accrue priority based on how long they have been queued.</li> <li>The scheduler will attempt to balance usage among all users. (See     Fairshare Policy below.)</li> <li>It is against our fair use policy to artificially increase the     priority of a job in the queue (e.g. by requesting more resources     which will not be used). Jobs found to be manipulating the scheduler     will be canceled, and users continuing to attempt this will be     suspended.</li> </ul>","tags":["reference","slurm"]},{"location":"job_policies/#more-about-queue-time","title":"More about queue time","text":"<p>This section gives a brief overview of the factors that affect how long your job sits in the SLURM queue. For more information, see the page on how jobs are scheduled by SLURM as well as other pages under \"Understanding the Scheduler.\"</p>","tags":["reference","slurm"]},{"location":"job_policies/#fairshare","title":"Fairshare","text":"<p>As jobs wait in the queue, they accrue priority to run. Another factor that contributes to a job's priority value is Fairshare. The scheduler will attempt to ensure fair resource utilization of all HPCC users by adjusting the initial priorities of the users who have recently used HPCC resources. Due to the policy, if users had jobs running with many resources recently, their current pending jobs might wait longer than before. Users can find the Fairshare contribution to a job priority by running command \"<code>sprio -u $USER</code>\":</p> <pre><code>[UserID@dev-intel18 UserID]$ sprio -u $USER\n          JOBID PARTITION     USER   PRIORITY       SITE        AGE  FAIRSHARE        QOS                 TRES\n       53381467 general-l   UserID      49432          0          0      49318          0       cpu=100,mem=15\n       53381467 general-s   UserID      49432          0          0      49318          0       cpu=100,mem=15\n</code></pre> <p>where it is found under <code>FAIRSHARE</code> column and the values are between 60,000 (highest priority contribution) and 0 (lowest priority contribution).  The more resources your jobs used recently, the less your Fairshare value will become, resulting in lower overall priority for your jobs. For other contributions of <code>sprio</code> results, please check Job Priority Factors.</p>","tags":["reference","slurm"]},{"location":"job_policies/#shorter-jobs-can-run-on-more-nodes","title":"Shorter jobs can run on more nodes","text":"<p>Jobs that request a total running (wall-clock) time of four hours or less can run on any available buy-in and specialized nodes. Because they can access any nodes, they are likely to start running more quickly than the jobs which have to wait for the general-long partition nodes.</p>","tags":["reference","slurm"]},{"location":"job_policies/#bigger-jobs-are-prioritized-small-jobs-are-backfilled","title":"Bigger jobs are prioritized &amp; small jobs are backfilled","text":"<p>The scheduler attempts to gather resources for large jobs and then backfill smaller jobs around them. The size of the job is determined by the number of CPUs and amount of memory requested.</p> <p>The scheduler packs small jobs together to allow more resources to be gathered for multi-core jobs.\u00a0 Resource requests are monitored. Abusive resource requests may violate MSU policy.</p>","tags":["reference","slurm"]},{"location":"js/","title":"js","text":"<p>The SLURM command sacct can be used to show the job steps of a job and the resource usages after it finished running.:</p> <pre><code>sacct -j 40410\n       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode\n------------ ---------- ---------- ---------- ---------- ---------- --------\n40410               job classres-+   classres         28  COMPLETED      0:0\n40410.batch       batch              classres         28  COMPLETED      0:0\n40410.extern     extern              classres         28  COMPLETED      0:0\n40410.0            pw.x              classres         28  COMPLETED      0:0\n40410.1            ph.x              classres         28  COMPLETED      0:0\n</code></pre> <p>However, to display your desired results, it might take you some time to look into the web site and learn how to use the command. Here we  introduce the powertools command \"<code>js</code>\" to display the resource usages of your jobs.</p>","tags":["reference"]},{"location":"js/#display-usage-info-of-a-job","title":"Display Usage Info of a Job","text":"<p>Users can simply run the powertools command \"<code>js</code>\" and it gives you most of the useful resource usages. To see the resource usages of a job, just use the command \"<code>js -j &lt;JobID&gt;</code>\", e.g.,</p> <pre><code>$ js -j 45251                      # powertools command\n\nSLURM Job ID: 45251\nWrkDir=/mnt/home/changc81/GetExample/GaAs\nstdout=/mnt/home/changc81/GetExample/GaAs/slurm-45251.out\n=========================================================================================================\n          JobID |               45251 |         45251.batch |        45251.extern |             45251.0 |\n        JobName |                 job |               batch |              extern |                pw.x |\n           User |            UserName |                     |                     |                     |\n       NodeList |             lac-421 |             lac-421 |             lac-421 |             lac-421 |\n         NNodes |                   1 |                   1 |                   1 |                   1 |\n         NTasks |                     |                   1 |                   1 |                  28 |\n          NCPUS |                  28 |                  28 |                  28 |                  28 |\n         ReqMem |               112Gn |               112Gn |               112Gn |               112Gn |\n      Timelimit |            04:00:00 |                     |                     |                     |\n        Elapsed |            00:00:16 |            00:00:16 |            00:00:16 |            00:00:14 |\n       TotalCPU |           05:39.544 |           00:00.999 |           00:00.001 |           05:38.543 |\n     AveCPULoad |             21.2215 |           0.0624375 |            6.25e-05 |             24.1816 |\n         MaxRSS |                     |                     |                 20K |              60296K |\n      MaxVMSize |                     |             189200K |               4184K |             605104K |\n          Start | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:40 | 2018-08-28T20:27:41 |\n            End | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:56 | 2018-08-28T20:27:55 |\n       ExitCode |                 0:0 |                 0:0 |                 0:0 |                 0:0 |\n          State |           COMPLETED |           COMPLETED |           COMPLETED |           COMPLETED |\n=========================================================================================================\n</code></pre> <p>If you would like to show more data of a job, you can also use the specification -F:</p> <pre><code>$ js -j &lt;Job ID&gt; -F                   # powertools command\n</code></pre> <p>to list all stored data of the job steps.</p>","tags":["reference"]},{"location":"js/#display-a-list-of-jobs","title":"Display a List of Jobs","text":"<p>If users would like to know a list of jobs submitted before, they can use\u00a0 \"<code>js -z</code>\" command. Simply provide a period of time when job was running with -S (start time of the period) and -E (end time of the period) options:</p> <pre><code>$ js -z -S &lt;Start Time&gt; -E &lt;End Time&gt;\n</code></pre> <p>and a list of the jobs with their properties and resource usages is displayed. For example, user can run the command:</p> <pre><code>$ js -z -S 2021-04-12 -E 2021-04-19\n       JobID    JobName NNo NTas NCPU   Timelimit     Elapsed  AveCPU     MaxRSS Stat Exit               Start          NodeList\n------------ ---------- --- ---- ---- ----------- ----------- ------- ---------- ---- ---- ------------------- -----------------\n    21043834 ondemand/+   1    1    1    01:00:00    01:00:16 0.05487    467.41M TIM+  0:0 2021-04-12T08:59:10           css-033\n    21127831    fi_info   1    1    2    00:05:00    00:03:19 1.18321  58191.45M COM+  0:0 2021-04-16T10:14:33           skl-033\n    21158898  hello.exe   1    8    1    00:20:00    00:01:09   0.236    644.61M COM+  0:0 2021-04-17T20:17:57           amr-133\n    21158916 interacti+   1    1    1    03:00:00    02:00:04 0.77325   1244.61M COM+  0:0 2021-04-18T20:18:29           css-033\n    21158973     SPAdes   1    4    8    09:30:00    09:00:04   7.254     13.20G FAI+  1:0 2021-04-19T20:20:44           lac-421\n</code></pre> <p>to see a list of jobs running between April 12th 2021 and April 19th 2021. If any one of the options -S or -E is not specified, the time will be considered as the current time of \"<code>js</code>\" execution.</p>","tags":["reference"]},{"location":"js/#more-selections-of-js-command","title":"More Selections of js Command","text":"<p>To see all possible usages of the command, please use the specification -h:</p> <pre><code>$ js -h\n\njs [&lt;OPTION&gt;]\n     Valid &lt;OPTION&gt; values are:\n     -a, --allusers:\n                   Display jobs for all users. By default, only the\n                   current user's jobs are displayed.  If ran by user root\n                   this is the default.\n     -A, --accounts:\n                   Use this comma separated list of accounts to select jobs\n                   to display.  By default, all accounts are selected.\n     -b, --brief:\n                   Equivalent to '--format=jobstep,state,error'.\n     -c, --completion: Use job completion instead of accounting data.\n         --delimiter:\n                   ASCII characters used to separate the fields when\n                   specifying the  -p  or  -P options. The default delimiter\n                   is a '|'. This options is ignored if -p or -P options\n                   are not specified.\n     -C:\n                   Display results in columns rather than rows. Each\n                   column shows all data of a job step. A number can\n                   be specified after -C for how many columns in a row.\n     -D, --duplicates:\n                   If Slurm job ids are reset, some job numbers may\n                   appear more than once referring to different jobs.\n                   Without this option only the most recent jobs will be\n                   displayed.\n     -e, --helpformat:\n                   Print a list of fields that can be specified with the\n                   '--format' option\n     -E, --endtime:\n                   Select jobs eligible before this time.  If states are\n                   given with the -s option return jobs in this state before\n                   this period.\n         --federation: Report jobs from federation if a member of a one.\n     -f, --file=file:\n                   Read data from the specified file, rather than Slurm's\n                   current accounting log file. (Only appliciable when\n                   running the filetxt plugin.)\n     -F:\n                   Display data of all fields (--format=ALL) in columns.\n                   By default, three columns are shown in a row. See -C\n                   to change the default column number.\n     -g, --gid, --group:\n                   Use this comma separated list of gids or group names\n                   to select jobs to display.  By default, all groups are\n                   selected.\n     -h, --help:   Print this description of use.\n     -i, --nnodes=N:\n                   Return jobs which ran on this many nodes (N = min[-max])\n     -I, --ncpus=N:\n                   Return jobs which ran on this many cpus (N = min[-max])\n     -j, --jobs:\n                   Format is &lt;job(.step)&gt;. Display information about this\n                   job or comma-separated list of jobs. The default is all\n                   jobs. Adding .step will display the specific job step of\n                   that job. (A step id of 'batch' will display the\n                   information about the batch step.)\n     -k, --timelimit-min:\n                   Only send data about jobs with this timelimit.\n                   If used with timelimit_max this will be the minimum\n                   timelimit of the range.  Default is no restriction.\n     -K, --timelimit-max:\n                   Ignored by itself, but if timelimit_min is set this will\n                   be the maximum timelimit of the range.  Default is no\n                   restriction.\n         --local   Report information only about jobs on the local cluster.\n                   Overrides --federation.\n     -l, --long:\n                   Equivalent to specifying\n                   '--format=jobid,jobname,partition,maxvmsize,maxvmsizenode,\n                             maxvmsizetask,avevmsize,maxrss,maxrssnode,\n                             maxrsstask,averss,maxpages,maxpagesnode,\n                             maxpagestask,avepages,mincpu,mincpunode,\n                             mincputask,avecpu,ntasks,alloccpus,elapsed,\n                             state,exitcode,avecpufreq,reqcpufreqmin,\n                             reqcpufreqmax,reqcpufreqgov,consumedenergy,\n                             maxdiskread,maxdiskreadnode,maxdiskreadtask,\n                             avediskread,maxdiskwrite,maxdiskwritenode,\n                             maxdiskwritetask,avediskread,allocgres,reqgres\n     -L, --allclusters:\n                   Display jobs ran on all clusters. By default, only jobs\n                   ran on the cluster from where sacct is called are\n                   displayed.\n     -M, --clusters:\n                   Only send data about these clusters. Use \"all\" for all\n                   clusters.\n     -n, --noheader:\n                   No header will be added to the beginning of output.\n                   The default is to print a header.\n     --noconvert:\n                   Don't convert units from their original type\n                   (e.g. 2048M won't be converted to 2G).\n     -N, --nodelist:\n                   Display jobs that ran on any of these nodes,\n                   can be one or more using a ranged string.\n     --name:\n                   Display jobs that have any of these name(s).\n     -o, --format:\n                   Comma separated list of fields. (use \"--helpformat\"\n                   for a list of available fields).\n     -p, --parsable: output will be '|' delimited with a '|' at the end\n     -P, --parsable2: output will be '|' delimited without a '|' at the end\n     -q, --qos:\n                   Only send data about jobs using these qos.  Default is all.\n     -r, --partition:\n                   Comma separated list of partitions to select jobs and\n                   job steps from. The default is all partitions.\n     -s, --state:\n                   Select jobs based on their current state or the state\n                   they were in during the time period given: running (r),\n                   completed (cd), failed (f), timeout (to), resizing (rs),\n                   deadline (dl) and node_fail (nf).\n     -S, --starttime:\n                   Select jobs eligible after this time.  Default is\n                   00:00:00 of the current day, unless '-s' is set then\n                   the default is 'now'.\n     -T, --truncate:\n                   Truncate time.  So if a job started before --starttime\n                   the start time would be truncated to --starttime.\n                   The same for end time and --endtime.\n     -u, --uid, --user:\n                   Use this comma separated list of uids or user names\n                   to select jobs to display.  By default, the running\n                   user's uid is used.\n     --units=[KMGTP]:\n                   Display values in specified unit type. Takes precedence\n                   over --noconvert option.\n     --usage:      Display brief usage message.\n     -v, --verbose:\n                   Primarily for debugging purposes, report the state of\n                   various variables during processing.\n     -V, --version: Print version.\n     -W, --wckeys:\n                   Only send data about these wckeys.  Default is all.\n     --whole-hetjob=[yes|no]:\n                   If set to 'yes' (or not set), then information about all\n                   the heterogeneous components will be retrieved. If set\n                   to 'no' only the specific filtered components will be\n                   retrieved.\n     -x, --associations:\n                   Only send data about these association id.  Default is all.\n     -X, --allocations:\n                   Only show statistics relevant to the job allocation\n                   itself, not taking steps into consideration.\n     -z:           Show simple summary data only.\n\n     Note, valid start/end time formats are...\n                   HH:MM[:SS] [AM|PM]\n                   MMDD[YY] or MM/DD[/YY] or MM.DD[.YY]\n                   MM/DD[/YY]-HH:MM[:SS]\n                   YYYY-MM-DD[THH:MM[:SS]]\n</code></pre>","tags":["reference"]},{"location":"make/","title":"<code>make</code>","text":"<p>The program <code>make</code> and Makefiles are a simple way to organize code compilation. This tutorial offers a very basic idea of what is possible using <code>make</code>.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#prerequisites","title":"Prerequisites","text":"<p>For this tutorial, please run</p> <pre><code>module load powertools\ngetexample makefile_example2\n</code></pre> <p>This will create a directory <code>makefile_example2</code> with a main program <code>hello.c</code>, a function <code>hellofunc.c</code>, and an include file <code>hello.h</code>.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#compiling-by-hand","title":"Compiling by hand","text":"<p>To compile this code, you would use the following command:</p> <pre><code>gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>This command compiles the two C files, and names the executable hello. With the <code>-I.</code> flag, <code>gcc</code> will look in the current directory for the include file <code>hello.h</code>. For future steps, please remove the executable with <code>rm hello</code>.</p> <p>With only two C files, it is easy to compile with the above approach, but with more files, it is harder to keep track of everything. In addition, if you are only making changes to one C file, the above approach recompiles all of C files every time which is time-consuming and inefficient.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#transferring-to-a-makefile","title":"Transferring to a Makefile","text":"<p>A Makefile will be helpful for such cases. A simple Makefile is included in the <code>getexample</code>, so before we start, please move it to <code>makefile.bak</code> with <code>mv makefile makefile.bak</code>.</p> <p>Now, in the <code>makefile_example2</code> directory, create a file called <code>makefile</code> which has the following two lines:</p> makefile<pre><code>hello: hello.c hellofunc.c\n    gcc -o hello hello.c hellofunc.c -I.\n</code></pre> <p>Here, the tab must actually be a tab character, not spaces.</p> <p>Now, type <code>make</code> on the terminal and check if the executable is created. The <code>make</code> command will execute the compile command as you have written it in the Makefile.</p> <p>Note that invoking <code>make</code> with no arguments executes the first rule in the file. Furthermore, by putting the list of files on which the command depends on the first line after the ':', <code>make</code> knows that the rule <code>hello</code> needs to be executed if any of those files change.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#useful-makefile-variables","title":"Useful Makefile variables","text":"<p>Can we make it a little bit more efficient? Let's modify <code>makefile</code> like so:</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>In this Makefile, we define the variables <code>CC</code> and <code>CFLAGS</code>, which are special macros communicating to <code>make</code> how we want to compile the files <code>hello.c</code> and <code>hellofunc.c</code>. In particular, <code>CC</code> is for the C compiler, and <code>CFLAGS</code> is the list of flags to pass to C compiler.</p> <p>By putting the object files (<code>hello.o</code> and <code>hellofunc.o</code>) in the dependency list and in the rule, <code>make</code> will automatically compile the <code>.c</code> files individually into object files, and then build the executable <code>hello</code>. If your project is small (just a few files), this form of Makefile is enough.</p>","tags":["tutorial","make","compilers"]},{"location":"make/#setting-up-dependencies","title":"Setting up dependencies","text":"<p>However, there is a problem. This Makefile misses the include files. For example, if you made a change to <code>hello.h</code>, <code>make</code> would not recompile the <code>.c</code> files, even though the change in <code>hello.h</code> may affect them. In order to fix this problem, we need to tell <code>make</code> that all <code>.c</code> files depend on certain <code>.h</code> files. This can be done by writing a simple rule and adding it to the Makefile.</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: hello.o hellofunc.o\n    $(CC) -o hello hello.o hellofunc.o\n</code></pre> <p>This addition first creates the macro <code>DEPS</code> (the macro name can be anything), which is the set of <code>.h</code> files on which the <code>.c</code> files depend.</p> <p>Then we define a rule for all <code>.o</code> files. The rule says that each <code>.o</code> file depends on </p> <ul> <li>the <code>.c</code> file with the same name, and </li> <li>the <code>.h</code> files which are included in <code>DEPS</code>.</li> </ul> <p>Next, the rule says that to generate the <code>.o</code> file, <code>make</code> needs to compile the <code>.c</code> file using the compiler defined in <code>CC</code>. The components are described as follows:</p> <ul> <li>the <code>-c</code> flag says to generate the object file, </li> <li>the <code>-o $@</code> says to put the output of the compilation in the file named on   the left side of the <code>:</code> (in this case, the <code>.o</code> file), </li> <li>the <code>$&lt;</code> is the first item in the dependencies list (in this case, the <code>.c</code>   file), </li> <li>and the <code>CFLAGS</code> macro is defined on the second line.</li> </ul>","tags":["tutorial","make","compilers"]},{"location":"make/#generalizing","title":"Generalizing","text":"<p>To simplify the final rule, you can use special macros <code>$@</code> and <code>$^</code>, which are the left and right sides of the <code>:</code>, respectively. This also generalizes the rule to work for multiple files at once.</p> <p>In the example below, all of the include files should be listed as part of the macro <code>DEPS</code>, and all of the object files should be listed as part of the macro <code>OBJ</code>.</p> makefile<pre><code>CC=gcc\nCFLAGS=-I.\nDEPS = hello.h\nOBJ = hello.o hellofunc.o\n\n%.o: %.c $(DEPS)\n    $(CC) -c -o $@ $&lt; $(CFLAGS)\n\nhello: $(OBJ)\n    $(CC) -o $@ $^ $(CFLAGS)\n</code></pre>","tags":["tutorial","make","compilers"]},{"location":"make/#further-resources","title":"Further resources","text":"<p>Now you have a good sense of the Makefile. For more information on Makefiles and the <code>make</code> function, check out the GNU Make Manual. </p> <p>You can download some other Makefile examples on the HPCC using getexample.</p>","tags":["tutorial","make","compilers"]},{"location":"modules_changelog/","title":"Module changelog","text":"<p>Note: this is for CentOS system software and so not applicable to the new Ubuntu system.</p> <ul> <li>2024-03-19 - Installation of moduel DFTB+/21.1 from source (Xiaoge Wang)</li> <li>2024-03-04 - Installed Module: DMTCP Version: 3.0.0 (Craig Gross)</li> <li>2024-03-04 - Installed Module: PAML Version: 4.10.5 (Craig Gross)</li> <li>2024-02-21 - Manually added an enviroment variable that sets the path to the plugin folder (Nicholas Louis Panchy)</li> <li>2024-02-20 - Installed Apache Spark and dependencies (Andrew Giles Fullard)</li> <li>2024-02-16 - Installed Module: htop Version: 2.0.0 and NO dependencies (Craig Gross)</li> <li>2024-01-31 - Installed Module: CGAL 5.5.2 (Andrew Giles Fullard)</li> <li>2024-01-29 - installation of Trilinos/13.4.1-GCC-12.2.0. (Xiaoge Wang)</li> <li>2024-01-26 - Partial install of Trilinos/13.4.1 by Xiaoge, work in progress (Nicholas Louis Panchy)</li> <li>2024-01-26 - Updated BCFtools, GSL, and HTSlib for BCFtools 1.17 (Nicholas Louis Panchy)</li> <li>2024-01-19 - Installed Module: Blender Version 3.4.1 and 1 dependency (Craig Gross)</li> <li>2024-01-11 - added abaqus 2023.lua (Jim Leikert)</li> <li>2024-01-04 - Julia 1.10 (Andrew Giles Fullard)</li> <li>2024-01-03 - Change EasyBuild Version 4.8.1 configuration (Craig Gross)</li> <li>2023-12-19 - Installed new version of Nextflow, 23.10.0 (Nicholas Louis Panchy)</li> <li>2023-12-19 - Installed Module: Molpro versio 2023.2.0, source build and binaries (Nicholas Louis Panchy)</li> <li>2023-12-19 - Installed Module: ESMF Version: 8.3.0 and 2 dependencies (Xiaoge Wang)</li> <li>2023-12-18 - Installed Module: M4 Version: 1.4.19 and 1 dependency (Craig Gross)</li> <li>2023-12-18 - Installed Module: VoyantServer Version: 2.6.10-Java-11.0.2 and NO dependencies (Craig Gross)</li> <li>2023-12-14 - Updaed breseq(0.38.1) for a BioinformaticsCore Project (Nicholas Louis Panchy)</li> <li>2023-12-14 - Added new versions of ulia (1.8.0,1.8.2,1.9.3) (Nicholas Louis Panchy)</li> <li>2023-12-06 - Installed Module: htop Version: 2.0.0 and NO dependencies (Craig Gross)</li> <li>2023-12-01 - Installation of GDB/13.2 and its dependency. (Xiaoge Wang)</li> <li>2023-11-30 - Fix a path error of .lua file for module OpenMPI/4.1.1-gcccuda-11.1.0-11.4.0 (Xiaoge Wang)</li> <li>2023-11-30 - added schrodinger 2023-4 module (Jim Leikert)</li> <li>2023-11-21 - added mathematica 13.3.1.lua modulefile (Jim Leikert)</li> <li>2023-11-16 - Installation of GROMACS/2023.1-CUDA-11.8.0 and its dependencies. (Xiaoge Wang)</li> <li>2023-11-16 - Installation of OpenBLAS/0.3.23 (Xiaoge Wang)</li> <li>2023-11-14 - reinstall PHP/8.2.12 with more extentions. (Xiaoge Wang)</li> <li>2023-11-13 - Install module phoronix-test-suite and its dependency php/8.2.12. (Xiaoge Wang)</li> <li>2023-11-06 - Add a warning message to the module file. This version only works on intel18 nodes. (Xiaoge Wang)</li> <li>2023-11-02 - Installation of VASP/6.2.1 revised version. See RT #74184 for details. (Xiaoge Wang)</li> <li>2023-10-30 - Remove the module HDF5 built with GCC toolchain (Xiaoge Wang)</li> <li>2023-10-30 - Install HDF5/1.14.0 with toolchain gompi/2023a (Xiaoge Wang)</li> <li>2023-10-30 - Create a toolchain gompi/2023a (Xiaoge Wang)</li> <li>2023-10-30 - Installation of HDF5/1.14.0. (Xiaoge Wang)</li> <li>2023-10-30 - Install libevent/2.1.12 and PMIx/4.2.4 for dependency of OpenMPI/5.0.0-CUDA-12.3.0. But there is some issue. Use bundled module instead. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libpciaccess/0.17, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of hwloc/2.9.1, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libfabri/1.18.0, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of UCX/1.14.1, a dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of libxml2/2.11.4, dependency of OpenMPI/5.0.0-CUDA-12.3.0. (Xiaoge Wang)</li> <li>2023-10-30 - Installation of module numactl/2.0.16, which is a dependency of OpenMPI/5.0.0-CUDA-12.3.0 (Xiaoge Wang)</li> <li>2023-10-30 - Installation of module OpenMPI/5.0.0-CUDA-12.3.0 (Xiaoge Wang)</li> <li>2023-10-29 - Installation of module pkg-config/0.29.2 (Xiaoge Wang)</li> <li>2023-10-29 - Installation of module GCC/12.3.0 (Xiaoge Wang)</li> <li>2023-10-29 - installation of CUDA/12.3.0 (Xiaoge Wang)</li> <li>2023-10-27 - Modify the lua file to fix the issue of installation paths of CUDA/8.0.44 with intel tool chain. (Xiaoge Wang)</li> <li>2023-10-26 - Finished Rust 1.70 install (Nicholas Louis Panchy)</li> <li>2023-10-26 - Finished Trinity 2.15.1 install (Nicholas Louis Panchy)</li> <li>2023-10-25 - New EasyBuild with updated easyblocks for recent software builds (Nicholas Louis Panchy)</li> <li>2023-10-25 - New alphafold singularity image and support scripts (Nicholas Louis Panchy)</li> <li>2023-10-25 - Parts of the GCC 12.3.0 tool chain for the incomplete build of Rust 1.70 (Nicholas Louis Panchy)</li> <li>2023-10-25 - Additional parts of the Trinit 2.51.1 build (Nicholas Louis Panchy)</li> <li>2023-10-25 - Updated bioinformatics software for incompleted build of Trinity 2.15.1 (Nicholas Louis Panchy)</li> <li>2023-10-18 - install elbencho/2.0-3 with CUDA support (Xiaoge Wang)</li> <li>2023-10-13 - testing push (Joel)</li> <li>2023-10-13 - testing push (Joel)</li> <li>2023-10-11 - Installation of Nektar++ and OrthoFinder (Xiaoge Wang)</li> <li>2023-10-06 - Add module gffcompare/0.10.6 (Xiaoge Wang)</li> <li>2023-10-04 - reinstall OrthoFinder/2.5.4 with explicitly add MAFF as dependency (Xiaoge Wang)</li> <li>2023-10-03 - Flow3d 2023R1 file cleanup (Jim Leikert)</li> <li>2023-10-02 - Build OrthoFinder module verion 2.5.4 with Scipy-boundle version 2019.10-Python-2.7.16 to fix the issue in the version built with Scipy-boundle version 2019.10-Python-3.7.4 (Xiaoge Wang)</li> <li>2023-09-27 - libaio/0.3.112 is installed as the dependcy of module elbencho/2.0-3 (Xiaoge Wang)</li> <li>2023-09-27 - elbencho is a distributed storage benchmark for files, objects &amp; blocks with support for GPUs (Xiaoge Wang)</li> <li>2023-09-27 - added flow3d 2023R1 (Jim Leikert)</li> <li>2023-09-22 - Module file of LAMMPS/2Aug2023-Python-3.9.6 (Xiaoge Wang)</li> <li>2023-09-22 - Module file of LAMMPS/2Aug2023-kokkos. (Xiaoge Wang)</li> <li>2023-09-22 - Added two new versions of RSEM/1.3.3 and associated software. New builds support updated versions of the STAR aligner. (Nicholas Louis Panchy)</li> <li>2023-09-21 - changed license server info for ansys v. 16,18,19 (Jim Leikert)</li> <li>2023-09-21 - added converge 3.0.28 ompi/impi module files (Jim Leikert)</li> <li>2023-09-14 - Install JAGS/4.3.1 (Compatible with R/4.2.1) (Craig Gross)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-09 - New install (Xiaoge Wang)</li> <li>2023-09-08 - New install (Xiaoge Wang)</li> <li>2023-09-08 - New install (Xiaoge Wang)</li> <li>2023-09-08 - new install. (Xiaoge Wang)</li> <li>2023-08-31 - Added WPS module, companion to WRF (Nicholas Louis Panchy)</li> <li>2023-08-31 - Add a path to fix the error. (Xiaoge Wang)</li> <li>2023-08-29 - new install (Nanye Long)</li> <li>2023-08-28 - Install RStudio-Server/2022.07.02 (Craig Gross)</li> <li>2023-08-22 - add NextDenovo lua (Nanye Long)</li> <li>2023-08-22 - VASP built with VTST tools. (Xiaoge Wang)</li> <li>2023-08-21 - Add hwloc dependency to OpenMPI/2.1.2 (Craig Gross)</li> <li>2023-08-17 - New installation of a dependent (Xiaoge Wang)</li> <li>2023-08-17 - added tecplot360 2023R1 (Jim Leikert)</li> <li>2023-08-09 - SciPy-bundle built with foss-2022b and associated tools for unfinished R/4.3.1 (Nicholas Louis Panchy)</li> <li>2023-08-09 - More of the foss-2022b install fromthe the unfinished R.4.3.1 install (Nicholas Louis Panchy)</li> <li>2023-08-09 - Installed AnsysEM 2023R2 (Andrew Giles Fullard)</li> <li>2023-08-09 - Installed (some) of foss-2022b as part of unfinished R.4.3.1 install (Nicholas Louis Panchy)</li> <li>2023-08-07 - Initial commit (Andrew Giles Fullard)</li> </ul>","tags":["reference","software"]},{"location":"obtain_an_hpcc_account/","title":"Obtaining an HPCC account","text":"<p>Every user needs to have an account to use the HPCC. Below, we provide directions for different situations.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#current-msu-affiliated-employees","title":"Current MSU-affiliated employees","text":"<p>HPCC accounts are free for all MSU researchers. To obtain an account, a Principal Investigator (PI) needs to complete an online New Account Request Form for themselves and their personnel (staff, students, staff, post-docs) who have an existing MSU NetID. Information needed includes:</p> <ul> <li>a list of the names and MSU NetIDs of personnel requiring HPCC accounts</li> <li>a statement on whether or not export controlled software or data will be used</li> <li>an abstract describing your research </li> </ul> <p>By applying for an HPCC account, the PI agrees that all group members will abide by MSU's Acceptable Use Policy.</p> <p>If you are going to attend an ICER workshop, you may get a temporary HPCC account. Please contact ICER for further instruction.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#previous-msu-affiliated-hpcc-users","title":"Previous MSU-affiliated HPCC users","text":"<p>If you were previously affiliated with MSU and used to have an HPCC account, your account can be disabled by our yearly departed user cleanup. In order to reactivate your old account, a PI (e.g., your collaborator) needs to sponsor the renewal for you. They need to fill out the HPCC Sponsored Renewal Form on an annual basis.</p> <p>Note: All MSU Net IDs used to access the HPCC must be associated with a valid email address. If your previous MSU email no longer works, please provide us with an alternative email.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#external-collaborators","title":"External collaborators","text":"<p>To request an HPCC account for an external collaborator, the PI must obtain a login-only, non-email, MSU Guest ID for their collaborator first. Either contact MSU ID Office at idoffice@msu.edu  or visit their website. Once the Guest ID is acquired for the collaborator, the PI can follow the above instructions, by filling out the New Account Request Form. Since the Guest ID is not associated with a valid email address, the PI is responsible for providing a working email of the collaborator when completing the request form.</p>","tags":["accounts"]},{"location":"obtain_an_hpcc_account/#other-affiliated-universities-in-michigan","title":"Other affiliated universities in Michigan","text":"<p>If you are affiliated with one of the universities listed below, please review their Documentation and email the Contact shown in the table below.</p> University Contact Documentation Oakland University Mario Nowak: nowak@oakland.eduTomas Hajek:hajek@oakland.edu University Technology Services Western Michigan University Leonard Peirce:leonard.peirce@wmich.edu Research and Innovation HPCHub Central Michigan University Mel Taylor:taylo1ml@cmich.edu Office of Information Technology <p>For MSU PIs who want to sponsor HPCC accounts for users from one of these affiliated universities, please follow the following instructions:</p> <ul> <li> <p>The affiliated user must first register for an MSU Guest Account. Make sure that an official .edu email address from the affiliated university is used for the registration; otherwise, the HPCC account will not be created.</p> </li> <li> <p>Once the MSU Guest Account has been activated for the affiliated user, they will need to complete the ICER Community ID Form, choosing MSU to authenticate, and using the credentials established while registering for the MSU Guest Account.</p> </li> <li> <p>Once the ICER Community ID Form is received by HPCC staff, we will reach out to the MSU PI for approval and create the account.</p> </li> </ul>","tags":["accounts"]},{"location":"orthomcl-pipeline/","title":"orthomcl-pipeline","text":"<p>OrthoMCL Pipeline (https://github.com/apetkau/orthomcl-pipeline) is a wrapper that automates running of OrthoMCL. If you prefer to run OrthoMCL from scratch, please skip this tutorial.</p>"},{"location":"orthomcl-pipeline/#installation-guide","title":"Installation guide","text":"<p>You could install orthomcl pipeline to your home directory (or research space), following the instruction of installing OrthoMCL pipeline. All the Perl dependencies have been installed by iCER staff, and you only need to run a couple of commands to complete the installation. Importantly, it's assumed that you have already prepared your MySQL configuration file.</p>"},{"location":"orthomcl-pipeline/#sample-installation","title":"Sample installation","text":"<p>I am going to install the pipeline in a subdirectory under my home <code>~/Software/</code>.</p> <p>Installing OrthoMCL Pipeline</p> <pre><code>ssh dev-intel18\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26 impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\n\n\n# Download source and configure\ncd Software\ngit clone https://github.com/apetkau/orthomcl-pipeline.git\ncd orthomcl-pipeline\nperl scripts/orthomcl-pipeline-setup.pl # set paths to dependencies\ncat etc/orthomcl-pipeline.conf # parameters in this file can be adjusted; consult the instruction linked above\n    # ---\n    # blast:\n    #   F: 'm S'\n    #   b: '100000'\n    #   e: '1e-5'\n    #   v: '100000'\n    # filter:\n    #   max_percent_stop: '20'\n    #   min_length: '10'\n    # mcl:\n    #   inflation: '1.5'\n    # path:\n    #   blastall: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/blastall\n    #   formatdb: /opt/software/BLAST/2.2.26-Linux_x86_64/bin/formatdb\n    #   mcl: /opt/software/MCL/14.137-intel-2016b/bin/mcl\n    #   orthomcl: /opt/software/OrthoMCL/orthomclsoftware-custom/bin\n    # scheduler: fork\n    # split: '4'\n\n\n# Testing\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\nperl t/test_pipeline.pl -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config -s fork -t ~/tmp # replace the path to orthomcl.config with your own\n</code></pre>"},{"location":"orthomcl-pipeline/#example-ortholog-identification","title":"Example: ortholog identification","text":"<p>The tutorial is adapted from a tutorial hosted at https://github.com/apetkau/microbial-informatics-2014/tree/master/labs/orthomcl. We strongly recommend that you read it fully before starting the hands-on practice below, which is a much simplified version of the original one and serves as a demo only. The datasets containing a set of V. Cholerae genomes are located in <code>mnt/research/common-data/Bio/orthomcl-data/</code>.</p> <pre><code>ssh dev-intel18\n\n# Then go to your orthomcl working directory\n\n# Load necessary modules\nmodule purge\nmodule load icc/2016.3.210-GCC-5.4.0-2.26  impi/5.1.3.181\nmodule load OrthoMCL/2.0.9-custom-Perl-5.24.0\nmodule load BLAST/2.2.26-Linux_x86_64\nmodule load GCCcore/5.4.0 libxml2/2.9.4\nexport PATH=~/Software/orthomcl-pipeline/bin:~/Software/orthomcl-pipeline/scripts:$PATH\n\n# Run orthomcl pipeline (replace the path to orthomcl.config with your own)\northomcl-pipeline -i /mnt/research/common-data/Bio/orthomcl-data -o orthomcl_out_tmp -m ~/Practice/general_test/orthomcl/my_orthomcl_dir/orthomcl.config --nocompliant\n\n# Visualize the results by drawing a Venn Diagram using a pipeline utility script\nnml_parse_orthomcl.pl -i orthomcl_out_tmp/groups/groups.txt -g /mnt/research/common-data/Bio/orthomcl-data/genome-groups.txt -s --draw -o orthomcl-stats.txt --genes\n\n# View the Venn Diagram plot (just for demo; you should transfer the svg to your local computer for better display effect)\njava -jar /opt/software/batik/batik-1.9/batik-squiggle-1.9.jar genome-groups.txt.svg\n</code></pre> <p>Note</p> <p>As mentioned in the full tutorial, you need to answer \"yes\" to the database removal question in the course of the run.</p>"},{"location":"parallel_computing/","title":"Parallel Computing","text":"<p>Here we introduce three basic parallel models: Shared Memory, Distributed Memory and Hybrid.   Images are taken from the Lawrence Livermore National Lab's Parallel Computing Tutorial. Visit their site to learn more.</p>"},{"location":"parallel_computing/#shared-memory-with-threads","title":"Shared Memory with Threads","text":"<ul> <li>A main program loads and acquires all of the necessary resources to run the \"heavy weight\" process.</li> <li>It performs some serial work, and then creates a number of threads (\"light weight\") run by CPU cores concurrently.</li> <li>Each thread can have local data, but also, shares the entire resources, including memory of the main program.</li> <li>Threads communicate with each other through global memory (RAM, Random Access Memory). This requires synchronization operations to ensure that no than one thread is updating the same RAM address at any time.</li> <li>Threads can come and go, but the main program remains present to provide the necessary shared resources until the application has completed.</li> </ul> <p>Examples: POSIX Threads, OpenMP, CUDA threads for GPUs</p>"},{"location":"parallel_computing/#distributed-memory-with-tasks","title":"Distributed Memory with Tasks","text":"<ul> <li> <p>A main program creates a set of tasks that use their own local memory during computation. Multiple tasks can reside on the same physical machine and/or across an arbitrary number of machines.</p> </li> <li> <p>Tasks exchange data through communications by sending and receiving messages through fast network (e.g. infinite band).</p> </li> <li> <p>Data transfer usually requires cooperative operations to be performed by each process. For example, a send operation must have a matching receive operation.</p> </li> <li> <p>Synchronization operations are also required to prevent a race condition. Example: Message Passing Interface (MPI)</p> </li> </ul>"},{"location":"parallel_computing/#hybrid-parallel","title":"Hybrid Parallel","text":"<ul> <li>A hybrid model combines more than one of the previously described programming models.</li> <li> <p>A simple example is the combination of the message passing model (MPI) with the threads model (OpenMP).</p> <ul> <li>Threads perform computationally intensive kernels using local, on-node data</li> <li>Communications between processes on different nodes occurs over the network using MPI</li> </ul> </li> <li> <p>Works well to the most popular hardware environment of clustered multi/many-core machines.</p> </li> <li> <p>Other example: MPI with CPU-GPU (Graphics Processing Unit)</p> </li> </ul> <p>Hybrid OpenMP-MPI Parallel Model:</p> <p></p> <p>Hybrid CUDA-MPI Parallel Model:</p> <p></p>"},{"location":"qs/","title":"qs","text":"<p>qs is a powertools program that will display list of jobs currently on the SLURM queue for a giver user account. For more infomration about scheduling jobs on SLURM, see here. Running qs on its own will generate the following table listing your jobs that are currently on the queue:</p> <pre><code>$ qs\n\nTue Feb  7 12:29:28 EST 2023\n                                                                                            Start_Time/\n          JobID         User    Account      Name  Node CPUs  TotMem    GPU    WallTime  ST  Elapsed_Time  NodeList(Reason)\n---------------------------------------------------------------------------------------------------------------------------\n         4304840     panchyni   general test_SLURM    1    1    750M    N/A        10:00  R         0:05   css-118\n         4304842     panchyni   general test_SLURM    1    1    750M    N/A        10:00  R         0:05   css-121\n</code></pre> <p>A full description of arguments for qs can be found by acccessing the help for the program by runnig \"qs -h\" on the command line (this information is also reproduced below)</p> <pre><code>$ qs -h\n\nUsage:   -a  --&gt;  all jobs\n         -F  --&gt;  all (52) fields\n         -E  --&gt;  every (107) fields\n         -j  --&gt;  specific job\n         -u  --&gt;  specific user\n         -r  --&gt;  all job array elements\n\nDefault: -u $USER\n</code></pre>","tags":["reference"]},{"location":"virtual_help_desk/","title":"Virtual Help Desk by Microsoft Teams and Zoom","text":"<p>ICER offers virtual helpdesk office hours (every Monday and Thursday 1:00-2:00pm) online without walk-in. Users can reach us either through Microsoft Teams App or just a web browser.</p> <p>Please click on the ICER Help Desk link . It will take you to the launcher web site of Microsoft Teams:</p> <p></p> <p>You can now choose to use a web browser or Microsoft Teams to access our Help Desk channel.</p> <p>If you do not want to install and use Microsoft Teams, you can click on Use the web app instead to enter ICER Help Desk channel: </p> <p>If you would like to use Microsoft Teams but have not installed one in your computer yet, please click on Get the Teams app. If Microsoft Teams is installed already, you can click on Launch it now. If a \"Launch Appliction\" window pops out, choose Microsoft Teams  to open our Help Desk channel link:</p> <p></p> <p>Once you are in the channel, please ask your questions in the text bar located at the bottom of the window:</p> <p></p> <p>Click on the  button so we are able to see the message and help you. We can start a conversation and arrange a Zoom\u00a0   Zoom's Microsoft Teams integration has been set up to start or join an instant meeting right from our conversation. You may enter \"@zoom help\" or type \"@zoom\" in the text bar and click on\u00a0  Zoom to see a list of commands. To find out how to download or use Zoom, please visit the MSU Zoom page.</p>"},{"location":"workshop_slides/","title":"Workshop materials","text":"<p>Note</p> <p>These materials are made available for reference by attendees of past ICER workshops. Though they contain useful information, they are not meant as standalone documents that will be regularly updated. If would like to participate in a future workshop covering these materials or something similar, please visit ICER's Upcoming Seminars and Workshops page.</p> <ul> <li>R for the HPCC (last update 09/19/2023, by Craig Gross): website</li> <li>Introduction to Linux for High-Performance Computing (last update 01/31/2023, by Mahmoud Parvizi): pdf</li> <li>Introduction to HPCC (last update 7/2022, by Nanye Long): pdf</li> <li>Writing SLURM job scripts (last update 7/2022, by Nanye Long): pdf</li> <li>From PC to HPC (by Xiaoge Wang): pdf</li> </ul>"},{"location":"available_software/overview/","title":"Available software (via modules)","text":"<p>This table gives an overview of all the available software in the HPCC module system per specific CPU target.</p> <p>NOTE: This list may be out of sync with the current system. Use \"module spider keyword\" to search the most up to date list on the system.</p> <p></p> Grace Nodes Everything(except Grace nodes) amd20 amd22 intel16 intel18amd20-v100amd21intel21 Grace Nodes Everything (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21"},{"location":"available_software/detail/ABINIT/","title":"ABINIT","text":"<p>ABINIT is a package whose main program allows one to find the total energy, charge density and electronic structure ofsystems made of electrons and nuclei (molecules and periodic solids) within Density Functional Theory (DFT), usingpseudopotentials and a planewave or wavelet basis.</p> <p>https://www.abinit.org/</p>"},{"location":"available_software/detail/ABINIT/#available-modules","title":"Available modules","text":"<p>The overview below shows which ABINIT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ABINIT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ABINIT/9.8.4-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ABINIT/9.8.4-foss-2023a - x - - - -"},{"location":"available_software/detail/ABySS/","title":"ABySS","text":"<p>Assembly By Short Sequences - a de novo, parallel, paired-end sequence assembler</p> <p>https://www.bcgsc.ca/platform/bioinfo/software/abyss</p>"},{"location":"available_software/detail/ABySS/#available-modules","title":"Available modules","text":"<p>The overview below shows which ABySS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ABySS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ABySS/2.3.7-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ABySS/2.3.7-foss-2023a - x - - - -"},{"location":"available_software/detail/ACTC/","title":"ACTC","text":"<p>ACTC converts independent triangles into triangle strips or fans.</p> <p>https://sourceforge.net/projects/actc</p>"},{"location":"available_software/detail/ACTC/#available-modules","title":"Available modules","text":"<p>The overview below shows which ACTC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ACTC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ACTC/1.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ACTC/1.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/ADMIXTURE/","title":"ADMIXTURE","text":"<p>ADMIXTURE is a software tool for maximum likelihood estimation of individual ancestries from multilocus SNP genotype datasets. It uses the same statistical model as STRUCTURE but calculates estimates much more rapidly using a fast numerical optimization algorithm.</p> <p>https://dalexander.github.io/admixture/</p>"},{"location":"available_software/detail/ADMIXTURE/#available-modules","title":"Available modules","text":"<p>The overview below shows which ADMIXTURE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ADMIXTURE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ADMIXTURE/1.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ADMIXTURE/1.3.0 - x - - - -"},{"location":"available_software/detail/AFNI/","title":"AFNI","text":"<p>AFNI is a set of C programs for processing, analyzing, and displaying functional MRI (FMRI) data - a technique for mapping human brain activity.</p> <p>http://afni.nimh.nih.gov/</p>"},{"location":"available_software/detail/AFNI/#available-modules","title":"Available modules","text":"<p>The overview below shows which AFNI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using AFNI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load AFNI/24.1.03-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 AFNI/24.1.03-foss-2023a - x - - - - AFNI/24.0.02-foss-2023a - x - - - -"},{"location":"available_software/detail/ALL/","title":"ALL","text":"<p>A Load Balancing Library (ALL) aims to provide an easy way to include dynamicdomain-based load balancing into particle based simulation codes. The libraryis developed in the Simulation Laboratory Molecular Systems of the J\u00fclichSupercomputing Centre at Forschungszentrum J\u00fclich.</p> <p>https://gitlab.jsc.fz-juelich.de/SLMS/loadbalancing</p>"},{"location":"available_software/detail/ALL/#available-modules","title":"Available modules","text":"<p>The overview below shows which ALL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ALL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ALL/0.9.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ALL/0.9.2-foss-2023a - x - - - -"},{"location":"available_software/detail/ANSYS/","title":"ANSYS","text":"<p>ANSYS simulation software enables organizations to confidently predict     how their products will operate in the real world. We believe that every product is     a promise of something greater.</p> <p>https://www.ansys.com</p>"},{"location":"available_software/detail/ANSYS/#available-modules","title":"Available modules","text":"<p>The overview below shows which ANSYS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ANSYS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ANSYS/2024R1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ANSYS/2024R1 - x - - - - ANSYS/2023R2 - x - - - - ANSYS/2022R2 - x - - - -"},{"location":"available_software/detail/AOFlagger/","title":"AOFlagger","text":"<p>The AOFlagger is a tool that can find and remove radio-frequency interference (RFI)in radio astronomical observations. It can make use of Lua scripts to make flagging strategies flexible,and the tools are applicable to a wide set of telescopes.</p> <p>https://aoflagger.readthedocs.io/</p>"},{"location":"available_software/detail/AOFlagger/#available-modules","title":"Available modules","text":"<p>The overview below shows which AOFlagger installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using AOFlagger, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load AOFlagger/3.4.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 AOFlagger/3.4.0-foss-2023b - x - - - -"},{"location":"available_software/detail/APR-util/","title":"APR-util","text":"<p>Apache Portable Runtime (APR) util libraries.</p> <p>https://apr.apache.org/</p>"},{"location":"available_software/detail/APR-util/#available-modules","title":"Available modules","text":"<p>The overview below shows which APR-util installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using APR-util, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load APR-util/1.6.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 APR-util/1.6.3-GCCcore-12.3.0 - x - - - - APR-util/1.6.1-GCCcore-12.2.0 - x - - - - APR-util/1.6.1-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/APR/","title":"APR","text":"<p>Apache Portable Runtime (APR) libraries.</p> <p>https://apr.apache.org/</p>"},{"location":"available_software/detail/APR/#available-modules","title":"Available modules","text":"<p>The overview below shows which APR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using APR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load APR/1.7.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 APR/1.7.4-GCCcore-12.3.0 - x - - - - APR/1.7.0-GCCcore-12.2.0 - x - - - - APR/1.7.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/ASE/","title":"ASE","text":"<p>ASE is a python package providing an open source Atomic Simulation Environment in the Python scripting language.From version 3.20.1 we also include the ase-ext package, it contains optional reimplementationsin C of functions in ASE.  ASE uses it automatically when installed.</p> <p>https://wiki.fysik.dtu.dk/ase</p>"},{"location":"available_software/detail/ASE/#available-modules","title":"Available modules","text":"<p>The overview below shows which ASE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ASE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ASE/3.22.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ASE/3.22.1-gfbf-2023a - x - - - -"},{"location":"available_software/detail/ASE/#ase3221-gfbf-2023a","title":"ASE/3.22.1-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>ase-3.22.1, ase-ext-20.9.0, pytest-mock-3.11.1</p>"},{"location":"available_software/detail/ATK/","title":"ATK","text":"<p>ATK provides the set of accessibility interfaces that are implemented by other toolkits and applications. Using the ATK interfaces, accessibility tools have full access to view and control running applications.</p> <p>https://developer.gnome.org/atk/</p>"},{"location":"available_software/detail/ATK/#available-modules","title":"Available modules","text":"<p>The overview below shows which ATK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ATK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ATK/2.38.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ATK/2.38.0-GCCcore-12.3.0 - x - - - - ATK/2.38.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/AUGUSTUS/","title":"AUGUSTUS","text":"<p>AUGUSTUS is a program that predicts genes in eukaryotic genomic sequences</p> <p>https://bioinf.uni-greifswald.de/augustus/</p>"},{"location":"available_software/detail/AUGUSTUS/#available-modules","title":"Available modules","text":"<p>The overview below shows which AUGUSTUS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using AUGUSTUS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load AUGUSTUS/3.5.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 AUGUSTUS/3.5.0-foss-2023a - x - - - - AUGUSTUS/3.5.0-foss-2022b - x - - - -"},{"location":"available_software/detail/Abseil/","title":"Abseil","text":"<p>Abseil is an open-source collection of C++ library code designed to augment theC++ standard library. The Abseil library code is collected from Google's ownC++ code base, has been extensively tested and used in production, and is thesame code we depend on in our daily coding lives.</p> <p>https://abseil.io/</p>"},{"location":"available_software/detail/Abseil/#available-modules","title":"Available modules","text":"<p>The overview below shows which Abseil installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Abseil, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Abseil/20230125.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Abseil/20230125.3-GCCcore-12.3.0 - x - - - - Abseil/20230125.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/AlphaFold/","title":"AlphaFold","text":"<p>AlphaFold Singularity Image (community build)</p> <p>https://github.com/prehensilecode/alphafold_singularity</p>"},{"location":"available_software/detail/AlphaFold/#available-modules","title":"Available modules","text":"<p>The overview below shows which AlphaFold installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using AlphaFold, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load AlphaFold/2.3.2-GCCcore-13.2.0-singularity\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 AlphaFold/2.3.2-GCCcore-13.2.0-singularity - x - - - -"},{"location":"available_software/detail/Amber/","title":"Amber","text":"<p>Amber (originally Assisted Model Building with Energy Refinement) is software for performing molecular dynamics and structure prediction.</p> <p>https://ambermd.org/amber.html</p>"},{"location":"available_software/detail/Amber/#available-modules","title":"Available modules","text":"<p>The overview below shows which Amber installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Amber, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Amber/22.5-foss-2023a-AmberTools-23.6-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Amber/22.5-foss-2023a-AmberTools-23.6-CUDA-12.1.1 - - - x - x Amber/22.4-foss-2022a-AmberTools-22.5-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/AnsysEM/","title":"AnsysEM","text":"<p>Description not found in module</p>"},{"location":"available_software/detail/AnsysEM/#available-modules","title":"Available modules","text":"<p>The overview below shows which AnsysEM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using AnsysEM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load AnsysEM/24.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 AnsysEM/24.1 - x - - - -"},{"location":"available_software/detail/Armadillo/","title":"Armadillo","text":"<p>Armadillo is an open-source C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions.</p> <p>https://arma.sourceforge.net/</p>"},{"location":"available_software/detail/Armadillo/#available-modules","title":"Available modules","text":"<p>The overview below shows which Armadillo installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Armadillo, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Armadillo/12.8.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Armadillo/12.8.0-foss-2023b - x - - - - Armadillo/12.6.2-foss-2023a - x - - - - Armadillo/11.4.3-foss-2022b - x - - - - Armadillo/10.5.3-foss-2020b - x - - - -"},{"location":"available_software/detail/Arrow/","title":"Arrow","text":"<p>Apache Arrow (incl. PyArrow Python bindings), a cross-language development platform for in-memory data.</p> <p>https://arrow.apache.org</p>"},{"location":"available_software/detail/Arrow/#available-modules","title":"Available modules","text":"<p>The overview below shows which Arrow installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Arrow, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Arrow/14.0.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Arrow/14.0.1-gfbf-2023a - x - - - - Arrow/11.0.0-gfbf-2022b - x - - - -"},{"location":"available_software/detail/Arrow/#arrow1401-gfbf-2023a","title":"Arrow/14.0.1-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>pyarrow-14.0.1</p>"},{"location":"available_software/detail/Aspera-Connect/","title":"Aspera-Connect","text":"<p>Connect is an install-on-demand Web browser plug-in that facilitates high-speed uploads and downloads with an Aspera transfer server.</p> <p>http://downloads.asperasoft.com/connect2/</p>"},{"location":"available_software/detail/Aspera-Connect/#available-modules","title":"Available modules","text":"<p>The overview below shows which Aspera-Connect installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Aspera-Connect, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Aspera-Connect/3.9.6\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Aspera-Connect/3.9.6 - x - - - -"},{"location":"available_software/detail/Autoconf/","title":"Autoconf","text":"<p>Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages. These scripts can adapt the packages to many kinds of UNIX-like systems without manual user intervention. Autoconf creates a configuration script for a package from a template file that lists the operating system features that the package can use, in the form of M4 macro calls.</p> <p>https://www.gnu.org/software/autoconf/</p>"},{"location":"available_software/detail/Autoconf/#available-modules","title":"Available modules","text":"<p>The overview below shows which Autoconf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Autoconf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Autoconf/2.71-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Autoconf/2.71-GCCcore-13.2.0 - x - - - - Autoconf/2.71-GCCcore-12.3.0 - x x x x x Autoconf/2.71-GCCcore-12.2.0 - x - - - - Autoconf/2.71-GCCcore-11.3.0 - x - - - - Autoconf/2.71 - x - - - - Autoconf/2.69-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Automake/","title":"Automake","text":"<p>Automake: GNU Standards-compliant Makefile generator</p> <p>https://www.gnu.org/software/automake/automake.html</p>"},{"location":"available_software/detail/Automake/#available-modules","title":"Available modules","text":"<p>The overview below shows which Automake installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Automake, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Automake/1.16.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Automake/1.16.5-GCCcore-13.2.0 - x - - - - Automake/1.16.5-GCCcore-12.3.0 - x x x x x Automake/1.16.5-GCCcore-12.2.0 - x - - - - Automake/1.16.5-GCCcore-11.3.0 - x - - - - Automake/1.16.5 - x - - - - Automake/1.16.2-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Autotools/","title":"Autotools","text":"<p>This bundle collect the standard GNU build tools: Autoconf, Automake and libtool</p> <p>https://autotools.io</p>"},{"location":"available_software/detail/Autotools/#available-modules","title":"Available modules","text":"<p>The overview below shows which Autotools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Autotools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Autotools/20220317-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Autotools/20220317-GCCcore-13.2.0 - x - - - - Autotools/20220317-GCCcore-12.3.0 - x x x x x Autotools/20220317-GCCcore-12.2.0 - x - - - - Autotools/20220317-GCCcore-11.3.0 - x - - - - Autotools/20220317 - x - - - - Autotools/20200321-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Avogadro2/","title":"Avogadro2","text":"<p>Avogadro is an advanced molecule editor and visualizer designed for cross-platform     use in computational chemistry, molecular modeling, bioinformatics, materials science,     and related areas.</p> <p>https://two.avogadro.cc/index.html</p>"},{"location":"available_software/detail/Avogadro2/#available-modules","title":"Available modules","text":"<p>The overview below shows which Avogadro2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Avogadro2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Avogadro2/1.97.0-linux-x86_64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Avogadro2/1.97.0-linux-x86_64 - x - - - -"},{"location":"available_software/detail/BBMap/","title":"BBMap","text":"<p>BBMap short read aligner, and other bioinformatic tools.</p> <p>https://sourceforge.net/projects/bbmap/</p>"},{"location":"available_software/detail/BBMap/#available-modules","title":"Available modules","text":"<p>The overview below shows which BBMap installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BBMap, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BBMap/39.01-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BBMap/39.01-GCC-12.3.0 - x - - - - BBMap/39.01-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/BCFtools/","title":"BCFtools","text":"<p>Samtools is a suite of programs for interacting with high-throughput sequencing data. BCFtools - Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants</p> <p>https://www.htslib.org/</p>"},{"location":"available_software/detail/BCFtools/#available-modules","title":"Available modules","text":"<p>The overview below shows which BCFtools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BCFtools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BCFtools/1.19-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BCFtools/1.19-GCC-13.2.0 - x - - - - BCFtools/1.18-GCC-12.3.0 - x - - - - BCFtools/1.17-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/BEDTools/","title":"BEDTools","text":"<p>BEDTools: a powerful toolset for genome arithmetic.The BEDTools utilities allow one to address common genomics tasks such as finding feature overlaps andcomputing coverage.The utilities are largely based on four widely-used file formats: BED, GFF/GTF, VCF, and SAM/BAM.</p> <p>https://bedtools.readthedocs.io/</p>"},{"location":"available_software/detail/BEDTools/#available-modules","title":"Available modules","text":"<p>The overview below shows which BEDTools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BEDTools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BEDTools/2.31.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BEDTools/2.31.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/BLAST%2B/","title":"BLAST+","text":"<p>Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences.</p> <p>https://blast.ncbi.nlm.nih.gov/</p>"},{"location":"available_software/detail/BLAST%2B/#available-modules","title":"Available modules","text":"<p>The overview below shows which BLAST+ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BLAST+, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BLAST+/2.14.1-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BLAST+/2.14.1-gompi-2023a - x - - - - BLAST+/2.14.0-gompi-2022b - x - - - -"},{"location":"available_software/detail/BLAT/","title":"BLAT","text":"<p>BLAT on DNA is designed to quickly find sequences of 95% andgreater similarity of length 25 bases or more.</p> <p>https://genome.ucsc.edu/goldenPath/help/blatSpec.html</p>"},{"location":"available_software/detail/BLAT/#available-modules","title":"Available modules","text":"<p>The overview below shows which BLAT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BLAT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BLAT/3.7-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BLAT/3.7-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/BLIS/","title":"BLIS","text":"<p>BLIS is a portable software framework for instantiating high-performanceBLAS-like dense linear algebra libraries.</p> <p>https://github.com/flame/blis/</p>"},{"location":"available_software/detail/BLIS/#available-modules","title":"Available modules","text":"<p>The overview below shows which BLIS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BLIS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BLIS/0.9.0-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BLIS/0.9.0-GCC-13.2.0 - x - - - - BLIS/0.9.0-GCC-12.3.0 - x x x x - BLIS/0.9.0-GCC-12.2.0 - x - - - - BLIS/0.9.0-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/BRAKER/","title":"BRAKER","text":"<p>BRAKER is a pipeline for fully automated prediction of protein coding genes with GeneMark-ES/ET and AUGUSTUS in novel eukaryotic genomes. Users may wish to personally install compleasm and MakeHub for additional functionality.</p> <p>https://github.com/Gaius-Augustus/BRAKER</p>"},{"location":"available_software/detail/BRAKER/#available-modules","title":"Available modules","text":"<p>The overview below shows which BRAKER installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BRAKER, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BRAKER/3.0.8-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BRAKER/3.0.8-foss-2023a - x - - - -"},{"location":"available_software/detail/BUSCO/","title":"BUSCO","text":"<p>BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs</p> <p>https://busco.ezlab.org/</p>"},{"location":"available_software/detail/BUSCO/#available-modules","title":"Available modules","text":"<p>The overview below shows which BUSCO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BUSCO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BUSCO/5.4.7-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BUSCO/5.4.7-foss-2022b - x - - - -"},{"location":"available_software/detail/BWA/","title":"BWA","text":"<p>Burrows-Wheeler Aligner (BWA) is an efficient program that aligns relatively short nucleotide sequences against a long reference sequence such as the human genome.</p> <p>http://bio-bwa.sourceforge.net/</p>"},{"location":"available_software/detail/BWA/#available-modules","title":"Available modules","text":"<p>The overview below shows which BWA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BWA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BWA/0.7.17-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BWA/0.7.17-GCCcore-12.3.0 - x - - - - BWA/0.7.17-20220923-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/BamTools/","title":"BamTools","text":"<p>BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files.</p> <p>https://github.com/pezmaster31/bamtools</p>"},{"location":"available_software/detail/BamTools/#available-modules","title":"Available modules","text":"<p>The overview below shows which BamTools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BamTools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BamTools/2.5.2-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BamTools/2.5.2-GCC-12.3.0 - x - - - - BamTools/2.5.2-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/BamUtil/","title":"BamUtil","text":"<p>BamUtil is a repository that contains several programs  that perform operations on SAM/BAM files. All of these programs  are built into a single executable, bam.</p> <p>http://genome.sph.umich.edu/wiki/BamUtil</p>"},{"location":"available_software/detail/BamUtil/#available-modules","title":"Available modules","text":"<p>The overview below shows which BamUtil installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BamUtil, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BamUtil/1.0.15-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BamUtil/1.0.15-foss-2023a - x - - - -"},{"location":"available_software/detail/Bazel/","title":"Bazel","text":"<p>Bazel is a build tool that builds code quickly and reliably.It is used to build the majority of Google's software.</p> <p>https://bazel.io/</p>"},{"location":"available_software/detail/Bazel/#available-modules","title":"Available modules","text":"<p>The overview below shows which Bazel installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Bazel, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Bazel/6.3.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Bazel/6.3.1-GCCcore-12.3.0 - x - - - - Bazel/5.1.1-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Beagle/","title":"Beagle","text":"<p>Beagle is a software package for phasing genotypes and for imputing ungenotyped markers.</p> <p>https://faculty.washington.edu/browning/beagle/beagle.html</p>"},{"location":"available_software/detail/Beagle/#available-modules","title":"Available modules","text":"<p>The overview below shows which Beagle installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Beagle, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Beagle/5.4.22Jul22.46e-Java-11\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Beagle/5.4.22Jul22.46e-Java-11 - x - - - -"},{"location":"available_software/detail/BeautifulSoup/","title":"BeautifulSoup","text":"<p>Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping.</p> <p>https://www.crummy.com/software/BeautifulSoup</p>"},{"location":"available_software/detail/BeautifulSoup/#available-modules","title":"Available modules","text":"<p>The overview below shows which BeautifulSoup installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BeautifulSoup, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BeautifulSoup/4.12.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BeautifulSoup/4.12.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/BeautifulSoup/#beautifulsoup4122-gcccore-1230","title":"BeautifulSoup/4.12.2-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>BeautifulSoup-4.12.2, soupsieve-2.4.1</p>"},{"location":"available_software/detail/Bio-SearchIO-hmmer/","title":"Bio-SearchIO-hmmer","text":"<p>Code to parse output from hmmsearch, hmmscan, phmmer and nhmmer, compatiblewith both version 2 and version 3 of the HMMER package from http://hmmer.org.</p> <p>https://metacpan.org/pod/Bio::SearchIO::hmmer3</p>"},{"location":"available_software/detail/Bio-SearchIO-hmmer/#available-modules","title":"Available modules","text":"<p>The overview below shows which Bio-SearchIO-hmmer installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Bio-SearchIO-hmmer, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Bio-SearchIO-hmmer/1.7.3-GCC-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Bio-SearchIO-hmmer/1.7.3-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/BioPerl/","title":"BioPerl","text":"<p>Bioperl is the product of a community effort to produce Perl code which is useful in biology. Examples include Sequence objects, Alignment objects and database searching objects.</p> <p>https://bioperl.org/</p>"},{"location":"available_software/detail/BioPerl/#available-modules","title":"Available modules","text":"<p>The overview below shows which BioPerl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using BioPerl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load BioPerl/1.7.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 BioPerl/1.7.8-GCCcore-12.3.0 - x - - - - BioPerl/1.7.8-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/BioPerl/#bioperl178-gcccore-1230","title":"BioPerl/1.7.8-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Bio::Procedural-1.7.4, BioPerl-1.7.8, XML::Writer-0.900</p>"},{"location":"available_software/detail/BioPerl/#bioperl178-gcccore-1220","title":"BioPerl/1.7.8-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Bio::Procedural-1.7.4, BioPerl-1.7.8, XML::Writer-0.900</p>"},{"location":"available_software/detail/Biopython/","title":"Biopython","text":"<p>Biopython is a set of freely available tools for biological computation written in Python by an international team of developers. It is a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics.</p> <p>https://www.biopython.org</p>"},{"location":"available_software/detail/Biopython/#available-modules","title":"Available modules","text":"<p>The overview below shows which Biopython installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Biopython, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Biopython/1.83-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Biopython/1.83-foss-2023a - x - - - - Biopython/1.81-foss-2022b - x - - - - Biopython/1.79-foss-2022a - x - - - -"},{"location":"available_software/detail/Bison/","title":"Bison","text":"<p>Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables.</p> <p>https://www.gnu.org/software/bison</p>"},{"location":"available_software/detail/Bison/#available-modules","title":"Available modules","text":"<p>The overview below shows which Bison installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Bison, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Bison/3.8.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Bison/3.8.2-GCCcore-13.2.0 - x - - - - Bison/3.8.2-GCCcore-12.3.0 - x x x x x Bison/3.8.2-GCCcore-12.2.0 - x - - - - Bison/3.8.2-GCCcore-11.3.0 - x - - - - Bison/3.8.2 - x x x x x Bison/3.7.6-GCCcore-11.2.0 - x - - - - Bison/3.7.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Blender/","title":"Blender","text":"<p>Blender is the free and open source 3D creation suite. It supports the entirety of the 3D pipeline-modeling, rigging, animation, simulation, rendering, compositing and motion tracking, even video editing and game creation.</p> <p>https://www.blender.org/</p>"},{"location":"available_software/detail/Blender/#available-modules","title":"Available modules","text":"<p>The overview below shows which Blender installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Blender, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Blender/4.0.1-linux-x86_64-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Blender/4.0.1-linux-x86_64-CUDA-12.1.1 - x - - - -"},{"location":"available_software/detail/Boost.MPI/","title":"Boost.MPI","text":"<p>Boost provides free peer-reviewed portable C++ source libraries.</p> <p>https://www.boost.org/</p>"},{"location":"available_software/detail/Boost.MPI/#available-modules","title":"Available modules","text":"<p>The overview below shows which Boost.MPI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Boost.MPI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Boost.MPI/1.82.0-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Boost.MPI/1.82.0-gompi-2023a - x - - - - Boost.MPI/1.81.0-gompi-2022b - x - - - -"},{"location":"available_software/detail/Boost.Python/","title":"Boost.Python","text":"<p>Boost.Python is a C++ library which enables seamless interoperability between C++ and the Python programming language.</p> <p>https://boostorg.github.io/python</p>"},{"location":"available_software/detail/Boost.Python/#available-modules","title":"Available modules","text":"<p>The overview below shows which Boost.Python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Boost.Python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Boost.Python/1.83.0-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Boost.Python/1.83.0-GCC-13.2.0 - x - - - - Boost.Python/1.79.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Boost/","title":"Boost","text":"<p>Boost provides free peer-reviewed portable C++ source libraries.</p> <p>https://www.boost.org/</p>"},{"location":"available_software/detail/Boost/#available-modules","title":"Available modules","text":"<p>The overview below shows which Boost installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Boost, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Boost/1.83.0-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Boost/1.83.0-GCC-13.2.0 - x - - - - Boost/1.82.0-GCC-12.3.0 - x x x x x Boost/1.81.0-GCC-12.2.0 - x - - - - Boost/1.79.0-GCC-11.3.0 - x - - - - Boost/1.74.0-GCC-10.2.0 - x - - - -"},{"location":"available_software/detail/Bowtie/","title":"Bowtie","text":"<p>Bowtie is an ultrafast, memory-efficient short read aligner. It aligns short DNA sequences (reads) to the human genome.</p> <p>http://bowtie-bio.sourceforge.net/index.shtml</p>"},{"location":"available_software/detail/Bowtie/#available-modules","title":"Available modules","text":"<p>The overview below shows which Bowtie installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Bowtie, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Bowtie/1.3.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Bowtie/1.3.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Bowtie2/","title":"Bowtie2","text":"<p>Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes. Bowtie 2 indexes the genome with an FM Index to keep its memory footprint small: for the human genome, its memory footprint is typically around 3.2 GB. Bowtie 2 supports gapped, local, and paired-end alignment modes.</p> <p>https://bowtie-bio.sourceforge.net/bowtie2/index.shtml</p>"},{"location":"available_software/detail/Bowtie2/#available-modules","title":"Available modules","text":"<p>The overview below shows which Bowtie2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Bowtie2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Bowtie2/2.5.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Bowtie2/2.5.1-GCC-12.3.0 - x - - - - Bowtie2/2.4.5-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/Brotli/","title":"Brotli","text":"<p>Brotli is a generic-purpose lossless compression algorithm that compresses data using a combination of a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling, with a compression ratio comparable to the best currently available general-purpose compression methods. It is similar in speed with deflate but offers more dense compression.The specification of the Brotli Compressed Data Format is defined in RFC 7932.</p> <p>https://github.com/google/brotli</p>"},{"location":"available_software/detail/Brotli/#available-modules","title":"Available modules","text":"<p>The overview below shows which Brotli installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Brotli, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Brotli/1.1.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Brotli/1.1.0-GCCcore-13.2.0 - x - - - - Brotli/1.0.9-GCCcore-12.3.0 - x x x x x Brotli/1.0.9-GCCcore-12.2.0 - x - - - - Brotli/1.0.9-GCCcore-11.3.0 - x - - - x Brotli/1.0.9-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Brunsli/","title":"Brunsli","text":"<p>Brunsli is a lossless JPEG repacking library.</p> <p>https://github.com/google/brunsli/</p>"},{"location":"available_software/detail/Brunsli/#available-modules","title":"Available modules","text":"<p>The overview below shows which Brunsli installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Brunsli, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Brunsli/0.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Brunsli/0.1-GCCcore-12.3.0 - x - - - - Brunsli/0.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/CD-HIT/","title":"CD-HIT","text":"<p>CD-HIT is a very widely used program for clustering and  comparing protein or nucleotide sequences.</p> <p>https://github.com/weizhongli/cdhit/wiki</p>"},{"location":"available_software/detail/CD-HIT/#available-modules","title":"Available modules","text":"<p>The overview below shows which CD-HIT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CD-HIT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CD-HIT/4.8.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CD-HIT/4.8.1-GCC-12.3.0 - x - - - - CD-HIT/4.8.1-GCC-12.2.0 - x - - - - CD-HIT/4.8.1-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/CDBtools/","title":"CDBtools","text":"<p>CDB (Constant DataBase) indexing and retrieval tools for FASTA files</p> <p>http://compbio.dfci.harvard.edu/tgi</p>"},{"location":"available_software/detail/CDBtools/#available-modules","title":"Available modules","text":"<p>The overview below shows which CDBtools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CDBtools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CDBtools/0.99-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CDBtools/0.99-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/CDO/","title":"CDO","text":"<p>CDO is a collection of command line Operators to manipulate and analyse Climate and NWP model Data.</p> <p>https://code.zmaw.de/projects/cdo</p>"},{"location":"available_software/detail/CDO/#available-modules","title":"Available modules","text":"<p>The overview below shows which CDO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CDO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CDO/2.2.2-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CDO/2.2.2-gompi-2023b - x - - - - CDO/2.2.2-gompi-2023a - x - - - -"},{"location":"available_software/detail/CESM-deps/","title":"CESM-deps","text":"<p>CESM is a fully-coupled, community, global climate model thatprovides state-of-the-art computer simulations of the Earth's past, present,and future climate states.</p> <p>https://www.cesm.ucar.edu/models/cesm2/</p>"},{"location":"available_software/detail/CESM-deps/#available-modules","title":"Available modules","text":"<p>The overview below shows which CESM-deps installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CESM-deps, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CESM-deps/2-foss-2022a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CESM-deps/2-foss-2022a - x - - - -"},{"location":"available_software/detail/CFITSIO/","title":"CFITSIO","text":"<p>CFITSIO is a library of C and Fortran subroutines for reading and writing data files inFITS (Flexible Image Transport System) data format.</p> <p>https://heasarc.gsfc.nasa.gov/fitsio/</p>"},{"location":"available_software/detail/CFITSIO/#available-modules","title":"Available modules","text":"<p>The overview below shows which CFITSIO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CFITSIO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CFITSIO/4.3.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CFITSIO/4.3.1-GCCcore-13.2.0 - x - - - - CFITSIO/4.3.0-GCCcore-12.3.0 - x - - - - CFITSIO/4.2.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/CGAL/","title":"CGAL","text":"<p>The goal of the CGAL Open Source Project is to provide easy access to efficient and reliable geometric algorithms in the form of a C++ library.</p> <p>https://www.cgal.org/</p>"},{"location":"available_software/detail/CGAL/#available-modules","title":"Available modules","text":"<p>The overview below shows which CGAL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CGAL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CGAL/5.6-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CGAL/5.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/CMake/","title":"CMake","text":"<p>CMake, the cross-platform, open-source build system.  CMake is a family of tools designed to build, test and package software.</p> <p>https://www.cmake.org</p>"},{"location":"available_software/detail/CMake/#available-modules","title":"Available modules","text":"<p>The overview below shows which CMake installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CMake, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CMake/3.27.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CMake/3.27.6-GCCcore-13.2.0 - x - - - - CMake/3.26.3-GCCcore-12.3.0 - x x x x x CMake/3.24.3-GCCcore-12.2.0 - x - - - - CMake/3.24.3-GCCcore-11.3.0 - x - - - - CMake/3.23.1-GCCcore-11.3.0 - x - - - - CMake/3.18.4-GCCcore-10.2.0 - x - - - - CMake/3.18.4 - x - - - -"},{"location":"available_software/detail/COMSOL/","title":"COMSOL","text":"<p>COMSOL Multiphysics is a general-purpose software platform, based onadvanced numerical methods, for modeling and simulating physics-basedproblems.</p> <p>https://www.comsol.se/</p>"},{"location":"available_software/detail/COMSOL/#available-modules","title":"Available modules","text":"<p>The overview below shows which COMSOL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using COMSOL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load COMSOL/6.2.0.415\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 COMSOL/6.2.0.415 - x - - - -"},{"location":"available_software/detail/CONVERGE/","title":"CONVERGE","text":"<p>CONVERGE</p> <p>https://convergecfd.com/getting-started-guide-v3/converge_installation_unix_linux.htm</p>"},{"location":"available_software/detail/CONVERGE/#available-modules","title":"Available modules","text":"<p>The overview below shows which CONVERGE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CONVERGE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CONVERGE/4.0.2-linux-x86_64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CONVERGE/4.0.2-linux-x86_64 - x - - - - CONVERGE/3.1.11-linux-x86_64 - x - - - - CONVERGE/3.0.28-linux-x86_64 - x - - - -"},{"location":"available_software/detail/CREST/","title":"CREST","text":"<p>CREST is an utility/driver program for the xtb program. Originally it was designed as conformer sampling program, hence the abbreviation Conformer\u2013Rotamer Ensemble Sampling Tool, but now offers also some utility functions for calculations with the GFNn\u2013xTB methods. Generally the program functions as an IO based OMP scheduler (i.e., calculations are performed by the xtb program) and tool for the creation and analysation of structure ensembles.</p> <p>https://xtb-docs.readthedocs.io/en/latest/crest.html</p>"},{"location":"available_software/detail/CREST/#available-modules","title":"Available modules","text":"<p>The overview below shows which CREST installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CREST, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CREST/2.12-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CREST/2.12-gfbf-2023a - x - - - -"},{"location":"available_software/detail/CRYSTAL/","title":"CRYSTAL","text":"<p>A powerful and scalable computational tool for solid state chemistry and physics</p> <p>https://www.crystal.unito.it/index.html</p>"},{"location":"available_software/detail/CRYSTAL/#available-modules","title":"Available modules","text":"<p>The overview below shows which CRYSTAL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CRYSTAL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CRYSTAL/23_v1.0.1_binary\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CRYSTAL/23_v1.0.1_binary - - x - x x CRYSTAL/23-intel-2023a - x x - - x CRYSTAL/23-MPP-intel-2023a - x - - - x CRYSTAL/17-intel-2023a-1.0.2 - x - - - x CRYSTAL/17-MPP-intel-2023a-1.0.2 - x - - - x"},{"location":"available_software/detail/CUDA-Samples/","title":"CUDA-Samples","text":"<p>Samples for CUDA Developers which demonstrates features in CUDA Toolkit</p> <p>https://github.com/NVIDIA/cuda-samples</p>"},{"location":"available_software/detail/CUDA-Samples/#available-modules","title":"Available modules","text":"<p>The overview below shows which CUDA-Samples installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CUDA-Samples, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CUDA-Samples/12.1-GCC-12.3.0-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CUDA-Samples/12.1-GCC-12.3.0-CUDA-12.1.1 - x - - - -"},{"location":"available_software/detail/CUDA/","title":"CUDA","text":"<p>CUDA (formerly Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. CUDA gives developers access to the virtual instruction set and memory of the parallel computational elements in CUDA GPUs.</p> <p>https://developer.nvidia.com/cuda-toolkit</p>"},{"location":"available_software/detail/CUDA/#available-modules","title":"Available modules","text":"<p>The overview below shows which CUDA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CUDA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CUDA/12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CUDA/12.3.0 - x - - - - CUDA/12.1.1 - x - - - x CUDA/11.7.0 - x - - - -"},{"location":"available_software/detail/Catch2/","title":"Catch2","text":"<p>A modern, C++-native, header-only, test framework for unit-tests, TDD and BDD - using C++11, C++14, C++17 and later</p> <p>https://github.com/catchorg/Catch2</p>"},{"location":"available_software/detail/Catch2/#available-modules","title":"Available modules","text":"<p>The overview below shows which Catch2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Catch2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Catch2/2.13.9-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Catch2/2.13.9-GCCcore-13.2.0 - x - - - - Catch2/2.13.9-GCCcore-12.3.0 - x - - - - Catch2/2.13.9-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Cbc/","title":"Cbc","text":"<p>Cbc (Coin-or branch and cut) is an open-source mixed integer linear programmingsolver written in C++. It can be used as a callable library or using astand-alone executable.</p> <p>https://github.com/coin-or/Cbc</p>"},{"location":"available_software/detail/Cbc/#available-modules","title":"Available modules","text":"<p>The overview below shows which Cbc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Cbc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Cbc/2.10.11-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Cbc/2.10.11-foss-2023a - x - - - -"},{"location":"available_software/detail/CellRanger-ARC/","title":"CellRanger-ARC","text":"<p>Cell Ranger ARC is a set of analysis pipelines that process Chromium Single Cell Multiome ATAC + Gene Expression sequencing data to generate a variety of analyses pertaining to gene expression, chromatin accessibility and their linkage. Furthermore, since the ATAC and gene expression measurements are on the very same cell, we are able to perform analyses that link chromatin accessibility and gene expression.</p> <p>https://support.10xgenomics.com/single-cell-multiome-atac-gex/software/pipelines/latest/what-is-cell-ranger-arc</p>"},{"location":"available_software/detail/CellRanger-ARC/#available-modules","title":"Available modules","text":"<p>The overview below shows which CellRanger-ARC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CellRanger-ARC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CellRanger-ARC/2.0.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CellRanger-ARC/2.0.2 - x - - - -"},{"location":"available_software/detail/CellRanger-ATAC/","title":"CellRanger-ATAC","text":"<p>Cell Ranger ATAC is a set of analysis pipelines that process Chromium Single Cell ATAC data.</p> <p>https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/what-is-cell-ranger-atac</p>"},{"location":"available_software/detail/CellRanger-ATAC/#available-modules","title":"Available modules","text":"<p>The overview below shows which CellRanger-ATAC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CellRanger-ATAC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CellRanger-ATAC/2.1.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CellRanger-ATAC/2.1.0 - x - - - -"},{"location":"available_software/detail/CellRanger/","title":"CellRanger","text":"<p>Cell Ranger is a set of analysis pipelines that process Chromium single-cell RNA-seq output to align reads, generate gene-cell matrices and perform clustering and gene expression analysis.</p> <p>https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger</p>"},{"location":"available_software/detail/CellRanger/#available-modules","title":"Available modules","text":"<p>The overview below shows which CellRanger installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CellRanger, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CellRanger/8.0.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CellRanger/8.0.1 - x - - - -"},{"location":"available_software/detail/Cereal/","title":"Cereal","text":"<p>cereal is a header-only C++11 serialization library. cereal takes arbitrary data types and reversibly turns them into different representations, such as compact binary encodings, XML, or JSON. cereal was designed to befast, light-weight, and easy to extend - it has no external dependencies and can be easily bundled with other code or used standalone.</p> <p>https://uscilab.github.io/cereal/</p>"},{"location":"available_software/detail/Cereal/#available-modules","title":"Available modules","text":"<p>The overview below shows which Cereal installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Cereal, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Cereal/1.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Cereal/1.3.2 - x - - - -"},{"location":"available_software/detail/Cgl/","title":"Cgl","text":"<p>The COIN-OR Cut Generation Library (Cgl) is a collection of cut generators thatcan be used with other COIN-OR packages that make use of cuts, such as, amongothers, the linear solver Clp or the mixed integer linear programming solversCbc or BCP. Cgl uses the abstract class OsiSolverInterface (see Osi) to use orcommunicate with a solver. It does not directly call a solver.</p> <p>https://github.com/coin-or/Cgl</p>"},{"location":"available_software/detail/Cgl/#available-modules","title":"Available modules","text":"<p>The overview below shows which Cgl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Cgl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Cgl/0.60.8-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Cgl/0.60.8-foss-2023a - x - - - -"},{"location":"available_software/detail/Clang/","title":"Clang","text":"<p>C, C++, Objective-C compiler, based on LLVM.  Does not include C++ standard library -- use libstdc++ from GCC.</p> <p>https://clang.llvm.org/</p>"},{"location":"available_software/detail/Clang/#available-modules","title":"Available modules","text":"<p>The overview below shows which Clang installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Clang, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Clang/16.0.6-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Clang/16.0.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Clp/","title":"Clp","text":"<p>Clp (Coin-or linear programming) is an open-source linear programming solver.It is primarily meant to be used as a callable library, but a basic,stand-alone executable version is also available.</p> <p>https://github.com/coin-or/Clp</p>"},{"location":"available_software/detail/Clp/#available-modules","title":"Available modules","text":"<p>The overview below shows which Clp installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Clp, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Clp/1.17.9-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Clp/1.17.9-foss-2023a - x - - - -"},{"location":"available_software/detail/CoinUtils/","title":"CoinUtils","text":"<p>CoinUtils (Coin-OR Utilities) is an open-source collection of classes andfunctions that are generally useful to more than one COIN-OR project.</p> <p>https://github.com/coin-or/CoinUtils</p>"},{"location":"available_software/detail/CoinUtils/#available-modules","title":"Available modules","text":"<p>The overview below shows which CoinUtils installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using CoinUtils, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load CoinUtils/2.11.10-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 CoinUtils/2.11.10-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Conda/","title":"Conda","text":"<p>Built to complement the rich, open source Python community,the Anaconda platform provides an enterprise-ready data analytics platform that empowers companies to adopt a modern open data science analytics architecture.</p> <p>https://www.anaconda.com</p>"},{"location":"available_software/detail/Conda/#available-modules","title":"Available modules","text":"<p>The overview below shows which Conda installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Conda, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Conda/3\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Conda/3 - x - - - -"},{"location":"available_software/detail/DB/","title":"DB","text":"<p>Berkeley DB enables the development of custom data management solutions, without the overhead traditionally associated with such custom projects.</p> <p>https://www.oracle.com/technetwork/products/berkeleydb</p>"},{"location":"available_software/detail/DB/#available-modules","title":"Available modules","text":"<p>The overview below shows which DB installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DB, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DB/18.1.40-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DB/18.1.40-GCCcore-12.3.0 - x - - - - DB/18.1.40-GCCcore-12.2.0 - x - - - - DB/18.1.40-GCCcore-11.3.0 - x - - - - DB/18.1.40-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/DB_File/","title":"DB_File","text":"<p>Perl5 access to Berkeley DB version 1.x.</p> <p>https://perldoc.perl.org/DB_File.html</p>"},{"location":"available_software/detail/DB_File/#available-modules","title":"Available modules","text":"<p>The overview below shows which DB_File installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DB_File, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DB_File/1.859-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DB_File/1.859-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/DBus/","title":"DBus","text":"<p>D-Bus is a message bus system, a simple way for applications to talk to one another.  In addition to interprocess communication, D-Bus helps coordinate process lifecycle; it makes it simple and reliable to code a \"single instance\" application or daemon, and to launch applications and daemons on demand when their services are needed.</p> <p>https://dbus.freedesktop.org/</p>"},{"location":"available_software/detail/DBus/#available-modules","title":"Available modules","text":"<p>The overview below shows which DBus installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DBus, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DBus/1.15.8-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DBus/1.15.8-GCCcore-13.2.0 - x - - - - DBus/1.15.4-GCCcore-12.3.0 - x - - - - DBus/1.15.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/DFTB%2B/","title":"DFTB+","text":"<p>DFTB+ is a fast and efficient versatile quantum mechanical simulation package.It is based on the Density Functional Tight Binding (DFTB) method, containingalmost all of the useful extensions which have been developed for the DFTBframework so far.  Using DFTB+ you can carry out quantum mechanical simulationslike with ab-initio density functional theory based packages, but in anapproximate way gaining typically around two order of magnitude in speed.</p> <p>https://www.dftb-plus.info</p>"},{"location":"available_software/detail/DFTB%2B/#available-modules","title":"Available modules","text":"<p>The overview below shows which DFTB+ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DFTB+, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DFTB+/24.1-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DFTB+/24.1-foss-2023a - - x - - -"},{"location":"available_software/detail/DFTB%2B/#dftb241-foss-2023a","title":"DFTB+/24.1-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>dptools-24.1</p>"},{"location":"available_software/detail/DIAMOND/","title":"DIAMOND","text":"<p>Accelerated BLAST compatible local sequence aligner</p> <p>https://github.com/bbuchfink/diamond</p>"},{"location":"available_software/detail/DIAMOND/#available-modules","title":"Available modules","text":"<p>The overview below shows which DIAMOND installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DIAMOND, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DIAMOND/2.1.8-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DIAMOND/2.1.8-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/DIRAC/","title":"DIRAC","text":"<p>DIRAC: Program for Atomic and Molecular Direct Iterative Relativistic All-electron Calculations</p> <p>http://www.diracprogram.org</p>"},{"location":"available_software/detail/DIRAC/#available-modules","title":"Available modules","text":"<p>The overview below shows which DIRAC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DIRAC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DIRAC/23.0-intel-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DIRAC/23.0-intel-2023a - x x x x x"},{"location":"available_software/detail/DMTCP/","title":"DMTCP","text":"<p>DMTCP (Distributed MultiThreaded Checkpointing) transparently checkpoints a single-host or distributed computation in user-space -- with no modifications to user code or to the O/S. It works on most Linux applications, including Python, Matlab, R, GUI desktops, MPI, etc.</p> <p>http://dmtcp.sourceforge.net/</p>"},{"location":"available_software/detail/DMTCP/#available-modules","title":"Available modules","text":"<p>The overview below shows which DMTCP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DMTCP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DMTCP/3.0.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DMTCP/3.0.0-GCCcore-12.3.0 - x - - - - DMTCP/3.0.0 - x - - - - DMTCP/2.6.0 - x - - - -"},{"location":"available_software/detail/DP3/","title":"DP3","text":"<p>DP3: streaming processing pipeline for radio interferometric data.</p> <p>https://dp3.readthedocs.io/</p>"},{"location":"available_software/detail/DP3/#available-modules","title":"Available modules","text":"<p>The overview below shows which DP3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DP3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DP3/6.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DP3/6.0-foss-2023b - x - - - -"},{"location":"available_software/detail/Dalton/","title":"Dalton","text":"<p>The Dalton code is a powerful tool for a wide range of molecular properties at different levels of theory. Any published work arising from use of one of the Dalton2016 programs must acknowledge that by a proper reference, https://www.daltonprogram.org/www/citation.html.</p> <p>https://daltonprogram.org/</p>"},{"location":"available_software/detail/Dalton/#available-modules","title":"Available modules","text":"<p>The overview below shows which Dalton installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Dalton, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Dalton/2020.1-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Dalton/2020.1-foss-2022b - - x - - x"},{"location":"available_software/detail/Delly/","title":"Delly","text":"<p>Delly is an integrated structural variant (SV) predictionmethod that can discover, genotype and visualize deletions, tandem duplications,inversions and translocations at single-nucleotide resolution in short-readmassively parallel sequencing data. It uses paired-ends, split-reads andread-depth to sensitively and accurately delineate genomic rearrangementsthroughout the genome.</p> <p>https://github.com/dellytools/delly/</p>"},{"location":"available_software/detail/Delly/#available-modules","title":"Available modules","text":"<p>The overview below shows which Delly installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Delly, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Delly/1.1.6-GCC-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Delly/1.1.6-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/DendroPy/","title":"DendroPy","text":"<p>A Python library for phylogenetics and phylogenetic computing: reading, writing, simulation, processing and manipulation of phylogenetic trees (phylogenies) and characters.</p> <p>https://dendropy.org/</p>"},{"location":"available_software/detail/DendroPy/#available-modules","title":"Available modules","text":"<p>The overview below shows which DendroPy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using DendroPy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load DendroPy/4.6.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 DendroPy/4.6.1-GCCcore-12.3.0 - x - - - - DendroPy/4.5.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Doxygen/","title":"Doxygen","text":"<p>Doxygen is a documentation system for C++, C, Java, Objective-C, Python, IDL (Corba and Microsoft flavors), Fortran, VHDL, PHP, C#, and to some extent D.</p> <p>https://www.doxygen.org</p>"},{"location":"available_software/detail/Doxygen/#available-modules","title":"Available modules","text":"<p>The overview below shows which Doxygen installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Doxygen, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Doxygen/1.9.8-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Doxygen/1.9.8-GCCcore-13.2.0 - x - - - - Doxygen/1.9.7-GCCcore-12.3.0 - x x x x x Doxygen/1.9.5-GCCcore-12.2.0 - x - - - - Doxygen/1.9.4-GCCcore-11.3.0 - x - - x x Doxygen/1.8.20-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/EDirect/","title":"EDirect","text":"<p>Entrez Direct (EDirect) provides access to the NCBI's suite ofinterconnected databases from a Unix terminal window. Search terms are enteredas command-line arguments. Individual operations are connected with Unix pipesto construct multi-step queries. Selected records can then be retrieved in avariety of formats.</p> <p>https://www.ncbi.nlm.nih.gov/books/NBK25501/</p>"},{"location":"available_software/detail/EDirect/#available-modules","title":"Available modules","text":"<p>The overview below shows which EDirect installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using EDirect, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load EDirect/20.5.20231006-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 EDirect/20.5.20231006-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/ELPA/","title":"ELPA","text":"<p>Eigenvalue SoLvers for Petaflop-Applications.</p> <p>https://elpa.mpcdf.mpg.de/</p>"},{"location":"available_software/detail/ELPA/#available-modules","title":"Available modules","text":"<p>The overview below shows which ELPA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ELPA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ELPA/2023.05.001-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ELPA/2023.05.001-foss-2023a - x - - - - ELPA/2022.05.001-foss-2022b - x - - - -"},{"location":"available_software/detail/ELSI/","title":"ELSI","text":"<p>ELSI provides and enhances scalable, open-source software library solutions for electronic structure calculations in  materials science, condensed matter physics, chemistry, and many other fields. ELSI focuses on methods that solve or circumvent eigenvalue problems in electronic structure theory. The ELSI infrastructure should also be useful for other challenging eigenvalue problems.</p> <p>https://wordpress.elsi-interchange.org/</p>"},{"location":"available_software/detail/ELSI/#available-modules","title":"Available modules","text":"<p>The overview below shows which ELSI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ELSI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ELSI/2.9.1-foss-2023a-PEXSI\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ELSI/2.9.1-foss-2023a-PEXSI - x - - - -"},{"location":"available_software/detail/ESMF/","title":"ESMF","text":"<p>The Earth System Modeling Framework (ESMF) is a suite of software tools for developing high-performance, multi-component Earth science modeling applications.</p> <p>https://www.earthsystemcog.org/projects/esmf/</p>"},{"location":"available_software/detail/ESMF/#available-modules","title":"Available modules","text":"<p>The overview below shows which ESMF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ESMF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ESMF/8.6.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ESMF/8.6.0-foss-2023a - x - - - - ESMF/8.3.0-foss-2022a - x - - - - ESMF/8.0.1-foss-2020b - x - - - -"},{"location":"available_software/detail/ESPResSo/","title":"ESPResSo","text":"<p>A software package for performing and analyzing scientific Molecular Dynamics simulations.</p> <p>https://espressomd.org/wordpress</p>"},{"location":"available_software/detail/ESPResSo/#available-modules","title":"Available modules","text":"<p>The overview below shows which ESPResSo installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ESPResSo, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ESPResSo/4.2.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ESPResSo/4.2.2-foss-2023a - x - - - - ESPResSo/4.2.1-foss-2023a - x - - - -"},{"location":"available_software/detail/EasyBuild/","title":"EasyBuild","text":"<p>EasyBuild is a software build and installation framework written in Python that allows you to install software in a structured, repeatable and robust way.</p> <p>https://easybuilders.github.io/easybuild</p>"},{"location":"available_software/detail/EasyBuild/#available-modules","title":"Available modules","text":"<p>The overview below shows which EasyBuild installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using EasyBuild, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load EasyBuild/4.9.4\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 EasyBuild/4.9.4 - x - - - - EasyBuild/4.9.3 - x - - - - EasyBuild/4.9.2 - x x x x x EasyBuild/4.9.1 - x - - - - EasyBuild/4.9.0 - x - - - - EasyBuild/4.8.2 - x - - - -"},{"location":"available_software/detail/Eigen/","title":"Eigen","text":"<p>Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.</p> <p>https://eigen.tuxfamily.org</p>"},{"location":"available_software/detail/Eigen/#available-modules","title":"Available modules","text":"<p>The overview below shows which Eigen installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Eigen, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Eigen/3.4.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Eigen/3.4.0-GCCcore-13.2.0 - x - - - - Eigen/3.4.0-GCCcore-12.3.0 - x - - - - Eigen/3.4.0-GCCcore-12.2.0 - x - - - - Eigen/3.4.0-GCCcore-11.3.0 - x - - - - Eigen/3.3.8-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/EveryBeam/","title":"EveryBeam","text":"<p>Library that provides the antenna response pattern for several instruments,such as LOFAR (and LOBES), SKA (OSKAR), MWA, JVLA, etc.</p> <p>https://everybeam.readthedocs.io/</p>"},{"location":"available_software/detail/EveryBeam/#available-modules","title":"Available modules","text":"<p>The overview below shows which EveryBeam installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using EveryBeam, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load EveryBeam/0.5.2-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 EveryBeam/0.5.2-foss-2023b - x - - - -"},{"location":"available_software/detail/ExaBayes/","title":"ExaBayes","text":"<p>ExaBayes is a software package for Bayesian tree inference. It is particularly suitable for large-scale analyses on computer clusters</p> <p>https://cme.h-its.org/exelixis/web/software/exabayes/</p>"},{"location":"available_software/detail/ExaBayes/#available-modules","title":"Available modules","text":"<p>The overview below shows which ExaBayes installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ExaBayes, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ExaBayes/1.5-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ExaBayes/1.5-foss-2023a - x - - - -"},{"location":"available_software/detail/ExaML/","title":"ExaML","text":"<p>Exascale Maximum Likelihood (ExaML) code for phylogenetic inference using MPI</p> <p>https://github.com/stamatak/ExaML/</p>"},{"location":"available_software/detail/ExaML/#available-modules","title":"Available modules","text":"<p>The overview below shows which ExaML installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ExaML, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ExaML/3.0.22-gompi-2023a-hybrid-avx\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ExaML/3.0.22-gompi-2023a-hybrid-avx - x - - - -"},{"location":"available_software/detail/Exonerate/","title":"Exonerate","text":"<p>Exonerate is a generic tool for pairwise sequence comparison. It allows you to align sequences using a many alignment models, using either  exhaustive dynamic programming, or a variety of heuristics.</p> <p>https://www.ebi.ac.uk/about/vertebrate-genomics/software/exonerate</p>"},{"location":"available_software/detail/Exonerate/#available-modules","title":"Available modules","text":"<p>The overview below shows which Exonerate installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Exonerate, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Exonerate/2.4.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Exonerate/2.4.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/FASTA/","title":"FASTA","text":"<p>The FASTA programs find regions of local or global (new) similarity between protein or DNA sequences, either by searching Protein or DNA databases, or by identifying local duplications within a sequence.</p> <p>https://fasta.bioch.virginia.edu/fasta_www2/fasta_list2.shtml</p>"},{"location":"available_software/detail/FASTA/#available-modules","title":"Available modules","text":"<p>The overview below shows which FASTA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FASTA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FASTA/36.3.8i-GCC-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FASTA/36.3.8i-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/FFTW.MPI/","title":"FFTW.MPI","text":"<p>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)in one or more dimensions, of arbitrary input size, and of both real and complex data.</p> <p>https://www.fftw.org</p>"},{"location":"available_software/detail/FFTW.MPI/#available-modules","title":"Available modules","text":"<p>The overview below shows which FFTW.MPI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FFTW.MPI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FFTW.MPI/3.3.10-nvompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FFTW.MPI/3.3.10-nvompi-2023a - - - x - - FFTW.MPI/3.3.10-nvompi-2022.07 - x - - - - FFTW.MPI/3.3.10-gompi-2023b - x - - - - FFTW.MPI/3.3.10-gompi-2023a - x x x x x FFTW.MPI/3.3.10-gompi-2022b - x - - - - FFTW.MPI/3.3.10-gompi-2022a - x - - - -"},{"location":"available_software/detail/FFTW/","title":"FFTW","text":"<p>FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)in one or more dimensions, of arbitrary input size, and of both real and complex data.</p> <p>https://www.fftw.org</p>"},{"location":"available_software/detail/FFTW/#available-modules","title":"Available modules","text":"<p>The overview below shows which FFTW installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FFTW, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FFTW/3.3.10-iimpi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FFTW/3.3.10-iimpi-2023a - x - - - - FFTW/3.3.10-NVHPC-23.7-CUDA-12.1.1 - - - x - - FFTW/3.3.10-NVHPC-22.7-CUDA-11.7.0 - x - - - - FFTW/3.3.10-GCC-13.2.0 - x - - - - FFTW/3.3.10-GCC-12.3.0 - x x x x x FFTW/3.3.10-GCC-12.2.0 - x - - - - FFTW/3.3.10-GCC-11.3.0 - x - - - - FFTW/3.3.8-gompi-2020b - x - - - -"},{"location":"available_software/detail/FFmpeg/","title":"FFmpeg","text":"<p>A complete, cross-platform solution to record, convert and stream audio and video.</p> <p>https://www.ffmpeg.org/</p>"},{"location":"available_software/detail/FFmpeg/#available-modules","title":"Available modules","text":"<p>The overview below shows which FFmpeg installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FFmpeg, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FFmpeg/6.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FFmpeg/6.0-GCCcore-12.3.0 - x - - - - FFmpeg/4.4.2-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/FLAC/","title":"FLAC","text":"<p>FLAC stands for Free Lossless Audio Codec, an audio format similar to MP3, but lossless, meaningthat audio is compressed in FLAC without any loss in quality.</p> <p>https://xiph.org/flac/</p>"},{"location":"available_software/detail/FLAC/#available-modules","title":"Available modules","text":"<p>The overview below shows which FLAC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FLAC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FLAC/1.4.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FLAC/1.4.2-GCCcore-12.3.0 - x - - - - FLAC/1.4.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/FLTK/","title":"FLTK","text":"<p>FLTK is a cross-platform C++ GUI toolkit for UNIX/Linux (X11), Microsoft Windows, and MacOS X. FLTK provides modern GUI functionality without the bloat and supports 3D graphics via OpenGL and its built-in GLUT emulation.</p> <p>https://www.fltk.org</p>"},{"location":"available_software/detail/FLTK/#available-modules","title":"Available modules","text":"<p>The overview below shows which FLTK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FLTK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FLTK/1.3.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FLTK/1.3.8-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/FastQC/","title":"FastQC","text":"<p>FastQC is a quality control application for high throughputsequence data. It reads in sequence data in a variety of formats and can eitherprovide an interactive application to review the results of several differentQC checks, or create an HTML based report which can be integrated into apipeline.</p> <p>https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</p>"},{"location":"available_software/detail/FastQC/#available-modules","title":"Available modules","text":"<p>The overview below shows which FastQC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FastQC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FastQC/0.12.1-Java-11\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FastQC/0.12.1-Java-11 - x - - - -"},{"location":"available_software/detail/Flask/","title":"Flask","text":"<p>Flask is a lightweight WSGI web application framework. It is designed to makegetting started quick and easy, with the ability to scale up to complexapplications.This module includes the Flask extensions: Flask-Cors</p> <p>https://www.palletsprojects.com/p/flask/</p>"},{"location":"available_software/detail/Flask/#available-modules","title":"Available modules","text":"<p>The overview below shows which Flask installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Flask, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Flask/2.3.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Flask/2.3.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Flask/#flask233-gcccore-1230","title":"Flask/2.3.3-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>asgiref-3.7.2, blinker-1.6.2, cachelib-0.10.2, flask-2.3.3, Flask-Cors-4.0.0, Flask-Session-0.5.0, itsdangerous-2.1.2, werkzeug-2.3.7</p>"},{"location":"available_software/detail/FlexiBLAS/","title":"FlexiBLAS","text":"<p>FlexiBLAS is a wrapper library that enables the exchange of the BLAS and LAPACK implementationused by a program without recompiling or relinking it.</p> <p>https://gitlab.mpi-magdeburg.mpg.de/software/flexiblas-release</p>"},{"location":"available_software/detail/FlexiBLAS/#available-modules","title":"Available modules","text":"<p>The overview below shows which FlexiBLAS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FlexiBLAS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FlexiBLAS/3.3.1-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FlexiBLAS/3.3.1-GCC-13.2.0 - x - - - x FlexiBLAS/3.3.1-GCC-12.3.0 - x x x x x FlexiBLAS/3.2.1-GCC-12.2.0 - x - - - - FlexiBLAS/3.2.0-NVHPC-22.7-CUDA-11.7.0 - x - - - - FlexiBLAS/3.2.0-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/Flow-3D/","title":"Flow-3D","text":"<p>Description not found in module</p>"},{"location":"available_software/detail/Flow-3D/#available-modules","title":"Available modules","text":"<p>The overview below shows which Flow-3D installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Flow-3D, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Flow-3D/2023R1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Flow-3D/2023R1 - x - - - -"},{"location":"available_software/detail/FreeSurfer/","title":"FreeSurfer","text":"<p>FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data.  FreeSurfer contains a fully automatic structural imaging stream for processing cross sectional and longitudinal data.</p> <p>https://surfer.nmr.mgh.harvard.edu/</p>"},{"location":"available_software/detail/FreeSurfer/#available-modules","title":"Available modules","text":"<p>The overview below shows which FreeSurfer installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FreeSurfer, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FreeSurfer/7.4.1-ubuntu22_amd64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FreeSurfer/7.4.1-ubuntu22_amd64 - x - - - -"},{"location":"available_software/detail/FriBidi/","title":"FriBidi","text":"<p>The Free Implementation of the Unicode Bidirectional Algorithm.</p> <p>https://github.com/fribidi/fribidi</p>"},{"location":"available_software/detail/FriBidi/#available-modules","title":"Available modules","text":"<p>The overview below shows which FriBidi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using FriBidi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load FriBidi/1.0.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 FriBidi/1.0.13-GCCcore-13.2.0 - x - - - - FriBidi/1.0.12-GCCcore-12.3.0 - x x x x x FriBidi/1.0.12-GCCcore-12.2.0 - x - - - - FriBidi/1.0.12-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/GAMESS-US/","title":"GAMESS-US","text":"<p>The General Atomic and Molecular Electronic Structure System (GAMESS)is a general ab initio quantum chemistry package.This module can be used on a maximum of 1024 nodes and 1024 CPUs per node.</p> <p>https://www.msg.chem.iastate.edu/gamess/</p>"},{"location":"available_software/detail/GAMESS-US/#available-modules","title":"Available modules","text":"<p>The overview below shows which GAMESS-US installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GAMESS-US, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GAMESS-US/20230930-R2-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GAMESS-US/20230930-R2-gompi-2023a - x - - - -"},{"location":"available_software/detail/GATK/","title":"GATK","text":"<p>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyse next-generation resequencing data. The toolkit offers a wide variety of tools, with a primary focus on variant discovery and genotyping as well as strong emphasis on data quality assurance. Its robust architecture, powerful processing engine and high-performance computing features make it capable of taking on projects of any size.</p> <p>https://www.broadinstitute.org/gatk/</p>"},{"location":"available_software/detail/GATK/#available-modules","title":"Available modules","text":"<p>The overview below shows which GATK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GATK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GATK/4.5.0.0-GCCcore-12.3.0-Java-17\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GATK/4.5.0.0-GCCcore-12.3.0-Java-17 - x - - - - GATK/4.3.0.0-GCCcore-12.3.0-Java-11 - x - - - -"},{"location":"available_software/detail/GCC/","title":"GCC","text":"<p>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...).</p> <p>https://gcc.gnu.org/</p>"},{"location":"available_software/detail/GCC/#available-modules","title":"Available modules","text":"<p>The overview below shows which GCC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GCC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GCC/13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GCC/13.2.0 - x - - - - GCC/12.3.0 - x x x x x GCC/12.2.0 - x - - - - GCC/11.3.0 - x - - - - GCC/11.2.0 - x - - - - GCC/10.2.0 - x - - - -"},{"location":"available_software/detail/GCCcore/","title":"GCCcore","text":"<p>The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Java, and Ada, as well as libraries for these languages (libstdc++, libgcj,...).</p> <p>https://gcc.gnu.org/</p>"},{"location":"available_software/detail/GCCcore/#available-modules","title":"Available modules","text":"<p>The overview below shows which GCCcore installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GCCcore, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GCCcore/13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GCCcore/13.2.0 - x - - - - GCCcore/12.3.0 - x x x x x GCCcore/12.2.0 - x - - - - GCCcore/11.3.0 - x - - - - GCCcore/11.2.0 - x - - - - GCCcore/10.2.0 - x - - - -"},{"location":"available_software/detail/GCTA/","title":"GCTA","text":"<p>GCTA (Genome-wide Complex Trait Analysis) is a software package, which was initially developed to estimate the proportion of phenotypic variance explained by all genome-wide SNPs for a complex trait but has been extensively extended for many other analyses of data from genome-wide association studies (GWASs).</p> <p>https://yanglab.westlake.edu.cn/software/gcta/</p>"},{"location":"available_software/detail/GCTA/#available-modules","title":"Available modules","text":"<p>The overview below shows which GCTA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GCTA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GCTA/1.94.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GCTA/1.94.1-gfbf-2023a - x - - - -"},{"location":"available_software/detail/GDAL/","title":"GDAL","text":"<p>GDAL is a translator library for raster geospatial data formats that is released under an X/MIT style Open Source license by the Open Source Geospatial Foundation. As a library, it presents a single abstract data model to the calling application for all supported formats. It also comes with a variety of useful commandline utilities for data translation and processing.</p> <p>https://www.gdal.org</p>"},{"location":"available_software/detail/GDAL/#available-modules","title":"Available modules","text":"<p>The overview below shows which GDAL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GDAL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GDAL/3.7.1-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GDAL/3.7.1-foss-2023a - x - - - - GDAL/3.6.2-foss-2022b - x - - - - GDAL/3.2.1-foss-2020b - x - - - -"},{"location":"available_software/detail/GDRCopy/","title":"GDRCopy","text":"<p>A low-latency GPU memory copy library based on NVIDIA GPUDirect RDMA technology.</p> <p>https://github.com/NVIDIA/gdrcopy</p>"},{"location":"available_software/detail/GDRCopy/#available-modules","title":"Available modules","text":"<p>The overview below shows which GDRCopy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GDRCopy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GDRCopy/2.3.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GDRCopy/2.3.1-GCCcore-12.3.0 - x - - - - GDRCopy/2.3-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/GEOS/","title":"GEOS","text":"<p>GEOS (Geometry Engine - Open Source) is a C++ port of the Java Topology Suite (JTS)</p> <p>https://trac.osgeo.org/geos</p>"},{"location":"available_software/detail/GEOS/#available-modules","title":"Available modules","text":"<p>The overview below shows which GEOS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GEOS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GEOS/3.12.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GEOS/3.12.0-GCC-12.3.0 - x - - - - GEOS/3.11.1-GCC-12.2.0 - x - - - - GEOS/3.9.1-GCC-10.2.0 - x - - - -"},{"location":"available_software/detail/GLM/","title":"GLM","text":"<p>OpenGL Mathematics (GLM) is a header only C++ mathematics library for graphics  software based on  the OpenGL Shading Language (GLSL) specifications.</p> <p>https://github.com/g-truc/glm</p>"},{"location":"available_software/detail/GLM/#available-modules","title":"Available modules","text":"<p>The overview below shows which GLM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GLM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GLM/0.9.9.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GLM/0.9.9.8-GCCcore-12.3.0 - x x - - -"},{"location":"available_software/detail/GLPK/","title":"GLPK","text":"<p>The GLPK (GNU Linear Programming Kit) package is intended for solving large-scale linear programming (LP), mixed integer programming (MIP), and other related problems. It is a set of routines written in ANSI C and organized in the form of a callable library.</p> <p>https://www.gnu.org/software/glpk/</p>"},{"location":"available_software/detail/GLPK/#available-modules","title":"Available modules","text":"<p>The overview below shows which GLPK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GLPK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GLPK/5.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GLPK/5.0-GCCcore-12.3.0 - x - - - - GLPK/5.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/GLVis/","title":"GLVis","text":"<p>GLVis is a lightweight tool for accurate and flexible finite element visualization.</p> <p>https://glvis.org/</p>"},{"location":"available_software/detail/GLVis/#available-modules","title":"Available modules","text":"<p>The overview below shows which GLVis installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GLVis, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GLVis/4.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GLVis/4.2-foss-2023a - x - - - -"},{"location":"available_software/detail/GLib/","title":"GLib","text":"<p>GLib is one of the base libraries of the GTK+ project</p> <p>https://www.gtk.org/</p>"},{"location":"available_software/detail/GLib/#available-modules","title":"Available modules","text":"<p>The overview below shows which GLib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GLib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GLib/2.78.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GLib/2.78.1-GCCcore-13.2.0 - x - - - - GLib/2.77.1-GCCcore-12.3.0 - x x x x x GLib/2.75.0-GCCcore-12.2.0 - x - - - - GLib/2.66.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/GMAP-GSNAP/","title":"GMAP-GSNAP","text":"<p>GMAP: A Genomic Mapping and Alignment Program for mRNA and EST Sequences GSNAP: Genomic Short-read Nucleotide Alignment Program</p> <p>http://research-pub.gene.com/gmap/</p>"},{"location":"available_software/detail/GMAP-GSNAP/#available-modules","title":"Available modules","text":"<p>The overview below shows which GMAP-GSNAP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GMAP-GSNAP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GMAP-GSNAP/2024-05-20-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GMAP-GSNAP/2024-05-20-GCC-12.3.0 - x - - - - GMAP-GSNAP/2023-04-20-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/GMP/","title":"GMP","text":"<p>GMP is a free library for arbitrary precision arithmetic, operating on signed integers, rational numbers, and floating point numbers.</p> <p>https://gmplib.org/</p>"},{"location":"available_software/detail/GMP/#available-modules","title":"Available modules","text":"<p>The overview below shows which GMP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GMP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GMP/6.3.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GMP/6.3.0-GCCcore-13.2.0 - x - - - - GMP/6.2.1-GCCcore-12.3.0 - x - - - - GMP/6.2.1-GCCcore-12.2.0 - x - - - - GMP/6.2.1-GCCcore-11.3.0 - x - - - - GMP/6.2.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/GObject-Introspection/","title":"GObject-Introspection","text":"<p>GObject introspection is a middleware layer between C libraries (using GObject) and language bindings. The C library can be scanned at compile time and generate a metadata file, in addition to the actual native C library. Then at runtime, language bindings can read this metadata and automatically provide bindings to call into the C library.</p> <p>https://gi.readthedocs.io/en/latest/</p>"},{"location":"available_software/detail/GObject-Introspection/#available-modules","title":"Available modules","text":"<p>The overview below shows which GObject-Introspection installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GObject-Introspection, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GObject-Introspection/1.78.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GObject-Introspection/1.78.1-GCCcore-13.2.0 - x - - - - GObject-Introspection/1.76.1-GCCcore-12.3.0 - x x x x x GObject-Introspection/1.74.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/GROMACS/","title":"GROMACS","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate theNewtonian equations of motion for systems with hundreds to millions ofparticles.This is a GPU enabled build, containing both MPI and threadMPI buildsfor both single and double precision.It also contains the gmxapi extension for the single precision MPI buildnext to PLUMED.</p> <p>https://www.gromacs.org</p>"},{"location":"available_software/detail/GROMACS/#available-modules","title":"Available modules","text":"<p>The overview below shows which GROMACS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GROMACS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GROMACS/2024.1-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GROMACS/2024.1-foss-2023b - - x x - x GROMACS/2023.3-foss-2023a-CUDA-12.1.1-PLUMED-2.9.0 - - - x - x"},{"location":"available_software/detail/GROMACS/#gromacs20241-foss-2023b","title":"GROMACS/2024.1-foss-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>gmxapi-0.5.0</p>"},{"location":"available_software/detail/GROMACS/#gromacs20233-foss-2023a-cuda-1211-plumed-290","title":"GROMACS/2023.3-foss-2023a-CUDA-12.1.1-PLUMED-2.9.0","text":"<p>This is a list of extensions included in the module:</p> <p>gmxapi-0.4.2</p>"},{"location":"available_software/detail/GSL/","title":"GSL","text":"<p>The GNU Scientific Library (GSL) is a numerical library for C and C++ programmers. The library provides a wide range of mathematical routines such as random number generators, special functions and least-squares fitting.</p> <p>https://www.gnu.org/software/gsl/</p>"},{"location":"available_software/detail/GSL/#available-modules","title":"Available modules","text":"<p>The overview below shows which GSL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GSL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GSL/2.7-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GSL/2.7-GCC-13.2.0 - x - - - - GSL/2.7-GCC-12.3.0 - x - - - - GSL/2.7-GCC-12.2.0 - x - - - - GSL/2.6-GCC-10.2.0 - x - - - -"},{"location":"available_software/detail/GTK3/","title":"GTK3","text":"<p>GTK+ is the primary library used to construct user interfaces in GNOME. It provides all the user interface controls, or widgets, used in a common graphical application. Its object-oriented API allows you to construct user interfaces without dealing with the low-level details of drawing and device interaction.</p> <p>https://developer.gnome.org/gtk3/stable/</p>"},{"location":"available_software/detail/GTK3/#available-modules","title":"Available modules","text":"<p>The overview below shows which GTK3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GTK3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GTK3/3.24.37-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GTK3/3.24.37-GCCcore-12.3.0 - x - - - - GTK3/3.24.35-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/GULP/","title":"GULP","text":"<p>GULP is a program for performing a variety of types of simulation on materials using boundary conditions of 0-D (molecules and clusters), 1-D (polymers), 2-D (surfaces, slabs and grain boundaries), or 3-D (periodic solids)Band Unfolding code for Plane-wave based calculations</p> <p>https://gulp.curtin.edu.au/gulp/</p>"},{"location":"available_software/detail/GULP/#available-modules","title":"Available modules","text":"<p>The overview below shows which GULP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GULP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GULP/6.2-intel-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GULP/6.2-intel-2023a - x - - - -"},{"location":"available_software/detail/Gaussian/","title":"Gaussian","text":"<p>Gaussian provides state-of-the-art capabilities for electronic structuremodeling. Gaussian 16 is licensed for a wide variety of computersystems. All versions of Gaussian 09 contain every scientific/modelingfeature, and none imposes any artificial limitations on calculationsother than your computing resources and patience.This is the official gaussian AVX2 build.</p> <p>https://www.gaussian.com/</p>"},{"location":"available_software/detail/Gaussian/#available-modules","title":"Available modules","text":"<p>The overview below shows which Gaussian installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Gaussian, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Gaussian/16.C.01-AVX2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Gaussian/16.C.01-AVX2 - x - - - - Gaussian/16.C.01-AVX - x - - - - Gaussian/16.B.01-AVX2 - x - - - - Gaussian/16.B.01-AVX - x - - - - Gaussian/16-AVX2 - x - - - - Gaussian/16-AVX - x - - - -"},{"location":"available_software/detail/Gdk-Pixbuf/","title":"Gdk-Pixbuf","text":"<p>The Gdk Pixbuf is a toolkit for image loading and pixel buffer manipulation. It is used by GTK+ 2 and GTK+ 3 to load and manipulate images. In the past it was distributed as part of GTK+ 2 but it was split off into a separate package in preparation for the change to GTK+ 3.</p> <p>https://docs.gtk.org/gdk-pixbuf/</p>"},{"location":"available_software/detail/Gdk-Pixbuf/#available-modules","title":"Available modules","text":"<p>The overview below shows which Gdk-Pixbuf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Gdk-Pixbuf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Gdk-Pixbuf/2.42.10-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Gdk-Pixbuf/2.42.10-GCCcore-12.3.0 - x - - - - Gdk-Pixbuf/2.42.10-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/GeneMark-ET/","title":"GeneMark-ET","text":"<p>Eukaryotic gene prediction suite with automatic training</p> <p>http://exon.gatech.edu/GeneMark</p>"},{"location":"available_software/detail/GeneMark-ET/#available-modules","title":"Available modules","text":"<p>The overview below shows which GeneMark-ET installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GeneMark-ET, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GeneMark-ET/4.72-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GeneMark-ET/4.72-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/GeneMark-ETP/","title":"GeneMark-ETP","text":"<p>Gene finding in eukaryotic genomes</p> <p>https://github.com/gatech-genemark/GeneMark-ETP</p>"},{"location":"available_software/detail/GeneMark-ETP/#available-modules","title":"Available modules","text":"<p>The overview below shows which GeneMark-ETP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GeneMark-ETP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GeneMark-ETP/1.02-preprint-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GeneMark-ETP/1.02-preprint-foss-2023a - x - - - -"},{"location":"available_software/detail/GenomeTools/","title":"GenomeTools","text":"<p>A comprehensive software library for efficient processing of structured genome annotations.</p> <p>http://genometools.org</p>"},{"location":"available_software/detail/GenomeTools/#available-modules","title":"Available modules","text":"<p>The overview below shows which GenomeTools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GenomeTools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GenomeTools/1.6.5-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GenomeTools/1.6.5-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Ghostscript/","title":"Ghostscript","text":"<p>Ghostscript is a versatile processor for PostScript data with the ability to render PostScript to different targets. It used to be part of the cups printing stack, but is no longer used for that.</p> <p>https://ghostscript.com</p>"},{"location":"available_software/detail/Ghostscript/#available-modules","title":"Available modules","text":"<p>The overview below shows which Ghostscript installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Ghostscript, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Ghostscript/10.01.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Ghostscript/10.01.2-GCCcore-12.3.0 - x - - - - Ghostscript/10.0.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/GitPython/","title":"GitPython","text":"<p>GitPython is a python library used to interact with Git repositories</p> <p>https://gitpython.readthedocs.org</p>"},{"location":"available_software/detail/GitPython/#available-modules","title":"Available modules","text":"<p>The overview below shows which GitPython installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GitPython, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GitPython/3.1.40-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GitPython/3.1.40-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/GitPython/#gitpython3140-gcccore-1230","title":"GitPython/3.1.40-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>gitdb-4.0.11, GitPython-3.1.40, smmap-5.0.1</p>"},{"location":"available_software/detail/GlobalArrays/","title":"GlobalArrays","text":"<p>Global Arrays (GA) is a Partitioned Global Address Space (PGAS) programming model</p> <p>https://hpc.pnl.gov/globalarrays</p>"},{"location":"available_software/detail/GlobalArrays/#available-modules","title":"Available modules","text":"<p>The overview below shows which GlobalArrays installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using GlobalArrays, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load GlobalArrays/5.8.2-intel-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 GlobalArrays/5.8.2-intel-2023a - x - - - -"},{"location":"available_software/detail/Go/","title":"Go","text":"<p>Go is an open source programming language that makes it easy to build simple, reliable, and efficient software.</p> <p>https://www.golang.org</p>"},{"location":"available_software/detail/Go/#available-modules","title":"Available modules","text":"<p>The overview below shows which Go installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Go, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Go/1.22.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Go/1.22.1 - x - - - - Go/1.21.2 - x - - - - Go/1.17.6 - x - - - -"},{"location":"available_software/detail/HDF-EOS/","title":"HDF-EOS","text":"<p>HDF-EOS libraries are software libraries built on HDF libraries. It supports three data structures for remote sensing data: Grid, Point and Swath.</p> <p>https://hdfeos.org/</p>"},{"location":"available_software/detail/HDF-EOS/#available-modules","title":"Available modules","text":"<p>The overview below shows which HDF-EOS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HDF-EOS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HDF-EOS/2.20-GCCcore-10.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HDF-EOS/2.20-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/HDF-EOS5/","title":"HDF-EOS5","text":"<p>HDF-EOS libraries are software libraries built on HDF libraries. It supports three data structures for remote sensing data: Grid, Point and Swath.</p> <p>https://hdfeos.org/</p>"},{"location":"available_software/detail/HDF-EOS5/#available-modules","title":"Available modules","text":"<p>The overview below shows which HDF-EOS5 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HDF-EOS5, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HDF-EOS5/1.16-gompi-2020b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HDF-EOS5/1.16-gompi-2020b - x - - - -"},{"location":"available_software/detail/HDF/","title":"HDF","text":"<p>HDF (also known as HDF4) is a library and multi-object file format for storing and managing data between machines.</p> <p>https://www.hdfgroup.org/products/hdf4/</p>"},{"location":"available_software/detail/HDF/#available-modules","title":"Available modules","text":"<p>The overview below shows which HDF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HDF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HDF/4.2.16-2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HDF/4.2.16-2-GCCcore-12.3.0 - x - - - - HDF/4.2.15-GCCcore-12.2.0 - x - - - - HDF/4.2.15-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/HDF5/","title":"HDF5","text":"<p>HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.</p> <p>https://portal.hdfgroup.org/display/support</p>"},{"location":"available_software/detail/HDF5/#available-modules","title":"Available modules","text":"<p>The overview below shows which HDF5 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HDF5, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HDF5/1.14.3-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HDF5/1.14.3-gompi-2023b - x - - - - HDF5/1.14.0-nvompi-2023a-CUDA-12.1.1 - - - x - - HDF5/1.14.0-iimpi-2023a - x - - - - HDF5/1.14.0-gompi-2023a - x - - - - HDF5/1.14.0-gompi-2022b - x - - - - HDF5/1.12.2-gompi-2022a - x - - x x HDF5/1.12.2-NVHPC-22.7-CUDA-11.7.0 - x - - - - HDF5/1.10.7-gompi-2020b - x - - - -"},{"location":"available_software/detail/HH-suite/","title":"HH-suite","text":"<p>The HH-suite is an open-source software package for sensitive protein sequence searching based on the pairwise alignment of hidden Markov models (HMMs).</p> <p>https://github.com/soedinglab/hh-suite</p>"},{"location":"available_software/detail/HH-suite/#available-modules","title":"Available modules","text":"<p>The overview below shows which HH-suite installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HH-suite, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HH-suite/3.3.0-gompi-2022a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HH-suite/3.3.0-gompi-2022a - x - - - -"},{"location":"available_software/detail/HISAT2/","title":"HISAT2","text":"<p>HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) against the general human population (as well as against a single reference genome).</p> <p>https://daehwankimlab.github.io/hisat2</p>"},{"location":"available_software/detail/HISAT2/#available-modules","title":"Available modules","text":"<p>The overview below shows which HISAT2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HISAT2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HISAT2/2.2.1-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HISAT2/2.2.1-gompi-2023a - x - - - -"},{"location":"available_software/detail/HMMER/","title":"HMMER","text":"<p>HMMER is used for searching sequence databases for homologs of protein sequences, and for making protein sequence alignments. It implements methods using probabilistic models called profile hidden Markov models (profile HMMs).  Compared to BLAST, FASTA, and other sequence alignment and database search tools based on older scoring methodology, HMMER aims to be significantly more accurate and more able to detect remote homologs because of the strength of its underlying mathematical models. In the past, this strength came at significant computational expense, but in the new HMMER3 project, HMMER is now essentially as fast as BLAST.</p> <p>http://hmmer.org/</p>"},{"location":"available_software/detail/HMMER/#available-modules","title":"Available modules","text":"<p>The overview below shows which HMMER installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HMMER, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HMMER/3.4-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HMMER/3.4-gompi-2023a - x - - - - HMMER/3.3.2-gompi-2022b - x - - - - HMMER/3.3.2-gompi-2022a - x - - - -"},{"location":"available_software/detail/HTSeq/","title":"HTSeq","text":"<p>HTSeq is a Python library to facilitate processing and analysis of data from high-throughput sequencing (HTS) experiments.</p> <p>https://github.com/simon-anders/htseq</p>"},{"location":"available_software/detail/HTSeq/#available-modules","title":"Available modules","text":"<p>The overview below shows which HTSeq installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HTSeq, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HTSeq/2.0.7-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HTSeq/2.0.7-foss-2023a - x - - - -"},{"location":"available_software/detail/HTSeq/#htseq207-foss-2023a","title":"HTSeq/2.0.7-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>htseq-2.0.7</p>"},{"location":"available_software/detail/HTSlib/","title":"HTSlib","text":"<p>A C library for reading/writing high-throughput sequencing data. This package includes the utilities bgzip and tabix</p> <p>https://www.htslib.org/</p>"},{"location":"available_software/detail/HTSlib/#available-modules","title":"Available modules","text":"<p>The overview below shows which HTSlib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HTSlib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HTSlib/1.19.1-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HTSlib/1.19.1-GCC-13.2.0 - x - - - - HTSlib/1.18-GCC-12.3.0 - x - - - - HTSlib/1.17-GCC-12.2.0 - x - - - - HTSlib/1.15.1-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/HarfBuzz/","title":"HarfBuzz","text":"<p>HarfBuzz is an OpenType text shaping engine.</p> <p>https://www.freedesktop.org/wiki/Software/HarfBuzz</p>"},{"location":"available_software/detail/HarfBuzz/#available-modules","title":"Available modules","text":"<p>The overview below shows which HarfBuzz installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HarfBuzz, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HarfBuzz/8.2.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HarfBuzz/8.2.2-GCCcore-13.2.0 - x - - - - HarfBuzz/5.3.1-GCCcore-12.3.0 - x x x x x HarfBuzz/5.3.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/HepMC3/","title":"HepMC3","text":"<p>HepMC is a standard for storing Monte Carlo event data.</p> <p>http://hepmc.web.cern.ch/hepmc/</p>"},{"location":"available_software/detail/HepMC3/#available-modules","title":"Available modules","text":"<p>The overview below shows which HepMC3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using HepMC3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load HepMC3/3.2.6-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 HepMC3/3.2.6-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Highway/","title":"Highway","text":"<p>Highway is a C++ library for SIMD (Single Instruction, Multiple Data), i.e. applying the sameoperation to 'lanes'.</p> <p>https://github.com/google/highway</p>"},{"location":"available_software/detail/Highway/#available-modules","title":"Available modules","text":"<p>The overview below shows which Highway installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Highway, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Highway/1.0.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Highway/1.0.4-GCCcore-12.3.0 - x - - - - Highway/1.0.3-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Homer/","title":"Homer","text":"<p>HOMER (Hypergeometric Optimization of Motif EnRichment) isa suite of tools for Motif Discovery and next-gen sequencing analysis.  Itis a collection of command line programs for UNIX-style operating systemswritten in Perl and C++.  HOMER was primarily written as a de novo motifdiscovery algorithm and is well suited for finding 8-20 bp motifs in largescale genomics data.  HOMER contains many useful tools for analyzingChIP-Seq, GRO-Seq, RNA-Seq, DNase-Seq, Hi-C and numerous other types offunctional genomics sequencing data sets.</p> <p>http://homer.ucsd.edu/homer/</p>"},{"location":"available_software/detail/Homer/#available-modules","title":"Available modules","text":"<p>The overview below shows which Homer installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Homer, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Homer/4.11-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Homer/4.11-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Hypre/","title":"Hypre","text":"<p>Hypre is a library for solving large, sparse linear systems of equations on massively parallel computers. The problems of interest arise in the simulation codes being developed at LLNL and elsewhere to study physical phenomena in the defense, environmental, energy, and biological sciences.</p> <p>https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods</p>"},{"location":"available_software/detail/Hypre/#available-modules","title":"Available modules","text":"<p>The overview below shows which Hypre installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Hypre, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Hypre/2.29.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Hypre/2.29.0-foss-2023a - x - - - -"},{"location":"available_software/detail/ICU/","title":"ICU","text":"<p>ICU is a mature, widely used set of C/C++ and Java libraries providing Unicode and Globalization support for software applications.</p> <p>https://icu.unicode.org</p>"},{"location":"available_software/detail/ICU/#available-modules","title":"Available modules","text":"<p>The overview below shows which ICU installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ICU, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ICU/74.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ICU/74.1-GCCcore-13.2.0 - x - - - - ICU/73.2-GCCcore-12.3.0 - x x x x x ICU/72.1-GCCcore-12.2.0 - x - - - - ICU/71.1-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/IDG/","title":"IDG","text":"<p>Image Domain Gridding (IDG) is a fast method for convolutional resampling (gridding/degridding)of radio astronomical data (visibilities). Direction dependent effects (DDEs) or A-tems can be appliedin the gridding process.The algorithm is described in \"Image Domain Gridding: a fast method for convolutional resampling of visibilities\",Van der Tol (2018).The implementation is described in \"Radio-astronomical imaging on graphics processors\", Veenboer (2020).Please cite these papers in publications using IDG.</p> <p>https://idg.readthedocs.io/</p>"},{"location":"available_software/detail/IDG/#available-modules","title":"Available modules","text":"<p>The overview below shows which IDG installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using IDG, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load IDG/1.2.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 IDG/1.2.0-foss-2023b - x - - - -"},{"location":"available_software/detail/IDL/","title":"IDL","text":"<p>Description not found in module</p>"},{"location":"available_software/detail/IDL/#available-modules","title":"Available modules","text":"<p>The overview below shows which IDL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using IDL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load IDL/9.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 IDL/9.0 - x - - - - IDL/8.8.2 - x - - - -"},{"location":"available_software/detail/IGV/","title":"IGV","text":"<p>This package contains command line utilities for preprocessing, computing feature count density (coverage),  sorting, and indexing data files.</p> <p>https://www.broadinstitute.org/software/igv/</p>"},{"location":"available_software/detail/IGV/#available-modules","title":"Available modules","text":"<p>The overview below shows which IGV installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using IGV, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load IGV/2.16.0-Java-11\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 IGV/2.16.0-Java-11 - x - - - -"},{"location":"available_software/detail/IPython/","title":"IPython","text":"<p>IPython provides a rich architecture for interactive computing with: Powerful interactive shells (terminal and Qt-based). A browser-based notebook with support for code, text, mathematical expressions, inline plots and other rich media. Support for interactive data visualization and use of GUI toolkits. Flexible, embeddable interpreters to load into your own projects. Easy to use, high performance tools for parallel computing.</p> <p>https://ipython.org/index.html</p>"},{"location":"available_software/detail/IPython/#available-modules","title":"Available modules","text":"<p>The overview below shows which IPython installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using IPython, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load IPython/8.14.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 IPython/8.14.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/IPython/#ipython8140-gcccore-1230","title":"IPython/8.14.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>asttokens-2.2.1, backcall-0.2.0, executing-1.2.0, ipython-8.14.0, jedi-0.19.0, matplotlib-inline-0.1.6, parso-0.8.3, pickleshare-0.7.5, prompt_toolkit-3.0.39, pure_eval-0.2.2, stack_data-0.6.2, traitlets-5.9.0</p>"},{"location":"available_software/detail/IQ-TREE/","title":"IQ-TREE","text":"<p>Efficient phylogenomic software by maximum likelihood</p> <p>http://www.iqtree.org/</p>"},{"location":"available_software/detail/IQ-TREE/#available-modules","title":"Available modules","text":"<p>The overview below shows which IQ-TREE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using IQ-TREE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load IQ-TREE/2.2.2.7-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 IQ-TREE/2.2.2.7-gompi-2023a - x - - - -"},{"location":"available_software/detail/ISA-L/","title":"ISA-L","text":"<p>Intelligent Storage Acceleration Library</p> <p>https://github.com/intel/isa-l</p>"},{"location":"available_software/detail/ISA-L/#available-modules","title":"Available modules","text":"<p>The overview below shows which ISA-L installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ISA-L, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ISA-L/2.30.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ISA-L/2.30.0-GCCcore-12.3.0 - x - - - - ISA-L/2.30.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/ImageMagick/","title":"ImageMagick","text":"<p>ImageMagick is a software suite to create, edit, compose, or convert bitmap images</p> <p>https://www.imagemagick.org/</p>"},{"location":"available_software/detail/ImageMagick/#available-modules","title":"Available modules","text":"<p>The overview below shows which ImageMagick installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ImageMagick, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ImageMagick/7.1.1-15-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ImageMagick/7.1.1-15-GCCcore-12.3.0 - x - - - - ImageMagick/7.1.0-53-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Imath/","title":"Imath","text":"<p>Imath is a C++ and python library of 2D and 3D vector, matrix, and math operations for computer graphics</p> <p>https://imath.readthedocs.io/en/latest/</p>"},{"location":"available_software/detail/Imath/#available-modules","title":"Available modules","text":"<p>The overview below shows which Imath installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Imath, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Imath/3.1.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Imath/3.1.7-GCCcore-12.3.0 - x - - - - Imath/3.1.6-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Infernal/","title":"Infernal","text":"<p>Infernal (\"INFERence of RNA ALignment\") is for searching DNA sequence databases for RNA structure and sequence similarities.</p> <p>http://eddylab.org/infernal/</p>"},{"location":"available_software/detail/Infernal/#available-modules","title":"Available modules","text":"<p>The overview below shows which Infernal installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Infernal, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Infernal/1.1.4-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Infernal/1.1.4-foss-2023a - x - - - -"},{"location":"available_software/detail/Inspector/","title":"Inspector","text":"<p>Intel Inspector is a dynamic memory and threading error checking tool for users developing serial and parallel applications</p> <p>https://software.intel.com/en-us/inspector</p>"},{"location":"available_software/detail/Inspector/#available-modules","title":"Available modules","text":"<p>The overview below shows which Inspector installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Inspector, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Inspector/2023.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Inspector/2023.2.0 - x - - - -"},{"location":"available_software/detail/JAGS/","title":"JAGS","text":"<p>JAGS is Just Another Gibbs Sampler.  It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation</p> <p>http://mcmc-jags.sourceforge.net/</p>"},{"location":"available_software/detail/JAGS/#available-modules","title":"Available modules","text":"<p>The overview below shows which JAGS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using JAGS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load JAGS/4.3.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 JAGS/4.3.2-foss-2023a - x x x x x"},{"location":"available_software/detail/JasPer/","title":"JasPer","text":"<p>The JasPer Project is an open-source initiative to provide a free software-based reference implementation of the codec specified in the JPEG-2000 Part-1 standard.</p> <p>https://www.ece.uvic.ca/~frodo/jasper/</p>"},{"location":"available_software/detail/JasPer/#available-modules","title":"Available modules","text":"<p>The overview below shows which JasPer installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using JasPer, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load JasPer/4.0.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 JasPer/4.0.0-GCCcore-13.2.0 - x - - - - JasPer/4.0.0-GCCcore-12.3.0 - x - - - - JasPer/4.0.0-GCCcore-12.2.0 - x - - - - JasPer/2.0.24-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Java/","title":"Java","text":""},{"location":"available_software/detail/Java/#available-modules","title":"Available modules","text":"<p>The overview below shows which Java installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Java, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Java/21.0.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Java/21.0.2 - x - - - - Java/17.0.6 - x - - - - Java/17(@Java/17.0.6) - x - - - - Java/11.0.20 - x x x x x Java/11(@Java/11.0.20) - x x x x x"},{"location":"available_software/detail/Jellyfish/","title":"Jellyfish","text":"<p>Jellyfish is a tool for fast, memory-efficient counting of k-mers in DNA.</p> <p>http://www.genome.umd.edu/jellyfish.html</p>"},{"location":"available_software/detail/Jellyfish/#available-modules","title":"Available modules","text":"<p>The overview below shows which Jellyfish installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Jellyfish, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Jellyfish/2.3.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Jellyfish/2.3.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/JsonCpp/","title":"JsonCpp","text":"<p>JsonCpp is a C++ library that allows manipulating JSON values, including serialization and deserialization to and from strings. It can also preserve existing comment in unserialization/serialization steps, making it a convenient format to store user input files.</p> <p>https://open-source-parsers.github.io/jsoncpp-docs/doxygen/index.html</p>"},{"location":"available_software/detail/JsonCpp/#available-modules","title":"Available modules","text":"<p>The overview below shows which JsonCpp installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using JsonCpp, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load JsonCpp/1.9.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 JsonCpp/1.9.5-GCCcore-12.3.0 - x - - - - JsonCpp/1.9.5-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/Judy/","title":"Judy","text":"<p>A C library that implements a dynamic array.</p> <p>http://judy.sourceforge.net/</p>"},{"location":"available_software/detail/Judy/#available-modules","title":"Available modules","text":"<p>The overview below shows which Judy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Judy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Judy/1.0.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Judy/1.0.5-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Julia/","title":"Julia","text":"<p>Julia is a high-level, high-performance dynamic programming language for numerical computing</p> <p>https://julialang.org</p>"},{"location":"available_software/detail/Julia/#available-modules","title":"Available modules","text":"<p>The overview below shows which Julia installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Julia, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Julia/1.9.3-linux-x86_64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Julia/1.9.3-linux-x86_64 - x - - - -"},{"location":"available_software/detail/JupyterLab/","title":"JupyterLab","text":"<p>JupyterLab is the next-generation user interface for Project Jupyter offering all the familiar building blocks of the classic Jupyter Notebook (notebook, terminal, text editor, file browser, rich outputs, etc.) in a flexible and powerful user interface. JupyterLab will eventually replace the classic Jupyter Notebook.</p> <p>https://jupyter.org/</p>"},{"location":"available_software/detail/JupyterLab/#available-modules","title":"Available modules","text":"<p>The overview below shows which JupyterLab installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using JupyterLab, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load JupyterLab/4.0.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 JupyterLab/4.0.5-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/JupyterLab/#jupyterlab405-gcccore-1230","title":"JupyterLab/4.0.5-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>async-lru-2.0.4, json5-0.9.14, jupyter-lsp-2.2.0, jupyterlab-4.0.5, jupyterlab_server-2.24.0</p>"},{"location":"available_software/detail/JupyterNotebook/","title":"JupyterNotebook","text":"<p>The Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience.</p> <p>https://jupyter.org/</p>"},{"location":"available_software/detail/JupyterNotebook/#available-modules","title":"Available modules","text":"<p>The overview below shows which JupyterNotebook installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using JupyterNotebook, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load JupyterNotebook/7.0.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 JupyterNotebook/7.0.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Kalign/","title":"Kalign","text":"<p>Kalign is a fast multiple sequence alignment program for biological sequences.</p> <p>https://github.com/TimoLassmann/kalign</p>"},{"location":"available_software/detail/Kalign/#available-modules","title":"Available modules","text":"<p>The overview below shows which Kalign installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Kalign, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Kalign/3.3.5-GCCcore-11.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Kalign/3.3.5-GCCcore-11.3.0 - - - - x x"},{"location":"available_software/detail/Kent_tools/","title":"Kent_tools","text":"<p>Kent utilities: collection of tools used by the UCSC genome browser.</p> <p>https://genome.cse.ucsc.edu/</p>"},{"location":"available_software/detail/Kent_tools/#available-modules","title":"Available modules","text":"<p>The overview below shows which Kent_tools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Kent_tools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Kent_tools/465-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Kent_tools/465-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Kraken2/","title":"Kraken2","text":"<p>Kraken is a system for assigning taxonomic labels to short DNA sequences, usually obtained through metagenomic studies. Previous attempts by other bioinformatics software to accomplish this task have often used sequence alignment or machine learning techniques that were quite slow, leading to the development of less sensitive but much faster abundance estimation programs. Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm.</p> <p>https://github.com/DerrickWood/kraken2/wiki</p>"},{"location":"available_software/detail/Kraken2/#available-modules","title":"Available modules","text":"<p>The overview below shows which Kraken2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Kraken2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Kraken2/2.1.2-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Kraken2/2.1.2-gompi-2023a - x - - - -"},{"location":"available_software/detail/LAME/","title":"LAME","text":"<p>LAME is a high quality MPEG Audio Layer III (MP3) encoder licensed under the LGPL.</p> <p>http://lame.sourceforge.net/</p>"},{"location":"available_software/detail/LAME/#available-modules","title":"Available modules","text":"<p>The overview below shows which LAME installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LAME, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LAME/3.100-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LAME/3.100-GCCcore-12.3.0 - x - - - - LAME/3.100-GCCcore-12.2.0 - x - - - - LAME/3.100-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/LAMMPS/","title":"LAMMPS","text":"<p>LAMMPS is a classical molecular dynamics code, and an acronymfor Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS haspotentials for solid-state materials (metals, semiconductors) and soft matter(biomolecules, polymers) and coarse-grained or mesoscopic systems. It can beused to model atoms or, more generically, as a parallel particle simulator atthe atomic, meso, or continuum scale. LAMMPS runs on single processors or inparallel using message-passing techniques and a spatial-decomposition of thesimulation domain. The code is designed to be easy to modify or extend with newfunctionality.</p> <p>https://www.lammps.org</p>"},{"location":"available_software/detail/LAMMPS/#available-modules","title":"Available modules","text":"<p>The overview below shows which LAMMPS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LAMMPS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LAMMPS/2Aug2023_update2-foss-2023a-kokkos-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LAMMPS/2Aug2023_update2-foss-2023a-kokkos-CUDA-12.1.1 - x - - - - LAMMPS/2Aug2023_update2-foss-2023a-kokkos - x - - - -"},{"location":"available_software/detail/LERC/","title":"LERC","text":"<p>LERC is an open-source image or raster format which supports rapid encoding and decodingfor any pixel type (not just RGB or Byte). Users set the maximum compression error per pixel while encoding,so the precision of the original input image is preserved (within user defined error bounds).</p> <p>https://github.com/Esri/lerc</p>"},{"location":"available_software/detail/LERC/#available-modules","title":"Available modules","text":"<p>The overview below shows which LERC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LERC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LERC/4.0.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LERC/4.0.0-GCCcore-12.3.0 - x - - - - LERC/4.0.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/LHAPDF/","title":"LHAPDF","text":"<p>Les Houches Parton Density FunctionLHAPDF is the standard tool for evaluating parton distribution functions (PDFs) in high-energy physics.</p> <p>http://lhapdf.hepforge.org/</p>"},{"location":"available_software/detail/LHAPDF/#available-modules","title":"Available modules","text":"<p>The overview below shows which LHAPDF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LHAPDF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LHAPDF/6.5.4-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LHAPDF/6.5.4-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/LLVM/","title":"LLVM","text":"<p>The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs (as well as some less common ones!) These libraries are built around a well specified code representation known as the LLVM intermediate representation (\"LLVM IR\"). The LLVM Core libraries are well documented, and it is particularly easy to invent your own language (or port an existing compiler) to use LLVM as an optimizer and code generator.</p> <p>https://llvm.org/</p>"},{"location":"available_software/detail/LLVM/#available-modules","title":"Available modules","text":"<p>The overview below shows which LLVM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LLVM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LLVM/16.0.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LLVM/16.0.6-GCCcore-13.2.0 - x - - - - LLVM/16.0.6-GCCcore-12.3.0 - x x x x x LLVM/15.0.5-GCCcore-12.2.0 - x - - - - LLVM/14.0.3-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/LMDB/","title":"LMDB","text":"<p>LMDB is a fast, memory-efficient database. With memory-mapped files, it has the read performance of a pure in-memory database while retaining the persistence of standard disk-based databases.</p> <p>https://symas.com/lmdb</p>"},{"location":"available_software/detail/LMDB/#available-modules","title":"Available modules","text":"<p>The overview below shows which LMDB installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LMDB, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LMDB/0.9.31-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LMDB/0.9.31-GCCcore-12.3.0 - x - - - - LMDB/0.9.29-GCCcore-12.2.0 - x - - - - LMDB/0.9.29-GCCcore-11.3.0 - x - - - x"},{"location":"available_software/detail/LM_Studio/","title":"LM_Studio","text":"<p>LM Studio - AI powered coding assistant.</p> <p>https://lmstudio.ai</p>"},{"location":"available_software/detail/LM_Studio/#available-modules","title":"Available modules","text":"<p>The overview below shows which LM_Studio installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LM_Studio, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LM_Studio/0.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LM_Studio/0.3.2 - - x - - - LM_Studio/0.2.27 - x - - - -"},{"location":"available_software/detail/LSD2/","title":"LSD2","text":"<p>Least-squares methods to estimate rates and dates from phylogenies</p> <p>https://github.com/tothuhien/lsd2</p>"},{"location":"available_software/detail/LSD2/#available-modules","title":"Available modules","text":"<p>The overview below shows which LSD2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LSD2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LSD2/2.4.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LSD2/2.4.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/LTR_retriever/","title":"LTR_retriever","text":"<p>LTR_retriever is a highly accurate and sensitive program for identification of LTR retrotransposons; The LTR Assembly Index (LAI) is also included in this package.</p> <p>https://github.com/oushujun/LTR_retriever</p>"},{"location":"available_software/detail/LTR_retriever/#available-modules","title":"Available modules","text":"<p>The overview below shows which LTR_retriever installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LTR_retriever, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LTR_retriever/2.9.8-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LTR_retriever/2.9.8-foss-2023a - x - - - - LTR_retriever/2.9.5-foss-2023a - x - - - -"},{"location":"available_software/detail/LZO/","title":"LZO","text":"<p>Portable lossless data compression library</p> <p>https://www.oberhumer.com/opensource/lzo/</p>"},{"location":"available_software/detail/LZO/#available-modules","title":"Available modules","text":"<p>The overview below shows which LZO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LZO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LZO/2.10-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LZO/2.10-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/LibTIFF/","title":"LibTIFF","text":"<p>tiff: Library and tools for reading and writing TIFF data files</p> <p>https://libtiff.gitlab.io/libtiff/</p>"},{"location":"available_software/detail/LibTIFF/#available-modules","title":"Available modules","text":"<p>The overview below shows which LibTIFF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LibTIFF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LibTIFF/4.6.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LibTIFF/4.6.0-GCCcore-13.2.0 - x - - - - LibTIFF/4.5.0-GCCcore-12.3.0 - x x x x x LibTIFF/4.4.0-GCCcore-12.2.0 - x - - - - LibTIFF/4.3.0-GCCcore-11.3.0 - x - - - - LibTIFF/4.1.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Libint/","title":"Libint","text":"<p>Libint library is used to evaluate the traditional (electron repulsion) and certain novel two-body matrix elements (integrals) over Cartesian Gaussian functions used in modern atomic and molecular theory.</p> <p>https://github.com/evaleev/libint</p>"},{"location":"available_software/detail/Libint/#available-modules","title":"Available modules","text":"<p>The overview below shows which Libint installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Libint, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Libint/2.7.2-GCC-12.3.0-lmax-6-cp2k\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Libint/2.7.2-GCC-12.3.0-lmax-6-cp2k - x - - - -"},{"location":"available_software/detail/LittleCMS/","title":"LittleCMS","text":"<p>Little CMS intends to be an OPEN SOURCE small-footprint color management engine, with special focus on accuracy and performance.</p> <p>https://www.littlecms.com/</p>"},{"location":"available_software/detail/LittleCMS/#available-modules","title":"Available modules","text":"<p>The overview below shows which LittleCMS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LittleCMS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LittleCMS/2.15-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LittleCMS/2.15-GCCcore-13.2.0 - x - - - - LittleCMS/2.15-GCCcore-12.3.0 - x - - - - LittleCMS/2.14-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/LoopTools/","title":"LoopTools","text":"<p>LoopTools is a package for evaluation of scalar and tensor one-loop integrals.It is based on the FF package by G.J. van Oldenborgh.</p> <p>https://feynarts.de/looptools/</p>"},{"location":"available_software/detail/LoopTools/#available-modules","title":"Available modules","text":"<p>The overview below shows which LoopTools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using LoopTools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load LoopTools/2.15-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 LoopTools/2.15-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Lua/","title":"Lua","text":"<p>Lua is a powerful, fast, lightweight, embeddable scripting language. Lua combines simple procedural syntax with powerful data description constructs based on associative arrays and extensible semantics. Lua is dynamically typed, runs by interpreting bytecode for a register-based virtual machine, and has automatic memory management with incremental garbage collection, making it ideal for configuration, scripting, and rapid prototyping.</p> <p>https://www.lua.org/</p>"},{"location":"available_software/detail/Lua/#available-modules","title":"Available modules","text":"<p>The overview below shows which Lua installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Lua, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Lua/5.4.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Lua/5.4.6-GCCcore-13.2.0 - x - - - - Lua/5.4.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/M4/","title":"M4","text":"<p>GNU M4 is an implementation of the traditional Unix macro processor. It is mostly SVR4 compatible although it has some extensions (for example, handling more than 9 positional parameters to macros). GNU M4 also has built-in functions for including files, running shell commands, doing arithmetic, etc.</p> <p>https://www.gnu.org/software/m4/m4.html</p>"},{"location":"available_software/detail/M4/#available-modules","title":"Available modules","text":"<p>The overview below shows which M4 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using M4, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load M4/1.4.19-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 M4/1.4.19-GCCcore-13.2.0 - x - - - - M4/1.4.19-GCCcore-12.3.0 - x x x x x M4/1.4.19-GCCcore-12.2.0 - x - - - - M4/1.4.19-GCCcore-11.3.0 - x - - - - M4/1.4.19-GCCcore-11.2.0 - x - - - - M4/1.4.19 - x x x x x M4/1.4.18-GCCcore-10.2.0 - x - - - - M4/1.4.18 - x - - - -"},{"location":"available_software/detail/MACS2/","title":"MACS2","text":"<p>Model Based Analysis for ChIP-Seq data</p> <p>https://github.com/taoliu/MACS</p>"},{"location":"available_software/detail/MACS2/#available-modules","title":"Available modules","text":"<p>The overview below shows which MACS2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MACS2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MACS2/2.2.9.1-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MACS2/2.2.9.1-foss-2022b - x - - - -"},{"location":"available_software/detail/MACS3/","title":"MACS3","text":"<p>Model Based Analysis for ChIP-Seq data</p> <p>https://macs3-project.github.io/MACS/</p>"},{"location":"available_software/detail/MACS3/#available-modules","title":"Available modules","text":"<p>The overview below shows which MACS3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MACS3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MACS3/3.0.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MACS3/3.0.1-gfbf-2023a - x - - - -"},{"location":"available_software/detail/MACS3/#macs3301-gfbf-2023a","title":"MACS3/3.0.1-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>cykhash-2.0.1, MACS3-3.0.1</p>"},{"location":"available_software/detail/MAFFT/","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program for unix-like operating systems.It offers a range of multiple alignment methods, L-INS-i (accurate; for alignmentof &lt;\u223c200 sequences), FFT-NS-2 (fast; for alignment of &lt;\u223c30,000 sequences), etc.</p> <p>https://mafft.cbrc.jp/alignment/software/source.html</p>"},{"location":"available_software/detail/MAFFT/#available-modules","title":"Available modules","text":"<p>The overview below shows which MAFFT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MAFFT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MAFFT/7.520-GCC-12.3.0-with-extensions\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MAFFT/7.520-GCC-12.3.0-with-extensions - x - - - -"},{"location":"available_software/detail/MAKER/","title":"MAKER","text":"<p>MAKER is a portable and easily configurable genome annotation pipeline. Itspurpose is to allow smaller eukaryotic and prokaryotic genome projects toindependently annotate their genomes and to create genome databases.</p> <p>https://yandell-lab.org/software/maker.html</p>"},{"location":"available_software/detail/MAKER/#available-modules","title":"Available modules","text":"<p>The overview below shows which MAKER installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MAKER, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MAKER/3.01.04-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MAKER/3.01.04-foss-2023a - x - - - -"},{"location":"available_software/detail/MAKER/#maker30104-foss-2023a","title":"MAKER/3.01.04-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>Acme::Damn-0.08, Bit::Vector-7.4, forks-0.36, Inline::C-0.82, IO::All-0.87, IO::Prompt-0.997004, MAKER-3.01.04, Perl::Unsafe::Signals-0.03, Sys::SigAction-0.23</p>"},{"location":"available_software/detail/MATIO/","title":"MATIO","text":"<p>matio is an C library for reading and writing Matlab MAT files.</p> <p>https://sourceforge.net/projects/matio/</p>"},{"location":"available_software/detail/MATIO/#available-modules","title":"Available modules","text":"<p>The overview below shows which MATIO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MATIO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MATIO/1.5.26-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MATIO/1.5.26-GCCcore-12.3.0 - - x - - -"},{"location":"available_software/detail/MATLAB/","title":"MATLAB","text":"<p>MATLAB is a high-level language and interactive environment that enables you to perform computationally intensive tasks faster than with traditional programming languages such as C, C++, and Fortran.</p> <p>https://www.mathworks.com/products/matlab</p>"},{"location":"available_software/detail/MATLAB/#available-modules","title":"Available modules","text":"<p>The overview below shows which MATLAB installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MATLAB, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MATLAB/2024a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MATLAB/2024a - x - - - - MATLAB/2023b - x x x x x"},{"location":"available_software/detail/MCR/","title":"MCR","text":"<p>The MATLAB Runtime is a standalone set of shared libraries that enables the execution of compiled MATLAB applications or components on computers that do not have MATLAB installed.</p> <p>https://www.mathworks.com/products/compiler/mcr/</p>"},{"location":"available_software/detail/MCR/#available-modules","title":"Available modules","text":"<p>The overview below shows which MCR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MCR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MCR/R2019b.8\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MCR/R2019b.8 - x - - - -"},{"location":"available_software/detail/MDI/","title":"MDI","text":"<p>The MolSSI Driver Interface (MDI) project provides a standardized API for fast, on-the-fly communication between computational chemistry codes. This greatly simplifies the process of implementing methods that require the cooperation of multiple software packages and enables developers to write a single implementation that works across many different codes. The API is sufficiently general to support a wide variety of techniques, including QM/MM, ab initio MD, machine learning, advanced sampling, and path integral MD, while also being straightforwardly extensible. Communication between codes is handled by the MDI Library, which enables tight coupling between codes using either the MPI or TCP/IP methods.</p> <p>https://github.com/MolSSI-MDI/MDI_Library</p>"},{"location":"available_software/detail/MDI/#available-modules","title":"Available modules","text":"<p>The overview below shows which MDI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MDI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MDI/1.4.26-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MDI/1.4.26-gompi-2023a - x - - - -"},{"location":"available_software/detail/MEGAHIT/","title":"MEGAHIT","text":"<p>An ultra-fast single-node solution for large and complexmetagenomics assembly via succinct de Bruijn graph</p> <p>https://github.com/voutcn/megahit</p>"},{"location":"available_software/detail/MEGAHIT/#available-modules","title":"Available modules","text":"<p>The overview below shows which MEGAHIT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MEGAHIT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MEGAHIT/1.2.9-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MEGAHIT/1.2.9-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/MEME/","title":"MEME","text":"<p>The MEME Suite allows you to: * discover motifs using MEME, DREME (DNA only) or GLAM2 on groups of related DNA or protein sequences, * search sequence databases with motifs using MAST, FIMO, MCAST or GLAM2SCAN, * compare a motif to all motifs in a database of motifs, * associate motifs with Gene Ontology terms via their putative target genes, and * analyse motif enrichment using SpaMo or CentriMo.</p> <p>https://meme-suite.org/meme/index.html</p>"},{"location":"available_software/detail/MEME/#available-modules","title":"Available modules","text":"<p>The overview below shows which MEME installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MEME, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MEME/5.5.4-gompi-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MEME/5.5.4-gompi-2022b - x - - - -"},{"location":"available_software/detail/METIS/","title":"METIS","text":"<p>METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. The algorithms implemented in METIS are based on the multilevel recursive-bisection, multilevel k-way, and multi-constraint partitioning schemes.</p> <p>http://glaros.dtc.umn.edu/gkhome/metis/metis/overview</p>"},{"location":"available_software/detail/METIS/#available-modules","title":"Available modules","text":"<p>The overview below shows which METIS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using METIS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load METIS/5.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 METIS/5.1.0-GCCcore-12.3.0 - x - - - - METIS/5.1.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/MFEM/","title":"MFEM","text":"<p>MFEM is a general, modular library for finite element methods.</p> <p>https://mfem.org/</p>"},{"location":"available_software/detail/MFEM/#available-modules","title":"Available modules","text":"<p>The overview below shows which MFEM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MFEM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MFEM/4.7-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MFEM/4.7-foss-2023a - x - - - -"},{"location":"available_software/detail/MMseqs2/","title":"MMseqs2","text":"<p>MMseqs2: ultra fast and sensitive search and clustering suite</p> <p>https://mmseqs.com</p>"},{"location":"available_software/detail/MMseqs2/#available-modules","title":"Available modules","text":"<p>The overview below shows which MMseqs2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MMseqs2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MMseqs2/14-7e284-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MMseqs2/14-7e284-gompi-2023a - x - - - -"},{"location":"available_software/detail/MODFLOW/","title":"MODFLOW","text":"<p>MODFLOW is the USGS's modular hydrologic model. MODFLOW is considered an international standard for simulating and predicting groundwater conditions and groundwater/surface-water interactions.</p> <p>https://www.usgs.gov/mission-areas/water-resources/science/modflow-and-related-programs</p>"},{"location":"available_software/detail/MODFLOW/#available-modules","title":"Available modules","text":"<p>The overview below shows which MODFLOW installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MODFLOW, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MODFLOW/6.4.4-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MODFLOW/6.4.4-foss-2023a - x - - - -"},{"location":"available_software/detail/MPC/","title":"MPC","text":"<p>Gnu Mpc is a C library for the arithmetic of complex numbers with arbitrarily high precision and correct rounding of the result. It extends the principles of the IEEE-754 standard for fixed precision real floating point numbers to complex numbers, providing well-defined semantics for every operation. At the same time, speed of operation at high precision is a major design goal.</p> <p>http://www.multiprecision.org/</p>"},{"location":"available_software/detail/MPC/#available-modules","title":"Available modules","text":"<p>The overview below shows which MPC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MPC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MPC/1.3.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MPC/1.3.1-GCCcore-12.3.0 - - - x - x"},{"location":"available_software/detail/MPFR/","title":"MPFR","text":"<p>The MPFR library is a C library for multiple-precision floating-point computations with correct rounding.</p> <p>https://www.mpfr.org</p>"},{"location":"available_software/detail/MPFR/#available-modules","title":"Available modules","text":"<p>The overview below shows which MPFR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MPFR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MPFR/4.2.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MPFR/4.2.0-GCCcore-12.3.0 - x - - - - MPFR/4.2.0-GCCcore-12.2.0 - x - - - - MPFR/4.1.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/MRtrix/","title":"MRtrix","text":"<p>MRtrix provides a set of tools to perform diffusion-weighted MR white-matter tractography in a manner robust to crossing fibres, using constrained spherical deconvolution (CSD) and probabilistic streamlines.</p> <p>http://www.brain.org.au/software/index.html#mrtrix</p>"},{"location":"available_software/detail/MRtrix/#available-modules","title":"Available modules","text":"<p>The overview below shows which MRtrix installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MRtrix, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MRtrix/3.0.4-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MRtrix/3.0.4-foss-2022b - x - - - -"},{"location":"available_software/detail/MUMPS/","title":"MUMPS","text":"<p>A parallel sparse direct solver</p> <p>https://graal.ens-lyon.fr/MUMPS/</p>"},{"location":"available_software/detail/MUMPS/#available-modules","title":"Available modules","text":"<p>The overview below shows which MUMPS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MUMPS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MUMPS/5.6.1-foss-2023a-metis\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MUMPS/5.6.1-foss-2023a-metis - x - - - -"},{"location":"available_software/detail/MUMmer/","title":"MUMmer","text":"<p>MUMmer is a system for rapidly aligning entire genomes, whether in complete or draft form. AMOS makes use of it.</p> <p>https://mummer.sourceforge.net/</p>"},{"location":"available_software/detail/MUMmer/#available-modules","title":"Available modules","text":"<p>The overview below shows which MUMmer installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MUMmer, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MUMmer/4.0.0rc1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MUMmer/4.0.0rc1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/MUSCLE/","title":"MUSCLE","text":"<p>MUSCLE is one of the best-performing multiple alignment programs according to published benchmark tests, with accuracy and speed that are consistently better than CLUSTALW. MUSCLE can align hundreds of sequences in seconds. Most users learn everything they need to know about MUSCLE in a few minutes-only a handful of command-line options are needed to perform common alignment tasks.</p> <p>https://drive5.com/muscle/</p>"},{"location":"available_software/detail/MUSCLE/#available-modules","title":"Available modules","text":"<p>The overview below shows which MUSCLE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MUSCLE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MUSCLE/5.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MUSCLE/5.1.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Mako/","title":"Mako","text":"<p>A super-fast templating language that borrows the best ideas from the existing templating languages</p> <p>https://www.makotemplates.org</p>"},{"location":"available_software/detail/Mako/#available-modules","title":"Available modules","text":"<p>The overview below shows which Mako installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Mako, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Mako/1.2.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Mako/1.2.4-GCCcore-13.2.0 - x - - - - Mako/1.2.4-GCCcore-12.3.0 - x x x x x Mako/1.2.4-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Mako/#mako124-gcccore-1320","title":"Mako/1.2.4-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Mako-1.2.4, MarkupSafe-2.1.3</p>"},{"location":"available_software/detail/Mako/#mako124-gcccore-1230","title":"Mako/1.2.4-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Mako-1.2.4, MarkupSafe-2.1.3</p>"},{"location":"available_software/detail/MariaDB/","title":"MariaDB","text":"<p>MariaDB is an enhanced, drop-in replacement for MySQL.Included engines: myISAM, Aria, InnoDB, RocksDB, TokuDB, OQGraph, Mroonga.</p> <p>https://mariadb.org/</p>"},{"location":"available_software/detail/MariaDB/#available-modules","title":"Available modules","text":"<p>The overview below shows which MariaDB installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MariaDB, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MariaDB/10.11.8-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MariaDB/10.11.8-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Mathematica/","title":"Mathematica","text":"<p>Mathematica is a computational software program used in many scientific, engineering, mathematicaland computing fields.</p> <p>https://www.wolfram.com/mathematica</p>"},{"location":"available_software/detail/Mathematica/#available-modules","title":"Available modules","text":"<p>The overview below shows which Mathematica installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Mathematica, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Mathematica/13.3.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Mathematica/13.3.1 - x - - - -"},{"location":"available_software/detail/Maven/","title":"Maven","text":"<p>Binary maven install, Apache Maven is a software project management and comprehension tool. Based onthe concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from acentral piece of information.</p> <p>https://maven.apache.org/index.html</p>"},{"location":"available_software/detail/Maven/#available-modules","title":"Available modules","text":"<p>The overview below shows which Maven installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Maven, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Maven/3.6.3\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Maven/3.6.3 - x - - - -"},{"location":"available_software/detail/Mesa/","title":"Mesa","text":"<p>Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics.</p> <p>https://www.mesa3d.org/</p>"},{"location":"available_software/detail/Mesa/#available-modules","title":"Available modules","text":"<p>The overview below shows which Mesa installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Mesa, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Mesa/23.1.9-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Mesa/23.1.9-GCCcore-13.2.0 - x - - - - Mesa/23.1.4-GCCcore-12.3.0 - x x x x x Mesa/22.2.4-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Meson/","title":"Meson","text":"<p>Meson is a cross-platform build system designed to be both as fast and as user friendly as possible.</p> <p>https://mesonbuild.com</p>"},{"location":"available_software/detail/Meson/#available-modules","title":"Available modules","text":"<p>The overview below shows which Meson installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Meson, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Meson/1.2.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Meson/1.2.3-GCCcore-13.2.0 - x - - - - Meson/1.1.1-GCCcore-12.3.0 - x x x x x Meson/0.64.0-GCCcore-12.2.0 - x - - - - Meson/0.62.1-GCCcore-11.3.0 - x - - - - Meson/0.55.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/MetaEuk/","title":"MetaEuk","text":"<p>MetaEuk is a modular toolkit designed for large-scale gene discovery and annotation in eukaryotic metagenomic contigs.</p> <p>https://metaeuk.soedinglab.org</p>"},{"location":"available_software/detail/MetaEuk/#available-modules","title":"Available modules","text":"<p>The overview below shows which MetaEuk installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MetaEuk, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MetaEuk/6-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MetaEuk/6-GCC-12.3.0 - x - - - - MetaEuk/6-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/Miniforge3/","title":"Miniforge3","text":"<p>Miniforge is a free minimal installer for conda and Mamba specific to conda-forge.</p> <p>https://github.com/conda-forge/miniforge</p>"},{"location":"available_software/detail/Miniforge3/#available-modules","title":"Available modules","text":"<p>The overview below shows which Miniforge3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Miniforge3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Miniforge3/24.3.0-0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Miniforge3/24.3.0-0 - x - - - -"},{"location":"available_software/detail/Molden/","title":"Molden","text":"<p>Molden is a package for displaying Molecular Density from the Ab Initio packages GAMESS-UK, GAMESS-US and GAUSSIAN and the Semi-Empirical packages Mopac/Ampac</p> <p>https://www.theochem.ru.nl/molden/</p>"},{"location":"available_software/detail/Molden/#available-modules","title":"Available modules","text":"<p>The overview below shows which Molden installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Molden, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Molden/7.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Molden/7.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Molpro/","title":"Molpro","text":"<p>Molpro is a complete system of ab initio programs for molecular electronic structure calculations.</p> <p>https://www.molpro.net</p>"},{"location":"available_software/detail/Molpro/#available-modules","title":"Available modules","text":"<p>The overview below shows which Molpro installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Molpro, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Molpro/mpp-2023.2.0.linux_x86_64_sockets\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Molpro/mpp-2023.2.0.linux_x86_64_sockets - x - - - - Molpro/mpp-2023.2.0.linux_x86_64_mpipr - x - - - -"},{"location":"available_software/detail/Mothur/","title":"Mothur","text":"<p>Mothur is a single piece of open-source, expandable software to fill the bioinformatics needs of the microbial ecology community.</p> <p>https://www.mothur.org/</p>"},{"location":"available_software/detail/Mothur/#available-modules","title":"Available modules","text":"<p>The overview below shows which Mothur installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Mothur, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Mothur/1.48.0-foss-2023a-Python-3.11.3\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Mothur/1.48.0-foss-2023a-Python-3.11.3 - x - - - -"},{"location":"available_software/detail/MultiQC/","title":"MultiQC","text":"<p>Aggregate results from bioinformatics analyses across many samples into a single report. MultiQC searches a given directory for analysis logs and compiles an HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.</p> <p>https://multiqc.info</p>"},{"location":"available_software/detail/MultiQC/#available-modules","title":"Available modules","text":"<p>The overview below shows which MultiQC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using MultiQC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load MultiQC/1.14-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 MultiQC/1.14-foss-2022b - x - - - -"},{"location":"available_software/detail/MultiQC/#multiqc114-foss-2022b","title":"MultiQC/1.14-foss-2022b","text":"<p>This is a list of extensions included in the module:</p> <p>coloredlogs-15.0.1, colormath-3.0.0, commonmark-0.9.1, humanfriendly-10.0, lzstring-1.0.4, Markdown-3.4.1, markdown-it-py-2.1.0, mdurl-0.1.2, multiqc-1.14, Pygments-2.14.0, rich-13.3.1, rich-click-1.6.1, spectra-0.0.11</p>"},{"location":"available_software/detail/NAMD/","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems.</p> <p>https://www.ks.uiuc.edu/Research/namd/</p>"},{"location":"available_software/detail/NAMD/#available-modules","title":"Available modules","text":"<p>The overview below shows which NAMD installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NAMD, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NAMD/2.14-foss-2023a-mpi\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NAMD/2.14-foss-2023a-mpi - x - - - -"},{"location":"available_software/detail/NASM/","title":"NASM","text":"<p>NASM: General-purpose x86 assembler</p> <p>https://www.nasm.us/</p>"},{"location":"available_software/detail/NASM/#available-modules","title":"Available modules","text":"<p>The overview below shows which NASM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NASM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NASM/2.16.01-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NASM/2.16.01-GCCcore-13.2.0 - x - - - - NASM/2.16.01-GCCcore-12.3.0 - x x x x x NASM/2.15.05-GCCcore-12.2.0 - x - - - - NASM/2.15.05-GCCcore-11.3.0 - x - - - x NASM/2.15.05-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/NBO/","title":"NBO","text":"<p>The Natural Bond Orbital (NBO) program is a discovery toolfor chemical insights from complex wavefunctions.</p> <p>http://nbo.chem.wisc.edu/</p>"},{"location":"available_software/detail/NBO/#available-modules","title":"Available modules","text":"<p>The overview below shows which NBO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NBO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NBO/7.0-gfbf-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NBO/7.0-gfbf-2022b - x - - - - NBO/7.0 - x - - - - NBO/6.0 - x - - - -"},{"location":"available_software/detail/NCCL/","title":"NCCL","text":"<p>The NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collectivecommunication primitives that are performance optimized for NVIDIA GPUs.</p> <p>https://developer.nvidia.com/nccl</p>"},{"location":"available_software/detail/NCCL/#available-modules","title":"Available modules","text":"<p>The overview below shows which NCCL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NCCL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NCCL/2.18.3-GCCcore-12.3.0-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NCCL/2.18.3-GCCcore-12.3.0-CUDA-12.1.1 - x - x - x NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/NCL/","title":"NCL","text":"<p>NCL is an interpreted language designed specifically for scientific data analysis and visualization.</p> <p>https://www.ncl.ucar.edu</p>"},{"location":"available_software/detail/NCL/#available-modules","title":"Available modules","text":"<p>The overview below shows which NCL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NCL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NCL/6.6.2-foss-2020b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NCL/6.6.2-foss-2020b - x - - - -"},{"location":"available_software/detail/NLopt/","title":"NLopt","text":"<p>NLopt is a free/open-source library for nonlinear optimization, providing a common interface for a number of different free optimization routines available online as well as original implementations of various other algorithms.</p> <p>http://ab-initio.mit.edu/wiki/index.php/NLopt</p>"},{"location":"available_software/detail/NLopt/#available-modules","title":"Available modules","text":"<p>The overview below shows which NLopt installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NLopt, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NLopt/2.7.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NLopt/2.7.1-GCCcore-12.3.0 - x - - - - NLopt/2.7.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/NSPR/","title":"NSPR","text":"<p>Netscape Portable Runtime (NSPR) provides a platform-neutral API for system level and libc-like functions.</p> <p>https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSPR</p>"},{"location":"available_software/detail/NSPR/#available-modules","title":"Available modules","text":"<p>The overview below shows which NSPR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NSPR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NSPR/4.35-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NSPR/4.35-GCCcore-13.2.0 - x - - - - NSPR/4.35-GCCcore-12.3.0 - x - - - - NSPR/4.35-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/NSS/","title":"NSS","text":"<p>Network Security Services (NSS) is a set of libraries designed to support cross-platform development of security-enabled client and server applications.</p> <p>https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS</p>"},{"location":"available_software/detail/NSS/#available-modules","title":"Available modules","text":"<p>The overview below shows which NSS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NSS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NSS/3.94-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NSS/3.94-GCCcore-13.2.0 - x - - - - NSS/3.89.1-GCCcore-12.3.0 - x - - - - NSS/3.85-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/NTPoly/","title":"NTPoly","text":"<p>is a massively parallel library for computing the functions of sparse, symmetric matrices based on polynomial expansions. For sufficiently sparse matrices, most of the matrix functions in NTPoly can be computed in linear time.</p> <p>https://github.com/william-dawson/NTPoly</p>"},{"location":"available_software/detail/NTPoly/#available-modules","title":"Available modules","text":"<p>The overview below shows which NTPoly installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NTPoly, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NTPoly/2.7.1-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NTPoly/2.7.1-foss-2023b - x - - - - NTPoly/2.7.1-foss-2023a - x - - - -"},{"location":"available_software/detail/NVHPC/","title":"NVHPC","text":"<p>C, C++ and Fortran compilers included with the NVIDIA HPC SDK (previously: PGI)</p> <p>https://developer.nvidia.com/hpc-sdk/</p>"},{"location":"available_software/detail/NVHPC/#available-modules","title":"Available modules","text":"<p>The overview below shows which NVHPC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NVHPC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NVHPC/23.7-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NVHPC/23.7-CUDA-12.1.1 - - - x - x NVHPC/22.11-CUDA-11.7.0 - x - - - - NVHPC/22.7-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/NWChem/","title":"NWChem","text":"<p>NWChem aims to provide its users with computational chemistry tools that are scalable both in their ability to treat large scientific computational chemistry problems efficiently, and in their use of available parallel computing resources from high-performance parallel supercomputers to conventional workstation clusters. NWChem software can handle: biomolecules, nanostructures, and solid-state; from quantum to classical, and all combinations; Gaussian basis functions or plane-waves; scaling from one to thousands of processors; properties and relativity.</p> <p>https://nwchemgit.github.io/</p>"},{"location":"available_software/detail/NWChem/#available-modules","title":"Available modules","text":"<p>The overview below shows which NWChem installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using NWChem, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load NWChem/7.2.2-intel-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 NWChem/7.2.2-intel-2023a - x - - - -"},{"location":"available_software/detail/Nextflow/","title":"Nextflow","text":"<p>Nextflow is a reactive workflow framework and a programming DSL that eases writing computational pipelines with complex data</p> <p>https://www.nextflow.io/</p>"},{"location":"available_software/detail/Nextflow/#available-modules","title":"Available modules","text":"<p>The overview below shows which Nextflow installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Nextflow, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Nextflow/23.10.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Nextflow/23.10.0 - x - - - - Nextflow/22.10.0 - x - - - -"},{"location":"available_software/detail/Nim/","title":"Nim","text":"<p>Nim is a systems and applications programming language.</p> <p>https://nim-lang.org/</p>"},{"location":"available_software/detail/Nim/#available-modules","title":"Available modules","text":"<p>The overview below shows which Nim installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Nim, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Nim/2.0.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Nim/2.0.4-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Ninja/","title":"Ninja","text":"<p>Ninja is a small build system with a focus on speed.</p> <p>https://ninja-build.org/</p>"},{"location":"available_software/detail/Ninja/#available-modules","title":"Available modules","text":"<p>The overview below shows which Ninja installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Ninja, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Ninja/1.11.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Ninja/1.11.1-GCCcore-13.2.0 - x - - - - Ninja/1.11.1-GCCcore-12.3.0 - x x x x x Ninja/1.11.1-GCCcore-12.2.0 - x - - - - Ninja/1.10.2-GCCcore-11.3.0 - x - - x x Ninja/1.10.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/ORCA/","title":"ORCA","text":"<p>ORCA is a flexible, efficient and easy-to-use general purpose tool for quantumchemistry with specific emphasis on spectroscopic properties of open-shellmolecules. It features a wide variety of standard quantum chemical methodsranging from semiempirical methods to DFT to single- and multireferencecorrelated ab initio methods. It can also treat environmental and relativisticeffects.</p> <p>https://orcaforum.kofo.mpg.de</p>"},{"location":"available_software/detail/ORCA/#available-modules","title":"Available modules","text":"<p>The overview below shows which ORCA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ORCA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ORCA/6.0.0-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ORCA/6.0.0-gompi-2023a - x - - - - ORCA/5.0.4-gompi-2023a - x - - - -"},{"location":"available_software/detail/OSU-Micro-Benchmarks/","title":"OSU-Micro-Benchmarks","text":"<p>OSU Micro-Benchmarks</p> <p>https://mvapich.cse.ohio-state.edu/benchmarks/</p>"},{"location":"available_software/detail/OSU-Micro-Benchmarks/#available-modules","title":"Available modules","text":"<p>The overview below shows which OSU-Micro-Benchmarks installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OSU-Micro-Benchmarks, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OSU-Micro-Benchmarks/7.2-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OSU-Micro-Benchmarks/7.2-gompi-2023b - x - - - - OSU-Micro-Benchmarks/7.2-gompi-2023a-CUDA-12.1.1 - x - - - - OSU-Micro-Benchmarks/7.1-1-gompi-2023a - x - - - -"},{"location":"available_software/detail/Ollama/","title":"Ollama","text":"<p>Ollama is a model management platform for AI models.</p> <p>https://github.com/ollama/ollama</p>"},{"location":"available_software/detail/Ollama/#available-modules","title":"Available modules","text":"<p>The overview below shows which Ollama installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Ollama, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Ollama/0.3.10\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Ollama/0.3.10 - - x - - -"},{"location":"available_software/detail/OpenBLAS/","title":"OpenBLAS","text":"<p>OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version.</p> <p>http://www.openblas.net/</p>"},{"location":"available_software/detail/OpenBLAS/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenBLAS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenBLAS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenBLAS/0.3.24-NVHPC-23.7-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenBLAS/0.3.24-NVHPC-23.7-CUDA-12.1.1 - - - x - - OpenBLAS/0.3.24-GCC-13.2.0 - x - - - x OpenBLAS/0.3.23-GCC-12.3.0-int8 - x - - - - OpenBLAS/0.3.23-GCC-12.3.0 - x x x x x OpenBLAS/0.3.21-GCC-12.2.0 - x - - - - OpenBLAS/0.3.20-NVHPC-22.7-CUDA-11.7.0 - x - - - - OpenBLAS/0.3.20-GCC-11.3.0 - x - - - - OpenBLAS/0.3.12-GCC-10.2.0 - x - - - -"},{"location":"available_software/detail/OpenEXR/","title":"OpenEXR","text":"<p>OpenEXR is a high dynamic-range (HDR) image file format developed by Industrial Light &amp; Magic for use in computer imaging applications</p> <p>https://www.openexr.com/</p>"},{"location":"available_software/detail/OpenEXR/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenEXR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenEXR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenEXR/3.1.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenEXR/3.1.7-GCCcore-12.3.0 - x - - - - OpenEXR/3.1.5-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/OpenFOAM/","title":"OpenFOAM","text":"<p>OpenFOAM is a free, open source CFD software package. OpenFOAM has an extensive range of features to solve anything from complex fluid flows involving chemical reactions, turbulence and heat transfer, to solid dynamics and electromagnetics.</p> <p>https://www.openfoam.org/</p>"},{"location":"available_software/detail/OpenFOAM/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenFOAM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenFOAM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenFOAM/12-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenFOAM/12-foss-2023a - x - - - - OpenFOAM/11-foss-2023a - x - - - - OpenFOAM/10-foss-2023a - x - - - -"},{"location":"available_software/detail/OpenJPEG/","title":"OpenJPEG","text":"<p>OpenJPEG is an open-source JPEG 2000 codec written in C language. It has been developed in order to promote the use of JPEG 2000, a still-image compression standard from the Joint Photographic Experts Group (JPEG). Since may 2015, it is officially recognized by ISO/IEC and ITU-T as a JPEG 2000 Reference Software.</p> <p>https://www.openjpeg.org/</p>"},{"location":"available_software/detail/OpenJPEG/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenJPEG installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenJPEG, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenJPEG/2.5.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenJPEG/2.5.0-GCCcore-13.2.0 - x - - - - OpenJPEG/2.5.0-GCCcore-12.3.0 - x - - - - OpenJPEG/2.5.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/OpenMM/","title":"OpenMM","text":"<p>OpenMM is a toolkit for molecular simulation.</p> <p>https://openmm.org</p>"},{"location":"available_software/detail/OpenMM/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenMM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenMM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenMM/8.0.0-foss-2022a-CUDA-11.7.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenMM/8.0.0-foss-2022a-CUDA-11.7.0 - - - - - x"},{"location":"available_software/detail/OpenMPI/","title":"OpenMPI","text":"<p>The Open MPI Project is an open source MPI-3 implementation.</p> <p>https://www.open-mpi.org/</p>"},{"location":"available_software/detail/OpenMPI/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenMPI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenMPI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenMPI/4.1.6-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenMPI/4.1.6-GCC-13.2.0 - x - - - - OpenMPI/4.1.5-intel-compilers-2023.1.0 - x - - - - OpenMPI/4.1.5-NVHPC-23.7-CUDA-12.1.1 - - - x - - OpenMPI/4.1.5-GCC-12.3.0 - x x x x x OpenMPI/4.1.4-NVHPC-22.7-CUDA-11.7.0 - x - - - - OpenMPI/4.1.4-GCC-12.2.0 - x - - - - OpenMPI/4.1.4-GCC-11.3.0 - x - - - - OpenMPI/4.0.5-GCC-10.2.0 - x - - - -"},{"location":"available_software/detail/OpenMolcas/","title":"OpenMolcas","text":"<p>OpenMolcas is a quantum chemistry software package.</p> <p>https://gitlab.com/Molcas/OpenMolcas</p>"},{"location":"available_software/detail/OpenMolcas/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenMolcas installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenMolcas, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenMolcas/23.06-intel-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenMolcas/23.06-intel-2023a - x - - - -"},{"location":"available_software/detail/OpenPGM/","title":"OpenPGM","text":"<p>OpenPGM is an open source implementation of the Pragmatic General Multicast (PGM) specification in RFC 3208 available at www.ietf.org. PGM is a reliable and scalable multicast protocol that enables receivers to detect loss, request retransmission of lost data, or notify an application of unrecoverable loss. PGM is a receiver-reliable protocol, which means the receiver is responsible for ensuring all data is received, absolving the sender of reception responsibility.</p> <p>https://code.google.com/p/openpgm/</p>"},{"location":"available_software/detail/OpenPGM/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenPGM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenPGM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenPGM/5.2.122-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenPGM/5.2.122-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/OpenRefine/","title":"OpenRefine","text":"<p>OpenRefine is a power tool that allows you to load data, understand it, clean it up, reconcile it, and augment it with data coming from the web.</p> <p>https://github.com/OpenRefine/OpenRefine</p>"},{"location":"available_software/detail/OpenRefine/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenRefine installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenRefine, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenRefine/3.8.2-Java-11\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenRefine/3.8.2-Java-11 - x - - - -"},{"location":"available_software/detail/OpenSSL/","title":"OpenSSL","text":"<p>The OpenSSL Project is a collaborative effort to develop a robust, commercial-grade, full-featured, and Open Source toolchain implementing the Secure Sockets Layer (SSL v2/v3) and Transport Layer Security (TLS v1) protocols as well as a full-strength general purpose cryptography library.</p> <p>https://www.openssl.org/</p>"},{"location":"available_software/detail/OpenSSL/#available-modules","title":"Available modules","text":"<p>The overview below shows which OpenSSL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using OpenSSL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load OpenSSL/1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 OpenSSL/1.1 - x x x x x"},{"location":"available_software/detail/Osi/","title":"Osi","text":"<p>Osi (Open Solver Interface) provides an abstract base class to a generic linearprogramming (LP) solver, along with derived classes for specific solvers. Manyapplications may be able to use the Osi to insulate themselves from a specificLP solver. That is, programs written to the OSI standard may be linked to anysolver with an OSI interface and should produce correct results. The OSI hasbeen significantly extended compared to its first incarnation. Currently, theOSI supports linear programming solvers and has rudimentary support for integerprogramming.</p> <p>https://github.com/coin-or/Osi</p>"},{"location":"available_software/detail/Osi/#available-modules","title":"Available modules","text":"<p>The overview below shows which Osi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Osi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Osi/0.108.9-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Osi/0.108.9-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/PAML/","title":"PAML","text":"<p>PAML is a package of programs for phylogenetic analyses of DNA or protein sequences using maximum likelihood.</p> <p>http://abacus.gene.ucl.ac.uk/software/paml.html</p>"},{"location":"available_software/detail/PAML/#available-modules","title":"Available modules","text":"<p>The overview below shows which PAML installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PAML, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PAML/4.10.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PAML/4.10.5-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PASA/","title":"PASA","text":"<p>PASA, acronym for Program to Assemble Spliced Alignments (and pronounced 'pass-uh'), is a eukaryotic genome annotation tool that exploits spliced alignments of expressed transcript sequences to automatically model gene structures, and to maintain gene structure annotation consistent with the most recently available experimental sequence data. PASA also identifies and classifies all splicing variations supported by the transcript alignments.</p> <p>https://github.com/PASApipeline/PASApipeline</p>"},{"location":"available_software/detail/PASA/#available-modules","title":"Available modules","text":"<p>The overview below shows which PASA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PASA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PASA/2.5.3-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PASA/2.5.3-foss-2022b - x - - - -"},{"location":"available_software/detail/PCRE/","title":"PCRE","text":"<p>The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.</p> <p>https://www.pcre.org/</p>"},{"location":"available_software/detail/PCRE/#available-modules","title":"Available modules","text":"<p>The overview below shows which PCRE installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PCRE, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PCRE/8.45-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PCRE/8.45-GCCcore-13.2.0 - x - - - - PCRE/8.45-GCCcore-12.3.0 - x x x x x PCRE/8.45-GCCcore-12.2.0 - x - - - - PCRE/8.45-GCCcore-11.3.0 - - - - x x PCRE/8.44-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/PCRE2/","title":"PCRE2","text":"<p>The PCRE library is a set of functions that implement regular expression pattern matching using the same syntax and semantics as Perl 5.</p> <p>https://www.pcre.org/</p>"},{"location":"available_software/detail/PCRE2/#available-modules","title":"Available modules","text":"<p>The overview below shows which PCRE2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PCRE2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PCRE2/10.42-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PCRE2/10.42-GCCcore-13.2.0 - x - - - - PCRE2/10.42-GCCcore-12.3.0 - x x x x x PCRE2/10.40-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/PETSc/","title":"PETSc","text":"<p>PETSc, pronounced PET-see (the S is silent), is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations.</p> <p>https://www.mcs.anl.gov/petsc</p>"},{"location":"available_software/detail/PETSc/#available-modules","title":"Available modules","text":"<p>The overview below shows which PETSc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PETSc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PETSc/3.20.3-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PETSc/3.20.3-foss-2023a - x - - - -"},{"location":"available_software/detail/PGPLOT/","title":"PGPLOT","text":"<p>The PGPLOT Graphics Subroutine Library is a Fortran- or C-callable,device-independent graphics package for making simple scientific graphs. It is intendedfor making graphical images of publication quality with minimum effort on the part ofthe user. For most applications, the program can be device-independent, and the outputcan be directed to the appropriate device at run time.</p> <p>https://sites.astro.caltech.edu/~tjp/pgplot/</p>"},{"location":"available_software/detail/PGPLOT/#available-modules","title":"Available modules","text":"<p>The overview below shows which PGPLOT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PGPLOT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PGPLOT/5.2.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PGPLOT/5.2.2-GCCcore-13.2.0 - x - - - -"},{"location":"available_software/detail/PICRUSt2/","title":"PICRUSt2","text":"<p>PICRUSt2 (Phylogenetic Investigation of Communities by Reconstruction of Unobserved States) is a software for predicting functional abundances based only on marker gene sequences.</p> <p>https://github.com/picrust/picrust2</p>"},{"location":"available_software/detail/PICRUSt2/#available-modules","title":"Available modules","text":"<p>The overview below shows which PICRUSt2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PICRUSt2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PICRUSt2/2.5.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PICRUSt2/2.5.2-foss-2023a - x - - - -"},{"location":"available_software/detail/PICRUSt2/#picrust2252-foss-2023a","title":"PICRUSt2/2.5.2-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>joblib-1.3.1, PICRUSt2-2.5.2</p>"},{"location":"available_software/detail/PLINK/","title":"PLINK","text":"<p>PLINK is a free, open-source whole genome association analysis toolset,designed to perform a range of basic, large-scale analyses in a computationallyefficient manner.</p> <p>https://www.cog-genomics.org/plink/2.0/</p>"},{"location":"available_software/detail/PLINK/#available-modules","title":"Available modules","text":"<p>The overview below shows which PLINK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PLINK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PLINK/2.00a3.7-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PLINK/2.00a3.7-gfbf-2023a - x - - - - PLINK/1.9b_6.21-x86_64 - x - - - -"},{"location":"available_software/detail/PLUMED/","title":"PLUMED","text":"<p>PLUMED is an open source library for free energy calculations in molecular systems which works together with some of the most popular molecular dynamics engines. Free energy calculations can be performed as a function of many order parameters with a particular  focus on biological problems, using state of the art methods such as metadynamics, umbrella sampling and Jarzynski-equation based steered MD. The software, written in C++, can be easily interfaced with both fortran and C/C++ codes.</p> <p>https://www.plumed.org</p>"},{"location":"available_software/detail/PLUMED/#available-modules","title":"Available modules","text":"<p>The overview below shows which PLUMED installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PLUMED, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PLUMED/2.9.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PLUMED/2.9.0-foss-2023a - x - - - - PLUMED/2.6.2-foss-2020b - x - - - -"},{"location":"available_software/detail/PLY/","title":"PLY","text":"<p>PLY is yet another implementation of lex and yacc for Python.</p> <p>https://www.dabeaz.com/ply/</p>"},{"location":"available_software/detail/PLY/#available-modules","title":"Available modules","text":"<p>The overview below shows which PLY installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PLY, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PLY/3.11-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PLY/3.11-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PMIx/","title":"PMIx","text":"<p>Process Management for Exascale EnvironmentsPMI Exascale (PMIx) represents an attempt toprovide an extended version of the PMI standard specifically designedto support clusters up to and including exascale sizes. The overallobjective of the project is not to branch the existing pseudo-standarddefinitions - in fact, PMIx fully supports both of the existing PMI-1and PMI-2 APIs - but rather to (a) augment and extend those APIs toeliminate some current restrictions that impact scalability, and (b)provide a reference implementation of the PMI-server that demonstratesthe desired level of scalability.</p> <p>https://pmix.org/</p>"},{"location":"available_software/detail/PMIx/#available-modules","title":"Available modules","text":"<p>The overview below shows which PMIx installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PMIx, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PMIx/4.2.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PMIx/4.2.6-GCCcore-13.2.0 - x - - - - PMIx/4.2.4-GCCcore-12.3.0 - x x x x x PMIx/4.2.2-GCCcore-12.2.0 - x - - - - PMIx/4.1.2-GCCcore-11.3.0 - x - - - - PMIx/3.1.5-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/POV-Ray/","title":"POV-Ray","text":"<p>The Persistence of Vision Raytracer, or POV-Ray, is a ray tracing program which generates images from a text-based scene description, and is available for a variety of computer platforms. POV-Ray is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports.</p> <p>https://www.povray.org/</p>"},{"location":"available_software/detail/POV-Ray/#available-modules","title":"Available modules","text":"<p>The overview below shows which POV-Ray installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using POV-Ray, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load POV-Ray/3.7.0.10-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 POV-Ray/3.7.0.10-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/PROJ/","title":"PROJ","text":"<p>Program proj is a standard Unix filter function which convertsgeographic longitude and latitude coordinates into cartesian coordinates</p> <p>https://proj.org</p>"},{"location":"available_software/detail/PROJ/#available-modules","title":"Available modules","text":"<p>The overview below shows which PROJ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PROJ, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PROJ/9.3.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PROJ/9.3.1-GCCcore-13.2.0 - x - - - - PROJ/9.2.0-GCCcore-12.3.0 - x - - - - PROJ/9.1.1-GCCcore-12.2.0 - x - - - - PROJ/7.2.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Pandoc/","title":"Pandoc","text":"<p>If you need to convert files from one markup format into another, pandoc is your swiss-army knife</p> <p>https://pandoc.org</p>"},{"location":"available_software/detail/Pandoc/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pandoc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pandoc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pandoc/3.1.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pandoc/3.1.2 - x - - - -"},{"location":"available_software/detail/Pango/","title":"Pango","text":"<p>Pango is a library for laying out and rendering of text, with an emphasis on internationalization.Pango can be used anywhere that text layout is needed, though most of the work on Pango so far has been done in thecontext of the GTK+ widget toolkit. Pango forms the core of text and font handling for GTK+-2.x.</p> <p>https://www.pango.org/</p>"},{"location":"available_software/detail/Pango/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pango installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pango, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pango/1.50.14-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pango/1.50.14-GCCcore-12.3.0 - x - - - - Pango/1.50.12-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/ParMETIS/","title":"ParMETIS","text":"<p>ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. ParMETIS extends the functionality provided by METIS and includes routines that are especially suited for parallel AMR computations and large scale numerical simulations. The algorithms implemented in ParMETIS are based on the parallel multilevel k-way graph-partitioning, adaptive repartitioning, and parallel multi-constrained partitioning schemes.</p> <p>http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview</p>"},{"location":"available_software/detail/ParMETIS/#available-modules","title":"Available modules","text":"<p>The overview below shows which ParMETIS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ParMETIS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ParMETIS/4.0.3-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ParMETIS/4.0.3-gompi-2023a - x - - - -"},{"location":"available_software/detail/ParaView/","title":"ParaView","text":"<p>ParaView is a scientific parallel visualizer.</p> <p>https://www.paraview.org</p>"},{"location":"available_software/detail/ParaView/#available-modules","title":"Available modules","text":"<p>The overview below shows which ParaView installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ParaView, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ParaView/5.11.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ParaView/5.11.2-foss-2023a - x - - - -"},{"location":"available_software/detail/Perl-bundle-BRAKER3/","title":"Perl-bundle-BRAKER3","text":"<p>The set of packages needed for BRAKER3 &amp; its dependency GeneMark-ETP</p> <p>https://www.perl.org/</p>"},{"location":"available_software/detail/Perl-bundle-BRAKER3/#available-modules","title":"Available modules","text":"<p>The overview below shows which Perl-bundle-BRAKER3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Perl-bundle-BRAKER3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Perl-bundle-BRAKER3/5.36.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Perl-bundle-BRAKER3/5.36.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Perl-bundle-BRAKER3/#perl-bundle-braker35361-gcccore-1230","title":"Perl-bundle-BRAKER3/5.36.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Class::Method::Modifiers-2.15, Clone::Choose-0.010, File::Temp-0.2311, File::Which-1.27, Hash::Merge-0.302, Math::Utils-1.14, MCE::Mutex-1.884, Moo-2.005005, Parallel::ForkManager-2.02, Role::Tiny-2.002004, Scalar::Util::Numeric-0.40, Spiffy-0.46, Statistics::LineFit-0.07, Sub::Quote-2.006008, Sub::Uplevel-0.2800, Test::Base-0.89, Test::Deep-1.204, Test::Warn-0.37, Test::Without::Module-0.21, Test::YAML-1.07, YAML-1.30, YAML::XS-0.89</p>"},{"location":"available_software/detail/Perl-bundle-CPAN/","title":"Perl-bundle-CPAN","text":"<p>A set of common packages from CPAN</p> <p>https://www.perl.org/</p>"},{"location":"available_software/detail/Perl-bundle-CPAN/#available-modules","title":"Available modules","text":"<p>The overview below shows which Perl-bundle-CPAN installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Perl-bundle-CPAN, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Perl-bundle-CPAN/5.38.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Perl-bundle-CPAN/5.38.0-GCCcore-13.2.0 - x - - - - Perl-bundle-CPAN/5.36.1-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/Perl-bundle-CPAN/#perl-bundle-cpan5380-gcccore-1320","title":"Perl-bundle-CPAN/5.38.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Algorithm::Dependency-1.112, Algorithm::Diff-1.201, aliased-0.34, AnyEvent-7.17, App::Cmd-0.336, App::cpanminus-1.7047, AppConfig-1.71, Archive::Extract-0.88, Array::Transpose-0.06, Array::Utils-0.5, Authen::NTLM-1.09, Authen::SASL-2.1700, AutoLoader-5.74, B::COW-0.007, B::Hooks::EndOfScope-0.26, B::Lint-1.20, boolean-0.46, Business::ISBN-3.008, Business::ISBN::Data-20231013.001, Canary::Stability-2013, Capture::Tiny-0.48, Carp::Clan-6.08, Carp::Heavy-1.50, CGI-4.59, Class::Accessor-0.51, Class::Data::Inheritable-0.09, Class::DBI-v3.0.17, Class::DBI::SQLite-0.11, Class::Inspector-1.36, Class::ISA-0.36, Class::Load-0.25, Class::Load::XS-0.10, Class::Method::Modifiers-2.15, Class::Singleton-1.6, Class::Tiny-1.008, Class::Trigger-0.15, Class::XSAccessor-1.19, Clone-0.46, Clone::Choose-0.010, common::sense-3.75, Compress::Raw::Zlib-2.206, Config::General-2.65, Config::INI-0.029, Config::MVP-2.200013, Config::MVP::Reader::INI-2.101465, Config::Simple-4.58, Config::Tiny-2.30, Const::Exporter-v1.2.3, Const::Fast-0.014, CPAN::Meta::Check-0.018, CPAN::Uploader-0.103018, CPANPLUS-0.9914, Crypt::DES-2.07, Crypt::Rijndael-1.16, Cwd-3.75, Cwd::Guard-0.05, Data::Dump-1.25, Data::Dumper::Concise-2.023, Data::Grove-0.08, Data::OptList-0.114, Data::Section-0.200008, Data::Section::Simple-0.07, Data::Stag-0.14, Data::Types-0.17, Data::UUID-1.226, Date::Handler-1.2, Date::Language-2.33, DateTime-1.59, DateTime::Locale-1.39, DateTime::TimeZone-2.60, DateTime::Tiny-1.07, DBD::CSV-0.60, DBD::SQLite-1.74, DBI-1.643, DBIx::Admin::CreateTable-2.11, DBIx::Admin::DSNManager-2.02, DBIx::Admin::TableInfo-3.04, DBIx::ContextualFetch-1.03, DBIx::Simple-1.37, Devel::CheckCompiler-0.07, Devel::CheckLib-1.16, Devel::Cycle-1.12, Devel::FindPerl-0.016, Devel::GlobalDestruction-0.14, Devel::OverloadInfo-0.007, Devel::Size-0.83, Devel::StackTrace-2.04, Digest::HMAC-1.04, Digest::MD5::File-0.08, Digest::SHA1-2.13, Dist::CheckConflicts-0.11, Dist::Zilla-6.030, Email::Date::Format-1.008, Encode-3.19, Encode::Locale-1.05, Error-0.17029, Eval::Closure-0.14, Exception::Class-1.45, Expect-1.35, Exporter::Declare-0.114, Exporter::Tiny-1.006002, ExtUtils::CBuilder-0.280236, ExtUtils::Config-0.008, ExtUtils::Constant-0.25, ExtUtils::CppGuess-0.26, ExtUtils::Helpers-0.026, ExtUtils::InstallPaths-0.012, ExtUtils::MakeMaker-7.70, ExtUtils::ParseXS-3.51, Fennec::Lite-0.004, File::CheckTree-4.42, File::Copy::Recursive-0.45, File::Copy::Recursive::Reduced-0.007, File::Find::Rule-0.34, File::Find::Rule::Perl-1.16, File::Grep-0.02, File::HomeDir-1.006, File::Listing-6.16, File::Next-1.18, File::pushd-1.016, File::Remove-1.61, File::ShareDir-1.118, File::ShareDir::Install-0.14, File::Slurp-9999.32, File::Slurp::Tiny-0.004, File::Slurper-0.014, File::Temp-0.2311, File::Which-1.27, Font::TTF-1.06, Getopt::Long::Descriptive-0.111, Git-0.42, GO-0.04, GO::Utils-0.15, Graph-0.9727, Graph::ReadWrite-2.10, Hash::Merge-0.302, Hash::Objectify-0.008, Heap-0.80, Hook::LexWrap-0.26, HTML::Entities::Interpolate-1.10, HTML::Form-6.11, HTML::Parser-3.81, HTML::Tagset-3.20, HTML::Template-2.97, HTML::Tree-5.07, HTTP::CookieJar-0.014, HTTP::Cookies-6.10, HTTP::Daemon-6.16, HTTP::Date-6.06, HTTP::Message-6.45, HTTP::Negotiate-6.01, HTTP::Tiny-0.088, if-0.0608, Ima::DBI-0.35, Import::Into-1.002005, Importer-0.026, Inline-0.86, IO::Compress::Zip-2.206, IO::HTML-1.004, IO::Socket::SSL-2.083, IO::String-1.08, IO::Stringy-2.113, IO::TieCombine-1.005, IO::Tty-1.17, IO::Tty-1.17, IPC::Cmd-1.04, IPC::Run-20231003.0, IPC::Run3-0.048, IPC::System::Simple-1.30, JSON-4.10, JSON::MaybeXS-1.004005, JSON::XS-4.03, Lingua::EN::PluralToSingular-0.21, List::AllUtils-0.19, List::MoreUtils-0.430, List::MoreUtils::XS-0.430, List::SomeUtils-0.59, List::UtilsBy-0.12, local::lib-2.000029, Locale::Maketext::Simple-0.21, Log::Dispatch-2.71, Log::Dispatch::Array-1.005, Log::Dispatchouli-3.007, Log::Handler-0.90, Log::Log4perl-1.57, Log::Message-0.08, Log::Message::Simple-0.10, Log::Report-1.34, Log::Report::Optional-1.07, Logger::Simple-2.0, LWP::MediaTypes-6.04, LWP::Protocol::https-6.11, LWP::Simple-6.72, Mail::Util-2.21, Math::Bezier-0.01, Math::CDF-0.1, Math::Round-0.08, Math::Utils-1.14, Math::VecStat-0.08, MCE::Mutex-1.889, Meta::Builder-0.004, MIME::Base64-3.16, MIME::Charset-v1.013.1, MIME::Lite-3.033, MIME::Types-2.24, Mixin::Linewise::Readers-0.111, Mock::Quick-1.111, Module::Build-0.4234, Module::Build::Tiny-0.047, Module::Build::XSUtil-0.19, Module::CoreList-5.20230920, Module::Implementation-0.09, Module::Install-1.21, Module::Load-0.36, Module::Load::Conditional-0.74, Module::Metadata-1.000038, Module::Path-0.19, Module::Path-0.19, Module::Pluggable-5.2, Module::Runtime-0.016, Module::Runtime::Conflicts-0.003, Moo-2.005005, Moose-2.2206, MooseX::LazyRequire-0.11, MooseX::OneArgNew-0.007, MooseX::Role::Parameterized-1.11, MooseX::SetOnce-0.203, MooseX::Types-0.50, MooseX::Types::Perl-0.101344, Mouse-v2.5.10, Mozilla::CA-20230821, MRO::Compat-0.15, namespace::autoclean-0.29, namespace::clean-0.27, Net::Domain-3.15, Net::HTTP-6.23, Net::SMTP::SSL-1.04, Net::SNMP-v6.0.1, Net::SSLeay-1.92, Number::Compare-0.03, Number::Format-1.76, Object::Accessor-0.48, Object::InsideOut-4.05, Object::InsideOut-4.05, Package::Constants-0.06, Package::DeprecationManager-0.18, Package::Stash-0.40, Package::Stash::XS-0.30, PadWalker-2.5, Parallel::ForkManager-2.02, Params::Check-0.38, Params::Util-1.102, Params::Validate-1.31, Params::ValidationCompiler-0.31, parent-0.241, Parse::RecDescent-1.967015, Parse::Yapp-1.21, Path::Tiny-0.144, PDF::API2-2.045, Perl::OSType-1.010, Perl::PrereqScanner-1.100, PerlIO::utf8_strict-0.010, Pod::Elemental-0.103006, Pod::Escapes-1.07, Pod::Eventual-0.094003, Pod::LaTeX-0.61, Pod::Man-5.01, Pod::Parser-1.66, Pod::Plainer-1.04, Pod::POM-2.01, Pod::Simple-3.45, Pod::Weaver-4.019, PPI-1.277, Readonly-2.05, Ref::Util-0.204, Regexp::Common-2017060201, Role::HasMessage-0.007, Role::Identifiable::HasIdent-0.009, Role::Tiny-2.002004, Scalar::Util-1.63, Scalar::Util::Numeric-0.40, Scope::Guard-0.21, Set::Array-0.30, Set::IntervalTree-0.12, Set::IntSpan-1.19, Set::IntSpan::Fast-1.15, Set::Object-1.42, Set::Scalar-1.29, Shell-0.73, Socket-2.037, Software::License-0.104004, Specio-0.48, Spiffy-0.46, SQL::Abstract-2.000001, SQL::Statement-1.414, Statistics::Basic-1.6611, Statistics::Descriptive-3.0801, Storable-3.25, strictures-2.000006, String::Errf-0.009, String::Flogger-1.101246, String::Formatter-1.235, String::Print-0.94, String::RewritePrefix-0.009, String::Truncate-1.100603, String::TtyLength-0.03, Sub::Exporter-0.990, Sub::Exporter::ForMethods-0.100055, Sub::Exporter::GlobExporter-0.006, Sub::Exporter::Progressive-0.001013, Sub::Identify-0.14, Sub::Info-0.002, Sub::Install-0.929, Sub::Name-0.27, Sub::Quote-2.006008, Sub::Uplevel-0.2800, SVG-2.87, Switch-2.17, Sys::Info-0.7811, Sys::Info::Base-0.7807, Sys::Info::Driver::Linux-0.7905, Sys::Info::Driver::Linux::Device::CPU-0.7905, Sys::Info::Driver::Unknown-0.79, Sys::Info::Driver::Unknown::Device::CPU-0.79, Template-3.101, Template::Plugin::Number::Format-1.06, Term::Encoding-0.03, Term::ReadKey-2.38, Term::ReadLine::Gnu-1.46, Term::Table-0.017, Term::UI-0.50, Test-1.26, Test2::Plugin::NoWarnings-0.09, Test2::Require::Module-0.000156, Test::Base-0.89, Test::CheckDeps-0.010, Test::ClassAPI-1.07, Test::CleanNamespaces-0.24, Test::Deep-1.204, Test::Differences-0.71, Test::Exception-0.43, Test::FailWarnings-0.008, Test::Fatal-0.017, Test::File-1.993, Test::File::ShareDir::Dist-1.001002, Test::Harness-3.48, Test::LeakTrace-0.17, Test::Memory::Cycle-1.06, Test::More::UTF8-0.05, Test::Most-0.38, Test::Needs-0.002010, Test::NoWarnings-1.06, Test::Object-0.08, Test::Output-1.034, Test::Pod-1.52, Test::Requires-0.11, Test::RequiresInternet-0.05, Test::Simple-1.302195, Test::SubCalls-1.10, Test::Sys::Info-0.23, Test::Version-2.09, Test::Warn-0.37, Test::Warnings-0.032, Test::Without::Module-0.21, Test::YAML-1.07, Text::Aligner-0.16, Text::Balanced-2.06, Text::CSV-2.03, Text::CSV_XS-1.52, Text::Diff-1.45, Text::Format-0.62, Text::Glob-0.11, Text::Iconv-1.7, Text::Soundex-3.05, Text::Table-1.135, Text::Table::Manifold-1.03, Text::Template-1.61, Throwable-1.001, Tie::Function-0.02, Tie::IxHash-1.23, Time::HiRes-1.9764, Time::Local-1.35, Time::Piece-1.3401, Time::Piece::MySQL-0.06, Tree::DAG_Node-1.32, Try::Tiny-0.31, Type::Tiny-2.004000, Types::Serialiser-1.01, Types::Serialiser-1.01, Unicode::EastAsianWidth-12.0, Unicode::LineBreak-2019.001, UNIVERSAL::moniker-0.08, Unix::Processors-2.046, Unix::Processors-2.046, URI-5.21, Variable::Magic-0.63, version-0.9930, Want-0.29, WWW::RobotRules-6.02, XML::Bare-0.53, XML::DOM-1.46, XML::Filter::BufferText-1.01, XML::NamespaceSupport-1.12, XML::Parser-2.46, XML::RegExp-0.04, XML::SAX-1.02, XML::SAX::Base-1.09, XML::SAX::Expat-0.51, XML::SAX::Writer-0.57, XML::Simple-2.25, XML::Tiny-2.07, XML::Twig-3.52, XML::Writer-0.900, XML::XPath-1.48, XSLoader-0.24, YAML-1.30, YAML::Tiny-1.74</p>"},{"location":"available_software/detail/Perl-bundle-CPAN/#perl-bundle-cpan5361-gcccore-1230","title":"Perl-bundle-CPAN/5.36.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Algorithm::Dependency-1.112, Algorithm::Diff-1.201, aliased-0.34, AnyEvent-7.17, App::Cmd-0.335, App::cpanminus-1.7046, AppConfig-1.71, Archive::Extract-0.88, Array::Transpose-0.06, Array::Utils-0.5, Authen::NTLM-1.09, Authen::SASL-2.16, AutoLoader-5.74, B::COW-0.007, B::Hooks::EndOfScope-0.26, B::Lint-1.20, boolean-0.46, Business::ISBN-3.008, Business::ISBN::Data-20230516.001, Canary::Stability-2013, Capture::Tiny-0.48, Carp::Clan-6.08, Carp::Heavy-1.50, CGI-4.57, Class::Accessor-0.51, Class::Data::Inheritable-0.09, Class::DBI-v3.0.17, Class::DBI::SQLite-0.11, Class::Inspector-1.36, Class::ISA-0.36, Class::Load-0.25, Class::Load::XS-0.10, Class::Method::Modifiers-2.15, Class::Singleton-1.6, Class::Tiny-1.008, Class::Trigger-0.15, Class::XSAccessor-1.19, Clone-0.46, Clone::Choose-0.010, common::sense-3.75, Compress::Raw::Zlib-2.204, Config::General-2.65, Config::INI-0.029, Config::MVP-2.200013, Config::MVP::Reader::INI-2.101465, Config::Simple-4.58, Config::Tiny-2.29, Const::Exporter-1.2.2, Const::Fast-0.014, CPAN::Meta::Check-0.017, CPAN::Uploader-0.103018, CPANPLUS-0.9914, Crypt::DES-2.07, Crypt::Rijndael-1.16, Cwd-3.75, Cwd::Guard-0.05, Data::Dump-1.25, Data::Dumper::Concise-2.023, Data::Grove-0.08, Data::OptList-0.114, Data::Section-0.200008, Data::Section::Simple-0.07, Data::Stag-0.14, Data::Types-0.17, Data::UUID-1.226, Date::Handler-1.2, Date::Language-2.33, DateTime-1.59, DateTime::Locale-1.38, DateTime::TimeZone-2.60, DateTime::Tiny-1.07, DBD::CSV-0.60, DBD::SQLite-1.72, DBI-1.643, DBIx::Admin::CreateTable-2.11, DBIx::Admin::DSNManager-2.02, DBIx::Admin::TableInfo-3.04, DBIx::ContextualFetch-1.03, DBIx::Simple-1.37, Devel::CheckCompiler-0.07, Devel::CheckLib-1.16, Devel::Cycle-1.12, Devel::FindPerl-0.016, Devel::GlobalDestruction-0.14, Devel::OverloadInfo-0.007, Devel::Size-0.83, Devel::StackTrace-2.04, Digest::HMAC-1.04, Digest::MD5::File-0.08, Digest::SHA1-2.13, Dist::CheckConflicts-0.11, Dist::Zilla-6.030, Email::Date::Format-1.008, Encode-3.19, Encode::Locale-1.05, Error-0.17029, Eval::Closure-0.14, Exception::Class-1.45, Expect-1.35, Exporter::Declare-0.114, Exporter::Tiny-1.006002, ExtUtils::CBuilder-0.280236, ExtUtils::Config-0.008, ExtUtils::Constant-0.25, ExtUtils::CppGuess-0.26, ExtUtils::Helpers-0.026, ExtUtils::InstallPaths-0.012, ExtUtils::MakeMaker-7.70, ExtUtils::ParseXS-3.44, Fennec::Lite-0.004, File::CheckTree-4.42, File::Copy::Recursive-0.45, File::Copy::Recursive::Reduced-0.006, File::Find::Rule-0.34, File::Find::Rule::Perl-1.16, File::Grep-0.02, File::HomeDir-1.006, File::Listing-6.15, File::Next-1.18, File::pushd-1.016, File::Remove-1.61, File::ShareDir-1.118, File::ShareDir::Install-0.14, File::Slurp-9999.32, File::Slurp::Tiny-0.004, File::Slurper-0.014, File::Temp-0.2311, File::Which-1.27, Font::TTF-1.06, Getopt::Long::Descriptive-0.111, Git-0.42, GO-0.04, GO::Utils-0.15, Graph-0.9726, Graph::ReadWrite-2.10, Hash::Merge-0.302, Hash::Objectify-0.008, Heap-0.80, Hook::LexWrap-0.26, HTML::Entities::Interpolate-1.10, HTML::Form-6.11, HTML::Parser-3.81, HTML::Tagset-3.20, HTML::Template-2.97, HTML::Tree-5.07, HTTP::CookieJar-0.014, HTTP::Cookies-6.10, HTTP::Daemon-6.16, HTTP::Date-6.05, HTTP::Message-6.44, HTTP::Negotiate-6.01, HTTP::Tiny-0.082, if-0.0608, Ima::DBI-0.35, Import::Into-1.002005, Importer-0.026, Inline-0.86, IO::Compress::Zip-2.204, IO::HTML-1.004, IO::Socket::SSL-2.083, IO::String-1.08, IO::Stringy-2.113, IO::TieCombine-1.005, IO::Tty-1.17, IO::Tty-1.17, IPC::Cmd-1.04, IPC::Run-20220807.0, IPC::Run3-0.048, IPC::System::Simple-1.30, JSON-4.10, JSON::MaybeXS-1.004005, JSON::XS-4.03, Lingua::EN::PluralToSingular-0.21, List::AllUtils-0.19, List::MoreUtils-0.430, List::MoreUtils::XS-0.430, List::SomeUtils-0.59, List::UtilsBy-0.12, local::lib-2.000029, Locale::Maketext::Simple-0.21, Log::Dispatch-2.71, Log::Dispatch::Array-1.005, Log::Dispatchouli-3.002, Log::Handler-0.90, Log::Log4perl-1.57, Log::Message-0.08, Log::Message::Simple-0.10, Log::Report-1.34, Log::Report::Optional-1.07, Logger::Simple-2.0, LWP::MediaTypes-6.04, LWP::Protocol::https-6.10, LWP::Simple-6.70, Mail::Util-2.21, Math::Bezier-0.01, Math::CDF-0.1, Math::Round-0.07, Math::Utils-1.14, Math::VecStat-0.08, MCE::Mutex-1.884, Meta::Builder-0.004, MIME::Base64-3.16, MIME::Charset-v1.013.1, MIME::Lite-3.033, MIME::Types-2.24, Mixin::Linewise::Readers-0.111, Mock::Quick-1.111, Module::Build-0.4234, Module::Build::Tiny-0.045, Module::Build::XSUtil-0.19, Module::CoreList-5.20230423, Module::Implementation-0.09, Module::Install-1.21, Module::Load-0.36, Module::Load::Conditional-0.74, Module::Metadata-1.000038, Module::Path-0.19, Module::Path-0.19, Module::Pluggable-5.2, Module::Runtime-0.016, Module::Runtime::Conflicts-0.003, Moo-2.005005, Moose-2.2203, MooseX::LazyRequire-0.11, MooseX::OneArgNew-0.007, MooseX::Role::Parameterized-1.11, MooseX::SetOnce-0.203, MooseX::Types-0.50, MooseX::Types::Perl-0.101344, Mouse-v2.5.10, Mozilla::CA-20221114, MRO::Compat-0.15, namespace::autoclean-0.29, namespace::clean-0.27, Net::Domain-3.15, Net::HTTP-6.22, Net::SMTP::SSL-1.04, Net::SNMP-v6.0.1, Net::SSLeay-1.92, Number::Compare-0.03, Number::Format-1.75, Object::Accessor-0.48, Object::InsideOut-4.05, Object::InsideOut-4.05, Package::Constants-0.06, Package::DeprecationManager-0.18, Package::Stash-0.40, Package::Stash::XS-0.30, PadWalker-2.5, Parallel::ForkManager-2.02, Params::Check-0.38, Params::Util-1.102, Params::Validate-1.31, Params::ValidationCompiler-0.31, parent-0.241, Parse::RecDescent-1.967015, Parse::Yapp-1.21, Path::Tiny-0.144, PDF::API2-2.044, Perl::OSType-1.010, Perl::PrereqScanner-1.100, PerlIO::utf8_strict-0.010, Pod::Elemental-0.103006, Pod::Escapes-1.07, Pod::Eventual-0.094003, Pod::LaTeX-0.61, Pod::Man-5.01, Pod::Parser-1.66, Pod::Plainer-1.04, Pod::POM-2.01, Pod::Simple-3.45, Pod::Weaver-4.019, PPI-1.276, Readonly-2.05, Ref::Util-0.204, Regexp::Common-2017060201, Role::HasMessage-0.007, Role::Identifiable::HasIdent-0.009, Role::Tiny-2.002004, Scalar::Util-1.63, Scalar::Util::Numeric-0.40, Scope::Guard-0.21, Set::Array-0.30, Set::IntervalTree-0.12, Set::IntSpan-1.19, Set::IntSpan::Fast-1.15, Set::Object-1.42, Set::Scalar-1.29, Shell-0.73, Socket-2.036, Software::License-0.104003, Specio-0.48, Spiffy-0.46, SQL::Abstract-2.000001, SQL::Statement-1.414, Statistics::Basic-1.6611, Statistics::Descriptive-3.0800, Storable-3.25, strictures-2.000006, String::Errf-0.009, String::Flogger-1.101246, String::Formatter-1.235, String::Print-0.94, String::RewritePrefix-0.009, String::Truncate-1.100603, String::TtyLength-0.03, Sub::Exporter-0.989, Sub::Exporter::ForMethods-0.100055, Sub::Exporter::GlobExporter-0.006, Sub::Exporter::Progressive-0.001013, Sub::Identify-0.14, Sub::Info-0.002, Sub::Install-0.929, Sub::Name-0.27, Sub::Quote-2.006008, Sub::Uplevel-0.2800, SVG-2.87, Switch-2.17, Sys::Info-0.7811, Sys::Info::Base-0.7807, Sys::Info::Driver::Linux-0.7905, Sys::Info::Driver::Linux::Device::CPU-0.7905, Sys::Info::Driver::Unknown-0.79, Sys::Info::Driver::Unknown::Device::CPU-0.79, Template-3.101, Template::Plugin::Number::Format-1.06, Term::Encoding-0.03, Term::ReadKey-2.38, Term::ReadLine::Gnu-1.45, Term::Table-0.016, Term::UI-0.50, Test-1.26, Test2::Plugin::NoWarnings-0.09, Test2::Require::Module-0.000155, Test::Base-0.89, Test::CheckDeps-0.010, Test::ClassAPI-1.07, Test::CleanNamespaces-0.24, Test::Deep-1.204, Test::Differences-0.69, Test::Exception-0.43, Test::FailWarnings-0.008, Test::Fatal-0.017, Test::File-1.993, Test::File::ShareDir::Dist-1.001002, Test::Harness-3.44, Test::LeakTrace-0.17, Test::Memory::Cycle-1.06, Test::More::UTF8-0.05, Test::Most-0.38, Test::Needs-0.002010, Test::NoWarnings-1.06, Test::Object-0.08, Test::Output-1.033, Test::Pod-1.52, Test::Requires-0.11, Test::RequiresInternet-0.05, Test::Simple-1.302195, Test::SubCalls-1.10, Test::Sys::Info-0.23, Test::Version-2.09, Test::Warn-0.37, Test::Warnings-0.031, Test::Without::Module-0.21, Test::YAML-1.07, Text::Aligner-0.16, Text::Balanced-2.06, Text::CSV-2.02, Text::CSV_XS-1.50, Text::Diff-1.45, Text::Format-0.62, Text::Glob-0.11, Text::Iconv-1.7, Text::Soundex-3.05, Text::Table-1.135, Text::Table::Manifold-1.03, Text::Template-1.61, Throwable-1.001, Tie::Function-0.02, Tie::IxHash-1.23, Time::HiRes-1.9764, Time::Local-1.35, Time::Piece-1.3401, Time::Piece::MySQL-0.06, Tree::DAG_Node-1.32, Try::Tiny-0.31, Type::Tiny-2.004000, Types::Serialiser-1.01, Types::Serialiser-1.01, Unicode::EastAsianWidth-12.0, Unicode::LineBreak-2019.001, UNIVERSAL::moniker-0.08, Unix::Processors-2.046, Unix::Processors-2.046, URI-5.19, Variable::Magic-0.63, version-0.9929, Want-0.29, WWW::RobotRules-6.02, XML::Bare-0.53, XML::DOM-1.46, XML::Filter::BufferText-1.01, XML::NamespaceSupport-1.12, XML::Parser-2.46, XML::RegExp-0.04, XML::SAX-1.02, XML::SAX::Base-1.09, XML::SAX::Expat-0.51, XML::SAX::Writer-0.57, XML::Simple-2.25, XML::Tiny-2.07, XML::Twig-3.52, XML::Writer-0.900, XML::XPath-1.48, XSLoader-0.24, YAML-1.30, YAML::Tiny-1.74</p>"},{"location":"available_software/detail/Perl/","title":"Perl","text":"<p>Larry Wall's Practical Extraction and Report LanguageIncludes a small selection of extra CPAN packages for core functionality.</p> <p>https://www.perl.org/</p>"},{"location":"available_software/detail/Perl/#available-modules","title":"Available modules","text":"<p>The overview below shows which Perl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Perl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Perl/5.38.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Perl/5.38.0-GCCcore-13.2.0 - x - - - - Perl/5.36.1-GCCcore-12.3.0 - x x x x x Perl/5.36.0-GCCcore-12.2.0 - x - - - - Perl/5.34.1-GCCcore-11.3.0 - x - - - - Perl/5.32.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Perl/#perl5380-gcccore-1320","title":"Perl/5.38.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Carp-1.50, constant-1.33, Data::Dumper-2.183, Exporter-5.77, File::Path-2.18, File::Spec-3.75, Getopt::Long-2.54, IO::File-1.51, Text::ParseWords-3.31, Thread::Queue-3.13, threads-2.21</p>"},{"location":"available_software/detail/Perl/#perl5361-gcccore-1230","title":"Perl/5.36.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Carp-1.50, constant-1.33, Data::Dumper-2.183, Exporter-5.77, File::Path-2.18, File::Spec-3.75, Getopt::Long-2.54, IO::File-1.51, Text::ParseWords-3.31, Thread::Queue-3.13, threads-2.21</p>"},{"location":"available_software/detail/Perl/#perl5360-gcccore-1220","title":"Perl/5.36.0-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Algorithm::Dependency-1.112, Algorithm::Diff-1.201, aliased-0.34, AnyEvent-7.17, App::Cmd-0.334, App::cpanminus-1.7046, AppConfig-1.71, Archive::Extract-0.88, Array::Transpose-0.06, Array::Utils-0.5, Authen::NTLM-1.09, Authen::SASL-2.16, AutoLoader-5.74, B::Hooks::EndOfScope-0.26, B::Lint-1.20, boolean-0.46, Business::ISBN-3.007, Business::ISBN::Data-20210112.006, Canary::Stability-2013, Capture::Tiny-0.48, Carp-1.50, Carp::Clan-6.08, Carp::Heavy-1.50, Class::Accessor-0.51, Class::Data::Inheritable-0.09, Class::DBI-v3.0.17, Class::DBI::SQLite-0.11, Class::Inspector-1.36, Class::ISA-0.36, Class::Load-0.25, Class::Load::XS-0.10, Class::Singleton-1.6, Class::Tiny-1.008, Class::Trigger-0.15, Clone-0.45, Clone::Choose-0.010, common::sense-3.75, Config::General-2.65, Config::INI-0.027, Config::MVP-2.200012, Config::Simple-4.58, Config::Tiny-2.28, constant-1.33, CPAN::Meta::Check-0.014, CPANPLUS-0.9914, Crypt::DES-2.07, Crypt::Rijndael-1.16, Cwd-3.75, Cwd::Guard-0.05, Data::Dump-1.25, Data::Dumper-2.183, Data::Dumper::Concise-2.023, Data::Grove-0.08, Data::OptList-0.112, Data::Section-0.200007, Data::Section::Simple-0.07, Data::Stag-0.14, Data::Types-0.17, Data::UUID-1.226, Date::Handler-1.2, Date::Language-2.33, DateTime-1.58, DateTime::Locale-1.36, DateTime::TimeZone-2.53, DateTime::Tiny-1.07, DBD::CSV-0.59, DBD::SQLite-1.70, DBI-1.643, DBIx::Admin::TableInfo-3.04, DBIx::ContextualFetch-1.03, DBIx::Simple-1.37, Devel::CheckCompiler-0.07, Devel::CheckLib-1.16, Devel::Cycle-1.12, Devel::GlobalDestruction-0.14, Devel::OverloadInfo-0.007, Devel::Size-0.83, Devel::StackTrace-2.04, Digest::HMAC-1.04, Digest::MD5::File-0.08, Digest::SHA1-2.13, Dist::CheckConflicts-0.11, Dist::Zilla-6.025, Email::Date::Format-1.005, Encode-3.19, Encode::Locale-1.05, Error-0.17029, Eval::Closure-0.14, Exception::Class-1.45, Expect-1.35, Exporter-5.74, Exporter::Declare-0.114, Exporter::Tiny-1.004000, ExtUtils::CBuilder-0.280236, ExtUtils::Config-0.008, ExtUtils::Constant-0.25, ExtUtils::CppGuess-0.26, ExtUtils::Helpers-0.026, ExtUtils::InstallPaths-0.012, ExtUtils::MakeMaker-7.64, ExtUtils::ParseXS-3.44, Fennec::Lite-0.004, File::CheckTree-4.42, File::Copy::Recursive-0.45, File::Copy::Recursive::Reduced-0.006, File::Find::Rule-0.34, File::Find::Rule::Perl-1.16, File::Grep-0.02, File::HomeDir-1.006, File::Listing-6.15, File::Next-1.18, File::Path-2.18, File::pushd-1.016, File::Remove-1.61, File::ShareDir-1.118, File::ShareDir::Install-0.14, File::Slurp-9999.32, File::Slurp::Tiny-0.004, File::Slurper-0.013, File::Spec-3.75, File::Temp-0.2311, File::Which-1.27, Font::TTF-1.06, Getopt::Long-2.52, Getopt::Long::Descriptive-0.110, Git-0.42, GO-0.04, GO::Utils-0.15, Graph-0.9725, Graph::ReadWrite-2.10, Hash::Merge-0.302, Heap-0.80, HTML::Entities::Interpolate-1.10, HTML::Form-6.10, HTML::Parser-3.78, HTML::Tagset-3.20, HTML::Template-2.97, HTML::Tree-5.07, HTTP::Cookies-6.10, HTTP::Daemon-6.14, HTTP::Date-6.05, HTTP::Negotiate-6.01, HTTP::Request-6.37, HTTP::Tiny-0.082, if-0.0608, Ima::DBI-0.35, Import::Into-1.002005, Importer-0.026, Inline-0.86, IO::HTML-1.004, IO::Socket::SSL-2.075, IO::String-1.08, IO::Stringy-2.113, IO::Tty-1.16, IPC::Cmd-1.04, IPC::Run-20220807.0, IPC::Run3-0.048, IPC::System::Simple-1.30, JSON-4.09, JSON::XS-4.03, Lingua::EN::PluralToSingular-0.21, List::AllUtils-0.19, List::MoreUtils-0.430, List::MoreUtils::XS-0.430, List::SomeUtils-0.58, List::Util-1.63, List::UtilsBy-0.12, local::lib-2.000029, Locale::Maketext::Simple-0.21, Log::Dispatch-2.70, Log::Dispatchouli-2.023, Log::Handler-0.90, Log::Log4perl-1.56, Log::Message-0.08, Log::Message::Simple-0.10, Log::Report-1.33, Log::Report::Optional-1.07, Logger::Simple-2.0, LWP::MediaTypes-6.04, LWP::Protocol::https-6.10, LWP::Simple-6.67, Mail::Util-2.21, Math::Bezier-0.01, Math::CDF-0.1, Math::Round-0.07, Math::Utils-1.14, Math::VecStat-0.08, MCE::Mutex-1.879, Meta::Builder-0.004, MIME::Base64-3.16, MIME::Charset-1.013.1, MIME::Lite-3.033, MIME::Types-2.22, Mixin::Linewise::Readers-0.110, Mock::Quick-1.111, Module::Build-0.4231, Module::Build::Tiny-0.039, Module::Build::XSUtil-0.19, Module::CoreList-5.20220820, Module::Implementation-0.09, Module::Install-1.19, Module::Load-0.36, Module::Load::Conditional-0.74, Module::Metadata-1.000037, Module::Path-0.19, Module::Pluggable-5.2, Module::Runtime-0.016, Module::Runtime::Conflicts-0.003, Moo-2.005004, Moose-2.2201, MooseX::LazyRequire-0.11, MooseX::OneArgNew-0.006, MooseX::Role::Parameterized-1.11, MooseX::SetOnce-0.201, MooseX::Types-0.50, MooseX::Types::Perl-0.101343, Mouse-v2.5.10, Mozilla::CA-20211001, MRO::Compat-0.15, namespace::autoclean-0.29, namespace::clean-0.27, Net::Domain-3.14, Net::HTTP-6.22, Net::SMTP::SSL-1.04, Net::SNMP-v6.0.1, Net::SSLeay-1.92, Number::Compare-0.03, Number::Format-1.75, Object::Accessor-0.48, Object::InsideOut-4.05, Package::Constants-0.06, Package::DeprecationManager-0.17, Package::Stash-0.40, Package::Stash::XS-0.30, PadWalker-2.5, Parallel::ForkManager-2.02, Params::Check-0.38, Params::Util-1.102, Params::Validate-1.30, Params::ValidationCompiler-0.30, parent-0.238, Parse::RecDescent-1.967015, Path::Tiny-0.124, PDF::API2-2.043, Perl::OSType-1.010, PerlIO::utf8_strict-0.009, Pod::Elemental-0.103005, Pod::Escapes-1.07, Pod::Eventual-0.094002, Pod::LaTeX-0.61, Pod::Man-4.14, Pod::Parser-1.66, Pod::Plainer-1.04, Pod::POM-2.01, Pod::Simple-3.43, Pod::Weaver-4.018, Readonly-2.05, Regexp::Common-2017060201, Role::HasMessage-0.006, Role::Identifiable::HasIdent-0.008, Role::Tiny-2.002004, Scalar::Util-1.63, Scalar::Util::Numeric-0.40, Scope::Guard-0.21, Set::Array-0.30, Set::IntervalTree-0.12, Set::IntSpan-1.19, Set::IntSpan::Fast-1.15, Set::Object-1.42, Set::Scalar-1.29, Shell-0.73, Socket-2.036, Software::License-0.104002, Specio-0.48, SQL::Abstract-2.000001, SQL::Statement-1.414, Statistics::Basic-1.6611, Statistics::Descriptive-3.0800, Storable-3.25, strictures-2.000006, String::Flogger-1.101245, String::Print-0.94, String::RewritePrefix-0.008, String::Truncate-1.100602, Sub::Exporter-0.988, Sub::Exporter::ForMethods-0.100054, Sub::Exporter::Progressive-0.001013, Sub::Identify-0.14, Sub::Info-0.002, Sub::Install-0.928, Sub::Name-0.26, Sub::Quote-2.006006, Sub::Uplevel-0.2800, Sub::Uplevel-0.2800, SVG-2.87, Switch-2.17, Sys::Info-0.7811, Sys::Info::Base-0.7807, Sys::Info::Driver::Linux-0.7905, Sys::Info::Driver::Unknown-0.79, Template-3.101, Template::Plugin::Number::Format-1.06, Term::Encoding-0.03, Term::ReadKey-2.38, Term::ReadLine::Gnu-1.42, Term::Table-0.016, Term::UI-0.50, Test-1.26, Test2::Plugin::NoWarnings-0.09, Test2::Require::Module-0.000145, Test::ClassAPI-1.07, Test::CleanNamespaces-0.24, Test::Deep-1.130, Test::Differences-0.69, Test::Exception-0.43, Test::Fatal-0.016, Test::File::ShareDir::Dist-1.001002, Test::Harness-3.44, Test::LeakTrace-0.17, Test::Memory::Cycle-1.06, Test::More-1.302191, Test::More::UTF8-0.05, Test::Most-0.37, Test::Needs-0.002009, Test::NoWarnings-1.06, Test::Output-1.033, Test::Pod-1.52, Test::Requires-0.11, Test::RequiresInternet-0.05, Test::Simple-1.302191, Test::Version-2.09, Test::Warn-0.37, Test::Warnings-0.031, Test::Without::Module-0.20, Text::Aligner-0.16, Text::Balanced-2.06, Text::CSV-2.02, Text::CSV_XS-1.48, Text::Diff-1.45, Text::Format-0.62, Text::Glob-0.11, Text::Iconv-1.7, Text::ParseWords-3.31, Text::Soundex-3.05, Text::Table-1.134, Text::Template-1.61, Thread::Queue-3.13, Throwable-1.000, Tie::Function-0.02, Tie::IxHash-1.23, Time::HiRes-1.9764, Time::Local-1.30, Time::Piece-1.3401, Time::Piece::MySQL-0.06, Tree::DAG_Node-1.32, Try::Tiny-0.31, Types::Serialiser-1.01, Unicode::LineBreak-2019.001, UNIVERSAL::moniker-0.08, Unix::Processors-2.046, URI-5.12, URI::Escape-5.12, Variable::Magic-0.62, version-0.9929, Want-0.29, WWW::RobotRules-6.02, XML::Bare-0.53, XML::DOM-1.46, XML::Filter::BufferText-1.01, XML::NamespaceSupport-1.12, XML::Parser-2.46, XML::RegExp-0.04, XML::SAX-1.02, XML::SAX::Base-1.09, XML::SAX::Expat-0.51, XML::SAX::Writer-0.57, XML::Simple-2.25, XML::Tiny-2.07, XML::Twig-3.52, XML::XPath-1.48, XSLoader-0.24, YAML-1.30, YAML::Tiny-1.73</p>"},{"location":"available_software/detail/Perl/#perl5341-gcccore-1130","title":"Perl/5.34.1-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Algorithm::Dependency-1.112, Algorithm::Diff-1.201, aliased-0.34, AnyEvent-7.17, App::Cmd-0.334, App::cpanminus-1.7046, AppConfig-1.71, Archive::Extract-0.88, Array::Transpose-0.06, Array::Utils-0.5, Authen::NTLM-1.09, Authen::SASL-2.16, AutoLoader-5.74, B::Hooks::EndOfScope-0.26, B::Lint-1.20, boolean-0.46, Business::ISBN-3.007, Business::ISBN::Data-20210112.006, Canary::Stability-2013, Capture::Tiny-0.48, Carp-1.50, Carp::Clan-6.08, Carp::Heavy-1.50, Class::Accessor-0.51, Class::Data::Inheritable-0.09, Class::DBI-v3.0.17, Class::DBI::SQLite-0.11, Class::Inspector-1.36, Class::ISA-0.36, Class::Load-0.25, Class::Load::XS-0.10, Class::Singleton-1.6, Class::Tiny-1.008, Class::Trigger-0.15, Clone-0.45, Clone::Choose-0.010, common::sense-3.75, Config::General-2.65, Config::INI-0.027, Config::MVP-2.200012, Config::Simple-4.58, Config::Tiny-2.28, constant-1.33, CPAN::Meta::Check-0.014, CPANPLUS-0.9914, Crypt::DES-2.07, Crypt::Rijndael-1.16, Cwd-3.75, Cwd::Guard-0.05, Data::Dump-1.25, Data::Dumper-2.183, Data::Dumper::Concise-2.023, Data::Grove-0.08, Data::OptList-0.112, Data::Section-0.200007, Data::Section::Simple-0.07, Data::Stag-0.14, Data::Types-0.17, Data::UUID-1.226, Date::Handler-1.2, Date::Language-2.33, DateTime-1.58, DateTime::Locale-1.35, DateTime::TimeZone-2.52, DateTime::Tiny-1.07, DBD::CSV-0.59, DBD::SQLite-1.70, DBI-1.643, DBIx::Admin::TableInfo-3.04, DBIx::ContextualFetch-1.03, DBIx::Simple-1.37, Devel::CheckCompiler-0.07, Devel::CheckLib-1.14, Devel::Cycle-1.12, Devel::GlobalDestruction-0.14, Devel::OverloadInfo-0.007, Devel::Size-0.83, Devel::StackTrace-2.04, Digest::HMAC-1.04, Digest::MD5::File-0.08, Digest::SHA1-2.13, Dist::CheckConflicts-0.11, Dist::Zilla-6.024, Email::Date::Format-1.005, Encode-3.17, Encode::Locale-1.05, Error-0.17029, Eval::Closure-0.14, Exception::Class-1.45, Expect-1.35, Exporter-5.74, Exporter::Declare-0.114, Exporter::Tiny-1.002002, ExtUtils::CBuilder-0.280236, ExtUtils::Config-0.008, ExtUtils::Constant-0.25, ExtUtils::CppGuess-0.26, ExtUtils::Helpers-0.026, ExtUtils::InstallPaths-0.012, ExtUtils::MakeMaker-7.64, ExtUtils::ParseXS-3.44, Fennec::Lite-0.004, File::CheckTree-4.42, File::Copy::Recursive-0.45, File::Copy::Recursive::Reduced-0.006, File::Find::Rule-0.34, File::Find::Rule::Perl-1.16, File::Grep-0.02, File::HomeDir-1.006, File::Listing-6.15, File::Next-1.18, File::Path-2.18, File::pushd-1.016, File::Remove-1.60, File::ShareDir-1.118, File::ShareDir::Install-0.13, File::Slurp-9999.32, File::Slurp::Tiny-0.004, File::Slurper-0.013, File::Spec-3.75, File::Temp-0.2311, File::Which-1.27, Font::TTF-1.06, Getopt::Long-2.52, Getopt::Long::Descriptive-0.110, Git-0.42, GO-0.04, GO::Utils-0.15, Graph-0.9725, Graph::ReadWrite-2.10, Hash::Merge-0.302, Heap-0.80, HTML::Entities::Interpolate-1.10, HTML::Form-6.07, HTML::Parser-3.78, HTML::Tagset-3.20, HTML::Template-2.97, HTML::Tree-5.07, HTTP::Cookies-6.10, HTTP::Daemon-6.14, HTTP::Date-6.05, HTTP::Negotiate-6.01, HTTP::Request-6.36, HTTP::Tiny-0.080, if-0.0608, Ima::DBI-0.35, Import::Into-1.002005, Importer-0.026, Inline-0.86, IO::HTML-1.004, IO::Socket::SSL-2.074, IO::String-1.08, IO::Stringy-2.113, IO::Tty-1.16, IPC::Cmd-1.04, IPC::Run-20200505.0, IPC::Run3-0.048, IPC::System::Simple-1.30, JSON-4.05, JSON::XS-4.03, Lingua::EN::PluralToSingular-0.21, List::AllUtils-0.19, List::MoreUtils-0.430, List::MoreUtils::XS-0.430, List::SomeUtils-0.58, List::Util-1.62, List::UtilsBy-0.11, local::lib-2.000029, Locale::Maketext::Simple-0.21, Log::Dispatch-2.70, Log::Dispatchouli-2.023, Log::Handler-0.90, Log::Log4perl-1.54, Log::Message-0.08, Log::Message::Simple-0.10, Log::Report-1.33, Log::Report::Optional-1.07, Logger::Simple-2.0, LWP::MediaTypes-6.04, LWP::Protocol::https-6.10, LWP::Simple-6.55, Mail::Util-2.21, Math::Bezier-0.01, Math::CDF-0.1, Math::Round-0.07, Math::Utils-1.14, Math::VecStat-0.08, MCE::Mutex-1.878, Meta::Builder-0.004, MIME::Base64-3.16, MIME::Charset-1.012.2, MIME::Lite-3.033, MIME::Types-2.22, Mixin::Linewise::Readers-0.110, Mock::Quick-1.111, Module::Build-0.4231, Module::Build::Tiny-0.039, Module::Build::XSUtil-0.19, Module::CoreList-5.20220420, Module::Implementation-0.09, Module::Install-1.19, Module::Load-0.36, Module::Load::Conditional-0.74, Module::Metadata-1.000037, Module::Path-0.19, Module::Pluggable-5.2, Module::Runtime-0.016, Module::Runtime::Conflicts-0.003, Moo-2.005004, Moose-2.2201, MooseX::LazyRequire-0.11, MooseX::OneArgNew-0.005, MooseX::Role::Parameterized-1.11, MooseX::SetOnce-0.201, MooseX::Types-0.50, MooseX::Types::Perl-0.101343, Mouse-v2.5.10, Mozilla::CA-20211001, MRO::Compat-0.15, namespace::autoclean-0.29, namespace::clean-0.27, Net::Domain-3.13, Net::HTTP-6.22, Net::SMTP::SSL-1.04, Net::SNMP-v6.0.1, Net::SSLeay-1.92, Number::Compare-0.03, Number::Format-1.75, Object::Accessor-0.48, Object::InsideOut-4.05, Package::Constants-0.06, Package::DeprecationManager-0.17, Package::Stash-0.40, Package::Stash::XS-0.29, PadWalker-2.5, Parallel::ForkManager-2.02, Params::Check-0.38, Params::Util-1.102, Params::Validate-1.30, Params::ValidationCompiler-0.30, parent-0.238, Parse::RecDescent-1.967015, Path::Tiny-0.122, PDF::API2-2.043, Perl::OSType-1.010, PerlIO::utf8_strict-0.009, Pod::Elemental-0.103005, Pod::Escapes-1.07, Pod::Eventual-0.094002, Pod::LaTeX-0.61, Pod::Man-4.14, Pod::Parser-1.65, Pod::Plainer-1.04, Pod::POM-2.01, Pod::Simple-3.43, Pod::Weaver-4.018, Readonly-2.05, Regexp::Common-2017060201, Role::HasMessage-0.006, Role::Identifiable::HasIdent-0.007, Role::Tiny-2.002004, Scalar::Util-1.56, Scalar::Util::Numeric-0.40, Scope::Guard-0.21, Set::Array-0.30, Set::IntervalTree-0.12, Set::IntSpan-1.19, Set::IntSpan::Fast-1.15, Set::Object-1.42, Set::Scalar-1.29, Shell-0.73, Socket-2.032, Software::License-0.104001, Specio-0.47, SQL::Abstract-2.000001, SQL::Statement-1.414, Statistics::Basic-1.6611, Statistics::Descriptive-3.0800, Storable-3.25, strictures-2.000006, String::Flogger-1.101245, String::Print-0.94, String::RewritePrefix-0.008, String::Truncate-1.100602, Sub::Exporter-0.988, Sub::Exporter::ForMethods-0.100054, Sub::Exporter::Progressive-0.001013, Sub::Identify-0.14, Sub::Info-0.002, Sub::Install-0.928, Sub::Name-0.26, Sub::Quote-2.006006, Sub::Uplevel-0.2800, Sub::Uplevel-0.2800, SVG-2.86, Switch-2.17, Sys::Info-0.7811, Sys::Info::Base-0.7807, Sys::Info::Driver::Linux-0.7905, Sys::Info::Driver::Unknown-0.79, Template-3.100, Template::Plugin::Number::Format-1.06, Term::Encoding-0.03, Term::ReadKey-2.38, Term::ReadLine::Gnu-1.42, Term::Table-0.016, Term::UI-0.50, Test-1.26, Test2::Plugin::NoWarnings-0.09, Test2::Require::Module-0.000145, Test::ClassAPI-1.07, Test::CleanNamespaces-0.24, Test::Deep-1.130, Test::Differences-0.69, Test::Exception-0.43, Test::Fatal-0.016, Test::File::ShareDir::Dist-1.001002, Test::Harness-3.44, Test::LeakTrace-0.17, Test::Memory::Cycle-1.06, Test::More-1.302190, Test::More::UTF8-0.05, Test::Most-0.37, Test::Needs-0.002009, Test::NoWarnings-1.06, Test::Output-1.033, Test::Pod-1.52, Test::Requires-0.11, Test::RequiresInternet-0.05, Test::Simple-1.302190, Test::Version-2.09, Test::Warn-0.36, Test::Warnings-0.031, Test::Without::Module-0.20, Text::Aligner-0.16, Text::Balanced-2.04, Text::CSV-2.01, Text::CSV_XS-1.47, Text::Diff-1.45, Text::Format-0.62, Text::Glob-0.11, Text::Iconv-1.7, Text::ParseWords-3.31, Text::Soundex-3.05, Text::Table-1.134, Text::Template-1.60, Thread::Queue-3.13, Throwable-1.000, Tie::Function-0.02, Tie::IxHash-1.23, Time::HiRes-1.9764, Time::Local-1.30, Time::Piece-1.3401, Time::Piece::MySQL-0.06, Tree::DAG_Node-1.32, Try::Tiny-0.31, Types::Serialiser-1.01, Unicode::LineBreak-2019.001, UNIVERSAL::moniker-0.08, Unix::Processors-2.046, URI-5.10, URI::Escape-5.10, Variable::Magic-0.62, version-0.9929, Want-0.29, WWW::RobotRules-6.02, XML::Bare-0.53, XML::DOM-1.46, XML::Filter::BufferText-1.01, XML::NamespaceSupport-1.12, XML::Parser-2.46, XML::RegExp-0.04, XML::SAX-1.02, XML::SAX::Base-1.09, XML::SAX::Expat-0.51, XML::SAX::Writer-0.57, XML::Simple-2.25, XML::Tiny-2.07, XML::Twig-3.52, XML::XPath-1.44, XSLoader-0.24, YAML-1.30, YAML::Tiny-1.73</p>"},{"location":"available_software/detail/Perl/#perl5320-gcccore-1020","title":"Perl/5.32.0-GCCcore-10.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Algorithm::Dependency-1.112, Algorithm::Diff-1.1903, aliased-0.34, AnyEvent-7.17, App::Cmd-0.331, App::cpanminus-1.7044, AppConfig-1.71, Archive::Extract-0.86, Array::Transpose-0.06, Array::Utils-0.5, Authen::NTLM-1.09, Authen::SASL-2.16, AutoLoader-5.74, B::Hooks::EndOfScope-0.24, B::Lint-1.20, boolean-0.46, Business::ISBN-3.005, Business::ISBN::Data-20191107, Canary::Stability-2013, Capture::Tiny-0.48, Carp-1.50, Carp::Clan-6.08, Carp::Heavy-1.50, Class::Accessor-0.51, Class::Data::Inheritable-0.08, Class::DBI-v3.0.17, Class::DBI::SQLite-0.11, Class::Inspector-1.36, Class::ISA-0.36, Class::Load-0.25, Class::Load::XS-0.10, Class::Singleton-1.5, Class::Trigger-0.15, Clone-0.45, Clone::Choose-0.010, common::sense-3.75, Config::General-2.63, Config::INI-0.025, Config::MVP-2.200011, Config::Simple-4.58, Config::Tiny-2.24, constant-1.33, CPANPLUS-0.9908, Crypt::DES-2.07, Crypt::Rijndael-1.14, Cwd-3.75, Cwd::Guard-0.05, Data::Dump-1.23, Data::Dumper-2.173, Data::Dumper::Concise-2.023, Data::Grove-0.08, Data::OptList-0.110, Data::Section-0.200007, Data::Section::Simple-0.07, Data::Stag-0.14, Data::Types-0.17, Data::UUID-1.226, Date::Handler-1.2, Date::Language-2.33, DateTime-1.52, DateTime::Locale-1.26, DateTime::TimeZone-2.39, DateTime::Tiny-1.07, DBD::CSV-0.55, DBD::SQLite-1.64, DBI-1.643, DBIx::Admin::TableInfo-3.03, DBIx::ContextualFetch-1.03, DBIx::Simple-1.37, Devel::CheckCompiler-0.07, Devel::CheckLib-1.14, Devel::GlobalDestruction-0.14, Devel::OverloadInfo-0.005, Devel::StackTrace-2.04, Digest::HMAC-1.03, Digest::MD5::File-0.08, Digest::SHA1-2.13, Dist::CheckConflicts-0.11, Dist::Zilla-6.015, Email::Date::Format-1.005, Encode-3.07, Encode::Locale-1.05, Error-0.17029, Eval::Closure-0.14, Exception::Class-1.44, Expect-1.35, Exporter-5.74, Exporter::Declare-0.114, Exporter::Tiny-1.002002, ExtUtils::CBuilder-0.280234, ExtUtils::Config-0.008, ExtUtils::Constant-0.25, ExtUtils::CppGuess-0.21, ExtUtils::Helpers-0.026, ExtUtils::InstallPaths-0.012, ExtUtils::MakeMaker-7.46, ExtUtils::ParseXS-3.35, Fennec::Lite-0.004, File::CheckTree-4.42, File::Copy::Recursive-0.45, File::Copy::Recursive::Reduced-0.006, File::Find::Rule-0.34, File::Find::Rule::Perl-1.15, File::Grep-0.02, File::HomeDir-1.004, File::Listing-6.04, File::Next-1.18, File::Path-2.17, File::pushd-1.016, File::Remove-1.58, File::ShareDir-1.116, File::ShareDir::Install-0.13, File::Slurp-9999.32, File::Slurp::Tiny-0.004, File::Slurper-0.012, File::Spec-3.75, File::Temp-0.2309, File::Which-1.23, Font::TTF-1.06, Getopt::Long-2.52, Getopt::Long::Descriptive-0.105, Git-0.42, GO-0.04, GO::Utils-0.15, Graph-0.9704, Graph::ReadWrite-2.09, Hash::Merge-0.302, HTML::Entities::Interpolate-1.10, HTML::Form-6.07, HTML::Parser-3.72, HTML::Tagset-3.20, HTML::Tree-5.07, HTTP::Cookies-6.08, HTTP::Daemon-6.12, HTTP::Date-6.05, HTTP::Negotiate-6.01, HTTP::Request-6.25, HTTP::Tiny-0.076, if-0.0608, Ima::DBI-0.35, Import::Into-1.002005, Inline-0.86, IO::Compress::Bzip2-2.093, IO::File-1.42, IO::Handle-1.42, IO::HTML-1.001, IO::Seekable-1.42, IO::Select-1.42, IO::Socket-1.42, IO::Socket::SSL-2.068, IO::String-1.08, IO::Stringy-2.113, IO::Tty-1.14, IPC::Cmd-1.04, IPC::Run-20200505.0, IPC::Run3-0.048, JSON-4.02, JSON::XS-4.03, Lingua::EN::PluralToSingular-0.21, List::AllUtils-0.16, List::MoreUtils-0.428, List::MoreUtils::XS-0.430, List::SomeUtils-0.58, List::Util-1.55, List::UtilsBy-0.11, Locale::Maketext::Simple-0.21, Log::Dispatch-2.70, Log::Dispatchouli-2.021, Log::Handler-0.90, Log::Log4perl-1.50, Log::Message-0.08, Log::Message::Simple-0.10, Logger::Simple-2.0, LWP::MediaTypes-6.04, LWP::Protocol::https-6.09, LWP::Simple-6.47, Mail::Util-2.21, Math::Bezier-0.01, Math::CDF-0.1, Math::Round-0.07, Math::Utils-1.14, Math::VecStat-0.08, MCE::Mutex-1.874, Meta::Builder-0.004, MIME::Base64-3.15, MIME::Lite-3.031, MIME::Types-2.17, Mixin::Linewise::Readers-0.108, Mock::Quick-1.111, Module::Build-0.4231, Module::Build::Tiny-0.039, Module::Build::XSUtil-0.19, Module::CoreList-5.20200717, Module::Implementation-0.09, Module::Install-1.19, Module::Load-0.34, Module::Load::Conditional-0.72, Module::Metadata-1.000037, Module::Pluggable-5.2, Module::Runtime-0.016, Moo-2.004000, Moose-2.2013, MooseX::LazyRequire-0.11, MooseX::OneArgNew-0.005, MooseX::Role::Parameterized-1.11, MooseX::SetOnce-0.200002, MooseX::Types-0.50, MooseX::Types::Perl-0.101343, Mouse-v2.5.10, Mozilla::CA-20200520, MRO::Compat-0.13, namespace::autoclean-0.29, namespace::clean-0.27, Net::Domain-3.11, Net::HTTP-6.19, Net::SMTP::SSL-1.04, Net::SNMP-v6.0.1, Net::SSLeay-1.88, Number::Compare-0.03, Number::Format-1.75, Object::Accessor-0.48, Object::InsideOut-4.05, Package::Constants-0.06, Package::DeprecationManager-0.17, Package::Stash-0.38, Package::Stash::XS-0.29, Parallel::ForkManager-2.02, Params::Check-0.38, Params::Util-1.07, Params::Validate-1.29, Params::ValidationCompiler-0.30, parent-0.238, Parse::RecDescent-1.967015, Path::Tiny-0.114, PDF::API2-2.037, Perl::OSType-1.010, PerlIO::utf8_strict-0.007, Pod::Elemental-0.103005, Pod::Escapes-1.07, Pod::Eventual-0.094001, Pod::LaTeX-0.61, Pod::Man-4.14, Pod::Plainer-1.04, Pod::POM-2.01, Pod::Simple-3.40, Pod::Weaver-4.015, Readonly-2.05, Regexp::Common-2017060201, Role::HasMessage-0.006, Role::Identifiable::HasIdent-0.007, Role::Tiny-2.001004, Scalar::List::Utils-1.55, Scalar::Util-1.55, Scalar::Util::Numeric-0.40, Set::Array-0.30, Set::IntervalTree-0.12, Set::IntSpan-1.19, Set::IntSpan::Fast-1.15, Set::Scalar-1.29, Shell-0.73, Socket-2.030, Software::License-0.103014, Specio-0.46, SQL::Abstract-1.87, SQL::Statement-1.412, Statistics::Basic-1.6611, Statistics::Descriptive-3.0702, Storable-3.15, strictures-2.000006, String::Flogger-1.101245, String::RewritePrefix-0.008, String::Truncate-1.100602, Sub::Exporter-0.987, Sub::Exporter::ForMethods-0.100052, Sub::Exporter::Progressive-0.001013, Sub::Identify-0.14, Sub::Install-0.928, Sub::Name-0.26, Sub::Quote-2.006006, Sub::Uplevel-0.2800, Sub::Uplevel-0.2800, SVG-2.85, Switch-2.17, Template-3.009, Template::Plugin::Number::Format-1.06, Term::Encoding-0.03, Term::ReadKey-2.38, Term::ReadLine::Gnu-1.36, Term::UI-0.46, Test-1.26, Test::ClassAPI-1.07, Test::Deep-1.130, Test::Differences-0.67, Test::Exception-0.43, Test::Fatal-0.016, Test::Harness-3.42, Test::LeakTrace-0.16, Test::More-1.302177, Test::Most-0.37, Test::NoWarnings-1.04, Test::Output-1.031, Test::Pod-1.52, Test::Requires-0.11, Test::RequiresInternet-0.05, Test::Simple-1.302177, Test::Version-2.09, Test::Warn-0.36, Test::Warnings-0.030, Text::Aligner-0.16, Text::Balanced-2.03, Text::CSV-2.00, Text::CSV_XS-1.44, Text::Diff-1.45, Text::Format-0.61, Text::Glob-0.11, Text::Iconv-1.7, Text::ParseWords-3.30, Text::Soundex-3.05, Text::Table-1.134, Text::Template-1.59, Thread::Queue-3.13, Throwable-0.200013, Tie::Function-0.02, Tie::IxHash-1.23, Time::HiRes-1.9764, Time::Local-1.30, Time::Piece-1.3401, Time::Piece::MySQL-0.06, Tree::DAG_Node-1.31, Try::Tiny-0.30, Types::Serialiser-1.0, UNIVERSAL::moniker-0.08, URI-1.76, URI::Escape-1.76, version-0.9924, Want-0.29, WWW::RobotRules-6.02, XML::Bare-0.53, XML::DOM-1.46, XML::Filter::BufferText-1.01, XML::NamespaceSupport-1.12, XML::Parser-2.46, XML::RegExp-0.04, XML::SAX-1.02, XML::SAX::Base-1.09, XML::SAX::Expat-0.51, XML::SAX::Writer-0.57, XML::Simple-2.25, XML::Tiny-2.07, XML::Twig-3.52, XML::XPath-1.44, XSLoader-0.24, YAML-1.30, YAML::Tiny-1.73</p>"},{"location":"available_software/detail/Pillow-SIMD/","title":"Pillow-SIMD","text":"<p>Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors.</p> <p>https://github.com/uploadcare/pillow-simd</p>"},{"location":"available_software/detail/Pillow-SIMD/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pillow-SIMD installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pillow-SIMD, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pillow-SIMD/9.5.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pillow-SIMD/9.5.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Pillow/","title":"Pillow","text":"<p>Pillow is the 'friendly PIL fork' by Alex Clark and Contributors. PIL is the Python Imaging Library by Fredrik Lundh and Contributors.</p> <p>https://pillow.readthedocs.org/</p>"},{"location":"available_software/detail/Pillow/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pillow installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pillow, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pillow/10.2.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pillow/10.2.0-GCCcore-13.2.0 - x - - - - Pillow/10.0.0-GCCcore-12.3.0 - x - - - - Pillow/9.4.0-GCCcore-12.2.0 - x - - - - Pillow/9.1.1-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Pint/","title":"Pint","text":"<p>Pint is a Python package to define, operate andmanipulate physical quantities: the product of a numerical value and aunit of measurement. It allows arithmetic operations between them andconversions from and to different units.</p> <p>https://github.com/hgrecco/pint</p>"},{"location":"available_software/detail/Pint/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pint installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pint, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pint/0.23-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pint/0.23-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PnetCDF/","title":"PnetCDF","text":"<p>Parallel netCDF: A Parallel I/O Library for NetCDF File Access</p> <p>https://parallel-netcdf.github.io/</p>"},{"location":"available_software/detail/PnetCDF/#available-modules","title":"Available modules","text":"<p>The overview below shows which PnetCDF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PnetCDF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PnetCDF/1.12.3-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PnetCDF/1.12.3-gompi-2023a - x - - - - PnetCDF/1.12.3-gompi-2022b - x - - - - PnetCDF/1.12.3-gompi-2022a - x - - - -"},{"location":"available_software/detail/PostgreSQL/","title":"PostgreSQL","text":"<p>PostgreSQL is a powerful, open source object-relational database system. It is fully ACID compliant, has full support for foreign keys, joins, views, triggers, and stored procedures (in multiple languages). It includes most SQL:2008 data types, including INTEGER, NUMERIC, BOOLEAN, CHAR, VARCHAR, DATE, INTERVAL, and TIMESTAMP. It also supports storage of binary large objects, including pictures, sounds, or video. It has native programming interfaces for C/C++, Java, .Net, Perl, Python, Ruby, Tcl, ODBC, among others, and exceptional documentation.</p> <p>https://www.postgresql.org/</p>"},{"location":"available_software/detail/PostgreSQL/#available-modules","title":"Available modules","text":"<p>The overview below shows which PostgreSQL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PostgreSQL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PostgreSQL/16.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PostgreSQL/16.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/ProtHint/","title":"ProtHint","text":"<p>ProtHint is a pipeline for predicting and scoring hints (in the form of introns, start and stop codons) in the genome of interest by mapping and spliced aligning predicted genes to a database of reference protein sequences.</p> <p>https://github.com/gatech-genemark/ProtHint</p>"},{"location":"available_software/detail/ProtHint/#available-modules","title":"Available modules","text":"<p>The overview below shows which ProtHint installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ProtHint, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ProtHint/2.6.0-foss-2023a-Python-3.11.3\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ProtHint/2.6.0-foss-2023a-Python-3.11.3 - x - - - -"},{"location":"available_software/detail/PuLP/","title":"PuLP","text":"<p>PuLP is an LP modeler written in Python. PuLP can generate MPS or LP files andcall GLPK, COIN-OR CLP/CBC, CPLEX, GUROBI, MOSEK, XPRESS, CHOCO, MIPCL, SCIP tosolve linear problems.</p> <p>https://github.com/coin-or/pulp</p>"},{"location":"available_software/detail/PuLP/#available-modules","title":"Available modules","text":"<p>The overview below shows which PuLP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PuLP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PuLP/2.8.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PuLP/2.8.0-foss-2023a - x - - - -"},{"location":"available_software/detail/PyBioLib/","title":"PyBioLib","text":"<p>PyBioLib is a Python package for running BioLib applications from Pythonscripts and the command line.BioLib is a library of biological data science applications. Applications onBioLib range from small bioinformatics utilities to state-of-the-art machinelearning algorithms for predicting characteristics of biological molecules.</p> <p>https://biolib.com/</p>"},{"location":"available_software/detail/PyBioLib/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyBioLib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyBioLib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyBioLib/1.1.988-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyBioLib/1.1.988-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PyBioLib/#pybiolib11988-gcccore-1230","title":"PyBioLib/1.1.988-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>commonmark-0.9.1, docker-6.0.1, gunicorn-20.1.0, pybiolib-1.1.988, pycryptodome-3.17, PyJWT-2.6.0, rich-12.6.0, websocket-client-1.5.1</p>"},{"location":"available_software/detail/PyInstaller/","title":"PyInstaller","text":"<p>PyInstaller bundles a Python application and all its dependencies into a single package. The user can run the packaged app without installing a Python interpreter or any modules.</p> <p>https://pyinstaller.org/en/stable/</p>"},{"location":"available_software/detail/PyInstaller/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyInstaller installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyInstaller, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyInstaller/6.3.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyInstaller/6.3.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PyInstaller/#pyinstaller630-gcccore-1230","title":"PyInstaller/6.3.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>altgraph-0.17.4, packaging-23.2, PyInstaller-6.3.0, pyinstaller-hooks-contrib-2023.12</p>"},{"location":"available_software/detail/PyMOL/","title":"PyMOL","text":"<p>PyMOL is a Python-enhanced molecular graphics tool. It excels at 3D             visualization of proteins, small molecules, density, surfaces, and trajectories.It also includes molecular editing, ray tracing, and movies. Open Source PyMOL  is free to everyone!</p> <p>https://github.com/schrodinger/pymol-open-source</p>"},{"location":"available_software/detail/PyMOL/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyMOL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyMOL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyMOL/2.5.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyMOL/2.5.0-foss-2023a - x - - - -"},{"location":"available_software/detail/PyMOL/#pymol250-foss-2023a","title":"PyMOL/2.5.0-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>Pmw-2.0.1, PyMOL-2.5.0</p>"},{"location":"available_software/detail/PyOpenGL/","title":"PyOpenGL","text":"<p>PyOpenGL is the most common cross platform Python binding to OpenGL and related APIs.</p> <p>http://pyopengl.sourceforge.net</p>"},{"location":"available_software/detail/PyOpenGL/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyOpenGL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyOpenGL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyOpenGL/3.1.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyOpenGL/3.1.7-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PyOpenGL/#pyopengl317-gcccore-1230","title":"PyOpenGL/3.1.7-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>PyOpenGL-3.1.7, PyOpenGL-accelerate-3.1.7</p>"},{"location":"available_software/detail/PyQt-builder/","title":"PyQt-builder","text":"<p>PyQt-builder is the PEP 517 compliant build system for PyQt and projects that   extend PyQt. It extends the SIP build system and uses Qt\u2019s qmake to perform the actual compilation and installation of extension modules.</p> <p>http://www.example.com</p>"},{"location":"available_software/detail/PyQt-builder/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyQt-builder installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyQt-builder, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyQt-builder/1.15.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyQt-builder/1.15.4-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PyQt-builder/#pyqt-builder1154-gcccore-1230","title":"PyQt-builder/1.15.4-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>PyQt-builder-1.15.4</p>"},{"location":"available_software/detail/PyQt5/","title":"PyQt5","text":"<p>PyQt5 is a set of Python bindings for v5 of the Qt application framework from The Qt Company.This bundle includes PyQtWebEngine, a set of Python bindings for The Qt Company\u2019s Qt WebEngine framework.</p> <p>https://www.riverbankcomputing.com/software/pyqt</p>"},{"location":"available_software/detail/PyQt5/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyQt5 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyQt5, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyQt5/5.15.10-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyQt5/5.15.10-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/PyTorch/","title":"PyTorch","text":"<p>Tensors and Dynamic neural networks in Python with strong GPU acceleration.PyTorch is a deep learning framework that puts Python first.</p> <p>https://pytorch.org/</p>"},{"location":"available_software/detail/PyTorch/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyTorch installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyTorch, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyTorch/2.1.2-foss-2023a-CUDA-12.1.1 - - - x - x PyTorch/1.13.1-foss-2022a-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/PyYAML/","title":"PyYAML","text":"<p>PyYAML is a YAML parser and emitter for the Python programming language.</p> <p>https://github.com/yaml/pyyaml</p>"},{"location":"available_software/detail/PyYAML/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyYAML installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyYAML, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyYAML/6.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyYAML/6.0-GCCcore-12.3.0 - x - - - - PyYAML/6.0-GCCcore-12.2.0 - x - - - - PyYAML/6.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/PyZMQ/","title":"PyZMQ","text":"<p>Python bindings for ZeroMQ</p> <p>https://www.zeromq.org/bindings:python</p>"},{"location":"available_software/detail/PyZMQ/#available-modules","title":"Available modules","text":"<p>The overview below shows which PyZMQ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using PyZMQ, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load PyZMQ/25.1.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 PyZMQ/25.1.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Pysam/","title":"Pysam","text":"<p>Pysam is a python module for reading and manipulating Samfiles. It's a lightweight wrapper of the samtools C-API. Pysam also includes an interface for tabix.</p> <p>https://github.com/pysam-developers/pysam</p>"},{"location":"available_software/detail/Pysam/#available-modules","title":"Available modules","text":"<p>The overview below shows which Pysam installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Pysam, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Pysam/0.22.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Pysam/0.22.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Python-bundle-PyPI/","title":"Python-bundle-PyPI","text":"<p>Bundle of Python packages from PyPI</p> <p>https://python.org/</p>"},{"location":"available_software/detail/Python-bundle-PyPI/#available-modules","title":"Available modules","text":"<p>The overview below shows which Python-bundle-PyPI installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Python-bundle-PyPI, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Python-bundle-PyPI/2023.10-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Python-bundle-PyPI/2023.10-GCCcore-13.2.0 - x - - - - Python-bundle-PyPI/2023.06-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/Python-bundle-PyPI/#python-bundle-pypi202310-gcccore-1320","title":"Python-bundle-PyPI/2023.10-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>alabaster-0.7.13, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.1, attrs-23.1.0, Babel-2.13.1, backports.entry-points-selectable-1.2.0, backports.functools_lru_cache-1.6.6, bitarray-2.8.2, bitstring-4.1.2, blist-1.3.6, cachecontrol-0.13.1, cachy-0.3.0, certifi-2023.7.22, cffi-1.16.0, chardet-5.2.0, charset-normalizer-3.3.1, cleo-2.0.1, click-8.1.7, cloudpickle-3.0.0, colorama-0.4.6, commonmark-0.9.1, crashtest-0.4.1, Cython-3.0.4, decorator-5.1.1, distlib-0.3.7, distro-1.8.0, docopt-0.6.2, docutils-0.20.1, doit-0.36.0, dulwich-0.21.6, ecdsa-0.18.0, editables-0.5, exceptiongroup-1.1.3, execnet-2.0.2, filelock-3.13.0, fsspec-2023.10.0, future-0.18.3, glob2-0.7, html5lib-1.1, idna-3.4, imagesize-1.4.1, importlib_metadata-6.8.0, importlib_resources-6.1.0, iniconfig-2.0.0, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jaraco.classes-3.3.0, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.3.2, jsonschema-4.17.3, keyring-24.2.0, keyrings.alt-5.0.0, liac-arff-2.5.0, lockfile-0.12.2, markdown-it-py-3.0.0, MarkupSafe-2.1.3, mdurl-0.1.2, mock-5.1.0, more-itertools-10.1.0, msgpack-1.0.7, netaddr-0.9.0, netifaces-0.11.0, packaging-23.2, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.11.2, pbr-5.11.1, pexpect-4.8.0, pkginfo-1.9.6, platformdirs-3.11.0, pluggy-1.3.0, pooch-1.8.0, psutil-5.9.6, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.5.0, pycparser-2.21, pycryptodome-3.19.0, pydevtool-0.3.0, Pygments-2.16.1, Pygments-2.16.1, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.1.1, pyrsistent-0.20.0, pytest-7.4.3, pytest-xdist-3.3.1, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2023.3.post1, rapidfuzz-2.15.2, regex-2023.10.3, requests-2.31.0, requests-toolbelt-1.0.0, rich-13.6.0, rich-click-1.7.0, scandir-1.10.0, SecretStorage-3.3.3, semantic_version-2.10.0, shellingham-1.5.4, simplegeneric-0.8.1, simplejson-3.19.2, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, sphinx-7.2.6, sphinx-bootstrap-theme-0.8.1, sphinxcontrib-jsmath-1.0.1, sphinxcontrib_applehelp-1.0.7, sphinxcontrib_devhelp-1.0.5, sphinxcontrib_htmlhelp-2.0.4, sphinxcontrib_qthelp-1.0.6, sphinxcontrib_serializinghtml-1.1.9, sphinxcontrib_websupport-1.2.6, tabulate-0.9.0, threadpoolctl-3.2.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.12.1, typing_extensions-4.8.0, ujson-5.8.0, urllib3-2.0.7, wcwidth-0.2.8, webencodings-0.5.1, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.17.0</p>"},{"location":"available_software/detail/Python-bundle-PyPI/#python-bundle-pypi202306-gcccore-1230","title":"Python-bundle-PyPI/2023.06-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>alabaster-0.7.13, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.1, attrs-23.1.0, Babel-2.12.1, backports.entry-points-selectable-1.2.0, backports.functools_lru_cache-1.6.5, bitstring-4.0.2, blist-1.3.6, CacheControl-0.12.14, cachy-0.3.0, certifi-2023.5.7, cffi-1.15.1, chardet-5.1.0, charset-normalizer-3.1.0, cleo-2.0.1, click-8.1.3, cloudpickle-2.2.1, colorama-0.4.6, commonmark-0.9.1, crashtest-0.4.1, Cython-0.29.35, decorator-5.1.1, distlib-0.3.6, distro-1.8.0, docopt-0.6.2, docutils-0.20.1, doit-0.36.0, dulwich-0.21.5, ecdsa-0.18.0, editables-0.3, exceptiongroup-1.1.1, execnet-1.9.0, filelock-3.12.2, fsspec-2023.6.0, future-0.18.3, glob2-0.7, html5lib-1.1, idna-3.4, imagesize-1.4.1, importlib_metadata-6.7.0, importlib_resources-5.12.0, iniconfig-2.0.0, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jaraco.classes-3.2.3, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.2.0, jsonschema-4.17.3, keyring-23.13.1, keyrings.alt-4.2.0, liac-arff-2.5.0, lockfile-0.12.2, markdown-it-py-3.0.0, MarkupSafe-2.1.3, mdurl-0.1.2, mock-5.0.2, more-itertools-9.1.0, msgpack-1.0.5, netaddr-0.8.0, netifaces-0.11.0, packaging-23.1, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.11.1, pbr-5.11.1, pexpect-4.8.0, pkginfo-1.9.6, platformdirs-3.8.0, pluggy-1.2.0, pooch-1.7.0, psutil-5.9.5, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.5.0, pycparser-2.21, pycryptodome-3.18.0, pydevtool-0.3.0, Pygments-2.15.1, Pygments-2.15.1, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.1.0, pyrsistent-0.19.3, pytest-7.4.0, pytest-xdist-3.3.1, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2023.3, rapidfuzz-2.15.1, regex-2023.6.3, requests-2.31.0, requests-toolbelt-1.0.0, rich-13.4.2, rich-click-1.6.1, scandir-1.10.0, SecretStorage-3.3.3, semantic_version-2.10.0, shellingham-1.5.0.post1, simplegeneric-0.8.1, simplejson-3.19.1, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, Sphinx-7.0.1, sphinx-bootstrap-theme-0.8.1, sphinxcontrib-applehelp-1.0.4, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-2.0.1, sphinxcontrib-jsmath-1.0.1, sphinxcontrib-qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.5, sphinxcontrib-websupport-1.2.4, tabulate-0.9.0, threadpoolctl-3.1.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.11.8, ujson-5.8.0, urllib3-1.26.16, wcwidth-0.2.6, webencodings-0.5.1, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.15.0</p>"},{"location":"available_software/detail/Python-bundle-alphafold-singularity/","title":"Python-bundle-alphafold-singularity","text":"<p>Bundle of Python packages from PyPI</p> <p>https://python.org/</p>"},{"location":"available_software/detail/Python-bundle-alphafold-singularity/#available-modules","title":"Available modules","text":"<p>The overview below shows which Python-bundle-alphafold-singularity installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Python-bundle-alphafold-singularity, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Python-bundle-alphafold-singularity/2.3.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Python-bundle-alphafold-singularity/2.3.2-GCCcore-13.2.0 - x - - - -"},{"location":"available_software/detail/Python-bundle-alphafold-singularity/#python-bundle-alphafold-singularity232-gcccore-1320","title":"Python-bundle-alphafold-singularity/2.3.2-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>absl-py-1.4.0, spython-0.3.13</p>"},{"location":"available_software/detail/Python-bundle-proxy/","title":"Python-bundle-proxy","text":"<p>Bundle of Python packages for OnDemand proxy</p> <p>https://python.org/</p>"},{"location":"available_software/detail/Python-bundle-proxy/#available-modules","title":"Available modules","text":"<p>The overview below shows which Python-bundle-proxy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Python-bundle-proxy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Python-bundle-proxy/2023a-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Python-bundle-proxy/2023a-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Python-bundle-proxy/#python-bundle-proxy2023a-gcccore-1230","title":"Python-bundle-proxy/2023a-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Automat-0.8.0, constantly-23.10.4, hyperlink-21.0.0, incremental-22.10.0, twisted-24.3.0, versioneer-0.29, zope.interface-6.4.post2</p>"},{"location":"available_software/detail/Python/","title":"Python","text":"<p>Python is a programming language that lets you work more quickly and integrate your systems more effectively.</p> <p>https://python.org/</p>"},{"location":"available_software/detail/Python/#available-modules","title":"Available modules","text":"<p>The overview below shows which Python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Python/3.11.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Python/3.11.5-GCCcore-13.2.0 - x - - - - Python/3.11.3-GCCcore-12.3.0 - x x x x x Python/3.10.8-GCCcore-12.2.0-bare - x - - - - Python/3.10.8-GCCcore-12.2.0 - x - - - - Python/3.10.4-GCCcore-11.3.0-bare - x - - - - Python/3.10.4-GCCcore-11.3.0 - x - - - - Python/3.8.6-GCCcore-10.2.0 - x - - - - Python/2.7.18-GCCcore-12.3.0 - x - - - - Python/2.7.18-GCCcore-12.2.0-bare - x - - - - Python/2.7.18-GCCcore-11.3.0-bare - x - - - -"},{"location":"available_software/detail/Python/#python3115-gcccore-1320","title":"Python/3.11.5-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>flit_core-3.9.0, pip-23.2.1, setuptools-68.2.2, wheel-0.41.2</p>"},{"location":"available_software/detail/Python/#python3113-gcccore-1230","title":"Python/3.11.3-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>flit_core-3.9.0, packaging-23.1, pip-23.1.2, setuptools-67.7.2, setuptools_scm-7.1.0, tomli-2.0.1, typing_extensions-4.6.3, wheel-0.40.0</p>"},{"location":"available_software/detail/Python/#python3108-gcccore-1220","title":"Python/3.10.8-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.1, attrs-22.1.0, Babel-2.11.0, backports.entry-points-selectable-1.2.0, backports.functools_lru_cache-1.6.4, bcrypt-4.0.1, bitstring-3.1.9, blist-1.3.6, CacheControl-0.12.11, cachy-0.3.0, certifi-2022.9.24, cffi-1.15.1, chardet-5.0.0, charset-normalizer-2.1.1, cleo-1.0.0a5, click-8.1.3, clikit-0.6.2, cloudpickle-2.2.0, colorama-0.4.6, commonmark-0.9.1, crashtest-0.3.1, cryptography-38.0.3, Cython-0.29.32, decorator-5.1.1, distlib-0.3.6, docopt-0.6.2, docutils-0.19, doit-0.36.0, dulwich-0.20.50, ecdsa-0.18.0, editables-0.3, exceptiongroup-1.0.1, execnet-1.9.0, filelock-3.8.0, flit-3.8.0, flit_core-3.8.0, flit_scm-1.7.0, fsspec-2022.11.0, future-0.18.2, glob2-0.7, hatch_fancy_pypi_readme-22.8.0, hatch_vcs-0.2.0, hatchling-1.11.1, html5lib-1.1, idna-3.4, imagesize-1.4.1, importlib_metadata-5.0.0, importlib_resources-5.10.0, iniconfig-1.1.1, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jaraco.classes-3.2.3, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.2.0, jsonschema-4.17.0, keyring-23.11.0, keyrings.alt-4.2.0, liac-arff-2.5.0, lockfile-0.12.2, MarkupSafe-2.1.1, mock-4.0.3, more-itertools-9.0.0, msgpack-1.0.4, netaddr-0.8.0, netifaces-0.11.0, packaging-21.3, paramiko-2.12.0, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.10.1, pbr-5.11.0, pexpect-4.8.0, pip-22.3.1, pkginfo-1.8.3, platformdirs-2.5.3, pluggy-1.0.0, poetry-1.2.2, poetry-core-1.3.2, poetry_plugin_export-1.2.0, pooch-1.6.0, psutil-5.9.4, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.4.8, pycparser-2.21, pycryptodome-3.17, pydevtool-0.3.0, Pygments-2.13.0, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.0.9, pyrsistent-0.19.2, pytest-7.2.0, pytest-xdist-3.1.0, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2022.6, regex-2022.10.31, requests-2.28.1, requests-toolbelt-0.9.1, rich-13.1.0, rich-click-1.6.0, scandir-1.10.0, SecretStorage-3.3.3, semantic_version-2.10.0, setuptools-63.4.3, setuptools-rust-1.5.2, setuptools_scm-7.0.5, shellingham-1.5.0, simplegeneric-0.8.1, simplejson-3.17.6, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, Sphinx-5.3.0, sphinx-bootstrap-theme-0.8.1, sphinxcontrib-applehelp-1.0.2, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-2.0.0, sphinxcontrib-jsmath-1.0.1, sphinxcontrib-qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.5, sphinxcontrib-websupport-1.2.4, tabulate-0.9.0, threadpoolctl-3.1.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.11.6, typing_extensions-4.4.0, ujson-5.5.0, urllib3-1.26.12, virtualenv-20.16.6, wcwidth-0.2.5, webencodings-0.5.1, wheel-0.38.4, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.10.0</p>"},{"location":"available_software/detail/Python/#python3104-gcccore-1130","title":"Python/3.10.4-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.5.1, atomicwrites-1.4.0, attrs-21.4.0, Babel-2.10.1, backports.entry_points_selectable-1.1.1, backports.functools_lru_cache-1.6.4, bcrypt-3.2.2, bitstring-3.1.9, blist-1.3.6, CacheControl-0.12.11, cachy-0.3.0, certifi-2021.10.8, cffi-1.15.0, chardet-4.0.0, charset-normalizer-2.0.12, cleo-0.8.1, click-8.1.3, clikit-0.6.2, colorama-0.4.4, crashtest-0.3.1, cryptography-37.0.1, Cython-0.29.28, decorator-5.1.1, distlib-0.3.4, docopt-0.6.2, docutils-0.17.1, ecdsa-0.17.0, editables-0.3, filelock-3.6.0, flit-3.7.1, flit-core-3.7.1, fsspec-2022.3.0, future-0.18.2, glob2-0.7, html5lib-1.1, idna-3.3, imagesize-1.3.0, importlib_metadata-4.11.3, importlib_resources-5.7.1, iniconfig-1.1.1, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jeepney-0.8.0, Jinja2-3.1.2, joblib-1.1.0, jsonschema-4.4.0, keyring-23.5.0, keyrings.alt-4.1.0, liac-arff-2.5.0, lockfile-0.12.2, MarkupSafe-2.1.1, mock-4.0.3, more-itertools-8.12.0, msgpack-1.0.3, netaddr-0.8.0, netifaces-0.11.0, packaging-20.9, paramiko-2.10.4, pastel-0.2.1, pathlib2-2.3.7.post1, pathspec-0.9.0, pbr-5.8.1, pexpect-4.8.0, pip-22.0.4, pkginfo-1.8.2, platformdirs-2.4.1, pluggy-1.0.0, poetry-1.1.13, poetry-core-1.0.8, psutil-5.9.0, ptyprocess-0.7.0, py-1.11.0, py_expression_eval-0.3.14, pyasn1-0.4.8, pycparser-2.21, pycryptodome-3.17, Pygments-2.12.0, pylev-1.4.0, PyNaCl-1.5.0, pyparsing-3.0.8, pyrsistent-0.18.1, pytest-7.1.2, python-dateutil-2.8.2, pytoml-0.1.21, pytz-2022.1, regex-2022.4.24, requests-2.27.1, requests-toolbelt-0.9.1, scandir-1.10.0, SecretStorage-3.3.2, semantic_version-2.9.0, setuptools-62.1.0, setuptools-rust-1.3.0, setuptools_scm-6.4.2, shellingham-1.4.0, simplegeneric-0.8.1, simplejson-3.17.6, six-1.16.0, snowballstemmer-2.2.0, sortedcontainers-2.4.0, Sphinx-4.5.0, sphinx-bootstrap-theme-0.8.1, sphinxcontrib-applehelp-1.0.2, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-2.0.0, sphinxcontrib-jsmath-1.0.1, sphinxcontrib-qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.5, sphinxcontrib-websupport-1.2.4, tabulate-0.8.9, threadpoolctl-3.1.0, toml-0.10.2, tomli-2.0.1, tomli_w-1.0.0, tomlkit-0.10.2, typing_extensions-4.2.0, ujson-5.2.0, urllib3-1.26.9, virtualenv-20.14.1, wcwidth-0.2.5, webencodings-0.5.1, wheel-0.37.1, xlrd-2.0.1, zipfile36-0.1.3, zipp-3.8.0</p>"},{"location":"available_software/detail/Python/#python386-gcccore-1020","title":"Python/3.8.6-GCCcore-10.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>alabaster-0.7.12, appdirs-1.4.4, asn1crypto-1.4.0, atomicwrites-1.4.0, attrs-20.2.0, Babel-2.8.0, bcrypt-3.2.0, bitstring-3.1.7, blist-1.3.6, CacheControl-0.12.6, cachy-0.3.0, certifi-2020.6.20, cffi-1.14.3, chardet-3.0.4, cleo-0.8.1, click-7.1.2, clikit-0.6.2, colorama-0.4.3, crashtest-0.3.1, cryptography-3.1.1, Cython-0.29.21, decorator-4.4.2, distlib-0.3.1, docopt-0.6.2, docutils-0.16, ecdsa-0.16.0, filelock-3.0.12, flit-3.0.0, flit-core-3.0.0, fsspec-0.8.4, future-0.18.2, html5lib-1.1, idna-2.10, imagesize-1.2.0, importlib_metadata-2.0.0, iniconfig-1.0.1, intervaltree-3.1.0, intreehooks-1.0, ipaddress-1.0.23, jeepney-0.4.3, Jinja2-2.11.2, joblib-0.17.0, jsonschema-3.2.0, keyring-21.4.0, keyrings.alt-4.0.0, liac-arff-2.5.0, lockfile-0.12.2, MarkupSafe-1.1.1, mock-4.0.2, more-itertools-8.5.0, msgpack-1.0.0, netaddr-0.8.0, netifaces-0.10.9, nose-1.3.7, packaging-20.4, paramiko-2.7.2, pastel-0.2.1, pathlib2-2.3.5, paycheck-1.0.2, pbr-5.5.0, pexpect-4.8.0, pip-20.2.3, pkginfo-1.5.0.1, pluggy-0.13.1, poetry-1.1.3, poetry-core-1.0.0, psutil-5.7.2, ptyprocess-0.6.0, py-1.9.0, py_expression_eval-0.3.10, pyasn1-0.4.8, pycparser-2.20, pycrypto-2.6.1, Pygments-2.7.1, pylev-1.3.0, PyNaCl-1.4.0, pyparsing-2.4.7, pyrsistent-0.17.3, pytest-6.1.1, python-dateutil-2.8.1, pytoml-0.1.21, pytz-2020.1, regex-2020.10.11, requests-2.24.0, requests-toolbelt-0.9.1, scandir-1.10.0, SecretStorage-3.1.2, setuptools-50.3.0, setuptools_scm-4.1.2, shellingham-1.3.2, simplegeneric-0.8.1, simplejson-3.17.2, six-1.15.0, snowballstemmer-2.0.0, sortedcontainers-2.2.2, Sphinx-3.2.1, sphinx-bootstrap-theme-0.7.1, sphinxcontrib-applehelp-1.0.2, sphinxcontrib-devhelp-1.0.2, sphinxcontrib-htmlhelp-1.0.3, sphinxcontrib-jsmath-1.0.1, sphinxcontrib-qthelp-1.0.3, sphinxcontrib-serializinghtml-1.1.4, sphinxcontrib-websupport-1.2.4, tabulate-0.8.7, threadpoolctl-2.1.0, toml-0.10.1, tomlkit-0.7.0, ujson-4.0.1, urllib3-1.25.10, virtualenv-20.0.34, wcwidth-0.2.5, webencodings-0.5.1, wheel-0.35.1, xlrd-1.2.0, zipp-3.3.0</p>"},{"location":"available_software/detail/Python/#python2718-gcccore-1230","title":"Python/2.7.18-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>pip-20.3.4, setuptools-44.1.1, wheel-0.37.1</p>"},{"location":"available_software/detail/QUAST/","title":"QUAST","text":"<p>QUAST evaluates genome assemblies by computing various metrics.It works both with and without reference genomes. The tool accepts multipleassemblies, thus is suitable for comparison.</p> <p>https://github.com/ablab/quast</p>"},{"location":"available_software/detail/QUAST/#available-modules","title":"Available modules","text":"<p>The overview below shows which QUAST installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using QUAST, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load QUAST/5.2.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 QUAST/5.2.0-foss-2023a - x - - - -"},{"location":"available_software/detail/QUAST/#quast520-foss-2023a","title":"QUAST/5.2.0-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>QUAST-5.2.0</p>"},{"location":"available_software/detail/Qhull/","title":"Qhull","text":"<p>Qhull computes the convex hull, Delaunay triangulation, Voronoi diagram, halfspace intersection about a point, furthest-site Delaunay triangulation, and furthest-site Voronoi diagram. The source code runs in 2-d, 3-d, 4-d, and higher dimensions. Qhull implements the Quickhull algorithm for computing the convex hull.</p> <p>http://www.qhull.org</p>"},{"location":"available_software/detail/Qhull/#available-modules","title":"Available modules","text":"<p>The overview below shows which Qhull installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Qhull, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Qhull/2020.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Qhull/2020.2-GCCcore-13.2.0 - x - - - - Qhull/2020.2-GCCcore-12.3.0 - x - - - - Qhull/2020.2-GCCcore-12.2.0 - x - - - - Qhull/2020.2-GCCcore-11.3.0 - x - - - x"},{"location":"available_software/detail/Qt5/","title":"Qt5","text":"<p>Qt is a comprehensive cross-platform C++ application framework.</p> <p>https://qt.io/</p>"},{"location":"available_software/detail/Qt5/#available-modules","title":"Available modules","text":"<p>The overview below shows which Qt5 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Qt5, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Qt5/5.15.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Qt5/5.15.13-GCCcore-13.2.0 - x - - - - Qt5/5.15.10-GCCcore-12.3.0 - x - - - - Qt5/5.15.7-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/QuantumESPRESSO/","title":"QuantumESPRESSO","text":"<p>Quantum ESPRESSO  is an integrated suite of computer codesfor electronic-structure calculations and materials modeling at the nanoscale.It is based on density-functional theory, plane waves, and pseudopotentials(both norm-conserving and ultrasoft).</p> <p>https://www.quantum-espresso.org</p>"},{"location":"available_software/detail/QuantumESPRESSO/#available-modules","title":"Available modules","text":"<p>The overview below shows which QuantumESPRESSO installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using QuantumESPRESSO, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load QuantumESPRESSO/7.3.1-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 QuantumESPRESSO/7.3.1-foss-2023a - - x x - x QuantumESPRESSO/7.3-foss-2023a - - x x - x QuantumESPRESSO/7.2-foss-2022b - x - - - -"},{"location":"available_software/detail/Qwt/","title":"Qwt","text":"<p>The Qwt library contains GUI Components and utility classes which are primarily useful for programs with a technical background.</p> <p>https://qwt.sourceforge.io/</p>"},{"location":"available_software/detail/Qwt/#available-modules","title":"Available modules","text":"<p>The overview below shows which Qwt installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Qwt, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Qwt/6.2.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Qwt/6.2.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/R-bundle-Bioconductor/","title":"R-bundle-Bioconductor","text":"<p>Bioconductor provides tools for the analysis and coprehension of high-throughput genomic data.</p> <p>https://bioconductor.org</p>"},{"location":"available_software/detail/R-bundle-Bioconductor/#available-modules","title":"Available modules","text":"<p>The overview below shows which R-bundle-Bioconductor installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using R-bundle-Bioconductor, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load R-bundle-Bioconductor/3.18-foss-2023a-R-4.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 R-bundle-Bioconductor/3.18-foss-2023a-R-4.3.2 - x - - - - R-bundle-Bioconductor/3.16-foss-2022b-R-4.2.2 - x - - - -"},{"location":"available_software/detail/R-bundle-Bioconductor/#r-bundle-bioconductor318-foss-2023a-r-432","title":"R-bundle-Bioconductor/3.18-foss-2023a-R-4.3.2","text":"<p>This is a list of extensions included in the module:</p> <p>affxparser-1.74.0, affy-1.80.0, affycoretools-1.74.0, affyio-1.72.0, AgiMicroRna-2.52.0, agricolae-1.3-7, ALDEx2-1.34.0, ALL-1.44.0, ANCOMBC-2.4.0, annaffy-1.74.0, annotate-1.80.0, AnnotationDbi-1.64.1, AnnotationFilter-1.26.0, AnnotationForge-1.44.0, AnnotationHub-3.10.0, anytime-0.3.9, aroma.affymetrix-3.2.1, aroma.apd-0.7.0, aroma.core-3.3.0, aroma.light-3.32.0, ash-1.0-15, ATACseqQC-1.26.0, AUCell-1.24.0, aws.s3-0.3.21, aws.signature-0.6.0, babelgene-22.9, ballgown-2.34.0, basilisk-1.14.2, basilisk.utils-1.14.1, batchelor-1.18.1, baySeq-2.36.0, beachmat-2.18.0, BH-1.84.0-0, Biobase-2.62.0, BiocBaseUtils-1.4.0, BiocFileCache-2.10.1, BiocGenerics-0.48.0, BiocIO-1.12.0, BiocManager-1.30.22, BiocNeighbors-1.20.2, BiocParallel-1.36.0, BiocSingular-1.18.0, BiocStyle-2.30.0, BiocVersion-3.18.1, biomaRt-2.58.0, biomformat-1.30.0, Biostrings-2.70.0, biovizBase-1.50.0, blme-1.0-5, bluster-1.12.0, bookdown-0.37, BSgenome-1.70.1, BSgenome.Cfamiliaris.UCSC.canFam3-1.4.0, BSgenome.Hsapiens.UCSC.hg19-1.4.3, BSgenome.Hsapiens.UCSC.hg38-1.4.5, BSgenome.Mmusculus.UCSC.mm10-1.4.3, bsseq-1.38.0, bumphunter-1.44.0, ca-0.71.1, CAGEfightR-1.22.0, CAGEr-2.8.0, CAMERA-1.58.0, Category-2.68.0, ccdata-1.28.0, ccmap-1.28.0, CGHbase-1.62.0, CGHcall-2.64.0, ChIPpeakAnno-3.36.0, chromVAR-1.24.0, clusterProfiler-4.10.0, CNEr-1.38.0, coloc-5.2.3, colorRamps-2.3.1, ComplexHeatmap-2.18.0, ConsensusClusterPlus-1.66.0, conumee-1.36.0, crossmeta-1.28.0, cummeRbund-2.44.0, cytolib-2.14.1, CytoML-2.14.0, dada2-1.30.0, ddPCRclust-1.22.0, DECIPHER-2.30.0, DeconRNASeq-1.44.0, decontam-1.22.0, decoupleR-2.8.0, DEGseq-1.56.1, DelayedArray-0.28.0, DelayedMatrixStats-1.24.0, densEstBayes-1.0-2.2, derfinder-1.36.0, derfinderHelper-1.36.0, DESeq2-1.42.0, diffcyt-1.22.0, dir.expiry-1.10.0, directlabels-2024.1.21, DirichletMultinomial-1.44.0, DNABarcodes-1.32.0, DNAcopy-1.76.0, DO.db-2.9, docopt-0.7.1, DOSE-3.28.2, dqrng-0.3.2, DRIMSeq-1.30.0, DropletUtils-1.22.0, DSS-2.50.1, dupRadar-1.32.0, DynDoc-1.80.0, EBImage-4.44.0, edgeR-4.0.12, egg-0.4.5, emmeans-1.10.0, enrichplot-1.22.0, EnsDb.Hsapiens.v75-2.99.0, EnsDb.Hsapiens.v79-2.99.0, EnsDb.Hsapiens.v86-2.99.0, ensembldb-2.26.0, escape-1.12.0, estimability-1.4.1, ExperimentHub-2.10.0, extraDistr-1.10.0, factoextra-1.0.7, fANCOVA-0.6-1, fda-6.1.4, FDb.InfiniumMethylation.hg19-2.2.0, fds-1.8, feature-1.2.15, fgsea-1.28.0, filelock-1.0.3, flowAI-1.32.0, flowClean-1.40.0, flowClust-3.40.0, flowCore-2.14.0, flowDensity-1.36.1, flowFP-1.60.0, flowMerge-2.50.0, flowPeaks-1.48.0, FlowSOM-2.10.0, FlowSorted.Blood.EPIC-2.6.0, FlowSorted.CordBloodCombined.450k-1.18.0, flowStats-4.14.1, flowViz-1.66.0, flowWorkspace-4.14.2, FRASER-1.14.0, fresh-0.2.0, gcrma-2.74.0, gdsfmt-1.38.0, genefilter-1.84.0, geneLenDataBase-1.38.0, geneplotter-1.80.0, GENESIS-2.32.0, GENIE3-1.24.0, GenomeInfoDb-1.38.5, GenomeInfoDbData-1.2.11, GenomicAlignments-1.38.2, GenomicFeatures-1.54.1, GenomicFiles-1.38.0, GenomicInteractions-1.36.0, GenomicRanges-1.54.1, GenomicScores-2.14.3, GEOmap-2.5-5, GEOquery-2.70.0, ggbio-1.50.0, ggcyto-1.30.0, ggdendro-0.1.23, ggnewscale-0.4.9, ggpointdensity-0.1.0, ggrastr-1.0.2, ggseqlogo-0.1, ggthemes-5.0.0, ggtree-3.10.0, GLAD-2.66.0, Glimma-2.12.0, GlobalAncova-4.20.0, globaltest-5.56.0, GO.db-3.18.0, GOSemSim-2.28.1, goseq-1.54.0, GOstats-2.68.0, graph-1.80.0, graphite-1.48.0, GSEABase-1.64.0, gsmoothr-0.1.7, gson-0.1.0, GSVA-1.50.0, Gviz-1.46.1, GWASExactHW-1.01, GWASTools-1.48.0, HDF5Array-1.30.0, HDO.db-0.99.1, hdrcde-3.4, heatmaply-1.5.0, hgu133plus2.db-3.13.0, HiCBricks-1.20.0, HiCcompare-1.24.0, HMMcopy-1.44.0, Homo.sapiens-1.3.1, IHW-1.30.0, IlluminaHumanMethylation450kanno.ilmn12.hg19-0.6.1, IlluminaHumanMethylation450kmanifest-0.4.0, IlluminaHumanMethylationEPICanno.ilm10b2.hg19-0.6.0, IlluminaHumanMethylationEPICanno.ilm10b4.hg19-0.6.0, IlluminaHumanMethylationEPICmanifest-0.3.0, illuminaio-0.44.0, impute-1.76.0, InteractionSet-1.30.0, interactiveDisplayBase-1.40.0, intervals-0.15.4, IRanges-2.36.0, isva-1.9, JASPAR2020-0.99.10, KEGGgraph-1.62.0, KEGGREST-1.42.0, LEA-3.14.0, limma-3.58.1, log4r-0.4.3, lpsymphony-1.30.0, lsa-0.73.3, lumi-2.54.0, M3Drop-1.28.0, marray-1.80.0, maSigPro-1.74.0, MassSpecWavelet-1.68.0, MatrixGenerics-1.14.0, MBA-0.1-0, MEDIPS-1.54.0, MetaboCoreUtils-1.10.0, metagenomeSeq-1.43.0, metaMA-3.1.3, metap-1.9, metapod-1.10.1, MethylSeekR-1.42.0, methylumi-2.48.0, Mfuzz-2.62.0, mia-1.10.0, minfi-1.48.0, missMethyl-1.36.0, mixOmics-6.26.0, mixsqp-0.3-54, MLInterfaces-1.82.0, MotifDb-1.44.0, motifmatchr-1.24.0, motifStack-1.46.0, MsCoreUtils-1.14.1, MsExperiment-1.4.0, MsFeatures-1.10.0, msigdbr-7.5.1, MSnbase-2.28.1, MSstats-4.10.0, MSstatsConvert-1.12.0, MSstatsLiP-1.8.1, MSstatsPTM-2.4.2, MSstatsTMT-2.10.0, MultiAssayExperiment-1.28.0, MultiDataSet-1.30.0, multtest-2.58.0, muscat-1.16.0, mutoss-0.1-13, mzID-1.40.0, mzR-2.36.0, NADA-1.6-1.1, ncdfFlow-2.48.0, NMF-0.26, NOISeq-2.46.0, numbat-1.3.2-1, oligo-1.66.0, oligoClasses-1.64.0, ontologyIndex-2.11, oompaBase-3.2.9, oompaData-3.1.3, openCyto-2.14.0, org.Hs.eg.db-3.18.0, org.Mm.eg.db-3.18.0, org.Rn.eg.db-3.18.0, OrganismDbi-1.44.0, OUTRIDER-1.20.0, pathview-1.42.0, pcaMethods-1.94.0, perm-1.0-0.4, PFAM.db-3.18.0, phyloseq-1.46.0, plyranges-1.22.0, pmp-1.14.0, polyester-1.38.0, poweRlaw-0.70.6, preprocessCore-1.64.0, pRoloc-1.42.0, pRolocdata-1.40.0, pRolocGUI-2.12.0, ProtGenerics-1.34.0, PRROC-1.3.1, PSCBS-0.66.0, PureCN-2.8.1, qap-0.1-2, QDNAseq-1.38.0, QFeatures-1.12.0, qlcMatrix-0.9.7, qqconf-1.3.2, quantsmooth-1.68.0, qvalue-2.34.0, R.devices-2.17.1, R.filesets-2.15.0, R.huge-0.10.1, rainbow-3.8, randomcoloR-1.1.0.1, rARPACK-0.11-0, RBGL-1.78.0, RcisTarget-1.22.0, RcppAnnoy-0.0.22, RcppHNSW-0.5.0, RcppML-0.3.7, RcppZiggurat-0.1.6, reactome.db-1.86.2, ReactomePA-1.46.0, regioneR-1.34.0, reldist-1.7-2, remaCor-0.0.16, Repitools-1.48.0, ReportingTools-2.42.3, ResidualMatrix-1.12.0, restfulr-0.0.15, Rfast-2.1.0, RFOC-3.4-10, rGADEM-2.50.0, Rgraphviz-2.46.0, rhdf5-2.46.1, rhdf5filters-1.14.1, Rhdf5lib-1.24.1, Rhtslib-2.4.1, Ringo-1.66.0, RNASeqPower-1.42.0, RnBeads-2.20.0, RnBeads.hg19-1.34.0, RnBeads.hg38-1.34.0, RnBeads.mm10-2.10.0, RnBeads.mm9-1.34.0, RnBeads.rn5-1.34.0, ROC-1.78.0, rols-2.30.0, ROntoTools-2.30.0, ropls-1.34.0, RPMG-2.2-7, RProtoBufLib-2.14.0, Rsamtools-2.18.0, RSEIS-4.1-6, Rsubread-2.16.1, rsvd-1.0.5, rtracklayer-1.62.0, Rwave-2.6-5, S4Arrays-1.2.0, S4Vectors-0.40.2, samr-3.0, SamSPECTRAL-1.56.0, SC3-1.30.0, ScaledMatrix-1.10.0, SCANVIS-1.16.0, scater-1.30.1, scattermore-1.2, scDblFinder-1.16.0, scistreer-1.2.0, scran-1.30.2, scrime-1.3.5, scuttle-1.12.0, SeqArray-1.42.0, seqLogo-1.68.0, SeqVarTools-1.40.0, seriation-1.5.4, Seurat-5.0.1, SeuratObject-5.0.1, shinyBS-0.61.1, shinydashboardPlus-2.0.3, shinyFiles-0.9.3, shinyhelper-0.3.2, shinypanel-0.1.5, shinyWidgets-0.8.1, ShortRead-1.60.0, siggenes-1.76.0, Signac-1.12.0, simplifyEnrichment-1.12.0, SingleCellExperiment-1.24.0, SingleR-2.4.1, sitmo-2.0.2, slingshot-2.10.0, SMVar-1.3.4, SNPRelate-1.36.0, snpStats-1.52.0, SparseArray-1.2.3, sparseMatrixStats-1.14.0, sparsesvd-0.2-2, SpatialExperiment-1.12.0, Spectra-1.12.0, SPIA-2.54.0, splancs-2.01-44, SPOTlight-1.6.7, stageR-1.24.0, struct-1.14.0, structToolbox-1.14.0, SummarizedExperiment-1.32.0, susieR-0.12.35, sva-3.50.0, TailRank-3.2.2, TFBSTools-1.40.0, TFMPvalue-0.0.9, tkWidgets-1.80.0, TrajectoryUtils-1.10.0, treeio-1.26.0, TreeSummarizedExperiment-2.10.0, TSP-1.2-4, TxDb.Hsapiens.UCSC.hg19.knownGene-3.2.2, TxDb.Mmusculus.UCSC.mm10.knownGene-3.10.0, tximport-1.30.0, UCell-2.6.2, uwot-0.1.16, variancePartition-1.32.2, VariantAnnotation-1.48.1, venn-1.12, vsn-3.70.0, waiter-0.2.5, wateRmelon-2.8.0, WGCNA-1.72-5, widgetTools-1.80.0, Wrench-1.20.0, xcms-4.0.2, XVector-0.42.0, zCompositions-1.5.0-1, zellkonverter-1.12.1, zlibbioc-1.48.0</p>"},{"location":"available_software/detail/R-bundle-Bioconductor/#r-bundle-bioconductor316-foss-2022b-r-422","title":"R-bundle-Bioconductor/3.16-foss-2022b-R-4.2.2","text":"<p>This is a list of extensions included in the module:</p> <p>affxparser-1.70.0, affy-1.76.0, affycoretools-1.70.0, affyio-1.68.0, AgiMicroRna-2.48.0, agricolae-1.3-5, ALDEx2-1.30.0, ALL-1.40.0, ANCOMBC-2.0.2, annaffy-1.70.0, annotate-1.76.0, AnnotationDbi-1.60.2, AnnotationFilter-1.22.0, AnnotationForge-1.40.1, AnnotationHub-3.6.0, anytime-0.3.9, aroma.affymetrix-3.2.1, aroma.apd-0.6.1, aroma.core-3.3.0, aroma.light-3.28.0, ash-1.0-15, ATACseqQC-1.22.0, AUCell-1.20.2, aws.s3-0.3.21, aws.signature-0.6.0, babelgene-22.9, ballgown-2.30.0, basilisk-1.10.2, basilisk.utils-1.10.0, batchelor-1.14.1, baySeq-2.31.0, beachmat-2.14.0, Biobase-2.58.0, BiocBaseUtils-1.0.0, BiocFileCache-2.6.1, BiocGenerics-0.44.0, BiocIO-1.8.0, BiocManager-1.30.20, BiocNeighbors-1.16.0, BiocParallel-1.32.5, BiocSingular-1.14.0, BiocStyle-2.26.0, BiocVersion-3.16.0, biomaRt-2.54.0, biomformat-1.26.0, Biostrings-2.66.0, biovizBase-1.46.0, blme-1.0-5, bluster-1.8.0, bookdown-0.33, BSgenome-1.66.3, BSgenome.Cfamiliaris.UCSC.canFam3-1.4.0, BSgenome.Hsapiens.UCSC.hg19-1.4.3, BSgenome.Hsapiens.UCSC.hg38-1.4.5, BSgenome.Mmusculus.UCSC.mm10-1.4.3, bsseq-1.34.0, bumphunter-1.40.0, ca-0.71.1, CAGEr-2.4.0, CAMERA-1.54.0, Category-2.64.0, ccdata-1.24.0, ccmap-1.24.0, CGHbase-1.58.0, CGHcall-2.60.0, ChIPpeakAnno-3.32.0, chromVAR-1.20.2, clusterProfiler-4.6.2, CNEr-1.34.0, coloc-5.1.0.1, colorRamps-2.3.1, ComplexHeatmap-2.14.0, ConsensusClusterPlus-1.62.0, conumee-1.32.0, crossmeta-1.24.0, cummeRbund-2.40.0, cytolib-2.10.1, CytoML-2.10.0, dada2-1.26.0, ddPCRclust-1.18.0, DECIPHER-2.26.0, DeconRNASeq-1.40.0, decontam-1.18.0, decoupleR-2.4.0, DEGseq-1.52.0, DelayedArray-0.24.0, DelayedMatrixStats-1.20.0, densEstBayes-1.0-2.1, derfinder-1.32.0, derfinderHelper-1.32.0, DESeq2-1.38.3, diffcyt-1.18.0, dir.expiry-1.6.0, DirichletMultinomial-1.40.0, DNABarcodes-1.28.0, DNAcopy-1.72.3, DO.db-2.9, docopt-0.7.1, DOSE-3.24.2, dqrng-0.3.0, DRIMSeq-1.26.0, DropletUtils-1.18.1, DSS-2.46.0, dupRadar-1.28.0, DynDoc-1.76.0, EBImage-4.40.0, edgeR-3.40.2, egg-0.4.5, emmeans-1.8.5, enrichplot-1.18.3, EnsDb.Hsapiens.v75-2.99.0, EnsDb.Hsapiens.v79-2.99.0, EnsDb.Hsapiens.v86-2.99.0, ensembldb-2.22.0, escape-1.8.0, estimability-1.4.1, ExperimentHub-2.6.0, extraDistr-1.9.1, factoextra-1.0.7, fda-6.0.5, FDb.InfiniumMethylation.hg19-2.2.0, fds-1.8, feature-1.2.15, fgsea-1.24.0, filelock-1.0.2, flowAI-1.28.0, flowClean-1.36.0, flowClust-3.36.0, flowCore-2.10.0, flowDensity-1.32.0, flowFP-1.56.3, flowMerge-2.46.0, flowPeaks-1.44.0, FlowSOM-2.6.0, FlowSorted.Blood.EPIC-2.2.0, FlowSorted.CordBloodCombined.450k-1.14.0, flowStats-4.10.0, flowViz-1.62.0, flowWorkspace-4.10.1, FRASER-1.10.2, fresh-0.2.0, gcrma-2.70.0, gdsfmt-1.34.0, genefilter-1.80.3, geneLenDataBase-1.34.0, geneplotter-1.76.0, GENESIS-2.28.0, GENIE3-1.20.0, GenomeInfoDb-1.34.9, GenomeInfoDbData-1.2.9, GenomicAlignments-1.34.1, GenomicFeatures-1.50.4, GenomicFiles-1.34.0, GenomicRanges-1.50.2, GenomicScores-2.10.0, GEOmap-2.5-0, GEOquery-2.66.0, ggbio-1.46.0, ggcyto-1.26.4, ggdendro-0.1.23, ggnewscale-0.4.8, ggpointdensity-0.1.0, ggrastr-1.0.1, ggseqlogo-0.1, ggthemes-4.2.4, ggtree-3.6.2, GLAD-2.62.0, Glimma-2.8.0, GlobalAncova-4.16.0, globaltest-5.52.0, GO.db-3.16.0, GOSemSim-2.24.0, goseq-1.50.0, GOstats-2.64.0, graph-1.76.0, graphite-1.44.0, GSEABase-1.60.0, gsmoothr-0.1.7, gson-0.1.0, GSVA-1.46.0, Gviz-1.42.1, GWASExactHW-1.01, GWASTools-1.44.0, HDF5Array-1.26.0, HDO.db-0.99.1, hdrcde-3.4, heatmaply-1.4.2, hgu133plus2.db-3.13.0, HiCBricks-1.16.0, HiCcompare-1.20.0, HMMcopy-1.40.0, Homo.sapiens-1.3.1, IHW-1.26.0, IlluminaHumanMethylation450kanno.ilmn12.hg19-0.6.1, IlluminaHumanMethylation450kmanifest-0.4.0, IlluminaHumanMethylationEPICanno.ilm10b2.hg19-0.6.0, IlluminaHumanMethylationEPICanno.ilm10b4.hg19-0.6.0, IlluminaHumanMethylationEPICmanifest-0.3.0, illuminaio-0.40.0, impute-1.72.3, InteractionSet-1.26.1, interactiveDisplayBase-1.36.0, intervals-0.15.4, IRanges-2.32.0, isva-1.9, JASPAR2020-0.99.10, KEGGgraph-1.58.3, KEGGREST-1.38.0, LEA-3.10.2, limma-3.54.2, log4r-0.4.3, lpsymphony-1.26.3, lsa-0.73.3, lumi-2.50.0, M3Drop-1.24.0, marray-1.76.0, maSigPro-1.70.0, MassSpecWavelet-1.64.1, MatrixGenerics-1.10.0, MBA-0.1-0, MEDIPS-1.50.0, metagenomeSeq-1.40.0, metaMA-3.1.3, metap-1.8, metapod-1.6.0, MethylSeekR-1.38.0, methylumi-2.44.0, Mfuzz-2.58.0, mia-1.6.0, minfi-1.44.0, missMethyl-1.32.0, mixOmics-6.22.0, mixsqp-0.3-48, MLInterfaces-1.78.0, MotifDb-1.40.0, motifmatchr-1.20.0, motifStack-1.42.0, MsCoreUtils-1.10.0, MsFeatures-1.6.0, msigdbr-7.5.1, MSnbase-2.24.2, MSstats-4.6.5, MSstatsConvert-1.8.3, MSstatsLiP-1.4.1, MSstatsPTM-2.0.3, MSstatsTMT-2.6.1, MultiAssayExperiment-1.24.0, MultiDataSet-1.26.0, multtest-2.54.0, muscat-1.12.1, mutoss-0.1-13, mzID-1.36.0, mzR-2.32.0, NADA-1.6-1.1, ncdfFlow-2.44.0, NMF-0.25, NOISeq-2.42.0, numbat-1.2.2, oligo-1.62.2, oligoClasses-1.60.0, ontologyIndex-2.10, oompaBase-3.2.9, oompaData-3.1.3, openCyto-2.10.1, org.Hs.eg.db-3.16.0, org.Mm.eg.db-3.16.0, org.Rn.eg.db-3.16.0, OrganismDbi-1.40.0, OUTRIDER-1.16.3, pathview-1.38.0, pcaMethods-1.90.0, perm-1.0-0.2, PFAM.db-3.16.0, phyloseq-1.42.0, pmp-1.10.0, polyester-1.34.0, poweRlaw-0.70.6, preprocessCore-1.60.2, pRoloc-1.38.2, pRolocdata-1.36.0, pRolocGUI-2.8.0, ProtGenerics-1.30.0, PRROC-1.3.1, PSCBS-0.66.0, PureCN-2.4.0, qap-0.1-2, QDNAseq-1.34.0, qlcMatrix-0.9.7, qqconf-1.3.1, quantsmooth-1.64.0, qvalue-2.30.0, R.devices-2.17.1, R.filesets-2.15.0, R.huge-0.9.0, rainbow-3.7, randomcoloR-1.1.0.1, rARPACK-0.11-0, RBGL-1.74.0, RcisTarget-1.18.2, RcppAnnoy-0.0.20, RcppHNSW-0.4.1, RcppML-0.3.7, RcppZiggurat-0.1.6, reactome.db-1.82.0, ReactomePA-1.42.0, regioneR-1.30.0, reldist-1.7-2, remaCor-0.0.11, Repitools-1.44.0, ReportingTools-2.38.0, ResidualMatrix-1.8.0, restfulr-0.0.15, Rfast-2.0.7, RFOC-3.4-6, rGADEM-2.46.0, Rgraphviz-2.42.0, rhdf5-2.42.0, rhdf5filters-1.10.0, Rhdf5lib-1.20.0, Rhtslib-2.0.0, Ringo-1.62.0, RNASeqPower-1.38.0, RnBeads-2.16.0, RnBeads.hg19-1.30.0, RnBeads.hg38-1.30.0, RnBeads.mm10-2.6.0, RnBeads.mm9-1.30.0, RnBeads.rn5-1.30.0, ROC-1.74.0, rols-2.26.0, ROntoTools-2.26.0, ropls-1.30.0, RPMG-2.2-3, RProtoBufLib-2.10.0, Rsamtools-2.14.0, RSEIS-4.1-4, Rsubread-2.12.3, rsvd-1.0.5, rtracklayer-1.58.0, Rwave-2.6-5, S4Vectors-0.36.2, samr-3.0, SamSPECTRAL-1.52.0, SC3-1.26.2, ScaledMatrix-1.6.0, SCANVIS-1.12.0, scater-1.26.1, scattermore-0.8, scDblFinder-1.12.0, scistreer-1.1.0, scran-1.26.2, scrime-1.3.5, scuttle-1.8.4, SeqArray-1.38.0, seqLogo-1.64.0, SeqVarTools-1.36.0, seriation-1.4.2, Seurat-4.3.0, SeuratObject-4.1.3, shinyBS-0.61.1, shinydashboardPlus-2.0.3, shinyFiles-0.9.3, shinyhelper-0.3.2, shinypanel-0.1.5, shinyWidgets-0.7.6, ShortRead-1.56.1, siggenes-1.72.0, Signac-1.9.0, simplifyEnrichment-1.8.0, SingleCellExperiment-1.20.0, SingleR-2.0.0, sitmo-2.0.2, slingshot-2.6.0, SMVar-1.3.4, SNPRelate-1.32.2, snpStats-1.48.0, sparseMatrixStats-1.10.0, sparsesvd-0.2-2, SpatialExperiment-1.8.1, SPIA-2.50.0, splancs-2.01-43, SPOTlight-1.2.0, stageR-1.20.0, struct-1.10.0, structToolbox-1.10.1, SummarizedExperiment-1.28.0, susieR-0.12.35, sva-3.46.0, TailRank-3.2.2, TFBSTools-1.36.0, TFMPvalue-0.0.9, tkWidgets-1.76.0, TrajectoryUtils-1.6.0, treeio-1.22.0, TreeSummarizedExperiment-2.6.0, TSP-1.2-3, TxDb.Hsapiens.UCSC.hg19.knownGene-3.2.2, TxDb.Mmusculus.UCSC.mm10.knownGene-3.10.0, tximport-1.26.1, UCell-2.2.0, uwot-0.1.14, variancePartition-1.28.7, VariantAnnotation-1.44.1, venn-1.11, vsn-3.66.0, waiter-0.2.5, wateRmelon-2.4.0, WGCNA-1.72-1, widgetTools-1.76.0, Wrench-1.16.0, xcms-3.20.0, XVector-0.38.0, zCompositions-1.4.0-1, zellkonverter-1.8.0, zlibbioc-1.44.0</p>"},{"location":"available_software/detail/R-bundle-CRAN/","title":"R-bundle-CRAN","text":"<p>Bundle of R packages from CRAN</p> <p>https://www.r-project.org/</p>"},{"location":"available_software/detail/R-bundle-CRAN/#available-modules","title":"Available modules","text":"<p>The overview below shows which R-bundle-CRAN installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using R-bundle-CRAN, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load R-bundle-CRAN/2023.12-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 R-bundle-CRAN/2023.12-foss-2023a - x - - - -"},{"location":"available_software/detail/R-bundle-CRAN/#r-bundle-cran202312-foss-2023a","title":"R-bundle-CRAN/2023.12-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>abc-2.2.1, abc.data-1.0, abe-3.0.1, abind-1.4-5, acepack-1.4.2, adabag-5.0, ade4-1.7-22, ADGofTest-0.3, admisc-0.34, aggregation-1.0.1, AICcmodavg-2.3-3, akima-0.6-3.4, alabama-2023.1.0, AlgDesign-1.2.1, alluvial-0.1-2, AMAPVox-1.0.1, animation-2.7, aod-1.3.2, apcluster-1.4.11, ape-5.7-1, aplot-0.2.2, argparse-2.2.2, aricode-1.0.3, arm-1.13-1, arrayhelpers-1.1-0, asnipe-1.1.17, assertive-0.3-6, assertive.base-0.0-9, assertive.code-0.0-4, assertive.data-0.0-3, assertive.data.uk-0.0-2, assertive.data.us-0.0-2, assertive.datetimes-0.0-3, assertive.files-0.0-2, assertive.matrices-0.0-2, assertive.models-0.0-2, assertive.numbers-0.0-2, assertive.properties-0.0-5, assertive.reflection-0.0-5, assertive.sets-0.0-3, assertive.strings-0.0-3, assertive.types-0.0-3, assertthat-0.2.1, AUC-0.3.2, audio-0.1-11, aws-2.5-3, awsMethods-1.1-1, backports-1.4.1, bacr-1.0.1, bartMachine-1.3.4.1, bartMachineJARs-1.2.1, base64-2.0.1, BatchJobs-1.9, batchmeans-1.0-4, BayesianTools-0.1.8, BayesLogit-2.1, bayesm-3.1-6, BayesPen-1.0, bayesplot-1.10.0, BB-2019.10-1, BBmisc-1.13, bbmle-1.0.25.1, BCEE-1.3.2, BDgraph-2.72, bdsmatrix-1.3-6, beanplot-1.3.1, beeswarm-0.4.0, berryFunctions-1.22.0, betareg-3.1-4, BH-1.81.0-1, BiasedUrn-2.0.11, bibtex-0.5.1, BIEN-1.2.6, bigD-0.2.0, BIGL-1.8.0, bigmemory-4.6.1, bigmemory.sri-0.1.6, bindr-0.1.1, bindrcpp-0.2.2, bio3d-2.4-4, biom-0.3.12, biomod2-4.2-4, bit-4.0.5, bit64-4.0.5, bitops-1.0-7, blavaan-0.5-2, blob-1.2.4, BMA-3.18.17, bmp-0.3, bnlearn-4.9.1, bold-1.3.0, boot-1.3-28.1, bootstrap-2019.6, Boruta-8.0.0, brglm-0.7.2, bridgedist-0.1.2, bridgesampling-1.1-2, brms-2.20.4, Brobdingnag-1.2-9, broom-1.0.5, broom.helpers-1.14.0, broom.mixed-0.2.9.4, bst-0.3-24, Cairo-1.6-2, calibrate-1.7.7, car-3.1-2, carData-3.0-5, caret-6.0-94, catlearn-1.0, caTools-1.18.2, CBPS-0.23, celestial-1.4.6, cellranger-1.1.0, cgdsr-1.3.0, cghFLasso-0.2-1, changepoint-2.2.4, checkmate-2.3.1, chemometrics-1.4.4, chk-0.9.1, chkptstanr-0.1.1, chron-2.3-61, circlize-0.4.15, circular-0.5-0, class-7.3-22, classInt-0.4-10, cld2-1.2.4, clisymbols-1.2.0, clock-0.7.0, clue-0.3-65, cluster-2.1.6, clusterGeneration-1.3.8, clusterRepro-0.9, clustree-0.5.1, clValid-0.7, cmprsk-2.2-11, cNORM-3.0.4, cobalt-4.5.2, cobs-1.3-5, coda-0.19-4, codetools-0.2-19, coin-1.4-3, collapse-2.0.7, colorspace-2.1-0, colourpicker-1.3.0, combinat-0.0-8, ComICS-1.0.4, ComplexUpset-1.3.3, compositions-2.0-6, CompQuadForm-1.4.3, conditionz-0.1.0, conflicted-1.2.0, conquer-1.3.3, ConsRank-2.1.3, contfrac-1.1-12, copCAR-2.0-4, copula-1.1-3, corpcor-1.6.10, corrplot-0.92, covr-3.6.4, CovSel-1.2.1, covsim-1.0.0, cowplot-1.1.1, coxed-0.3.3, coxme-2.2-18.1, crfsuite-0.4.2, crosstalk-1.2.1, crul-1.4.0, cSEM-0.5.0, csSAM-1.2.4, ctmle-0.1.2, cubature-2.1.0, cubelyr-1.0.2, cvAUC-1.1.4, CVST-0.2-3, CVXR-1.0-11, d3Network-0.5.2.1, dagitty-0.3-4, data.table-1.14.10, data.tree-1.1.0, DataCombine-0.2.21, date-1.2-42, dbarts-0.9-25, DBI-1.1.3, dbplyr-2.4.0, dbscan-1.1-12, dcurver-0.9.2, ddalpha-1.3.13, deal-1.2-42, debugme-1.1.0, deldir-2.0-2, dendextend-1.17.1, DEoptim-2.2-8, DEoptimR-1.1-3, DepthProc-2.1.5, Deriv-4.1.3, DescTools-0.99.52, deSolve-1.40, dfidx-0.0-5, DHARMa-0.4.6, dHSIC-2.1, diagram-1.6.5, DiagrammeR-1.0.10, DiceKriging-1.6.0, dichromat-2.0-0.1, dimRed-0.2.6, diptest-0.77-0, DiscriMiner-0.1-29, dismo-1.3-14, distillery-1.2-1, distr-2.9.2, distrEx-2.9.0, distributional-0.3.2, DistributionUtils-0.6-1, diveRsity-1.9.90, dlm-1.1-6, DMCfun-2.0.2, doc2vec-0.2.0, docstring-1.0.0, doMC-1.3.8, doParallel-1.0.17, doRNG-1.8.6, doSNOW-1.0.20, dotCall64-1.1-1, downloader-0.4, dplyr-1.1.4, dr-3.0.10, dreamerr-1.4.0, drgee-1.1.10, DRR-0.0.4, drugCombo-1.2.1, DT-0.31, dtangle-2.0.9, dtplyr-1.3.1, DTRreg-2.0, dtw-1.23-1, dummies-1.5.6, dygraphs-1.1.1.6, dynamicTreeCut-1.63-1, e1071-1.7-14, earth-5.3.2, EasyABC-1.5.2, ECOSolveR-0.5.5, ellipse-0.5.0, elliptic-1.4-0, emdbook-1.3.13, emmeans-1.8.9, emoa-0.5-0.2, emulator-1.2-21, energy-1.7-11, ENMeval-2.0.4, entropy-1.3.1, EnvStats-2.8.1, epitools-0.5-10.1, ergm-4.5.0, ergm.count-4.1.1, ergm.multi-0.2.0, estimability-1.4.1, EValue-4.1.3, evd-2.3-6.1, Exact-3.2, expm-0.999-8, ExPosition-2.8.23, expsmooth-2.3, extrafont-0.19, extrafontdb-1.0, extRemes-2.1-3, FactoMineR-2.9, FactorCopula-0.9.3, fail-1.3, farver-2.1.1, fastcluster-1.2.3, fastDummies-1.7.3, fasterize-1.0.5, fastICA-1.2-4, fastmatch-1.1-4, fdrtool-1.2.17, feather-0.3.5, ff-4.0.9, fftw-1.0-7, fftwtools-0.9-11, fields-15.2, filehash-2.4-5, finalfit-1.0.7, findpython-1.0.8, fishMod-0.29, fitdistrplus-1.1-11, fixest-0.11.2, FKSUM-1.0.1, flashClust-1.01-2, flexclust-1.4-1, flexmix-2.3-19, flextable-0.9.4, fma-2.5, FME-1.3.6.3, fmri-1.9.12, FNN-1.1.3.2, fontBitstreamVera-0.1.1, fontLiberation-0.1.0, fontquiver-0.2.1, forcats-1.0.0, foreach-1.5.2, forecast-8.21.1, foreign-0.8-86, formatR-1.14, Formula-1.2-5, formula.tools-1.7.1, fossil-0.4.0, fpc-2.2-10, fpp-0.5, fracdiff-1.5-2, furrr-0.3.1, futile.logger-1.4.3, futile.options-1.0.1, future-1.33.0, future.apply-1.11.0, gam-1.22-3, gamlss-5.4-20, gamlss.data-6.0-2, gamlss.dist-6.1-1, gamlss.tr-5.1-7, gamm4-0.2-6, gap-1.5-3, gap.datasets-0.0.6, gapfill-0.9.6-1, gargle-1.5.2, gaussquad-1.0-3, gbm-2.1.8.1, gbRd-0.4-11, gclus-1.3.2, gdalUtils-2.0.3.2, gdata-3.0.0, gdistance-1.6.4, gdtools-0.3.5, gee-4.13-26, geeM-0.10.1, geepack-1.3.9, geex-1.1.1, geiger-2.0.11, GeneNet-1.2.16, generics-0.1.3, genoPlotR-0.8.11, GenSA-1.1.10.1, geojsonsf-2.0.3, geometries-0.2.3, geometry-0.4.7, getopt-1.20.4, GetoptLong-1.0.5, gfonts-0.2.0, GGally-2.2.0, ggbeeswarm-0.7.2, ggdag-0.2.10, ggdist-3.3.1, ggExtra-0.10.1, ggfan-0.1.3, ggforce-0.4.1, ggformula-0.12.0, ggfun-0.1.3, ggh4x-0.2.6, ggnetwork-0.5.12, ggplot2-3.4.4, ggplotify-0.1.2, ggpubr-0.6.0, ggraph-2.1.0, ggrepel-0.9.4, ggridges-0.5.4, ggsci-3.0.0, ggsignif-0.6.4, ggstance-0.3.6, ggstats-0.5.1, ggvenn-0.1.10, ggvis-0.4.8, GillespieSSA-0.6.2, git2r-0.33.0, GJRM-0.2-6.4, glasso-1.11, gld-2.6.6, gllvm-1.4.3, glmmML-1.1.6, glmmTMB-1.1.8, glmnet-4.1-8, GlobalOptions-0.1.2, globals-0.16.2, gmm-1.8, gmodels-2.18.1.1, gmp-0.7-3, gnumeric-0.7-10, goftest-1.2-3, gomms-1.0, googledrive-2.1.1, googlesheets4-1.1.1, gower-1.0.1, GPArotation-2023.11-1, gplots-3.1.3, graphlayouts-1.0.2, grf-2.3.1, gridBase-0.4-7, gridExtra-2.3, gridGraphics-0.5-1, grImport2-0.3-1, grpreg-3.4.0, GSA-1.03.2, gsalib-2.2.1, gsl-2.1-8, gsw-1.1-1, gt-0.10.0, gtable-0.3.4, gtools-3.9.5, gtsummary-1.7.2, GUTS-1.2.5, gWidgets2-1.0-9, gWidgets2tcltk-1.0-8, GxEScanR-2.0.2, h2o-3.42.0.2, hal9001-0.4.6, haldensify-0.2.3, hardhat-1.3.0, harmony-1.2.0, hash-2.2.6.3, haven-2.5.4, hdf5r-1.3.8, hdm-0.3.1, heatmap3-1.1.9, here-1.0.1, hexbin-1.28.3, HGNChelper-0.8.1, HiddenMarkov-1.8-13, Hmisc-5.1-1, hms-1.1.3, Hmsc-3.0-13, htmlTable-2.4.2, httpcode-0.3.0, huge-1.3.5, hunspell-3.0.3, hwriter-1.3.2.1, HWxtest-1.1.9, hypergeo-1.2-13, ica-1.0-3, IDPmisc-1.1.20, idr-1.3, ids-1.0.1, ie2misc-0.9.1, igraph-1.5.1, image.binarization-0.1.3, imager-0.45.2, imagerExtra-1.3.2, ineq-0.2-13, influenceR-0.1.5, infotheo-1.2.0.1, inline-0.3.19, intergraph-2.0-3, interp-1.1-5, interpretR-0.2.5, intrinsicDimension-1.2.0, inum-1.0-5, ipred-0.9-14, irace-3.5, irlba-2.3.5.1, ismev-1.42, Iso-0.0-21, isoband-0.2.7, ISOcodes-2023.12.07, ISOweek-0.6-2, iterators-1.0.14, itertools-0.1-3, JADE-2.0-4, janeaustenr-1.0.0, JBTools-0.7.2.9, jiebaR-0.11, jiebaRD-0.1, jomo-2.7-6, jpeg-0.1-10, jsonify-1.2.2, jstable-1.1.3, juicyjuice-0.1.0, kde1d-1.0.5, kedd-1.0.3, kernlab-0.9-32, KernSmooth-2.23-22, kinship2-1.9.6, klaR-1.7-2, KODAMA-2.4, kohonen-3.0.12, ks-1.14.1, labdsv-2.1-0, labeling-0.4.3, labelled-2.12.0, laeken-0.5.2, lambda.r-1.2.4, LaplacesDemon-16.1.6, lars-1.3, lassosum-0.4.5, lattice-0.22-5, latticeExtra-0.6-30, lava-1.7.3, lavaan-0.6-16, lazy-1.2-18, lazyeval-0.2.2, LCFdata-2.0, lda-1.4.2, ldbounds-2.0.2, leafem-0.2.3, leaflet-2.2.1, leaflet.providers-2.0.0, leafsync-0.1.0, leaps-3.1, LearnBayes-2.15.1, leiden-0.4.3.1, lhs-1.1.6, libcoin-1.0-10, limSolve-1.5.7, linkcomm-1.0-14, linprog-0.9-4, liquidSVM-1.2.4, listenv-0.9.0, lme4-1.1-35.1, LMERConvenienceFunctions-3.0, lmerTest-3.1-3, lmom-3.0, Lmoments-1.3-1, lmtest-0.9-40, lobstr-1.1.2, locfdr-1.1-8, locfit-1.5-9.8, logcondens-2.1.8, logger-0.2.2, logistf-1.26.0, logspline-2.1.21, longitudinal-1.1.13, longmemo-1.1-2, loo-2.6.0, lpSolve-5.6.19, lpSolveAPI-5.5.2.0-17.11, lqa-1.0-3, lsei-1.3-0, lslx-0.6.11, lubridate-1.9.3, lwgeom-0.2-13, magic-1.6-1, magick-2.8.1, MALDIquant-1.22.1, manipulateWidget-0.11.1, mapproj-1.2.11, maps-3.4.1.1, maptools-1.1-8, markdown-1.12, MASS-7.3-60, Matching-4.10-14, MatchIt-4.5.5, mathjaxr-1.6-0, matlab-1.0.4, Matrix-1.6-4, matrixcalc-1.0-6, MatrixModels-0.5-3, matrixStats-1.1.0, maxLik-1.5-2, maxlike-0.1-10, maxnet-0.1.4, mboost-2.9-9, mclogit-0.9.6, mclust-6.0.1, mcmc-0.9-8, MCMCpack-1.6-3, mcmcse-1.5-0, mda-0.5-4, medflex-0.6-10, mediation-4.5.0, memisc-0.99.31.6, memuse-4.2-3, MESS-0.5.12, metadat-1.2-0, metafor-4.4-0, MetaUtility-2.1.2, mets-1.3.3, mgcv-1.9-0, mgsub-1.7.3, mhsmm-0.4.21, mi-1.1, mice-3.16.0, miceadds-3.16-18, microbenchmark-1.4.10, MIIVsem-0.5.8, minerva-1.5.10, minpack.lm-1.2-4, minqa-1.2.6, mirt-1.41, misc3d-0.9-1, miscTools-0.6-28, missForest-1.5, mitml-0.4-5, mitools-2.4, mixtools-2.0.0, mlbench-2.1-3.1, mlegp-3.1.9, MLmetrics-1.1.1, mlogit-1.1-1, mlr-2.19.1, mlrMBO-1.1.5.1, mltools-0.3.5, mnormt-2.1.1, ModelMetrics-1.2.2.2, modelr-0.1.11, modeltools-0.2-23, momentfit-0.5, moments-0.14.1, MonteCarlo-1.0.6, mosaicCore-0.9.4.0, mpath-0.4-2.23, mRMRe-2.1.2.1, msm-1.7.1, mstate-0.3.2, multcomp-1.4-25, multcompView-0.1-9, multicool-1.0.0, multipol-1.0-9, munsell-0.5.0, mvabund-4.2.1, mvnfast-0.2.8, mvtnorm-1.2-4, nabor-0.5.0, naniar-1.0.0, natserv-1.0.0, naturalsort-0.1.3, ncbit-2013.03.29.1, ncdf4-1.22, NCmisc-1.2.0, network-1.18.2, networkDynamic-0.11.3, networkLite-1.0.5, neuralnet-1.44.2, neuRosim-0.2-14, ngspatial-1.2-2, NISTunits-1.0.1, nleqslv-3.3.5, nlme-3.1-164, nloptr-2.0.3, NLP-0.2-1, nlsem-0.8-1, nnet-7.3-19, nnls-1.5, nonnest2-0.5-6, nor1mix-1.3-2, norm-1.0-11.1, nortest-1.0-4, np-0.60-17, npsurv-0.5-0, numDeriv-2016.8-1.1, oai-0.4.0, oce-1.8-2, OceanView-1.0.6, oddsratio-2.0.1, officer-0.6.3, openair-2.18-0, OpenMx-2.21.11, openxlsx-4.2.5.2, operator.tools-1.6.3, optextras-2019-12.4, optimParallel-1.0-2, optimr-2019-12.16, optimx-2023-10.21, optmatch-0.10.7, optparse-1.7.3, ordinal-2023.12-4, origami-1.0.7, oro.nifti-0.11.4, orthopolynom-1.0-6.1, osqp-0.6.3.2, outliers-0.15, packrat-0.9.2, pacman-0.5.1, pammtools-0.5.92, pamr-1.56.1, pan-1.9, parallelDist-0.2.6, parallelly-1.36.0, parallelMap-1.5.1, ParamHelpers-1.14.1, parsedate-1.3.1, party-1.3-14, partykit-1.2-20, pastecs-1.3.21, patchwork-1.1.3, pbapply-1.7-2, pbivnorm-0.6.0, pbkrtest-0.5.2, PCAmatchR-0.3.3, pcaPP-2.0-4, pdp-0.8.1, PearsonDS-1.3.0, pec-2023.04.12, penalized-0.9-52, penfa-0.1.1, peperr-1.5, PermAlgo-1.2, permute-0.9-7, phangorn-2.11.1, pheatmap-1.0.12, phylobase-0.8.10, phytools-2.0-3, pim-2.0.2, pinfsc50-1.3.0, pixmap-0.4-12, pkgmaker-0.32.10, plogr-0.2.0, plot3D-1.4, plot3Drgl-1.0.4, plotly-4.10.3, plotmo-3.6.2, plotrix-3.8-4, pls-2.8-3, plyr-1.8.9, PMA-1.2-2, png-0.1-8, PoissonSeq-1.1.2, poLCA-1.6.0.1, polspline-1.1.24, Polychrome-1.5.1, polyclip-1.10-6, polycor-0.8-1, polynom-1.4-1, posterior-1.5.0, ppcor-1.1, prabclus-2.3-3, pracma-2.4.4, PresenceAbsence-1.1.11, preseqR-4.0.0, prettyGraphs-2.1.6, princurve-2.1.6, pROC-1.18.5, prodlim-2023.08.28, profileModel-0.6.1, proftools-0.99-3, progress-1.2.3, progressr-0.14.0, projpred-2.7.0, proto-1.0.0, proxy-0.4-27, proxyC-0.3.4, pryr-0.1.6, pscl-1.5.5.1, pspline-1.0-19, psych-2.3.9, Publish-2023.01.17, pulsar-0.3.11, pvclust-2.2-0, qgam-1.3.4, qgraph-1.9.8, qqman-0.1.9, qrnn-2.1, quadprog-1.5-8, quanteda-3.3.1, quantmod-0.4.25, quantreg-5.97, questionr-0.7.8, QuickJSR-1.0.8, R.cache-0.16.0, R.matlab-3.7.0, R.methodsS3-1.8.2, R.oo-1.25.0, R.rsp-0.45.0, R.utils-2.12.3, R2WinBUGS-2.1-21, random-0.2.6, randomForest-4.7-1.1, randomForestSRC-3.2.3, randtoolbox-2.0.4, rangeModelMetadata-0.1.5, ranger-0.16.0, RANN-2.6.1, rapidjsonr-1.2.0, rARPACK-0.11-0, raster-3.6-26, rasterVis-0.51.6, ratelimitr-0.4.1, RBesT-1.7-2, rbibutils-2.2.16, rbison-1.0.0, Rborist-0.3-5, RCAL-2.0, Rcgmin-2022-4.30, RCircos-1.2.2, RColorBrewer-1.1-3, RcppArmadillo-0.12.6.6.1, RcppEigen-0.3.3.9.4, RcppGSL-0.3.13, RcppParallel-5.1.7, RcppProgress-0.4.2, RcppRoll-0.3.0, RcppThread-2.1.6, RcppTOML-0.2.2, RCurl-1.98-1.13, rda-1.2-1, Rdpack-2.6, rdrop2-0.8.2.1, reactable-0.4.4, reactR-0.5.0, readbitmap-0.1.5, reader-1.0.6, readODS-2.1.0, readr-2.1.4, readxl-1.4.3, rebird-1.3.0, recipes-1.0.8, RefFreeEWAS-2.2, registry-0.5-1, regsem-1.9.5, relsurv-2.2-9, rematch-2.0.0, rentrez-1.2.3, renv-1.0.3, reprex-2.0.2, resample-0.6, reshape-0.8.9, reshape2-1.4.4, reticulate-1.34.0, rex-1.2.1, rgbif-3.7.8, RGCCA-3.0.2, rgdal-1.6-7, rgeos-0.6-4, rgexf-0.16.2, rgl-1.2.8, Rglpk-0.6-5, RhpcBLASctl-0.23-42, ridge-3.3, ridigbio-0.3.7, RInside-0.2.18, rio-1.0.1, riskRegression-2023.09.08, ritis-1.0.0, RItools-0.3-3, rJava-1.0-10, rjson-0.2.21, RJSONIO-1.3-1.9, rle-0.9.2, rlecuyer-0.3-8, rlemon-0.2.1, rlist-0.4.6.2, rmeta-3.0, Rmpfr-0.9-4, rms-6.7-1, RMTstat-0.3.1, rncl-0.8.7, rnetcarto-0.2.6, RNeXML-2.4.11, rngtools-1.5.2, rngWELL-0.10-9, RNifti-1.5.1, robustbase-0.99-1, ROCR-1.0-11, ROI-1.0-1, ROI.plugin.glpk-1.0-0, Rook-1.2, rootSolve-1.8.2.4, roptim-0.1.6, rotl-3.1.0, rpact-3.4.0, rpart-4.1.23, rpf-1.0.14, RPMM-1.25, RPostgreSQL-0.7-5, rrcov-1.7-4, rredlist-0.7.1, rsample-1.2.0, rsconnect-1.1.1, Rserve-1.8-13, RSNNS-0.4-17, Rsolnp-1.16, RSpectra-0.16-1, RSQLite-2.3.4, Rssa-1.0.5, rstan-2.32.3, rstantools-2.3.1.1, rstatix-0.7.2, rtdists-0.11-5, Rtsne-0.17, Rttf2pt1-1.3.12, RUnit-0.4.32, ruv-0.9.7.1, rvertnet-0.8.2, rvest-1.0.3, rvinecopulib-0.6.3.1.1, Rvmmin-2018-4.17.1, RWeka-0.4-46, RWekajars-3.9.3-2, s2-1.1.4, sampling-2.10, sandwich-3.0-2, SBdecomp-1.2, scales-1.3.0, scam-1.2-14, scatterpie-0.2.1, scatterplot3d-0.3-44, scs-3.2.4, sctransform-0.4.1, SDMTools-1.1-221.2, seewave-2.2.3, segmented-2.0-0, selectr-0.4-2, sem-3.1-15, semPLS-1.0-10, semTools-0.5-6, sendmailR-1.4-0, sensemakr-0.1.4, sentometrics-1.0.0, seqinr-4.2-36, servr-0.27, setRNG-2022.4-1, sf-1.0-14, sfheaders-0.4.3, sfsmisc-1.1-16, shadowtext-0.1.2, shape-1.4.6, shapefiles-0.7.2, shinycssloaders-1.0.0, shinydashboard-0.7.2, shinyjs-2.1.0, shinystan-2.6.0, shinythemes-1.2.0, signal-1.8-0, SignifReg-4.3, simex-1.8, SimSeq-1.4.0, SKAT-2.2.5, slam-0.1-50, slider-0.3.1, sm-2.2-5.7.1, smoof-1.6.0.3, smoother-1.1, sn-2.1.1, sna-2.7-2, SNFtool-2.3.1, snow-0.4-4, SnowballC-0.7.1, snowfall-1.84-6.3, SOAR-0.99-11, solrium-1.2.0, som-0.3-5.1, soundecology-1.3.3, sp-2.1-2, spaa-0.2.2, spam-2.10-0, spaMM-4.4.0, SparseM-1.81, SPAtest-3.1.2, spatial-7.3-17, spatstat-3.0-7, spatstat.core-2.4-4, spatstat.data-3.0-3, spatstat.explore-3.2-5, spatstat.geom-3.2-7, spatstat.linnet-3.1-3, spatstat.model-3.2-8, spatstat.random-3.2-2, spatstat.sparse-3.0-3, spatstat.utils-3.0-4, spData-2.3.0, spdep-1.3-1, splitstackshape-1.4.8, spls-2.2-3, spocc-1.2.2, spThin-0.2.0, SQUAREM-2021.1, stabledist-0.7-1, stabs-0.6-4, StanHeaders-2.26.28, stargazer-5.2.3, stars-0.6-4, startupmsg-0.9.6, StatMatch-1.4.1, statmod-1.5.0, statnet-2019.6, statnet.common-4.9.0, stdReg-3.4.1, stopwords-2.3, stringdist-0.9.12, stringmagic-1.0.0, strucchange-1.5-3, styler-1.10.2, subplex-1.8, SuperLearner-2.0-28.1, SuppDists-1.1-9.7, survey-4.2-1, survival-3.5-7, survivalROC-1.0.3.1, svd-0.5.5, svglite-2.1.3, svUnit-1.0.6, swagger-3.33.1, symmoments-1.2.1, tableone-0.13.2, tabletools-0.1.0, tau-0.0-25, taxize-0.9.100, tcltk2-1.2-11, tclust-1.5-5, TeachingDemos-2.12, tensor-1.5, tensorA-0.36.2, tergm-4.2.0, terra-1.7-55, testit-0.13, textcat-1.0-8, textplot-0.2.2, TFisher-0.2.0, TH.data-1.1-2, threejs-0.3.3, tictoc-1.2, tidybayes-3.0.6, tidygraph-1.2.3, tidyr-1.3.0, tidyselect-1.2.0, tidytext-0.4.1, tidytree-0.4.5, tidyverse-2.0.0, tiff-0.1-12, timechange-0.2.0, timeDate-4022.108, timereg-2.0.5, tkrplot-0.0-27, tm-0.7-11, tmap-3.3-4, tmaptools-3.1-1, TMB-1.9.9, tmle-2.0.0, tmvnsim-1.0-2, tmvtnorm-1.6, tokenizers-0.3.0, topicmodels-0.2-15, TraMineR-2.2-8, tree-1.0-43, triebeard-0.4.1, trimcluster-0.1-5, tripack-1.3-9.1, TruncatedNormal-2.2.2, truncnorm-1.0-9, trust-0.1-8, tseries-0.10-55, tseriesChaos-0.1-13.1, tsna-0.3.5, tsne-0.1-3.1, TTR-0.24.4, tuneR-1.4.6, twang-2.6, tweedie-2.3.5, tweenr-2.0.2, tzdb-0.4.0, ucminf-1.2.0, udpipe-0.8.11, umap-0.2.10.0, unbalanced-2.0, unikn-0.9.0, uniqueAtomMat-0.1-3-2, units-0.8-5, unmarked-1.3.2, UpSetR-1.4.0, urca-1.3-3, urltools-1.7.3, uroot-2.1-2, uuid-1.1-1, V8-4.4.1, varhandle-2.0.6, vcd-1.4-11, vcfR-1.15.0, vegan-2.6-4, VennDiagram-1.7.3, VGAM-1.1-9, VIM-6.2.2, VineCopula-2.5.0, vioplot-0.4.0, vipor-0.4.5, viridis-0.6.4, viridisLite-0.4.2, visdat-0.6.0, visNetwork-2.1.2, vroom-1.6.5, VSURF-1.2.0, warp-0.2.1, waveslim-1.8.4, wdm-0.2.4, webshot-0.5.5, webutils-1.2.0, weights-1.0.4, WeightSVM-1.7-13, wellknown-0.7.4, widgetframe-0.3.1, WikidataQueryServiceR-1.0.0, WikidataR-2.3.3, WikipediR-1.5.0, wikitaxa-0.4.0, wk-0.9.1, word2vec-0.4.0, wordcloud-2.6, worrms-0.4.3, writexl-1.4.2, WriteXLS-6.4.0, xgboost-1.7.6.1, xlsx-0.6.5, xlsxjars-0.6.1, XML-3.99-0.16, xts-0.13.1, yaImpute-1.0-33, yulab.utils-0.1.0, zeallot-0.1.0, zoo-1.8-12</p>"},{"location":"available_software/detail/R-bundle-ICER/","title":"R-bundle-ICER","text":"<p>Bundle of R packages requested by ICER users</p> <p>https://www.r-project.org/</p>"},{"location":"available_software/detail/R-bundle-ICER/#available-modules","title":"Available modules","text":"<p>The overview below shows which R-bundle-ICER installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using R-bundle-ICER, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load R-bundle-ICER/2024.06-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 R-bundle-ICER/2024.06-foss-2023a - x - - - -"},{"location":"available_software/detail/R-bundle-ICER/#r-bundle-icer202406-foss-2023a","title":"R-bundle-ICER/2024.06-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>rjags-4-15, Rmpi-0.7-2, Seurat-5.1.0</p>"},{"location":"available_software/detail/R/","title":"R","text":"<p>R is a free software environment for statistical computing and graphics.</p> <p>https://www.r-project.org/</p>"},{"location":"available_software/detail/R/#available-modules","title":"Available modules","text":"<p>The overview below shows which R installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using R, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load R/4.3.3-gfbf-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 R/4.3.3-gfbf-2023b - x - - - - R/4.3.2-gfbf-2023a-ICER - x - - - - R/4.3.2-gfbf-2023a - x x x x x R/4.2.2-foss-2022b - x - - - - R/3.6.3-foss-2022b - x - - - -"},{"location":"available_software/detail/R/#r433-gfbf-2023b","title":"R/4.3.3-gfbf-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>askpass-1.2.0, base, base64enc-0.1-3, brew-1.0-10, brio-1.1.4, bslib-0.6.1, cachem-1.0.8, callr-3.7.5, cli-3.6.2, clipr-0.8.0, commonmark-1.9.1, compiler, cpp11-0.4.7, crayon-1.5.2, credentials-2.0.1, curl-5.2.1, datasets, desc-1.4.3, devtools-2.4.5, diffobj-0.3.5, digest-0.6.34, downlit-0.4.3, ellipsis-0.3.2, evaluate-0.23, fansi-1.0.6, fastmap-1.1.1, fontawesome-0.5.2, fs-1.6.3, gert-2.0.1, gh-1.4.0, gitcreds-0.1.2, glue-1.7.0, graphics, grDevices, grid, highr-0.10, htmltools-0.5.7, htmlwidgets-1.6.4, httpuv-1.6.14, httr-1.4.7, httr2-1.0.0, ini-0.3.1, jquerylib-0.1.4, jsonlite-1.8.8, knitr-1.45, later-1.3.2, lifecycle-1.0.4, magrittr-2.0.3, memoise-2.0.1, methods, mime-0.12, miniUI-0.1.1.1, openssl-2.1.1, parallel, pillar-1.9.0, pkgbuild-1.4.3, pkgconfig-2.0.3, pkgdown-2.0.7, pkgload-1.3.4, praise-1.0.0, prettyunits-1.2.0, processx-3.8.3, profvis-0.3.8, promises-1.2.1, ps-1.7.6, purrr-1.0.2, R6-2.5.1, ragg-1.2.7, rappdirs-0.3.3, rcmdcheck-1.4.0, Rcpp-1.0.12, rematch2-2.1.2, remotes-2.4.2.1, rlang-1.1.3, rmarkdown-2.25, roxygen2-7.3.1, rprojroot-2.0.4, rstudioapi-0.15.0, rversions-2.1.2, sass-0.4.8, sessioninfo-1.2.2, shiny-1.8.0, sourcetools-0.1.7-1, splines, stats, stats4, stringi-1.8.3, stringr-1.5.1, sys-3.4.2, systemfonts-1.0.5, tcltk, testthat-3.2.1, textshaping-0.3.7, tibble-3.2.1, tinytex-0.49, tools, urlchecker-1.0.1, usethis-2.2.3, utf8-1.2.4, utils, vctrs-0.6.5, waldo-0.5.2, whisker-0.4.1, withr-3.0.0, xfun-0.42, xml2-1.3.6, xopen-1.0.0, xtable-1.8-4, yaml-2.3.8, zip-2.3.1</p>"},{"location":"available_software/detail/R/#r432-gfbf-2023a-icer","title":"R/4.3.2-gfbf-2023a-ICER","text":"<p>This is a list of extensions included in the module:</p> <p>askpass-1.2.0, base, base64enc-0.1-3, brew-1.0-8, brio-1.1.3, bslib-0.5.1, cachem-1.0.8, callr-3.7.3, cli-3.6.1, clipr-0.8.0, commonmark-1.9.0, compiler, cpp11-0.4.6, crayon-1.5.2, credentials-2.0.1, curl-5.1.0, datasets, desc-1.4.2, devtools-2.4.5, diffobj-0.3.5, digest-0.6.33, downlit-0.4.3, ellipsis-0.3.2, evaluate-0.23, fansi-1.0.5, fastmap-1.1.1, fontawesome-0.5.2, fs-1.6.3, gert-2.0.0, gh-1.4.0, gitcreds-0.1.2, glue-1.6.2, graphics, grDevices, grid, highr-0.10, htmltools-0.5.7, htmlwidgets-1.6.2, httpuv-1.6.12, httr-1.4.7, httr2-0.2.3, ini-0.3.1, jquerylib-0.1.4, jsonlite-1.8.7, knitr-1.45, later-1.3.1, lifecycle-1.0.3, magrittr-2.0.3, memoise-2.0.1, methods, mime-0.12, miniUI-0.1.1.1, openssl-2.1.1, parallel, pillar-1.9.0, pkgbuild-1.4.2, pkgconfig-2.0.3, pkgdown-2.0.7, pkgload-1.3.3, praise-1.0.0, prettyunits-1.2.0, processx-3.8.2, profvis-0.3.8, promises-1.2.1, ps-1.7.5, purrr-1.0.2, R6-2.5.1, ragg-1.2.6, rappdirs-0.3.3, rcmdcheck-1.4.0, Rcpp-1.0.11, rematch2-2.1.2, remotes-2.4.2.1, rlang-1.1.2, rmarkdown-2.25, roxygen2-7.2.3, rprojroot-2.0.4, rstudioapi-0.15.0, rversions-2.1.2, sass-0.4.7, sessioninfo-1.2.2, shiny-1.7.5.1, sourcetools-0.1.7-1, splines, stats, stats4, stringi-1.7.12, stringr-1.5.0, sys-3.4.2, systemfonts-1.0.5, tcltk, testthat-3.2.0, textshaping-0.3.7, tibble-3.2.1, tinytex-0.48, tools, urlchecker-1.0.1, usethis-2.2.2, utf8-1.2.4, utils, vctrs-0.6.4, waldo-0.5.2, whisker-0.4.1, withr-2.5.2, xfun-0.41, xml2-1.3.5, xopen-1.0.0, xtable-1.8-4, yaml-2.3.7, zip-2.3.0</p>"},{"location":"available_software/detail/R/#r432-gfbf-2023a","title":"R/4.3.2-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>askpass-1.2.0, base, base64enc-0.1-3, brew-1.0-8, brio-1.1.3, bslib-0.5.1, cachem-1.0.8, callr-3.7.3, cli-3.6.1, clipr-0.8.0, commonmark-1.9.0, compiler, cpp11-0.4.6, crayon-1.5.2, credentials-2.0.1, curl-5.1.0, datasets, desc-1.4.2, devtools-2.4.5, diffobj-0.3.5, digest-0.6.33, downlit-0.4.3, ellipsis-0.3.2, evaluate-0.23, fansi-1.0.5, fastmap-1.1.1, fontawesome-0.5.2, fs-1.6.3, gert-2.0.0, gh-1.4.0, gitcreds-0.1.2, glue-1.6.2, graphics, grDevices, grid, highr-0.10, htmltools-0.5.7, htmlwidgets-1.6.2, httpuv-1.6.12, httr-1.4.7, httr2-0.2.3, ini-0.3.1, jquerylib-0.1.4, jsonlite-1.8.7, knitr-1.45, later-1.3.1, lifecycle-1.0.3, magrittr-2.0.3, memoise-2.0.1, methods, mime-0.12, miniUI-0.1.1.1, openssl-2.1.1, parallel, pillar-1.9.0, pkgbuild-1.4.2, pkgconfig-2.0.3, pkgdown-2.0.7, pkgload-1.3.3, praise-1.0.0, prettyunits-1.2.0, processx-3.8.2, profvis-0.3.8, promises-1.2.1, ps-1.7.5, purrr-1.0.2, R6-2.5.1, ragg-1.2.6, rappdirs-0.3.3, rcmdcheck-1.4.0, Rcpp-1.0.11, rematch2-2.1.2, remotes-2.4.2.1, rlang-1.1.2, rmarkdown-2.25, roxygen2-7.2.3, rprojroot-2.0.4, rstudioapi-0.15.0, rversions-2.1.2, sass-0.4.7, sessioninfo-1.2.2, shiny-1.7.5.1, sourcetools-0.1.7-1, splines, stats, stats4, stringi-1.7.12, stringr-1.5.0, sys-3.4.2, systemfonts-1.0.5, tcltk, testthat-3.2.0, textshaping-0.3.7, tibble-3.2.1, tinytex-0.48, tools, urlchecker-1.0.1, usethis-2.2.2, utf8-1.2.4, utils, vctrs-0.6.4, waldo-0.5.2, whisker-0.4.1, withr-2.5.2, xfun-0.41, xml2-1.3.5, xopen-1.0.0, xtable-1.8-4, yaml-2.3.7, zip-2.3.0</p>"},{"location":"available_software/detail/R/#r422-foss-2022b","title":"R/4.2.2-foss-2022b","text":"<p>This is a list of extensions included in the module:</p> <p>abc-2.2.1, abc.data-1.0, abe-3.0.1, abind-1.4-5, acepack-1.4.1, adabag-4.2, ade4-1.7-22, ADGofTest-0.3, admisc-0.31, aggregation-1.0.1, AICcmodavg-2.3-1, akima-0.6-3.4, alabama-2022.4-1, AlgDesign-1.2.1, alluvial-0.1-2, AMAPVox-1.0.0, animation-2.7, aod-1.3.2, apcluster-1.4.10, ape-5.7-1, aplot-0.1.10, argparse-2.2.2, aricode-1.0.2, arm-1.13-1, askpass-1.1, asnipe-1.1.16, assertive-0.3-6, assertive.base-0.0-9, assertive.code-0.0-3, assertive.data-0.0-3, assertive.data.uk-0.0-2, assertive.data.us-0.0-2, assertive.datetimes-0.0-3, assertive.files-0.0-2, assertive.matrices-0.0-2, assertive.models-0.0-2, assertive.numbers-0.0-2, assertive.properties-0.0-5, assertive.reflection-0.0-5, assertive.sets-0.0-3, assertive.strings-0.0-3, assertive.types-0.0-3, assertthat-0.2.1, AUC-0.3.2, audio-0.1-10, aws-2.5-1, awsMethods-1.1-1, backports-1.4.1, bacr-1.0.1, bartMachine-1.3.3.1, bartMachineJARs-1.2.1, base, base64-2.0.1, base64enc-0.1-3, BatchJobs-1.9, batchmeans-1.0-4, BayesianTools-0.1.8, BayesLogit-2.1, bayesm-3.1-5, BayesPen-1.0, bayesplot-1.10.0, BB-2019.10-1, BBmisc-1.13, bbmle-1.0.25, BCEE-1.3.1, BDgraph-2.72, bdsmatrix-1.3-6, beanplot-1.3.1, beeswarm-0.4.0, berryFunctions-1.22.0, betareg-3.1-4, BH-1.81.0-1, BiasedUrn-2.0.9, bibtex-0.5.1, bigD-0.2.0, BIGL-1.7.0, bigmemory-4.6.1, bigmemory.sri-0.1.6, bindr-0.1.1, bindrcpp-0.2.2, bio3d-2.4-4, biom-0.3.12, biomod2-4.2-2, bit-4.0.5, bit64-4.0.5, bitops-1.0-7, blavaan-0.4-7, blob-1.2.4, BMA-3.18.17, bmp-0.3, bnlearn-4.8.1, bold-1.2.0, boot-1.3-28.1, bootstrap-2019.6, Boruta-8.0.0, brew-1.0-8, brglm-0.7.2, bridgedist-0.1.2, bridgesampling-1.1-2, brio-1.1.3, brms-2.19.0, Brobdingnag-1.2-9, broom-1.0.4, broom.helpers-1.12.0, broom.mixed-0.2.9.4, bslib-0.4.2, bst-0.3-24, cachem-1.0.7, Cairo-1.6-0, calibrate-1.7.7, callr-3.7.3, car-3.1-1, carData-3.0-5, caret-6.0-93, catlearn-0.9.1, caTools-1.18.2, CBPS-0.23, celestial-1.4.6, cellranger-1.1.0, cgdsr-1.2.10, cghFLasso-0.2-1, changepoint-2.2.4, checkmate-2.1.0, chemometrics-1.4.2, chkptstanr-0.1.1, chron-2.3-60, circlize-0.4.15, circular-0.4-95, class-7.3-21, classInt-0.4-9, cld2-1.2.4, cli-3.6.0, clipr-0.8.0, clisymbols-1.2.0, clock-0.6.1, clue-0.3-64, cluster-2.1.4, clusterGeneration-1.3.7, clusterRepro-0.9, clustree-0.5.0, clValid-0.7, cmprsk-2.2-11, cNORM-3.0.2, cobalt-4.4.1, cobs-1.3-5, coda-0.19-4, codetools-0.2-19, coin-1.4-2, collapse-1.9.3, colorspace-2.1-0, colourpicker-1.2.0, combinat-0.0-8, ComICS-1.0.4, commonmark-1.8.1, compiler, ComplexUpset-1.3.3, compositions-2.0-5, CompQuadForm-1.4.3, conditionz-0.1.0, conflicted-1.2.0, conquer-1.3.3, contfrac-1.1-12, copCAR-2.0-4, copula-1.1-2, corpcor-1.6.10, corrplot-0.92, covr-3.6.1, CovSel-1.2.1, covsim-1.0.0, cowplot-1.1.1, coxed-0.3.3, coxme-2.2-18.1, cpp11-0.4.3, crayon-1.5.2, credentials-1.3.2, crfsuite-0.4.1, crosstalk-1.2.0, crul-1.3, cSEM-0.5.0, csSAM-1.2.4, ctmle-0.1.2, cubature-2.0.4.6, cubelyr-1.0.2, curl-5.0.0, cvAUC-1.1.4, CVST-0.2-3, CVXR-1.0-11, d3Network-0.5.2.1, dagitty-0.3-1, data.table-1.14.8, data.tree-1.0.0, DataCombine-0.2.21, datasets, date-1.2-42, dbarts-0.9-23, DBI-1.1.3, dbplyr-2.3.1, dbscan-1.1-11, dcurver-0.9.2, ddalpha-1.3.13, deal-1.2-42, debugme-1.1.0, deldir-1.0-6, dendextend-1.16.0, DEoptim-2.2-8, DEoptimR-1.0-11, DepthProc-2.1.5, Deriv-4.1.3, desc-1.4.2, DescTools-0.99.48, deSolve-1.35, devtools-2.4.5, dfidx-0.0-5, DHARMa-0.4.6, dHSIC-2.1, diagram-1.6.5, DiagrammeR-1.0.9, DiceKriging-1.6.0, dichromat-2.0-0.1, diffobj-0.3.5, digest-0.6.31, dimRed-0.2.6, diptest-0.76-0, DiscriMiner-0.1-29, dismo-1.3-9, distillery-1.2-1, distr-2.9.1, distrEx-2.9.0, distributional-0.3.1, DistributionUtils-0.6-0, diveRsity-1.9.90, dlm-1.1-6, DMCfun-2.0.2, doc2vec-0.2.0, docstring-1.0.0, doMC-1.3.8, doParallel-1.0.17, doRNG-1.8.6, doSNOW-1.0.20, dotCall64-1.0-2, downlit-0.4.2, downloader-0.4, dplyr-1.1.0, dr-3.0.10, drgee-1.1.10, DRR-0.0.4, drugCombo-1.2.1, DT-0.27, dtangle-2.0.9, dtplyr-1.3.0, DTRreg-1.7, dtw-1.23-1, dummies-1.5.6, dygraphs-1.1.1.6, dynamicTreeCut-1.63-1, e1071-1.7-13, earth-5.3.2, EasyABC-1.5.2, ECOSolveR-0.5.5, elementR-1.3.7, ellipse-0.4.3, ellipsis-0.3.2, elliptic-1.4-0, emdbook-1.3.12, emmeans-1.8.5, emoa-0.5-0.1, emulator-1.2-21, energy-1.7-11, ENMeval-2.0.4, entropy-1.3.1, EnvStats-2.7.0, epitools-0.5-10.1, ergm-4.4.0, ergm.count-4.1.1, estimability-1.4.1, evaluate-0.20, EValue-4.1.3, evd-2.3-6.1, Exact-3.2, expm-0.999-7, ExPosition-2.8.23, expsmooth-2.3, extrafont-0.19, extrafontdb-1.0, extRemes-2.1-3, FactoMineR-2.7, FactorCopula-0.9.3, fail-1.3, fansi-1.0.4, farver-2.1.1, fastcluster-1.2.3, fastDummies-1.6.3, fasterize-1.0.4, fastICA-1.2-3, fastmap-1.1.1, fastmatch-1.1-3, fdrtool-1.2.17, feather-0.3.5, ff-4.0.9, fftw-1.0-7, fftwtools-0.9-11, fields-14.1, filehash-2.4-5, finalfit-1.0.6, findpython-1.0.8, fishMod-0.29, fitdistrplus-1.1-8, FKSUM-1.0.1, flashClust-1.01-2, flexclust-1.4-1, flexmix-2.3-19, flextable-0.9.2, fma-2.5, FME-1.3.6.2, fmri-1.9.11, FNN-1.1.3.1, fontawesome-0.5.0, fontBitstreamVera-0.1.1, fontLiberation-0.1.0, fontquiver-0.2.1, forcats-1.0.0, foreach-1.5.2, forecast-8.21, foreign-0.8-84, formatR-1.14, Formula-1.2-5, formula.tools-1.7.1, fossil-0.4.0, fpc-2.2-10, fpp-0.5, fracdiff-1.5-2, fs-1.6.1, furrr-0.3.1, futile.logger-1.4.3, futile.options-1.0.1, future-1.32.0, future.apply-1.10.0, gam-1.22-1, gamlss-5.4-12, gamlss.data-6.0-2, gamlss.dist-6.0-5, gamlss.tr-5.1-7, gamm4-0.2-6, gap-1.5-1, gap.datasets-0.0.5, gapfill-0.9.6-1, gargle-1.3.0, gaussquad-1.0-3, gbm-2.1.8.1, gbRd-0.4-11, gclus-1.3.2, gdalUtilities-1.2.5, gdalUtils-2.0.3.2, gdata-2.18.0.1, gdistance-1.6, gdtools-0.3.3, gee-4.13-25, geeM-0.10.1, geepack-1.3.9, geex-1.1.1, geiger-2.0.10, GeneNet-1.2.16, generics-0.1.3, genoPlotR-0.8.11, GenSA-1.1.8, geojson-0.3.5, geojsonio-0.11.3, geojsonsf-2.0.3, geometries-0.2.2, geometry-0.4.7, gert-1.9.2, getopt-1.20.3, GetoptLong-1.0.5, gfonts-0.2.0, GGally-2.1.2, ggbeeswarm-0.7.1, ggdag-0.2.7, ggExtra-0.10.0, ggfan-0.1.3, ggforce-0.4.1, ggformula-0.10.2, ggfun-0.0.9, ggh4x-0.2.3, ggnetwork-0.5.12, ggplot2-3.4.1, ggplotify-0.1.0, ggpubr-0.6.0, ggraph-2.1.0, ggrepel-0.9.3, ggridges-0.5.4, ggsci-3.0.0, ggsignif-0.6.4, ggstance-0.3.6, ggvenn-0.1.9, ggvis-0.4.8, gh-1.4.0, GillespieSSA-0.6.2, git2r-0.31.0, gitcreds-0.1.2, GJRM-0.2-6.1, glasso-1.11, gld-2.6.6, gllvm-1.4.1, glmmML-1.1.4, glmmTMB-1.1.5, glmnet-4.1-6, GlobalOptions-0.1.2, globals-0.16.2, glue-1.6.2, gmm-1.7, gmodels-2.18.1.1, gmp-0.7-1, gnumeric-0.7-8, goftest-1.2-3, gomms-1.0, googledrive-2.0.0, googlesheets4-1.0.1, gower-1.0.1, GPArotation-2022.10-2, gplots-3.1.3, graphics, graphlayouts-0.8.4, grDevices, grf-2.2.1, grid, gridBase-0.4-7, gridExtra-2.3, gridGraphics-0.5-1, grImport2-0.2-0, grpreg-3.4.0, GSA-1.03.2, gsalib-2.2.1, gsl-2.1-8, gsw-1.1-1, gt-0.8.0, gtable-0.3.1, gtools-3.9.4, gtsummary-1.7.0, GUTS-1.2.3, gWidgets2-1.0-9, gWidgets2tcltk-1.0-8, GxEScanR-2.0.2, h2o-3.40.0.1, hal9001-0.4.3, haldensify-0.2.3, hardhat-1.2.0, harmony-0.1.1, hash-2.2.6.2, haven-2.5.2, hdf5r-1.3.8, hdm-0.3.1, heatmap3-1.1.9, here-1.0.1, hexbin-1.28.2, HGNChelper-0.8.1, HiddenMarkov-1.8-13, highr-0.10, Hmisc-5.0-1, hms-1.1.2, Hmsc-3.0-13, htmlTable-2.4.1, htmltools-0.5.4, htmlwidgets-1.6.1, httpcode-0.3.0, httpuv-1.6.9, httr-1.4.5, httr2-0.2.2, huge-1.3.5, hunspell-3.0.2, hwriter-1.3.2.1, HWxtest-1.1.9, hypergeo-1.2-13, ica-1.0-3, IDPmisc-1.1.20, idr-1.3, ids-1.0.1, ie2misc-0.9.0, igraph-1.4.1, image.binarization-0.1.3, imager-0.42.18, imagerExtra-1.3.2, ineq-0.2-13, influenceR-0.1.0.1, infotheo-1.2.0.1, ini-0.3.1, inline-0.3.19, intergraph-2.0-2, interp-1.1-3, interpretR-0.2.4, intrinsicDimension-1.2.0, inum-1.0-5, ipred-0.9-14, irace-3.5, irlba-2.3.5.1, ismev-1.42, Iso-0.0-18.1, isoband-0.2.7, ISOcodes-2022.09.29, ISOweek-0.6-2, iterators-1.0.14, itertools-0.1-3, JADE-2.0-3, janeaustenr-1.0.0, JBTools-0.7.2.9, jiebaR-0.11, jiebaRD-0.1, jomo-2.7-4, jpeg-0.1-10, jqr-1.3.1, jquerylib-0.1.4, jsonify-1.2.2, jsonlite-1.8.4, jstable-1.0.7, juicyjuice-0.1.0, kde1d-1.0.5, kedd-1.0.3, kernlab-0.9-32, KernSmooth-2.23-20, kinship2-1.9.6, klaR-1.7-1, knitr-1.42, KODAMA-2.4, kohonen-3.0.11, ks-1.14.0, labdsv-2.0-1, labeling-0.4.2, labelled-2.10.0, laeken-0.5.2, lambda.r-1.2.4, LaplacesDemon-16.1.6, lars-1.3, lassosum-0.4.5, later-1.3.0, lattice-0.20-45, latticeExtra-0.6-30, lava-1.7.2.1, lavaan-0.6-15, lazy-1.2-18, lazyeval-0.2.2, LCFdata-2.0, lda-1.4.2, ldbounds-2.0.0, leafem-0.2.0, leaflet-2.1.2, leaflet.providers-1.9.0, leafsync-0.1.0, leaps-3.1, LearnBayes-2.15.1, leiden-0.4.3, lhs-1.1.6, libcoin-1.0-9, lifecycle-1.0.3, limSolve-1.5.6, linkcomm-1.0-14, linprog-0.9-4, liquidSVM-1.2.4, listenv-0.9.0, lme4-1.1-32, LMERConvenienceFunctions-3.0, lmerTest-3.1-3, lmom-2.9, Lmoments-1.3-1, lmtest-0.9-40, lobstr-1.1.2, locfdr-1.1-8, locfit-1.5-9.7, logcondens-2.1.7, logger-0.2.2, logistf-1.24.1, logspline-2.1.19, longitudinal-1.1.13, longmemo-1.1-2, loo-2.5.1, lpSolve-5.6.18, lpSolveAPI-5.5.2.0-17.9, lqa-1.0-3, lsei-1.3-0, lslx-0.6.11, lubridate-1.9.2, lwgeom-0.2-11, magic-1.6-1, magick-2.7.4, magrittr-2.0.3, MALDIquant-1.22, manipulateWidget-0.11.1, mapproj-1.2.11, maps-3.4.1, maptools-1.1-6, markdown-1.5, MASS-7.3-58.3, Matching-4.10-8, MatchIt-4.5.1, mathjaxr-1.6-0, matlab-1.0.4, Matrix-1.5-3, matrixcalc-1.0-6, MatrixModels-0.5-1, matrixStats-0.63.0, maxLik-1.5-2, maxlike-0.1-9, maxnet-0.1.4, mboost-2.9-7, mclogit-0.9.6, mclust-6.0.0, mcmc-0.9-7, MCMCpack-1.6-3, mcmcse-1.5-0, mda-0.5-3, medflex-0.6-7, mediation-4.5.0, memisc-0.99.31.6, memoise-2.0.1, memuse-4.2-3, MESS-0.5.9, metadat-1.2-0, metafor-3.8-1, MetaUtility-2.1.2, methods, mets-1.3.2, mgcv-1.8-42, mgsub-1.7.3, mhsmm-0.4.16, mi-1.1, mice-3.15.0, miceadds-3.16-18, microbenchmark-1.4.9, MIIVsem-0.5.8, mime-0.12, minerva-1.5.10, miniUI-0.1.1.1, minpack.lm-1.2-3, minqa-1.2.5, mirt-1.38.1, misc3d-0.9-1, miscTools-0.6-26, missForest-1.5, mitml-0.4-5, mitools-2.4, mixtools-2.0.0, mlbench-2.1-3, mlegp-3.1.9, MLmetrics-1.1.1, mlogit-1.1-1, mlr-2.19.1, mlrMBO-1.1.5.1, mltools-0.3.5, mnormt-2.1.1, ModelMetrics-1.2.2.2, modelr-0.1.10, modeltools-0.2-23, MODIStsp-2.1.0, momentfit-0.3, moments-0.14.1, MonteCarlo-1.0.6, mosaicCore-0.9.2.1, mpath-0.4-2.23, mRMRe-2.1.2, msm-1.7, mstate-0.3.2, multcomp-1.4-23, multcompView-0.1-8, multicool-0.1-12, multipol-1.0-7, munsell-0.5.0, mvabund-4.2.1, mvnfast-0.2.8, mvtnorm-1.1-3, nabor-0.5.0, naniar-1.0.0, natserv-1.0.0, naturalsort-0.1.3, ncbit-2013.03.29.1, ncdf4-1.21, NCmisc-1.2.0, network-1.18.1, networkDynamic-0.11.3, networkLite-1.0.5, neuralnet-1.44.2, neuRosim-0.2-13, ngspatial-1.2-2, NISTunits-1.0.1, nleqslv-3.3.4, nlme-3.1-162, nloptr-2.0.3, NLP-0.2-1, nlsem-0.8, nnet-7.3-18, nnls-1.4, nonnest2-0.5-5, nor1mix-1.3-0, norm-1.0-10.0, nortest-1.0-4, np-0.60-17, npsurv-0.5-0, numDeriv-2016.8-1.1, oai-0.4.0, oce-1.7-10, OceanView-1.0.6, oddsratio-2.0.1, officer-0.6.2, openair-2.16-0, OpenMx-2.21.1, openssl-2.0.6, openxlsx-4.2.5.2, operator.tools-1.6.3, optextras-2019-12.4, optimParallel-1.0-2, optimr-2019-12.16, optimx-2022-4.30, optmatch-0.10.6, optparse-1.7.3, ordinal-2022.11-16, origami-1.0.7, oro.nifti-0.11.4, orthopolynom-1.0-6.1, osqp-0.6.0.8, outliers-0.15, packrat-0.9.1, pacman-0.5.1, pammtools-0.5.8, pamr-1.56.1, pan-1.6, parallel, parallelDist-0.2.6, parallelly-1.34.0, parallelMap-1.5.1, ParamHelpers-1.14.1, parsedate-1.3.1, party-1.3-13, partykit-1.2-18, pastecs-1.3.21, patchwork-1.1.2, pbapply-1.7-0, pbivnorm-0.6.0, pbkrtest-0.5.2, PCAmatchR-0.3.3, pcaPP-2.0-3, pdp-0.8.1, PearsonDS-1.2.3, pec-2022.05.04, penalized-0.9-52, penfa-0.1.1, peperr-1.4, PermAlgo-1.2, permute-0.9-7, phangorn-2.11.1, pheatmap-1.0.12, phylobase-0.8.10, phytools-1.5-1, pillar-1.8.1, pim-2.0.2, pinfsc50-1.2.0, pixmap-0.4-12, pkgbuild-1.4.0, pkgconfig-2.0.3, pkgdown-2.0.7, pkgload-1.3.2, pkgmaker-0.32.8, plogr-0.2.0, plot3D-1.4, plot3Drgl-1.0.4, plotly-4.10.1, plotmo-3.6.2, plotrix-3.8-2, pls-2.8-1, plyr-1.8.8, PMA-1.2.1, png-0.1-8, PoissonSeq-1.1.2, poLCA-1.6.0.1, polspline-1.1.22, Polychrome-1.5.1, polyclip-1.10-4, polycor-0.8-1, polynom-1.4-1, posterior-1.4.1, ppcor-1.1, prabclus-2.3-2, pracma-2.4.2, praise-1.0.0, PresenceAbsence-1.1.11, preseqR-4.0.0, prettyGraphs-2.1.6, prettyunits-1.1.1, princurve-2.1.6, pROC-1.18.0, processx-3.8.0, prodlim-2019.11.13, profileModel-0.6.1, proftools-0.99-3, profvis-0.3.7, progress-1.2.2, progressr-0.13.0, projpred-2.4.0, promises-1.2.0.1, proto-1.0.0, protolite-2.3.0, proxy-0.4-27, proxyC-0.3.3, pryr-0.1.6, ps-1.7.2, pscl-1.5.5, pspline-1.0-19, psych-2.2.9, Publish-2023.01.17, pulsar-0.3.10, purrr-1.0.1, pvclust-2.2-0, qgam-1.3.4, qgraph-1.9.3, qqman-0.1.8, qrnn-2.0.5, quadprog-1.5-8, quanteda-3.3.0, quantmod-0.4.20, quantreg-5.94, questionr-0.7.8, R.cache-0.16.0, R.matlab-3.7.0, R.methodsS3-1.8.2, R.oo-1.25.0, R.rsp-0.45.0, R.utils-2.12.2, R2WinBUGS-2.1-21, R6-2.5.1, ragg-1.2.5, random-0.2.6, randomForest-4.7-1.1, randomForestSRC-3.2.1, randtoolbox-2.0.4, rangeModelMetadata-0.1.4, ranger-0.14.1, RANN-2.6.1, rapidjsonr-1.2.0, rappdirs-0.3.3, rARPACK-0.11-0, raster-3.6-20, rasterVis-0.51.5, ratelimitr-0.4.1, RBesT-1.6-6, rbibutils-2.2.13, rbison-1.0.0, Rborist-0.3-2, RCAL-2.0, Rcgmin-2022-4.30, RCircos-1.2.2, rcmdcheck-1.4.0, RColorBrewer-1.1-3, Rcpp-1.0.10, RcppArmadillo-0.12.0.1.0, RcppEigen-0.3.3.9.3, RcppGSL-0.3.13, RcppParallel-5.1.7, RcppProgress-0.4.2, RcppRoll-0.3.0, RcppThread-2.1.3, RcppTOML-0.2.2, RCurl-1.98-1.10, rda-1.2-1, Rdpack-2.4, rdrop2-0.8.2.1, readbitmap-0.1.5, reader-1.0.6, readODS-1.8.0, readr-2.1.4, readxl-1.4.2, rebird-1.3.0, recipes-1.0.5, RefFreeEWAS-2.2, registry-0.5-1, regsem-1.9.3, relsurv-2.2-9, rematch-1.0.1, rematch2-2.1.2, remotes-2.4.2, rentrez-1.2.3, renv-0.17.1, reprex-2.0.2, resample-0.6, reshape-0.8.9, reshape2-1.4.4, reticulate-1.28, rex-1.2.1, rgbif-3.7.5, RGCCA-2.1.2, rgdal-1.6-5, rgeos-0.6-2, rgexf-0.16.2, rgl-1.0.1, Rglpk-0.6-4, RhpcBLASctl-0.23-42, ridge-3.3, ridigbio-0.3.6, RInside-0.2.18, rio-0.5.29, riskRegression-2022.11.28, ritis-1.0.0, RItools-0.3-3, rJava-1.0-6, rjson-0.2.21, RJSONIO-1.3-1.8, rlang-1.1.0, rle-0.9.2, rlecuyer-0.3-5, rlemon-0.2.1, rlist-0.4.6.2, rmarkdown-2.20, rmeta-3.0, Rmpfr-0.9-1, rms-6.5-0, RMTstat-0.3.1, rncl-0.8.7, rnetcarto-0.2.6, RNeXML-2.4.11, rngtools-1.5.2, rngWELL-0.10-9, RNifti-1.4.5, robustbase-0.95-0, ROCR-1.0-11, ROI-1.0-0, ROI.plugin.glpk-1.0-0, Rook-1.2, rootSolve-1.8.2.3, roptim-0.1.6, rotl-3.0.14, roxygen2-7.2.3, rpact-3.3.4, rpart-4.1.19, rpf-1.0.11, RPMM-1.25, rprojroot-2.0.3, rrcov-1.7-2, rredlist-0.7.1, rsample-1.1.1, rsconnect-0.8.29, Rserve-1.8-11, RSNNS-0.4-15, Rsolnp-1.16, RSpectra-0.16-1, RSQLite-2.3.0, Rssa-1.0.5, rstan-2.21.8, rstantools-2.3.0, rstatix-0.7.2, rstudioapi-0.14, rtdists-0.11-5, Rtsne-0.16, Rttf2pt1-1.3.12, RUnit-0.4.32, ruv-0.9.7.1, rversions-2.1.2, rvertnet-0.8.2, rvest-1.0.3, rvinecopulib-0.6.3.1.1, Rvmmin-2018-4.17.1, RWeka-0.4-46, RWekajars-3.9.3-2, s2-1.1.2, sampling-2.9, sandwich-3.0-2, sass-0.4.5, SBdecomp-1.2, scales-1.2.1, scam-1.2-13, scatterpie-0.1.8, scatterplot3d-0.3-43, scs-3.2.4, sctransform-0.3.5, SDMTools-1.1-221.2, seewave-2.2.0, segmented-1.6-2, selectr-0.4-2, sem-3.1-15, semPLS-1.0-10, semTools-0.5-6, sendmailR-1.4-0, sensemakr-0.1.4, sentometrics-1.0.0, seqinr-4.2-23, servr-0.25, sessioninfo-1.2.2, setRNG-2022.4-1, sf-1.0-11, sfheaders-0.4.2, sfsmisc-1.1-14, shadowtext-0.1.2, shape-1.4.6, shapefiles-0.7.2, shiny-1.7.4, shinycssloaders-1.0.0, shinydashboard-0.7.2, shinyjs-2.1.0, shinystan-2.6.0, shinythemes-1.2.0, signal-0.7-7, SignifReg-4.3, simex-1.8, SimSeq-1.4.0, SKAT-2.2.5, slam-0.1-50, slider-0.3.0, sm-2.2-5.7.1, smoof-1.6.0.3, smoother-1.1, sn-2.1.0, sna-2.7-1, SNFtool-2.3.1, snow-0.4-4, SnowballC-0.7.0, snowfall-1.84-6.2, SOAR-0.99-11, solrium-1.2.0, som-0.3-5.1, soundecology-1.3.3, sourcetools-0.1.7-1, sp-1.6-0, spaa-0.2.2, spam-2.9-1, spaMM-4.2.1, SparseM-1.81, SPAtest-3.1.2, spatial-7.3-16, spatstat-3.0-3, spatstat.core-2.4-4, spatstat.data-3.0-1, spatstat.explore-3.1-0, spatstat.geom-3.1-0, spatstat.linnet-3.0-6, spatstat.model-3.2-1, spatstat.random-3.1-4, spatstat.sparse-3.0-1, spatstat.utils-3.0-2, spData-2.2.2, splines, splitstackshape-1.4.8, spls-2.2-3, spocc-1.2.1, spThin-0.2.0, SQUAREM-2021.1, stabledist-0.7-1, stabs-0.6-4, StanHeaders-2.21.0-7, stargazer-5.2.3, stars-0.6-0, startupmsg-0.9.6, StatMatch-1.4.1, statmod-1.5.0, statnet-2019.6, statnet.common-4.8.0, stats, stats4, stdReg-3.4.1, stopwords-2.3, stringdist-0.9.10, stringi-1.7.12, stringr-1.5.0, strucchange-1.5-3, styler-1.9.1, subplex-1.8, SuperLearner-2.0-28, SuppDists-1.1-9.7, survey-4.1-1, survival-3.5-5, survivalROC-1.0.3.1, svd-0.5.3, svglite-2.1.1, swagger-3.33.1, symmoments-1.2.1, sys-3.4.1, systemfonts-1.0.4, tableone-0.13.2, tabletools-0.1.0, tau-0.0-24, taxize-0.9.100, tcltk, tcltk2-1.2-11, tclust-1.5-2, TeachingDemos-2.12, tensor-1.5, tensorA-0.36.2, tergm-4.1.1, terra-1.7-18, testit-0.13, testthat-3.1.7, textcat-1.0-8, textplot-0.2.2, textshaping-0.3.6, TFisher-0.2.0, TH.data-1.1-1, threejs-0.3.3, tibble-3.2.0, tictoc-1.1, tidygraph-1.2.3, tidyr-1.3.0, tidyselect-1.2.0, tidytext-0.4.1, tidytree-0.4.2, tidyverse-2.0.0, tiff-0.1-11, timechange-0.2.0, timeDate-4022.108, timereg-2.0.5, tinytex-0.44, tkrplot-0.0-27, tm-0.7-11, tmap-3.3-3, tmaptools-3.1-1, TMB-1.9.2, tmle-1.5.0.2, tmvnsim-1.0-2, tmvtnorm-1.5, tokenizers-0.3.0, tools, topicmodels-0.2-13, TraMineR-2.2-6, tree-1.0-43, triebeard-0.4.1, trimcluster-0.1-5, tripack-1.3-9.1, TruncatedNormal-2.2.2, truncnorm-1.0-8, trust-0.1-8, tseries-0.10-53, tseriesChaos-0.1-13.1, tsna-0.3.5, tsne-0.1-3.1, TTR-0.24.3, tuneR-1.4.3, twang-2.5, tweedie-2.3.5, tweenr-2.0.2, tzdb-0.3.0, ucminf-1.1-4.1, udpipe-0.8.11, umap-0.2.10.0, unbalanced-2.0, unikn-0.8.0, uniqueAtomMat-0.1-3-2, units-0.8-1, unmarked-1.2.5, UpSetR-1.4.0, urca-1.3-3, urlchecker-1.0.1, urltools-1.7.3, uroot-2.1-2, usethis-2.1.6, utf8-1.2.3, utils, uuid-1.1-0, V8-4.2.2, varhandle-2.0.5, vcd-1.4-11, vcfR-1.14.0, vctrs-0.6.0, vegan-2.6-4, VennDiagram-1.7.3, VGAM-1.1-8, VIM-6.2.2, VineCopula-2.4.5, vioplot-0.4.0, vipor-0.4.5, viridis-0.6.2, viridisLite-0.4.1, visdat-0.6.0, visNetwork-2.1.2, vroom-1.6.1, VSURF-1.2.0, waldo-0.4.0, warp-0.2.0, waveslim-1.8.4, wdm-0.2.3, webshot-0.5.4, webutils-1.1, weights-1.0.4, WeightSVM-1.7-11, wellknown-0.7.4, whisker-0.4.1, widgetframe-0.3.1, WikidataQueryServiceR-1.0.0, WikidataR-2.3.3, WikipediR-1.5.0, wikitaxa-0.4.0, withr-2.5.0, wk-0.7.1, word2vec-0.3.4, wordcloud-2.6, worrms-0.4.2, WriteXLS-6.4.0, xfun-0.37, xgboost-1.7.3.1, xlsx-0.6.5, xlsxjars-0.6.1, XML-3.99-0.13, xml2-1.3.3, xopen-1.0.0, xtable-1.8-4, xts-0.13.0, yaImpute-1.0-33, yaml-2.3.7, yulab.utils-0.0.6, zeallot-0.1.0, zip-2.2.2, zoo-1.8-11</p>"},{"location":"available_software/detail/R/#r363-foss-2022b","title":"R/3.6.3-foss-2022b","text":"<p>This is a list of extensions included in the module:</p> <p>base, compiler, datasets, graphics, grDevices, grid, methods, parallel, splines, stats, stats4, tcltk, tools, utils</p>"},{"location":"available_software/detail/RASPA2/","title":"RASPA2","text":"<p>RASPA is a software package for simulating adsorption and diffusion ofmolecules in flexible nanoporous materials.</p> <p>https://iraspa.org/raspa/</p>"},{"location":"available_software/detail/RASPA2/#available-modules","title":"Available modules","text":"<p>The overview below shows which RASPA2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RASPA2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RASPA2/2.0.47-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RASPA2/2.0.47-foss-2022b - x - - - -"},{"location":"available_software/detail/RAxML/","title":"RAxML","text":"<p>RAxML search algorithm for maximum likelihood based inference of phylogenetic trees.</p> <p>https://github.com/stamatak/standard-RAxML</p>"},{"location":"available_software/detail/RAxML/#available-modules","title":"Available modules","text":"<p>The overview below shows which RAxML installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RAxML, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RAxML/8.2.12-GCC-12.3.0-pthreads-avx\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RAxML/8.2.12-GCC-12.3.0-pthreads-avx - x - - - -"},{"location":"available_software/detail/RE2/","title":"RE2","text":"<p>RE2 is a fast, safe, thread-friendly alternative to backtracking regularexpression engines like those used in PCRE, Perl, and Python. It is a C++library.</p> <p>https://github.com/google/re2</p>"},{"location":"available_software/detail/RE2/#available-modules","title":"Available modules","text":"<p>The overview below shows which RE2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RE2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RE2/2023-08-01-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RE2/2023-08-01-GCCcore-12.3.0 - x - - - - RE2/2023-03-01-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/RECON/","title":"RECON","text":"<p>Patched version of RECON to be used with RepeatModeler.</p> <p>https://www.repeatmasker.org/RepeatModeler/</p>"},{"location":"available_software/detail/RECON/#available-modules","title":"Available modules","text":"<p>The overview below shows which RECON installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RECON, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RECON/1.08-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RECON/1.08-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/RMBlast/","title":"RMBlast","text":"<p>RMBlast is a RepeatMasker compatible version of the standard NCBI BLAST suite. The primary difference between this distribution and the NCBI distribution is the addition of a new program 'rmblastn' for use with RepeatMasker and RepeatModeler.</p> <p>https://www.repeatmasker.org/rmblast/</p>"},{"location":"available_software/detail/RMBlast/#available-modules","title":"Available modules","text":"<p>The overview below shows which RMBlast installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RMBlast, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RMBlast/2.14.1-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RMBlast/2.14.1-gompi-2023a - x - - - -"},{"location":"available_software/detail/RSEM/","title":"RSEM","text":"<p>RNA-Seq by Expectation-Maximization</p> <p>https://deweylab.github.io/RSEM/</p>"},{"location":"available_software/detail/RSEM/#available-modules","title":"Available modules","text":"<p>The overview below shows which RSEM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RSEM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RSEM/1.3.3-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RSEM/1.3.3-foss-2023a - x - - - -"},{"location":"available_software/detail/RStudio-Server/","title":"RStudio-Server","text":"<p>This is the RStudio Server version.RStudio is a set of integrated tools designed to help you be more productive with R.The server can be started with:  rserver --server-daemonize=0 --www-port 8787 --rsession-which-r=$(which R)</p> <p>https://www.rstudio.com/</p>"},{"location":"available_software/detail/RStudio-Server/#available-modules","title":"Available modules","text":"<p>The overview below shows which RStudio-Server installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RStudio-Server, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RStudio-Server/2023.12.1-402-jammy-amd64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RStudio-Server/2023.12.1-402-jammy-amd64 - x - - - -"},{"location":"available_software/detail/RapidJSON/","title":"RapidJSON","text":"<p>A fast JSON parser/generator for C++ with both SAX/DOM style API</p> <p>https://rapidjson.org</p>"},{"location":"available_software/detail/RapidJSON/#available-modules","title":"Available modules","text":"<p>The overview below shows which RapidJSON installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RapidJSON, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RapidJSON/1.1.0-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RapidJSON/1.1.0-GCCcore-12.2.0 - x - - - - RapidJSON/1.1.0-20230928-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/RepeatMasker/","title":"RepeatMasker","text":"<p>RepeatMasker is a program that screens DNA sequences for interspersed repeats and low complexity DNA sequences.</p> <p>https://www.repeatmasker.org/</p>"},{"location":"available_software/detail/RepeatMasker/#available-modules","title":"Available modules","text":"<p>The overview below shows which RepeatMasker installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RepeatMasker, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RepeatMasker/4.1.6-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RepeatMasker/4.1.6-foss-2023a - x - - - -"},{"location":"available_software/detail/RepeatModeler/","title":"RepeatModeler","text":"<p>RepeatModeler is a de novo transposable element (TE) family identification and modeling package.</p> <p>https://www.repeatmasker.org/</p>"},{"location":"available_software/detail/RepeatModeler/#available-modules","title":"Available modules","text":"<p>The overview below shows which RepeatModeler installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RepeatModeler, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RepeatModeler/2.0.5-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RepeatModeler/2.0.5-foss-2023a - x - - - -"},{"location":"available_software/detail/RepeatScout/","title":"RepeatScout","text":"<p>De Novo Repeat Finder, Price A.L., Jones N.C. and Pevzner P.A. Developed and tested with our multiple sequence version of RepeatScout ( 1.0.6 )</p> <p>https://www.repeatmasker.org/</p>"},{"location":"available_software/detail/RepeatScout/#available-modules","title":"Available modules","text":"<p>The overview below shows which RepeatScout installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using RepeatScout, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load RepeatScout/1.0.6-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 RepeatScout/1.0.6-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Rivet/","title":"Rivet","text":"<p>Rivet toolkit (Robust Independent Validation of Experiment and Theory)To use your own analysis you must append the path to <code>RIVET_ANALYSIS_PATH</code>.</p> <p>https://gitlab.com/hepcedar/rivet</p>"},{"location":"available_software/detail/Rivet/#available-modules","title":"Available modules","text":"<p>The overview below shows which Rivet installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Rivet, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Rivet/3.1.9-gompi-2023a-HepMC3-3.2.6\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Rivet/3.1.9-gompi-2023a-HepMC3-3.2.6 - x - - - -"},{"location":"available_software/detail/Ruby/","title":"Ruby","text":"<p>Ruby is a dynamic, open source programming language with a focus on simplicity and productivity. It has an elegant syntax that is natural to read and easy to write.</p> <p>https://www.ruby-lang.org</p>"},{"location":"available_software/detail/Ruby/#available-modules","title":"Available modules","text":"<p>The overview below shows which Ruby installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Ruby, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Ruby/3.3.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Ruby/3.3.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Rust/","title":"Rust","text":"<p>Rust is a systems programming language that runs blazingly fast, prevents segfaults, and guarantees thread safety.</p> <p>https://www.rust-lang.org</p>"},{"location":"available_software/detail/Rust/#available-modules","title":"Available modules","text":"<p>The overview below shows which Rust installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Rust, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Rust/1.73.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Rust/1.73.0-GCCcore-13.2.0 - x - - - - Rust/1.70.0-GCCcore-12.3.0 - x x x x x Rust/1.65.0-GCCcore-12.2.0 - x - - - - Rust/1.60.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/SAMtools/","title":"SAMtools","text":"<p>SAM Tools provide various utilities for manipulating alignments in the SAM format,  including sorting, merging, indexing and generating alignments in a per-position format.</p> <p>https://www.htslib.org/</p>"},{"location":"available_software/detail/SAMtools/#available-modules","title":"Available modules","text":"<p>The overview below shows which SAMtools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SAMtools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SAMtools/1.19.2-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SAMtools/1.19.2-GCC-13.2.0 - x - - - - SAMtools/1.18-GCC-12.3.0 - x - - - - SAMtools/1.17-GCC-12.2.0 - x - - - - SAMtools/1.16.1-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/SCOTCH/","title":"SCOTCH","text":"<p>Software package and libraries for sequential and parallel graph partitioning,static mapping, and sparse matrix block ordering, and sequential mesh and hypergraph partitioning.</p> <p>https://www.labri.fr/perso/pelegrin/scotch/</p>"},{"location":"available_software/detail/SCOTCH/#available-modules","title":"Available modules","text":"<p>The overview below shows which SCOTCH installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SCOTCH, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SCOTCH/7.0.3-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SCOTCH/7.0.3-gompi-2023a - x - - - -"},{"location":"available_software/detail/SCons/","title":"SCons","text":"<p>SCons is a software construction tool.</p> <p>https://www.scons.org</p>"},{"location":"available_software/detail/SCons/#available-modules","title":"Available modules","text":"<p>The overview below shows which SCons installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SCons, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SCons/4.5.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SCons/4.5.2-GCCcore-12.3.0 - x - - - - SCons/4.4.0-GCCcore-12.2.0 - x - - - - SCons/4.4.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/SCons/#scons452-gcccore-1230","title":"SCons/4.5.2-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>SCons-4.5.2</p>"},{"location":"available_software/detail/SCons/#scons440-gcccore-1220","title":"SCons/4.4.0-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>SCons-4.4.0</p>"},{"location":"available_software/detail/SCons/#scons440-gcccore-1130","title":"SCons/4.4.0-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>SCons-4.4.0</p>"},{"location":"available_software/detail/SDL2/","title":"SDL2","text":"<p>SDL: Simple DirectMedia Layer, a cross-platform multimedia library</p> <p>https://www.libsdl.org/</p>"},{"location":"available_software/detail/SDL2/#available-modules","title":"Available modules","text":"<p>The overview below shows which SDL2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SDL2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SDL2/2.28.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SDL2/2.28.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/SEPP/","title":"SEPP","text":"<p>SATe-enabled Phylogenetic Placement - addresses the problem of phylogeneticplacement of short reads into reference alignments and trees.</p> <p>https://github.com/smirarab/sepp</p>"},{"location":"available_software/detail/SEPP/#available-modules","title":"Available modules","text":"<p>The overview below shows which SEPP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SEPP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SEPP/4.5.5-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SEPP/4.5.5-foss-2023a - x - - - - SEPP/4.5.1-foss-2022b - x - - - -"},{"location":"available_software/detail/SHAPEIT4/","title":"SHAPEIT4","text":"<p>SHAPEIT4 is a fast and accurate method for estimation of haplotypes(aka phasing) for SNP array and high coverage sequencing data.</p> <p>https://odelaneau.github.io/shapeit4/</p>"},{"location":"available_software/detail/SHAPEIT4/#available-modules","title":"Available modules","text":"<p>The overview below shows which SHAPEIT4 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SHAPEIT4, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SHAPEIT4/4.2.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SHAPEIT4/4.2.2-foss-2023a - x - - - -"},{"location":"available_software/detail/SIP/","title":"SIP","text":"<p>SIP is a tool that makes it very easy to create Python bindings for C and C++ libraries.</p> <p>http://www.riverbankcomputing.com/software/sip/</p>"},{"location":"available_software/detail/SIP/#available-modules","title":"Available modules","text":"<p>The overview below shows which SIP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SIP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SIP/6.8.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SIP/6.8.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/SNAP-HMM/","title":"SNAP-HMM","text":"<p>SNAP is a general purpose gene finding program suitable for both eukaryotic andprokaryotic genomes. SNAP is an acroynm for Semi-HMM-based Nucleic AcidParser.</p> <p>https://korflab.github.io/</p>"},{"location":"available_software/detail/SNAP-HMM/#available-modules","title":"Available modules","text":"<p>The overview below shows which SNAP-HMM installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SNAP-HMM, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SNAP-HMM/20221022-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SNAP-HMM/20221022-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/SPAdes/","title":"SPAdes","text":"<p>Genome assembler for single-cell and isolates data sets</p> <p>https://github.com/ablab/spades</p>"},{"location":"available_software/detail/SPAdes/#available-modules","title":"Available modules","text":"<p>The overview below shows which SPAdes installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SPAdes, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SPAdes/3.15.4-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SPAdes/3.15.4-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/SQLite/","title":"SQLite","text":"<p>SQLite: SQL Database Engine in a C Library</p> <p>https://www.sqlite.org/</p>"},{"location":"available_software/detail/SQLite/#available-modules","title":"Available modules","text":"<p>The overview below shows which SQLite installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SQLite, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SQLite/3.43.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SQLite/3.43.1-GCCcore-13.2.0 - x - - - - SQLite/3.42.0-GCCcore-12.3.0 - x x x x x SQLite/3.39.4-GCCcore-12.2.0 - x - - - - SQLite/3.38.3-GCCcore-11.3.0 - x - - - - SQLite/3.33.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/SRA-Toolkit/","title":"SRA-Toolkit","text":"<p>The SRA Toolkit, and the source-code SRA System Development Kit (SDK), will allow you to programmatically access data housed within SRA and convert it from the SRA format</p> <p>https://github.com/ncbi/sra-tools</p>"},{"location":"available_software/detail/SRA-Toolkit/#available-modules","title":"Available modules","text":"<p>The overview below shows which SRA-Toolkit installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SRA-Toolkit, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SRA-Toolkit/3.0.10-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SRA-Toolkit/3.0.10-gompi-2023a - x - - - -"},{"location":"available_software/detail/SSW/","title":"SSW","text":"<p>SSW is a fast implementation of the Smith-Waterman algorithm, which uses the Single-Instruction Multiple-Data (SIMD) instructions to parallelize the algorithm at the instruction level. SSW library provides an API that can be flexibly used by programs written in C, C++ and other languages. We also provide a software that can do protein and genome alignment directly. Current version of our implementation is ~50 times faster than an ordinary Smith-Waterman. It can return the Smith-Waterman score, alignment location and traceback path (cigar) of the optimal alignment accurately; and return the sub-optimal alignment score and location heuristically.</p> <p>https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library</p>"},{"location":"available_software/detail/SSW/#available-modules","title":"Available modules","text":"<p>The overview below shows which SSW installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SSW, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SSW/1.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SSW/1.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/STAR/","title":"STAR","text":"<p>STAR aligns RNA-seq reads to a reference genome using uncompressed suffix arrays.</p> <p>https://github.com/alexdobin/STAR</p>"},{"location":"available_software/detail/STAR/#available-modules","title":"Available modules","text":"<p>The overview below shows which STAR installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using STAR, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load STAR/2.7.11b-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 STAR/2.7.11b-GCC-13.2.0 - x - - - - STAR/2.7.11b-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/SUNDIALS/","title":"SUNDIALS","text":"<p>SUNDIALS: SUite of Nonlinear and DIfferential/ALgebraic Equation Solvers</p> <p>https://computing.llnl.gov/projects/sundials</p>"},{"location":"available_software/detail/SUNDIALS/#available-modules","title":"Available modules","text":"<p>The overview below shows which SUNDIALS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SUNDIALS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SUNDIALS/7.1.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SUNDIALS/7.1.0-foss-2023a - x - - - - SUNDIALS/6.6.0-foss-2023a - x - - - -"},{"location":"available_software/detail/SWIG/","title":"SWIG","text":"<p>SWIG is a software development tool that connects programs written in C and C++ with a variety of high-level programming languages.</p> <p>http://www.swig.org/</p>"},{"location":"available_software/detail/SWIG/#available-modules","title":"Available modules","text":"<p>The overview below shows which SWIG installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SWIG, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SWIG/4.1.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SWIG/4.1.1-GCCcore-12.3.0 - x - - - - SWIG/4.1.1-GCCcore-12.2.0 - x - - - - SWIG/4.0.2-GCCcore-11.3.0 - - - - x x"},{"location":"available_software/detail/Salmon/","title":"Salmon","text":"<p>Salmon is a wicked-fast program to produce a highly-accurate, transcript-level quantification estimates from RNA-seq data.</p> <p>https://github.com/COMBINE-lab/salmon</p>"},{"location":"available_software/detail/Salmon/#available-modules","title":"Available modules","text":"<p>The overview below shows which Salmon installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Salmon, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Salmon/1.10.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Salmon/1.10.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/ScaFaCoS/","title":"ScaFaCoS","text":"<p>ScaFaCoS is a library of scalable fast coulomb solvers.</p> <p>http://www.scafacos.de/</p>"},{"location":"available_software/detail/ScaFaCoS/#available-modules","title":"Available modules","text":"<p>The overview below shows which ScaFaCoS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ScaFaCoS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ScaFaCoS/1.0.4-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ScaFaCoS/1.0.4-foss-2023a - x - - - -"},{"location":"available_software/detail/ScaLAPACK/","title":"ScaLAPACK","text":"<p>The ScaLAPACK (or Scalable LAPACK) library includes a subset of LAPACK routines redesigned for distributed memory MIMD parallel computers.</p> <p>https://www.netlib.org/scalapack/</p>"},{"location":"available_software/detail/ScaLAPACK/#available-modules","title":"Available modules","text":"<p>The overview below shows which ScaLAPACK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ScaLAPACK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ScaLAPACK/2.2.0-nvompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ScaLAPACK/2.2.0-nvompi-2023a - - - x - - ScaLAPACK/2.2.0-nvompi-2022.07-fb - x - - - - ScaLAPACK/2.2.0-gompi-2023b-fb - x - - - - ScaLAPACK/2.2.0-gompi-2023a-fb - x x x x x ScaLAPACK/2.2.0-gompi-2022b-fb - x - - - - ScaLAPACK/2.2.0-gompi-2022a-fb - x - - - - ScaLAPACK/2.1.0-gompi-2020b - x - - - -"},{"location":"available_software/detail/Schrodinger/","title":"Schrodinger","text":"<p>Schrodinger aims to provide integrated software solutions and services  that truly meet its customers needs. We want to empower researchers around the world to  achieve their goals of improving human health and quality of life through advanced computational  techniques that transform the way chemists design compounds and materials.</p> <p>https://www.schrodinger.com/</p>"},{"location":"available_software/detail/Schrodinger/#available-modules","title":"Available modules","text":"<p>The overview below shows which Schrodinger installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Schrodinger, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Schrodinger/2024-1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Schrodinger/2024-1 - x - - - - Schrodinger/2023-4 - x - - - -"},{"location":"available_software/detail/SciPy-bundle/","title":"SciPy-bundle","text":"<p>Bundle of Python packages for scientific software</p> <p>https://python.org/</p>"},{"location":"available_software/detail/SciPy-bundle/#available-modules","title":"Available modules","text":"<p>The overview below shows which SciPy-bundle installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SciPy-bundle, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SciPy-bundle/2023.11-gfbf-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SciPy-bundle/2023.11-gfbf-2023b - x - - - - SciPy-bundle/2023.07-gfbf-2023a - x - - - - SciPy-bundle/2023.02-gfbf-2022b - x - - - - SciPy-bundle/2022.05-foss-2022a - x - - - - SciPy-bundle/2020.11-foss-2020b - x - - - -"},{"location":"available_software/detail/SciPy-bundle/#scipy-bundle202311-gfbf-2023b","title":"SciPy-bundle/2023.11-gfbf-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>beniget-0.4.1, Bottleneck-1.3.7, deap-1.4.1, gast-0.5.4, mpmath-1.3.0, numexpr-2.8.7, numpy-1.26.2, pandas-2.1.3, ply-3.11, pythran-0.14.0, scipy-1.11.4, tzdata-2023.3, versioneer-0.29</p>"},{"location":"available_software/detail/SciPy-bundle/#scipy-bundle202307-gfbf-2023a","title":"SciPy-bundle/2023.07-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>beniget-0.4.1, Bottleneck-1.3.7, deap-1.4.0, gast-0.5.4, mpmath-1.3.0, numexpr-2.8.4, numpy-1.25.1, pandas-2.0.3, ply-3.11, pythran-0.13.1, scipy-1.11.1, tzdata-2023.3, versioneer-0.29</p>"},{"location":"available_software/detail/SciPy-bundle/#scipy-bundle202302-gfbf-2022b","title":"SciPy-bundle/2023.02-gfbf-2022b","text":"<p>This is a list of extensions included in the module:</p> <p>beniget-0.4.1, Bottleneck-1.3.5, deap-1.3.3, gast-0.5.3, mpmath-1.2.1, numexpr-2.8.4, numpy-1.24.2, pandas-1.5.3, ply-3.11, pythran-0.12.1, scipy-1.10.1</p>"},{"location":"available_software/detail/SciPy-bundle/#scipy-bundle202205-foss-2022a","title":"SciPy-bundle/2022.05-foss-2022a","text":"<p>This is a list of extensions included in the module:</p> <p>beniget-0.4.1, Bottleneck-1.3.4, deap-1.3.3, gast-0.5.3, mpi4py-3.1.3, mpmath-1.2.1, numexpr-2.8.1, numpy-1.22.3, pandas-1.4.2, ply-3.11, pythran-0.11.0, scipy-1.8.1</p>"},{"location":"available_software/detail/SciPy-bundle/#scipy-bundle202011-foss-2020b","title":"SciPy-bundle/2020.11-foss-2020b","text":"<p>This is a list of extensions included in the module:</p> <p>Bottleneck-1.3.2, deap-1.3.1, mpi4py-3.0.3, mpmath-1.1.0, numexpr-2.7.1, numpy-1.19.4, pandas-1.1.4, scipy-1.5.4</p>"},{"location":"available_software/detail/SeqKit/","title":"SeqKit","text":"<p>SeqKit - a cross-platform and ultrafast toolkit for FASTA/Q file manipulation</p> <p>https://bioinf.shenwei.me/seqkit/</p>"},{"location":"available_software/detail/SeqKit/#available-modules","title":"Available modules","text":"<p>The overview below shows which SeqKit installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SeqKit, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SeqKit/2.3.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SeqKit/2.3.1 - x - - - -"},{"location":"available_software/detail/SeqLib/","title":"SeqLib","text":"<p>C++ interface to HTSlib, BWA-MEM and Fermi.</p> <p>https://github.com/walaj/SeqLib</p>"},{"location":"available_software/detail/SeqLib/#available-modules","title":"Available modules","text":"<p>The overview below shows which SeqLib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SeqLib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SeqLib/1.2.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SeqLib/1.2.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Serf/","title":"Serf","text":"<p>The serf library is a high performance C-based HTTP client library built upon the Apache Portable Runtime (APR) library</p> <p>https://serf.apache.org/</p>"},{"location":"available_software/detail/Serf/#available-modules","title":"Available modules","text":"<p>The overview below shows which Serf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Serf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Serf/1.3.9-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Serf/1.3.9-GCCcore-12.3.0 - x - - - - Serf/1.3.9-GCCcore-12.2.0 - x - - - - Serf/1.3.9-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Simple-DFTD3/","title":"Simple-DFTD3","text":"<p>Reimplementation of the D3 dispersion correction. The s-dftd3 project aims toprovide a user-friendly and uniform interface to the D3 dispersion model andfor the calculation of DFT-D3 dispersion corrections.</p> <p>https://dftd3.readthedocs.io</p>"},{"location":"available_software/detail/Simple-DFTD3/#available-modules","title":"Available modules","text":"<p>The overview below shows which Simple-DFTD3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Simple-DFTD3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Simple-DFTD3/0.7.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Simple-DFTD3/0.7.0-foss-2023a - x - - - -"},{"location":"available_software/detail/SpaceRanger/","title":"SpaceRanger","text":"<p>Space Ranger is a set of analysis pipelines that process Visium spatial RNA-seq outputand brightfield microscope images in order to detect tissue, align reads, generate feature-spot matrices,perform clustering and gene expression analysis, and place spots in spatial context on the slide image.</p> <p>https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger</p>"},{"location":"available_software/detail/SpaceRanger/#available-modules","title":"Available modules","text":"<p>The overview below shows which SpaceRanger installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SpaceRanger, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SpaceRanger/3.0.1-GCC-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SpaceRanger/3.0.1-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/SpectrA/","title":"SpectrA","text":"<p>Spectra stands for Sparse Eigenvalue Computation Toolkit as a Redesigned ARPACK. It is a C++ library for large scale eigenvalue problems, built on top of Eigen, an open source linear algebra library.</p> <p>https://spectralib.org/</p>"},{"location":"available_software/detail/SpectrA/#available-modules","title":"Available modules","text":"<p>The overview below shows which SpectrA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SpectrA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SpectrA/1.0.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SpectrA/1.0.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Stacks/","title":"Stacks","text":"<p>Stacks is a software pipeline for building loci from short-read sequences, such as those generated on the Illumina platform. Stacks was developed to work with restriction enzyme-based data, such as RAD-seq, for the purpose of building genetic maps and conducting population genomics and phylogeography.</p> <p>https://catchenlab.life.illinois.edu/stacks/</p>"},{"location":"available_software/detail/Stacks/#available-modules","title":"Available modules","text":"<p>The overview below shows which Stacks installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Stacks, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Stacks/2.64-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Stacks/2.64-foss-2023a - x - - - - Stacks/2.62-foss-2023a - x - - - -"},{"location":"available_software/detail/Stata/","title":"Stata","text":"<p>Stata is a complete, integrated statistical software package that provides everything you need for data analysis, data management, and graphics.</p> <p>https://www.stata.com/</p>"},{"location":"available_software/detail/Stata/#available-modules","title":"Available modules","text":"<p>The overview below shows which Stata installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Stata, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Stata/18-SE\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Stata/18-SE - x - - - - Stata/18-MP - x - - - -"},{"location":"available_software/detail/Subread/","title":"Subread","text":"<p>High performance read alignment, quantification and mutation discovery</p> <p>https://subread.sourceforge.net/</p>"},{"location":"available_software/detail/Subread/#available-modules","title":"Available modules","text":"<p>The overview below shows which Subread installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Subread, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Subread/2.0.6-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Subread/2.0.6-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Subversion/","title":"Subversion","text":"<p>Subversion is an open source version control system.</p> <p>https://subversion.apache.org/</p>"},{"location":"available_software/detail/Subversion/#available-modules","title":"Available modules","text":"<p>The overview below shows which Subversion installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Subversion, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Subversion/1.14.2-GCCcore-11.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Subversion/1.14.2-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/SuiteSparse/","title":"SuiteSparse","text":"<p>SuiteSparse is a collection of libraries to manipulate sparse matrices.</p> <p>https://faculty.cse.tamu.edu/davis/suitesparse.html</p>"},{"location":"available_software/detail/SuiteSparse/#available-modules","title":"Available modules","text":"<p>The overview below shows which SuiteSparse installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SuiteSparse, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SuiteSparse/7.1.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SuiteSparse/7.1.0-foss-2023a - x - - - - SuiteSparse/5.13.0-foss-2023a-METIS-5.1.0 - x - - - - SuiteSparse/5.13.0-foss-2022b-METIS-5.1.0 - x - - - -"},{"location":"available_software/detail/SuperLU_DIST/","title":"SuperLU_DIST","text":"<p>SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations on high performance machines.</p> <p>https://crd-legacy.lbl.gov/~xiaoye/SuperLU/</p>"},{"location":"available_software/detail/SuperLU_DIST/#available-modules","title":"Available modules","text":"<p>The overview below shows which SuperLU_DIST installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using SuperLU_DIST, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load SuperLU_DIST/8.1.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 SuperLU_DIST/8.1.2-foss-2023a - x - - - -"},{"location":"available_software/detail/Szip/","title":"Szip","text":"<p>Szip compression software, providing lossless compression of scientific data</p> <p>https://www.hdfgroup.org/doc_resource/SZIP/</p>"},{"location":"available_software/detail/Szip/#available-modules","title":"Available modules","text":"<p>The overview below shows which Szip installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Szip, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Szip/2.1.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Szip/2.1.1-GCCcore-13.2.0 - x - - - - Szip/2.1.1-GCCcore-12.3.0 - x - - - - Szip/2.1.1-GCCcore-12.2.0 - x - - - - Szip/2.1.1-GCCcore-11.3.0 - x - - x x Szip/2.1.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/TEsorter/","title":"TEsorter","text":"<p>An accurate and fast method to classify LTR-retrotransposons in plant genomes.</p> <p>https://github.com/zhangrengang/TEsorter</p>"},{"location":"available_software/detail/TEsorter/#available-modules","title":"Available modules","text":"<p>The overview below shows which TEsorter installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TEsorter, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TEsorter/1.4.6-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TEsorter/1.4.6-foss-2023a - x - - - -"},{"location":"available_software/detail/TEsorter/#tesorter146-foss-2023a","title":"TEsorter/1.4.6-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>isal-1.7.0, TEsorter-1.4.6, versioningit-3.1.2, xopen-2.0.2, zlib-ng-0.5.0</p>"},{"location":"available_software/detail/TOML-Fortran/","title":"TOML-Fortran","text":"<p>TOML parser for Fortran projects</p> <p>https://github.com/toml-f/toml-f</p>"},{"location":"available_software/detail/TOML-Fortran/#available-modules","title":"Available modules","text":"<p>The overview below shows which TOML-Fortran installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TOML-Fortran, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TOML-Fortran/0.3.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TOML-Fortran/0.3.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/TRF/","title":"TRF","text":"<p>Tandem Repeats Finder: a program to analyze DNA sequences.</p> <p>https://tandem.bu.edu/trf/trf.html</p>"},{"location":"available_software/detail/TRF/#available-modules","title":"Available modules","text":"<p>The overview below shows which TRF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TRF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TRF/4.09.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TRF/4.09.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/TSEBRA/","title":"TSEBRA","text":"<p>TSEBRA is a combiner tool that selects transcripts from gene predictions basedon the support by extrisic evidence in form of introns and start/stop codons.It was developed to combine BRAKER1 and BRAKER2 predicitons to increase theiraccuracies.</p> <p>https://github.com/Gaius-Augustus/TSEBRA</p>"},{"location":"available_software/detail/TSEBRA/#available-modules","title":"Available modules","text":"<p>The overview below shows which TSEBRA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TSEBRA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TSEBRA/1.1.2.5-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TSEBRA/1.1.2.5-foss-2023a - x - - - -"},{"location":"available_software/detail/TWL-NINJA/","title":"TWL-NINJA","text":"<p>Nearly Infinite Neighbor Joining Application.</p> <p>https://github.com/TravisWheelerLab/NINJA</p>"},{"location":"available_software/detail/TWL-NINJA/#available-modules","title":"Available modules","text":"<p>The overview below shows which TWL-NINJA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TWL-NINJA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TWL-NINJA/0.99-cluster_only-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TWL-NINJA/0.99-cluster_only-GCC-12.3.0 - x - - - - TWL-NINJA/0.98-cluster_only-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Tcl/","title":"Tcl","text":"<p>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more.</p> <p>https://www.tcl.tk/</p>"},{"location":"available_software/detail/Tcl/#available-modules","title":"Available modules","text":"<p>The overview below shows which Tcl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Tcl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Tcl/8.6.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Tcl/8.6.13-GCCcore-13.2.0 - x - - - - Tcl/8.6.13-GCCcore-12.3.0 - x x x x x Tcl/8.6.12-GCCcore-12.2.0 - x - - - - Tcl/8.6.12-GCCcore-11.3.0 - x - - - - Tcl/8.6.10-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/TensorFlow/","title":"TensorFlow","text":"<p>An open-source software library for Machine Intelligence</p> <p>https://www.tensorflow.org/</p>"},{"location":"available_software/detail/TensorFlow/#available-modules","title":"Available modules","text":"<p>The overview below shows which TensorFlow installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TensorFlow, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TensorFlow/2.13.0-foss-2023a - x - - - -"},{"location":"available_software/detail/TensorFlow/#tensorflow2130-foss-2023a","title":"TensorFlow/2.13.0-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>absl-py-1.4.0, astor-0.8.1, astunparse-1.6.3, cachetools-5.3.1, google-auth-2.22.0, google-auth-oauthlib-1.0.0, google-pasta-0.2.0, grpcio-1.57.0, gviz-api-1.10.0, keras-2.13.1, Markdown-3.4.4, oauthlib-3.2.2, opt-einsum-3.3.0, portpicker-1.5.2, pyasn1-modules-0.3.0, requests-oauthlib-1.3.1, rsa-4.9, tblib-2.0.0, tensorboard-2.13.0, tensorboard-data-server-0.7.1, tensorboard-plugin-profile-2.13.1, tensorboard-plugin-wit-1.8.1, TensorFlow-2.13.0, tensorflow-estimator-2.13.0, termcolor-2.3.0, Werkzeug-2.3.7, wrapt-1.15.0</p>"},{"location":"available_software/detail/TensorRT/","title":"TensorRT","text":"<p>NVIDIA TensorRT is a platform for high-performance deep learning inference</p> <p>https://developer.nvidia.com/tensorrt</p>"},{"location":"available_software/detail/TensorRT/#available-modules","title":"Available modules","text":"<p>The overview below shows which TensorRT installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TensorRT, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TensorRT/8.6.1-foss-2022a-CUDA-11.7.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TensorRT/8.6.1-foss-2022a-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/Tk/","title":"Tk","text":"<p>Tk is an open source, cross-platform widget toolchain that provides a library of basic elements for building a graphical user interface (GUI) in many different programming languages.</p> <p>https://www.tcl.tk/</p>"},{"location":"available_software/detail/Tk/#available-modules","title":"Available modules","text":"<p>The overview below shows which Tk installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Tk, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Tk/8.6.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Tk/8.6.13-GCCcore-13.2.0 - x - - - - Tk/8.6.13-GCCcore-12.3.0 - x x x x x Tk/8.6.12-GCCcore-12.2.0 - x - - - - Tk/8.6.12-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Tkinter/","title":"Tkinter","text":"<p>Tkinter module, built with the Python buildsystem</p> <p>https://python.org/</p>"},{"location":"available_software/detail/Tkinter/#available-modules","title":"Available modules","text":"<p>The overview below shows which Tkinter installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Tkinter, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Tkinter/3.11.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Tkinter/3.11.5-GCCcore-13.2.0 - x - - - - Tkinter/3.11.3-GCCcore-12.3.0 - x - - - - Tkinter/3.10.8-GCCcore-12.2.0 - x - - - - Tkinter/3.10.4-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/TopHat/","title":"TopHat","text":"<p>TopHat is a fast splice junction mapper for RNA-Seq reads.</p> <p>http://ccb.jhu.edu/software/tophat/</p>"},{"location":"available_software/detail/TopHat/#available-modules","title":"Available modules","text":"<p>The overview below shows which TopHat installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TopHat, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TopHat/2.1.2-GCC-11.3.0-Python-2.7.18\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TopHat/2.1.2-GCC-11.3.0-Python-2.7.18 - x - - - -"},{"location":"available_software/detail/TotalView/","title":"TotalView","text":"<p>TotalView debugging software provides the specialized toolsyou need to quickly debug, analyze, and scale high-performance computing (HPC)applications. This includes highly dynamic, parallel, and multicore applicationsthat run on diverse hardware \u2014 from desktops to supercomputers.Improve HPC development efficiency, code quality, and time-to-market with TotalView\u2019s powerful tools for faster fault isolation, improved memory optimization,and dynamic visualization.</p> <p>https://totalview.io/products/totalview</p>"},{"location":"available_software/detail/TotalView/#available-modules","title":"Available modules","text":"<p>The overview below shows which TotalView installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TotalView, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TotalView/2023.4.16-linux-x86-64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TotalView/2023.4.16-linux-x86-64 - x - - - -"},{"location":"available_software/detail/TransDecoder/","title":"TransDecoder","text":"<p>TransDecoder identifies candidate coding regions within transcript sequences, such as those generated by de novo RNA-Seq transcript assembly using Trinity, or constructed based on RNA-Seq alignments to the genome using Tophat and Cufflinks.</p> <p>https://github.com/TransDecoder/TransDecoder/wiki</p>"},{"location":"available_software/detail/TransDecoder/#available-modules","title":"Available modules","text":"<p>The overview below shows which TransDecoder installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TransDecoder, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TransDecoder/5.7.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TransDecoder/5.7.0-GCC-12.3.0 - x - - - - TransDecoder/5.5.0-GCC-11.3.0 - x - - - -"},{"location":"available_software/detail/TrimGalore/","title":"TrimGalore","text":"<p>Trim Galore is a wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.</p> <p>https://github.com/FelixKrueger/TrimGalore</p>"},{"location":"available_software/detail/TrimGalore/#available-modules","title":"Available modules","text":"<p>The overview below shows which TrimGalore installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using TrimGalore, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load TrimGalore/0.6.10-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 TrimGalore/0.6.10-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Trimmomatic/","title":"Trimmomatic","text":"<p>Trimmomatic performs a variety of useful trimming tasks for illumina  paired-end and single ended data.The selection of trimming steps and their associated  parameters are supplied on the command line.</p> <p>http://www.usadellab.org/cms/?page=trimmomatic</p>"},{"location":"available_software/detail/Trimmomatic/#available-modules","title":"Available modules","text":"<p>The overview below shows which Trimmomatic installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Trimmomatic, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Trimmomatic/0.39-Java-17\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Trimmomatic/0.39-Java-17 - x - - - -"},{"location":"available_software/detail/Trinity/","title":"Trinity","text":"<p>Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-Seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-Seq reads.</p> <p>https://trinityrnaseq.github.io</p>"},{"location":"available_software/detail/Trinity/#available-modules","title":"Available modules","text":"<p>The overview below shows which Trinity installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Trinity, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Trinity/2.15.1-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Trinity/2.15.1-foss-2023a - x - - - -"},{"location":"available_software/detail/Trinotate/","title":"Trinotate","text":"<p>Trinotate is a comprehensive annotation suite designed for automatic functionalannotation of transcriptomes, particularly de novo assembled transcriptomes,from model or non-model organisms. Trinotate makes use of a number of differentwell referenced methods for functional annotation including homology search toknown sequence data (BLAST+/SwissProt), protein domain identification(HMMER/PFAM), protein signal peptide and transmembrane domain prediction(signalP/tmHMM), and leveraging various annotation databases (eggNOG/GO/Keggdatabases). All functional annotation data derived from the analysis oftranscripts is integrated into a SQLite database which allows fast efficientsearching for terms with specific qualities related to a desired scientifichypothesis or a means to create a whole annotation report for a transcriptome.</p> <p>https://github.com/Trinotate/Trinotate/wiki</p>"},{"location":"available_software/detail/Trinotate/#available-modules","title":"Available modules","text":"<p>The overview below shows which Trinotate installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Trinotate, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Trinotate/4.0.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Trinotate/4.0.2-foss-2023a - x - - - -"},{"location":"available_software/detail/UCC-CUDA/","title":"UCC-CUDA","text":"<p>UCC (Unified Collective Communication) is a collectivecommunication operations API and library that is flexible, complete, and feature-rich for current and emerging programming models and runtimes.This module adds the UCC CUDA support.</p> <p>https://www.openucx.org/</p>"},{"location":"available_software/detail/UCC-CUDA/#available-modules","title":"Available modules","text":"<p>The overview below shows which UCC-CUDA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UCC-CUDA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UCC-CUDA/1.2.0-GCCcore-12.3.0-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UCC-CUDA/1.2.0-GCCcore-12.3.0-CUDA-12.1.1 - x - - - - UCC-CUDA/1.0.0-GCCcore-11.3.0-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/UCC/","title":"UCC","text":"<p>UCC (Unified Collective Communication) is a collectivecommunication operations API and library that is flexible, complete, and feature-rich for current and emerging programming models and runtimes.</p> <p>https://www.openucx.org/</p>"},{"location":"available_software/detail/UCC/#available-modules","title":"Available modules","text":"<p>The overview below shows which UCC installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UCC, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UCC/1.2.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UCC/1.2.0-GCCcore-13.2.0 - x - - - - UCC/1.2.0-GCCcore-12.3.0 - x x x x x UCC/1.1.0-GCCcore-12.2.0 - x - - - - UCC/1.0.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/UCSC_Tools/","title":"UCSC_Tools","text":"<p>UCSC Genome Browser</p> <p>https://hgdownload.soe.ucsc.edu/downloads.html#utilities_downloads</p>"},{"location":"available_software/detail/UCSC_Tools/#available-modules","title":"Available modules","text":"<p>The overview below shows which UCSC_Tools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UCSC_Tools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UCSC_Tools/19.4.2024-linux-x86_64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UCSC_Tools/19.4.2024-linux-x86_64 - x - - - -"},{"location":"available_software/detail/UCX-CUDA/","title":"UCX-CUDA","text":"<p>Unified Communication XAn open-source production grade communication framework for data centricand high-performance applicationsThis module adds the UCX CUDA support.</p> <p>http://www.openucx.org/</p>"},{"location":"available_software/detail/UCX-CUDA/#available-modules","title":"Available modules","text":"<p>The overview below shows which UCX-CUDA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UCX-CUDA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UCX-CUDA/1.14.1-GCCcore-12.3.0-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UCX-CUDA/1.14.1-GCCcore-12.3.0-CUDA-12.1.1 - x - - - - UCX-CUDA/1.12.1-GCCcore-12.3.0-CUDA-11.7.0 - x - - - - UCX-CUDA/1.12.1-GCCcore-11.3.0-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/UCX/","title":"UCX","text":"<p>Unified Communication XAn open-source production grade communication framework for data centricand high-performance applications</p> <p>https://www.openucx.org/</p>"},{"location":"available_software/detail/UCX/#available-modules","title":"Available modules","text":"<p>The overview below shows which UCX installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UCX, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UCX/1.16.0-rc4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UCX/1.16.0-rc4-GCCcore-12.3.0 - x - - - - UCX/1.15.0-GCCcore-13.2.0 - x - - - - UCX/1.14.1-GCCcore-12.3.0 - x x x x x UCX/1.13.1-GCCcore-12.2.0 - x - - - - UCX/1.12.1-GCCcore-11.3.0 - x - - - - UCX/1.9.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/UDUNITS/","title":"UDUNITS","text":"<p>UDUNITS supports conversion of unit specifications between formatted and binary forms, arithmetic manipulation of units, and conversion of values between compatible scales of measurement.</p> <p>https://www.unidata.ucar.edu/software/udunits/</p>"},{"location":"available_software/detail/UDUNITS/#available-modules","title":"Available modules","text":"<p>The overview below shows which UDUNITS installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UDUNITS, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UDUNITS/2.2.28-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UDUNITS/2.2.28-GCCcore-13.2.0 - x - - - - UDUNITS/2.2.28-GCCcore-12.3.0 - x - - - - UDUNITS/2.2.28-GCCcore-12.2.0 - x - - - - UDUNITS/2.2.26-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/UnZip/","title":"UnZip","text":"<p>UnZip is an extraction utility for archives compressedin .zip format (also called \"zipfiles\"). Although highly compatible bothwith PKWARE's PKZIP and PKUNZIP utilities for MS-DOS and with Info-ZIP'sown Zip program, our primary objectives have been portability andnon-MSDOS functionality.</p> <p>http://www.info-zip.org/UnZip.html</p>"},{"location":"available_software/detail/UnZip/#available-modules","title":"Available modules","text":"<p>The overview below shows which UnZip installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using UnZip, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load UnZip/6.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 UnZip/6.0-GCCcore-13.2.0 - x - - - - UnZip/6.0-GCCcore-12.3.0 - x x x x x UnZip/6.0-GCCcore-12.2.0 - x - - - - UnZip/6.0-GCCcore-11.3.0 - x - - - - UnZip/6.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/VASP/","title":"VASP","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scalematerials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics,from first principles.</p> <p>https://www.vasp.at</p>"},{"location":"available_software/detail/VASP/#available-modules","title":"Available modules","text":"<p>The overview below shows which VASP installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using VASP, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load VASP/6.2.1-nvofbf-2022.07-OpenMP\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 VASP/6.2.1-nvofbf-2022.07-OpenMP - x - - - - VASP/6.2.1-nvofbf-2022.07-CUDA-11.7.0 - - - - - x"},{"location":"available_software/detail/VCFtools/","title":"VCFtools","text":"<p>The aim of VCFtools is to provide  easily accessible methods for working with complex  genetic variation data in the form of VCF files.</p> <p>https://vcftools.github.io</p>"},{"location":"available_software/detail/VCFtools/#available-modules","title":"Available modules","text":"<p>The overview below shows which VCFtools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using VCFtools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load VCFtools/0.1.16-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 VCFtools/0.1.16-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/VMD/","title":"VMD","text":"<p>VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting.</p> <p>https://www.ks.uiuc.edu/Research/vmd</p>"},{"location":"available_software/detail/VMD/#available-modules","title":"Available modules","text":"<p>The overview below shows which VMD installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using VMD, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load VMD/1.9.4a57-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 VMD/1.9.4a57-foss-2023a - x - - - -"},{"location":"available_software/detail/VSEARCH/","title":"VSEARCH","text":"<p>VSEARCH supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, rereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering, conversion and merging of paired-end reads.</p> <p>https://github.com/torognes/vsearch</p>"},{"location":"available_software/detail/VSEARCH/#available-modules","title":"Available modules","text":"<p>The overview below shows which VSEARCH installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using VSEARCH, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load VSEARCH/2.28.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 VSEARCH/2.28.1-GCC-12.3.0 - x - - - - VSEARCH/2.25.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/VTK/","title":"VTK","text":"<p>The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, image processing and visualization. VTK consists of a C++ class library and several interpreted interface layers including Tcl/Tk, Java, and Python. VTK supports a wide variety of visualization algorithms including: scalar, vector, tensor, texture, and volumetric methods; and advanced modeling techniques such as: implicit modeling, polygon reduction, mesh smoothing, cutting, contouring, and Delaunay triangulation.</p> <p>https://www.vtk.org</p>"},{"location":"available_software/detail/VTK/#available-modules","title":"Available modules","text":"<p>The overview below shows which VTK installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using VTK, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load VTK/9.3.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 VTK/9.3.0-foss-2023a - x - - - -"},{"location":"available_software/detail/Valgrind/","title":"Valgrind","text":"<p>Valgrind: Debugging and profiling tools</p> <p>https://valgrind.org</p>"},{"location":"available_software/detail/Valgrind/#available-modules","title":"Available modules","text":"<p>The overview below shows which Valgrind installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Valgrind, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Valgrind/3.21.0-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Valgrind/3.21.0-gompi-2023a - x - - - - Valgrind/3.21.0-gompi-2022b - x - - - -"},{"location":"available_software/detail/Vim/","title":"Vim","text":"<p>Vim is an advanced text editor that seeks to provide the power  of the de-facto Unix editor 'Vi', with a more complete feature set.</p> <p>http://www.vim.org</p>"},{"location":"available_software/detail/Vim/#available-modules","title":"Available modules","text":"<p>The overview below shows which Vim installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Vim, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Vim/9.1.0004-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Vim/9.1.0004-GCCcore-12.3.0 - x - - - - Vim/9.1.0004-GCCcore-12.2.0 - x - - - - Vim/9.0.1434-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/Voro%2B%2B/","title":"Voro++","text":"<p>Voro++ is a software library for carrying out three-dimensional computations of the Voronoitessellation. A distinguishing feature of the Voro++ library is that it carries out cell-based calculations,computing the Voronoi cell for each particle individually. It is particularly well-suited for applications thatrely on cell-based statistics, where features of Voronoi cells (eg. volume, centroid, number of faces) can be usedto analyze a system of particles.</p> <p>http://math.lbl.gov/voro++/</p>"},{"location":"available_software/detail/Voro%2B%2B/#available-modules","title":"Available modules","text":"<p>The overview below shows which Voro++ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Voro++, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Voro++/0.4.6-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Voro++/0.4.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/WCSLIB/","title":"WCSLIB","text":"<p>The FITS \"World Coordinate System\" (WCS) standard defines keywordsand usage that provide for the description of astronomical coordinate systems in aFITS image header.</p> <p>https://www.atnf.csiro.au/people/mcalabre/WCS/</p>"},{"location":"available_software/detail/WCSLIB/#available-modules","title":"Available modules","text":"<p>The overview below shows which WCSLIB installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using WCSLIB, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load WCSLIB/7.11-GCC-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 WCSLIB/7.11-GCC-13.2.0 - x - - - -"},{"location":"available_software/detail/WFA2/","title":"WFA2","text":"<p>The wavefront alignment (WFA) algorithm is an exact gap-affine algorithm that takes advantage of homologous regions between the sequences to accelerate the alignment process.</p> <p>https://github.com/smarco/WFA2-lib</p>"},{"location":"available_software/detail/WFA2/#available-modules","title":"Available modules","text":"<p>The overview below shows which WFA2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using WFA2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load WFA2/2.3.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 WFA2/2.3.4-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/WRF/","title":"WRF","text":"<p>The Weather Research and Forecasting (WRF) Model is a next-generation mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs.</p> <p>https://www.wrf-model.org</p>"},{"location":"available_software/detail/WRF/#available-modules","title":"Available modules","text":"<p>The overview below shows which WRF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using WRF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load WRF/4.4.1-foss-2022b-dmpar\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 WRF/4.4.1-foss-2022b-dmpar - x - - - -"},{"location":"available_software/detail/WSClean/","title":"WSClean","text":"<p>WSClean (w-stacking clean) is a fast generic widefield imager.It implements several gridding algorithms and offers fully-automated multi-scalemulti-frequency deconvolution.</p> <p>https://wsclean.readthedocs.io/</p>"},{"location":"available_software/detail/WSClean/#available-modules","title":"Available modules","text":"<p>The overview below shows which WSClean installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using WSClean, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load WSClean/3.4-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 WSClean/3.4-foss-2023b - x - - - -"},{"location":"available_software/detail/Wannier90/","title":"Wannier90","text":"<p>A tool for obtaining maximally-localised Wannier functions</p> <p>http://www.wannier.org</p>"},{"location":"available_software/detail/Wannier90/#available-modules","title":"Available modules","text":"<p>The overview below shows which Wannier90 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Wannier90, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Wannier90/3.1.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Wannier90/3.1.0-foss-2023a - x - - - -"},{"location":"available_software/detail/Wayland/","title":"Wayland","text":"<p>Wayland is a project to define a protocol for a compositor to talk to its clients as well as a library implementation of the protocol.  The compositor can be a standalone display server running on Linux kernel modesetting and evdev input devices, an X application, or a wayland client itself.  The clients can be traditional applications, X servers (rootless or fullscreen) or other display servers.</p> <p>https://wayland.freedesktop.org/</p>"},{"location":"available_software/detail/Wayland/#available-modules","title":"Available modules","text":"<p>The overview below shows which Wayland installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Wayland, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Wayland/1.22.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Wayland/1.22.0-GCCcore-13.2.0 - x - - - - Wayland/1.22.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/X11/","title":"X11","text":"<p>The X Window System (X11) is a windowing system for bitmap displays</p> <p>https://www.x.org</p>"},{"location":"available_software/detail/X11/#available-modules","title":"Available modules","text":"<p>The overview below shows which X11 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using X11, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load X11/20231019-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 X11/20231019-GCCcore-13.2.0 - x - - - - X11/20230603-GCCcore-12.3.0 - x x x x x X11/20221110-GCCcore-12.2.0 - x - - - - X11/20220504-GCCcore-11.3.0 - x - - - - X11/20201008-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/XML-Compile/","title":"XML-Compile","text":"<p>Perl module for compilation based XML processing</p> <p>https://metacpan.org/pod/XML::Compile</p>"},{"location":"available_software/detail/XML-Compile/#available-modules","title":"Available modules","text":"<p>The overview below shows which XML-Compile installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using XML-Compile, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load XML-Compile/1.63-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 XML-Compile/1.63-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/XML-Compile/#xml-compile163-gcccore-1220","title":"XML-Compile/1.63-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>XML::Compile-1.63, XML::Compile::Cache-1.06, XML::Compile::SOAP-3.28, XML::Compile::WSDL11-3.08, XML::LibXML::Simple-1.01</p>"},{"location":"available_software/detail/XML-LibXML/","title":"XML-LibXML","text":"<p>Perl binding for libxml2</p> <p>https://metacpan.org/pod/distribution/XML-LibXML/LibXML.pod</p>"},{"location":"available_software/detail/XML-LibXML/#available-modules","title":"Available modules","text":"<p>The overview below shows which XML-LibXML installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using XML-LibXML, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load XML-LibXML/2.0209-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 XML-LibXML/2.0209-GCCcore-12.3.0 - x - - - - XML-LibXML/2.0208-GCCcore-12.2.0 - x - - - - XML-LibXML/2.0207-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/XML-LibXML/#xml-libxml20209-gcccore-1230","title":"XML-LibXML/2.0209-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Alien::Base-2.80, Alien::Build::Plugin::Download::GitLab-0.01, Alien::Libxml2-0.19, File::chdir-0.1011, XML::LibXML-2.0209</p>"},{"location":"available_software/detail/XML-LibXML/#xml-libxml20208-gcccore-1220","title":"XML-LibXML/2.0208-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>Alien::Base-2.80, Alien::Build::Plugin::Download::GitLab-0.01, Alien::Libxml2-0.19, File::chdir-0.1011, XML::LibXML-2.0208</p>"},{"location":"available_software/detail/XML-LibXML/#xml-libxml20207-gcccore-1130","title":"XML-LibXML/2.0207-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>Alien::Base-2.68, Alien::Libxml2-0.17, File::chdir-0.1011, XML::LibXML-2.0207</p>"},{"location":"available_software/detail/XZ/","title":"XZ","text":"<p>xz: XZ utilities</p> <p>https://tukaani.org/xz/</p>"},{"location":"available_software/detail/XZ/#available-modules","title":"Available modules","text":"<p>The overview below shows which XZ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using XZ, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load XZ/5.4.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 XZ/5.4.4-GCCcore-13.2.0 - x - - - - XZ/5.4.2-GCCcore-12.3.0 - x x x x x XZ/5.2.7-GCCcore-12.2.0 - x - - - - XZ/5.2.5-GCCcore-11.3.0 - x - - - - XZ/5.2.5-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/Xerces-C%2B%2B/","title":"Xerces-C++","text":"<p>Xerces-C++ is a validating XML parser written in a portablesubset of C++. Xerces-C++ makes it easy to give your application the ability toread and write XML data. A shared library is provided for parsing, generating,manipulating, and validating XML documents using the DOM, SAX, and SAX2APIs.</p> <p>https://xerces.apache.org/xerces-c/</p>"},{"location":"available_software/detail/Xerces-C%2B%2B/#available-modules","title":"Available modules","text":"<p>The overview below shows which Xerces-C++ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Xerces-C++, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Xerces-C++/3.2.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Xerces-C++/3.2.4-GCCcore-12.3.0 - x - - - - Xerces-C++/3.2.4-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/XlsxWriter/","title":"XlsxWriter","text":"<p>A Python module for creating Excel XLSX files</p> <p>https://xlsxwriter.readthedocs.io/</p>"},{"location":"available_software/detail/XlsxWriter/#available-modules","title":"Available modules","text":"<p>The overview below shows which XlsxWriter installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using XlsxWriter, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load XlsxWriter/3.1.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 XlsxWriter/3.1.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Xvfb/","title":"Xvfb","text":"<p>Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory.</p> <p>https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml</p>"},{"location":"available_software/detail/Xvfb/#available-modules","title":"Available modules","text":"<p>The overview below shows which Xvfb installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Xvfb, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Xvfb/21.1.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Xvfb/21.1.8-GCCcore-12.3.0 - x - - - - Xvfb/21.1.6-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/YODA/","title":"YODA","text":"<p>Yet more Objects for (High Energy Physics) Data Analysis</p> <p>https://yoda.hepforge.org/</p>"},{"location":"available_software/detail/YODA/#available-modules","title":"Available modules","text":"<p>The overview below shows which YODA installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using YODA, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load YODA/1.9.9-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 YODA/1.9.9-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/Yasm/","title":"Yasm","text":"<p>Yasm: Complete rewrite of the NASM assembler with BSD license</p> <p>https://www.tortall.net/projects/yasm/</p>"},{"location":"available_software/detail/Yasm/#available-modules","title":"Available modules","text":"<p>The overview below shows which Yasm installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Yasm, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Yasm/1.3.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Yasm/1.3.0-GCCcore-12.3.0 - x - - - - Yasm/1.3.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Z3/","title":"Z3","text":"<p>Z3 is a theorem prover from Microsoft Research with support for bitvectors,booleans, arrays, floating point numbers, strings, and other data types. Thismodule includes z3-solver, the Python interface of Z3.</p> <p>https://github.com/Z3Prover/z3</p>"},{"location":"available_software/detail/Z3/#available-modules","title":"Available modules","text":"<p>The overview below shows which Z3 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Z3, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Z3/4.12.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Z3/4.12.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Z3/#z34122-gcccore-1230","title":"Z3/4.12.2-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>z3-solver-4.12.2.0</p>"},{"location":"available_software/detail/ZeroMQ/","title":"ZeroMQ","text":"<p>ZeroMQ looks like an embeddable networking library but acts like a concurrency framework. It gives you sockets that carry atomic messages across various transports like in-process, inter-process, TCP, and multicast. You can connect sockets N-to-N with patterns like fanout, pub-sub, task distribution, and request-reply. It's fast enough to be the fabric for clustered products. Its asynchronous I/O model gives you scalable multicore applications, built as asynchronous message-processing tasks. It has a score of language APIs and runs on most operating systems.</p> <p>https://www.zeromq.org/</p>"},{"location":"available_software/detail/ZeroMQ/#available-modules","title":"Available modules","text":"<p>The overview below shows which ZeroMQ installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ZeroMQ, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ZeroMQ/4.3.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ZeroMQ/4.3.4-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/Zip/","title":"Zip","text":"<p>Zip is a compression and file packaging/archive utility.Although highly compatible both with PKWARE's PKZIP and PKUNZIPutilities for MS-DOS and with Info-ZIP's own UnZip, our primary objectiveshave been portability and other-than-MSDOS functionality</p> <p>http://www.info-zip.org/Zip.html</p>"},{"location":"available_software/detail/Zip/#available-modules","title":"Available modules","text":"<p>The overview below shows which Zip installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Zip, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Zip/3.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Zip/3.0-GCCcore-12.3.0 - x - - - - Zip/3.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/Zoltan/","title":"Zoltan","text":"<p>Zoltan Dynamic Load Balancing and Graph Algorithm Toolkit</p> <p>https://sandialabs.github.io/Zoltan/</p>"},{"location":"available_software/detail/Zoltan/#available-modules","title":"Available modules","text":"<p>The overview below shows which Zoltan installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using Zoltan, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load Zoltan/3.901-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 Zoltan/3.901-foss-2023a - x - - - -"},{"location":"available_software/detail/ant/","title":"ant","text":"<p>Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications.</p> <p>https://ant.apache.org/</p>"},{"location":"available_software/detail/ant/#available-modules","title":"Available modules","text":"<p>The overview below shows which ant installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ant, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ant/1.10.12-Java-17\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ant/1.10.12-Java-17 - x - - - - ant/1.10.12-Java-11 - x - - - - ant/1.10.8-Java-11 - x - - - -"},{"location":"available_software/detail/archspec/","title":"archspec","text":"<p>A library for detecting, labeling, and reasoning about microarchitectures</p> <p>https://github.com/archspec/archspec</p>"},{"location":"available_software/detail/archspec/#available-modules","title":"Available modules","text":"<p>The overview below shows which archspec installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using archspec, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load archspec/0.2.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 archspec/0.2.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/arpack-ng/","title":"arpack-ng","text":"<p>ARPACK is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.</p> <p>https://github.com/opencollab/arpack-ng</p>"},{"location":"available_software/detail/arpack-ng/#available-modules","title":"Available modules","text":"<p>The overview below shows which arpack-ng installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using arpack-ng, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load arpack-ng/3.9.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 arpack-ng/3.9.0-foss-2023b - x - - - - arpack-ng/3.9.0-foss-2023a - x - - - - arpack-ng/3.8.0-foss-2022b - x - - - - arpack-ng/3.8.0-foss-2020b - x - - - -"},{"location":"available_software/detail/arrow-R/","title":"arrow-R","text":"<p>R interface to the Apache Arrow C++ library</p> <p>https://cran.r-project.org/web/packages/arrow</p>"},{"location":"available_software/detail/arrow-R/#available-modules","title":"Available modules","text":"<p>The overview below shows which arrow-R installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using arrow-R, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load arrow-R/14.0.0.2-foss-2023a-R-4.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 arrow-R/14.0.0.2-foss-2023a-R-4.3.2 - x - - - - arrow-R/11.0.0.3-foss-2022b-R-4.2.2 - x - - - -"},{"location":"available_software/detail/at-spi2-atk/","title":"at-spi2-atk","text":"<p>AT-SPI 2 toolkit bridge</p> <p>https://wiki.gnome.org/Accessibility</p>"},{"location":"available_software/detail/at-spi2-atk/#available-modules","title":"Available modules","text":"<p>The overview below shows which at-spi2-atk installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using at-spi2-atk, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load at-spi2-atk/2.38.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 at-spi2-atk/2.38.0-GCCcore-12.3.0 - x - - - - at-spi2-atk/2.38.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/at-spi2-core/","title":"at-spi2-core","text":"<p>Assistive Technology Service Provider Interface.</p> <p>https://wiki.gnome.org/Accessibility</p>"},{"location":"available_software/detail/at-spi2-core/#available-modules","title":"Available modules","text":"<p>The overview below shows which at-spi2-core installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using at-spi2-core, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load at-spi2-core/2.49.91-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 at-spi2-core/2.49.91-GCCcore-12.3.0 - x - - - - at-spi2-core/2.46.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/awscli/","title":"awscli","text":"<p>Universal Command Line Environment for AWS</p> <p>https://pypi.python.org/pypi/awscli</p>"},{"location":"available_software/detail/awscli/#available-modules","title":"Available modules","text":"<p>The overview below shows which awscli installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using awscli, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load awscli/2.15.2-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 awscli/2.15.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/awscli/#awscli2152-gcccore-1220","title":"awscli/2.15.2-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>awscli-2.15.2, awscrt-0.19.19, botocore-1.33.13, distro-1.8.0, jmespath-1.0.1, prompt-toolkit-3.0.38, s3transfer-0.8.2</p>"},{"location":"available_software/detail/bcl2fastq2/","title":"bcl2fastq2","text":"<p>bcl2fastq Conversion Software both demultiplexes data and converts BCL files generated by Illumina sequencing systems to standard FASTQ file formats for downstream analysis.</p> <p>https://support.illumina.com/sequencing/sequencing_software/bcl2fastq-conversion-software.html</p>"},{"location":"available_software/detail/bcl2fastq2/#available-modules","title":"Available modules","text":"<p>The overview below shows which bcl2fastq2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using bcl2fastq2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load bcl2fastq2/2.20.0-GCC-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 bcl2fastq2/2.20.0-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/beagle-lib/","title":"beagle-lib","text":"<p>beagle-lib is a high-performance library that can perform the core calculations at the heart of most Bayesian and Maximum Likelihood phylogenetics packages.</p> <p>https://github.com/beagle-dev/beagle-lib</p>"},{"location":"available_software/detail/beagle-lib/#available-modules","title":"Available modules","text":"<p>The overview below shows which beagle-lib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using beagle-lib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load beagle-lib/4.0.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 beagle-lib/4.0.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/binutils/","title":"binutils","text":"<p>binutils: GNU binary utilities</p> <p>https://directory.fsf.org/project/binutils/</p>"},{"location":"available_software/detail/binutils/#available-modules","title":"Available modules","text":"<p>The overview below shows which binutils installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using binutils, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load binutils/2.40-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 binutils/2.40-GCCcore-13.2.0 - x - - - - binutils/2.40-GCCcore-12.3.0 - x x x x x binutils/2.40 - x x x x x binutils/2.39-GCCcore-12.2.0 - x - - - - binutils/2.39 - x - - - - binutils/2.38-GCCcore-11.3.0 - x - - - - binutils/2.38 - x - - - - binutils/2.37-GCCcore-11.2.0 - x - - - - binutils/2.37 - x - - - - binutils/2.35 - x - - - - binutils/2.34 - x - - - -"},{"location":"available_software/detail/bioawk/","title":"bioawk","text":"<p>Bioawk is an extension to Brian Kernighan's awk, adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names.</p> <p>https://github.com/lh3/bioawk</p>"},{"location":"available_software/detail/bioawk/#available-modules","title":"Available modules","text":"<p>The overview below shows which bioawk installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using bioawk, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load bioawk/1.0-GCC-11.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 bioawk/1.0-GCC-11.2.0 - x - - - -"},{"location":"available_software/detail/biom-format/","title":"biom-format","text":"<p>The BIOM file format (canonically pronounced biome) is designed to be a general-use format for representing biological sample by observation contingency tables. BIOM is a recognized standard for the Earth Microbiome Project and is a Genomics Standards Consortium supported project.</p> <p>https://biom-format.org</p>"},{"location":"available_software/detail/biom-format/#available-modules","title":"Available modules","text":"<p>The overview below shows which biom-format installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using biom-format, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load biom-format/2.1.15-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 biom-format/2.1.15-foss-2023a - x - - - -"},{"location":"available_software/detail/bokeh/","title":"bokeh","text":"<p>Statistical and novel interactive HTML plots for Python</p> <p>https://github.com/bokeh/bokeh</p>"},{"location":"available_software/detail/bokeh/#available-modules","title":"Available modules","text":"<p>The overview below shows which bokeh installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using bokeh, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load bokeh/3.2.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 bokeh/3.2.2-foss-2023a - x - - - -"},{"location":"available_software/detail/bokeh/#bokeh322-foss-2023a","title":"bokeh/3.2.2-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>bokeh-3.2.2, contourpy-1.0.7, xyzservices-2023.7.0</p>"},{"location":"available_software/detail/btllib/","title":"btllib","text":"<p>Bioinformatics Technology Lab common code library</p> <p>https://github.com/bcgsc/btllib</p>"},{"location":"available_software/detail/btllib/#available-modules","title":"Available modules","text":"<p>The overview below shows which btllib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using btllib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load btllib/1.7.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 btllib/1.7.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/bwa-mem2/","title":"bwa-mem2","text":"<p>The tool bwa-mem2 is the next version of the bwa-mem algorithm in bwa. Itproduces alignment identical to bwa and is ~1.3-3.1x faster depending on theuse-case, dataset and the running machine.</p> <p>https://github.com/bwa-mem2/bwa-mem2</p>"},{"location":"available_software/detail/bwa-mem2/#available-modules","title":"Available modules","text":"<p>The overview below shows which bwa-mem2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using bwa-mem2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load bwa-mem2/2.2.1-intel-compilers-2023.1.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 bwa-mem2/2.2.1-intel-compilers-2023.1.0 - x - - - -"},{"location":"available_software/detail/bzip2/","title":"bzip2","text":"<p>bzip2 is a freely available, patent free, high-quality data compressor. It typically compresses files to within 10% to 15% of the best available techniques (the PPM family of statistical compressors), whilst being around twice as fast at compression and six times faster at decompression.</p> <p>https://sourceware.org/bzip2</p>"},{"location":"available_software/detail/bzip2/#available-modules","title":"Available modules","text":"<p>The overview below shows which bzip2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using bzip2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load bzip2/1.0.8-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 bzip2/1.0.8-GCCcore-13.2.0 - x - - - - bzip2/1.0.8-GCCcore-12.3.0 - x x x x x bzip2/1.0.8-GCCcore-12.2.0 - x - - - - bzip2/1.0.8-GCCcore-11.3.0 - x - - - - bzip2/1.0.8-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/cURL/","title":"cURL","text":"<p>libcurl is a free and easy-to-use client-side URL transfer library, supporting DICT, FILE, FTP, FTPS, Gopher, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMTP, SMTPS, Telnet and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP uploading, HTTP form based upload, proxies, cookies, user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file transfer resume, http proxy tunneling and more.</p> <p>https://curl.haxx.se</p>"},{"location":"available_software/detail/cURL/#available-modules","title":"Available modules","text":"<p>The overview below shows which cURL installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cURL, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cURL/8.3.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cURL/8.3.0-GCCcore-13.2.0 - x - - - - cURL/8.0.1-GCCcore-12.3.0 - x x x x x cURL/7.86.0-GCCcore-12.2.0 - x - - - - cURL/7.83.0-GCCcore-11.3.0 - x - - - - cURL/7.72.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/cairo/","title":"cairo","text":"<p>Cairo is a 2D graphics library with support for multiple output devices. Currently supported output targets include the X Window System (via both Xlib and XCB), Quartz, Win32, image buffers, PostScript, PDF, and SVG file output. Experimental backends include OpenGL, BeOS, OS/2, and DirectFB</p> <p>https://cairographics.org</p>"},{"location":"available_software/detail/cairo/#available-modules","title":"Available modules","text":"<p>The overview below shows which cairo installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cairo, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cairo/1.18.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cairo/1.18.0-GCCcore-13.2.0 - x - - - - cairo/1.17.8-GCCcore-12.3.0 - x x x x x cairo/1.17.4-GCCcore-12.2.0 - x - - - - cairo/1.16.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/casacore/","title":"casacore","text":"<p>A suite of C++ libraries for radio astronomy data processing.The ephemerides data needs to be in DATA_DIR and the location must be specified at runtime.Thus user's can update them.</p> <p>https://github.com/casacore/casacore</p>"},{"location":"available_software/detail/casacore/#available-modules","title":"Available modules","text":"<p>The overview below shows which casacore installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using casacore, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load casacore/3.5.0-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 casacore/3.5.0-foss-2023b - x - - - -"},{"location":"available_software/detail/cffi/","title":"cffi","text":"<p>C Foreign Function Interface for Python. Interact with almost any C code fromPython, based on C-like declarations that you can often copy-paste from headerfiles or documentation.</p> <p>https://cffi.readthedocs.io/en/latest/</p>"},{"location":"available_software/detail/cffi/#available-modules","title":"Available modules","text":"<p>The overview below shows which cffi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cffi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cffi/1.15.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cffi/1.15.1-GCCcore-13.2.0 - x - - - - cffi/1.15.1-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/cffi/#cffi1151-gcccore-1320","title":"cffi/1.15.1-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>cffi-1.15.1, pycparser-2.21</p>"},{"location":"available_software/detail/cffi/#cffi1151-gcccore-1230","title":"cffi/1.15.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>cffi-1.15.1, pycparser-2.21</p>"},{"location":"available_software/detail/charmm/","title":"charmm","text":"<p>CHARMM (Chemistry at HARvard Macromolecular Mechanics) is a versatileand widely used molecular simulation program with broad application to many-particle systems.charmm provides all the functionality of CHARMM except its performance enhancements.</p> <p>http://www.charmm.org</p>"},{"location":"available_software/detail/charmm/#available-modules","title":"Available modules","text":"<p>The overview below shows which charmm installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using charmm, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load charmm/48b2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 charmm/48b2-foss-2023a - x - - - -"},{"location":"available_software/detail/code-server/","title":"code-server","text":"<p>Run VS Code on any machine anywhere and access it in the browser.</p> <p>https://github.com/cdr/code-server</p>"},{"location":"available_software/detail/code-server/#available-modules","title":"Available modules","text":"<p>The overview below shows which code-server installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using code-server, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load code-server/4.16.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 code-server/4.16.1 - x - - - -"},{"location":"available_software/detail/compleasm/","title":"compleasm","text":"<p>Compleasm: a faster and more accurate reimplementation of BUSCO. A genome completeness evaluation toolbased on miniprot</p> <p>https://github.com/huangnengCSU/compleasm</p>"},{"location":"available_software/detail/compleasm/#available-modules","title":"Available modules","text":"<p>The overview below shows which compleasm installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using compleasm, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load compleasm/0.2.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 compleasm/0.2.2-foss-2023a - x - - - -"},{"location":"available_software/detail/cotainr/","title":"cotainr","text":"<p>cotainr is a tool that helps making Singularity/Apptainer containers.</p>"},{"location":"available_software/detail/cotainr/#available-modules","title":"Available modules","text":"<p>The overview below shows which cotainr installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cotainr, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cotainr/2023.01.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cotainr/2023.01.0 - x - - - -"},{"location":"available_software/detail/cppy/","title":"cppy","text":"<p>A small C++ header library which makes it easier to writePython extension modules. The primary feature is a PyObject smart pointerwhich automatically handles reference counting and provides conveniencemethods for performing common object operations.</p> <p>https://github.com/nucleic/cppy</p>"},{"location":"available_software/detail/cppy/#available-modules","title":"Available modules","text":"<p>The overview below shows which cppy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cppy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cppy/1.2.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cppy/1.2.1-GCCcore-13.2.0 - x - - - - cppy/1.2.1-GCCcore-12.3.0 - x - - - - cppy/1.2.1-GCCcore-12.2.0 - x - - - - cppy/1.2.1-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/cryptography/","title":"cryptography","text":"<p>cryptography is a package designed to expose cryptographic primitives and recipes to Python developers.</p> <p>https://github.com/pyca/cryptography</p>"},{"location":"available_software/detail/cryptography/#available-modules","title":"Available modules","text":"<p>The overview below shows which cryptography installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cryptography, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cryptography/41.0.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cryptography/41.0.5-GCCcore-13.2.0 - x - - - - cryptography/41.0.1-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/cuDNN/","title":"cuDNN","text":"<p>The NVIDIA CUDA Deep Neural Network library (cuDNN) isa GPU-accelerated library of primitives for deep neural networks.</p> <p>https://developer.nvidia.com/cudnn</p>"},{"location":"available_software/detail/cuDNN/#available-modules","title":"Available modules","text":"<p>The overview below shows which cuDNN installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cuDNN, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cuDNN/8.9.7.29-CUDA-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cuDNN/8.9.7.29-CUDA-12.3.0 - x - - - - cuDNN/8.9.2.26-CUDA-12.1.1 - - - x - x cuDNN/8.4.1.50-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/cufflinks/","title":"cufflinks","text":"<p>Cufflinks</p> <p>http://cole-trapnell-lab.github.io/cufflinks/</p>"},{"location":"available_software/detail/cufflinks/#available-modules","title":"Available modules","text":"<p>The overview below shows which cufflinks installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cufflinks, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cufflinks/2.2.1-linux-x86_64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cufflinks/2.2.1-linux-x86_64 - x - - - -"},{"location":"available_software/detail/cutadapt/","title":"cutadapt","text":"<p>Cutadapt finds and removes adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.</p> <p>https://opensource.scilifelab.se/projects/cutadapt/</p>"},{"location":"available_software/detail/cutadapt/#available-modules","title":"Available modules","text":"<p>The overview below shows which cutadapt installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using cutadapt, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load cutadapt/4.4-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 cutadapt/4.4-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/cutadapt/#cutadapt44-gcccore-1220","title":"cutadapt/4.4-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>cutadapt-4.4, dnaio-0.10.0, xopen-1.7.0</p>"},{"location":"available_software/detail/dask/","title":"dask","text":"<p>Dask natively scales Python. Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love.</p> <p>https://dask.org/</p>"},{"location":"available_software/detail/dask/#available-modules","title":"Available modules","text":"<p>The overview below shows which dask installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using dask, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load dask/2023.9.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 dask/2023.9.2-foss-2023a - x - - - -"},{"location":"available_software/detail/dask/#dask202392-foss-2023a","title":"dask/2023.9.2-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>dask-2023.9.2, dask-jobqueue-0.8.2, dask-mpi-2022.4.0, distributed-2023.9.2, docrep-0.3.2, HeapDict-1.0.1, locket-1.0.0, partd-1.4.0, tblib-2.0.0, toolz-0.12.0, zict-3.0.0</p>"},{"location":"available_software/detail/deepTools/","title":"deepTools","text":"<p>deepTools is a suite of python tools particularly developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq.</p> <p>https://deeptools.readthedocs.io/</p>"},{"location":"available_software/detail/deepTools/#available-modules","title":"Available modules","text":"<p>The overview below shows which deepTools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using deepTools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load deepTools/3.5.5-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 deepTools/3.5.5-foss-2023a - x - - - -"},{"location":"available_software/detail/deepTools/#deeptools355-foss-2023a","title":"deepTools/3.5.5-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>deepTools-3.5.5, deeptoolsintervals-0.1.9, numpydoc-1.5.0, py2bit-0.3.0</p>"},{"location":"available_software/detail/dftd3-lib/","title":"dftd3-lib","text":"<p>This is a repackaged version of the DFTD3 program by S. Grimme and his coworkers.The original program (V3.1 Rev 1) was downloaded at 2016-04-03. It has beenconverted to free format and encapsulated into modules.</p> <p>https://github.com/dftbplus/dftd3-lib</p>"},{"location":"available_software/detail/dftd3-lib/#available-modules","title":"Available modules","text":"<p>The overview below shows which dftd3-lib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using dftd3-lib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load dftd3-lib/0.9-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 dftd3-lib/0.9-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/dill/","title":"dill","text":"<p>dill extends python's pickle module for serializing and de-serializing python objects to the majority of the built-in python types. Serialization is the process of converting an object to a byte stream, and the inverse of which is converting a byte stream back to on python object hierarchy.</p> <p>https://pypi.org/project/dill/</p>"},{"location":"available_software/detail/dill/#available-modules","title":"Available modules","text":"<p>The overview below shows which dill installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using dill, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load dill/0.3.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 dill/0.3.7-GCCcore-12.3.0 - x - - - - dill/0.3.6-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/double-conversion/","title":"double-conversion","text":"<p>Efficient binary-decimal and decimal-binary conversion routines for IEEE doubles.</p> <p>https://github.com/google/double-conversion</p>"},{"location":"available_software/detail/double-conversion/#available-modules","title":"Available modules","text":"<p>The overview below shows which double-conversion installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using double-conversion, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load double-conversion/3.3.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 double-conversion/3.3.0-GCCcore-13.2.0 - x - - - - double-conversion/3.3.0-GCCcore-12.3.0 - x - - - - double-conversion/3.2.1-GCCcore-12.2.0 - x - - - - double-conversion/3.2.0-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/ecBuild/","title":"ecBuild","text":"<p>A CMake-based build system, consisting of a collection of CMake macros andfunctions that ease the managing of software build systems</p> <p>https://ecbuild.readthedocs.io/</p>"},{"location":"available_software/detail/ecBuild/#available-modules","title":"Available modules","text":"<p>The overview below shows which ecBuild installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ecBuild, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ecBuild/3.8.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ecBuild/3.8.0 - x - - - -"},{"location":"available_software/detail/ecCodes/","title":"ecCodes","text":"<p>ecCodes is a package developed by ECMWF which provides an application programming interface and a set of tools for decoding and encoding messages in the following formats: WMO FM-92 GRIB edition 1 and edition 2, WMO FM-94 BUFR edition 3 and edition 4, WMO GTS abbreviated header (only decoding).</p> <p>https://software.ecmwf.int/wiki/display/ECC/ecCodes+Home</p>"},{"location":"available_software/detail/ecCodes/#available-modules","title":"Available modules","text":"<p>The overview below shows which ecCodes installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ecCodes, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ecCodes/2.31.0-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ecCodes/2.31.0-gompi-2023b - x - - - - ecCodes/2.31.0-gompi-2023a - x - - - -"},{"location":"available_software/detail/eggnog-mapper/","title":"eggnog-mapper","text":"<p>EggNOG-mapper is a tool for fast functional annotation of novelsequences. It uses precomputed orthologous groups and phylogenies from theeggNOG database (http://eggnog5.embl.de) to transfer functional information fromfine-grained orthologs only. Common uses of eggNOG-mapper include the annotationof novel genomes, transcriptomes or even metagenomic gene catalogs.</p> <p>https://github.com/eggnogdb/eggnog-mapper</p>"},{"location":"available_software/detail/eggnog-mapper/#available-modules","title":"Available modules","text":"<p>The overview below shows which eggnog-mapper installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using eggnog-mapper, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load eggnog-mapper/2.1.12-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 eggnog-mapper/2.1.12-foss-2023a - x - - - -"},{"location":"available_software/detail/expat/","title":"expat","text":"<p>Expat is an XML parser library written in C. It is a stream-oriented parserin which an application registers handlers for things the parser might findin the XML document (like start tags).</p> <p>https://libexpat.github.io</p>"},{"location":"available_software/detail/expat/#available-modules","title":"Available modules","text":"<p>The overview below shows which expat installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using expat, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load expat/2.5.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 expat/2.5.0-GCCcore-13.2.0 - x - - - - expat/2.5.0-GCCcore-12.3.0 - x x x x x expat/2.4.9-GCCcore-12.2.0 - x - - - - expat/2.4.8-GCCcore-11.3.0 - x - - - - expat/2.2.9-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/expecttest/","title":"expecttest","text":"<p>This library implements expect tests (also known as \"golden\" tests). Expect tests are a method of writing tests where instead of hard-coding the expected output of a test, you run the test to get the output, and the test framework automatically populates the expected output. If the output of the test changes, you can rerun the test with the environment variable EXPECTTEST_ACCEPT=1 to automatically update the expected output.</p> <p>https://github.com/ezyang/expecttest</p>"},{"location":"available_software/detail/expecttest/#available-modules","title":"Available modules","text":"<p>The overview below shows which expecttest installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using expecttest, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load expecttest/0.1.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 expecttest/0.1.5-GCCcore-12.3.0 - - - x - x expecttest/0.1.3-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/fastahack/","title":"fastahack","text":"<p>Utilities for indexing and sequence extraction from FASTA files.</p> <p>https://github.com/ekg/fastahack</p>"},{"location":"available_software/detail/fastahack/#available-modules","title":"Available modules","text":"<p>The overview below shows which fastahack installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fastahack, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fastahack/1.0.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fastahack/1.0.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/fastjet-contrib/","title":"fastjet-contrib","text":"<p>3rd party extensions of FastJet</p> <p>https://fastjet.hepforge.org/contrib/</p>"},{"location":"available_software/detail/fastjet-contrib/#available-modules","title":"Available modules","text":"<p>The overview below shows which fastjet-contrib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fastjet-contrib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fastjet-contrib/1.053-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fastjet-contrib/1.053-gompi-2023a - x - - - -"},{"location":"available_software/detail/fastjet/","title":"fastjet","text":"<p>A software package for jet finding in pp and e+e- collisions</p> <p>https://fastjet.fr/</p>"},{"location":"available_software/detail/fastjet/#available-modules","title":"Available modules","text":"<p>The overview below shows which fastjet installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fastjet, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fastjet/3.4.2-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fastjet/3.4.2-gompi-2023a - x - - - -"},{"location":"available_software/detail/fermi-lite/","title":"fermi-lite","text":"<p>Standalone C library for assembling Illumina short reads in small regions.</p> <p>https://github.com/lh3/fermi-lite</p>"},{"location":"available_software/detail/fermi-lite/#available-modules","title":"Available modules","text":"<p>The overview below shows which fermi-lite installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fermi-lite, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fermi-lite/20190320-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fermi-lite/20190320-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/ffnvcodec/","title":"ffnvcodec","text":"<p>FFmpeg nvidia headers. Adds support for nvenc and nvdec. Requires Nvidia GPU and drivers to be present(picked up dynamically).</p> <p>https://git.videolan.org/?p=ffmpeg/nv-codec-headers.git</p>"},{"location":"available_software/detail/ffnvcodec/#available-modules","title":"Available modules","text":"<p>The overview below shows which ffnvcodec installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ffnvcodec, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ffnvcodec/12.0.16.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ffnvcodec/12.0.16.0 - x - - - - ffnvcodec/11.1.5.2 - x - - - -"},{"location":"available_software/detail/file/","title":"file","text":"<p>The file command is 'a file type guesser', that is, a command-line tool that tells you in words what kind of data a file contains.</p> <p>https://www.darwinsys.com/file/</p>"},{"location":"available_software/detail/file/#available-modules","title":"Available modules","text":"<p>The overview below shows which file installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using file, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load file/5.43-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 file/5.43-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/filevercmp/","title":"filevercmp","text":"<p>filevercmp function as in sort --version-sort.</p> <p>https://github.com/ekg/filevercmp</p>"},{"location":"available_software/detail/filevercmp/#available-modules","title":"Available modules","text":"<p>The overview below shows which filevercmp installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using filevercmp, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load filevercmp/20191210-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 filevercmp/20191210-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/flatbuffers-python/","title":"flatbuffers-python","text":"<p>Python Flatbuffers runtime library.</p> <p>https://github.com/google/flatbuffers/</p>"},{"location":"available_software/detail/flatbuffers-python/#available-modules","title":"Available modules","text":"<p>The overview below shows which flatbuffers-python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using flatbuffers-python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load flatbuffers-python/23.5.26-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 flatbuffers-python/23.5.26-GCCcore-12.3.0 - x - - - - flatbuffers-python/2.0-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/flatbuffers/","title":"flatbuffers","text":"<p>FlatBuffers: Memory Efficient Serialization Library</p> <p>https://github.com/google/flatbuffers/</p>"},{"location":"available_software/detail/flatbuffers/#available-modules","title":"Available modules","text":"<p>The overview below shows which flatbuffers installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using flatbuffers, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load flatbuffers/23.5.26-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 flatbuffers/23.5.26-GCCcore-12.3.0 - x - - - - flatbuffers/2.0.7-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/flatbuffers/#flatbuffers207-gcccore-1130","title":"flatbuffers/2.0.7-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>flatbuffers-2.0.7</p>"},{"location":"available_software/detail/flex/","title":"flex","text":"<p>Flex (Fast Lexical Analyzer) is a tool for generating scanners. A scanner, sometimes called a tokenizer, is a program which recognizes lexical patterns in text.</p> <p>http://flex.sourceforge.net/</p>"},{"location":"available_software/detail/flex/#available-modules","title":"Available modules","text":"<p>The overview below shows which flex installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using flex, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load flex/2.6.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 flex/2.6.4-GCCcore-13.2.0 - x - - - - flex/2.6.4-GCCcore-12.3.0 - x x x x x flex/2.6.4-GCCcore-12.2.0 - x - - - - flex/2.6.4-GCCcore-11.3.0 - x - - - - flex/2.6.4-GCCcore-11.2.0 - x - - - - flex/2.6.4-GCCcore-10.2.0 - x - - - - flex/2.6.4 - x x x x x"},{"location":"available_software/detail/flit/","title":"flit","text":"<p>A simple packaging tool for simple packages.</p> <p>https://github.com/pypa/flit</p>"},{"location":"available_software/detail/flit/#available-modules","title":"Available modules","text":"<p>The overview below shows which flit installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using flit, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load flit/3.9.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 flit/3.9.0-GCCcore-13.2.0 - x - - - - flit/3.9.0-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/flit/#flit390-gcccore-1320","title":"flit/3.9.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>certifi-2023.7.22, charset-normalizer-3.3.1, docutils-0.20.1, flit-3.9.0, flit_scm-1.7.0, idna-3.4, packaging-23.2, requests-2.31.0, setuptools-scm-8.0.4, tomli_w-1.0.0, typing_extensions-4.8.0, urllib3-2.0.7</p>"},{"location":"available_software/detail/flit/#flit390-gcccore-1230","title":"flit/3.9.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>certifi-2023.5.7, charset-normalizer-3.1.0, docutils-0.20.1, flit-3.9.0, flit_scm-1.7.0, idna-3.4, packaging-23.1, requests-2.31.0, setuptools_scm-7.1.0, tomli_w-1.0.0, typing_extensions-4.6.3, urllib3-1.26.16</p>"},{"location":"available_software/detail/fontconfig/","title":"fontconfig","text":"<p>Fontconfig is a library designed to provide system-wide font configuration, customization and application access.</p> <p>https://www.freedesktop.org/wiki/Software/fontconfig/</p>"},{"location":"available_software/detail/fontconfig/#available-modules","title":"Available modules","text":"<p>The overview below shows which fontconfig installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fontconfig, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fontconfig/2.14.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fontconfig/2.14.2-GCCcore-13.2.0 - x - - - - fontconfig/2.14.2-GCCcore-12.3.0 - x x x x x fontconfig/2.14.1-GCCcore-12.2.0 - x - - - - fontconfig/2.14.0-GCCcore-11.3.0 - x - - - - fontconfig/2.13.92-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/foss/","title":"foss","text":"<p>GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support, OpenBLAS (BLAS and LAPACK support), FFTW and ScaLAPACK.</p> <p>https://easybuild.readthedocs.io/en/master/Common-toolchains.html#foss-toolchain</p>"},{"location":"available_software/detail/foss/#available-modules","title":"Available modules","text":"<p>The overview below shows which foss installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using foss, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load foss/2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 foss/2023b - x - - - - foss/2023a - x x x x x foss/2022b - x - - - - foss/2022a - x - - - - foss/2020b - x - - - -"},{"location":"available_software/detail/freebayes/","title":"freebayes","text":"<p>Bayesian haplotype-based genetic polymorphism discovery and genotyping.</p> <p>https://github.com/freebayes/freebayes</p>"},{"location":"available_software/detail/freebayes/#available-modules","title":"Available modules","text":"<p>The overview below shows which freebayes installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using freebayes, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load freebayes/1.3.7-gfbf-2023a-R-4.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 freebayes/1.3.7-gfbf-2023a-R-4.3.2 - x - - - -"},{"location":"available_software/detail/freeglut/","title":"freeglut","text":"<p>freeglut is a completely OpenSourced alternative to the OpenGL Utility Toolkit (GLUT) library.</p> <p>http://freeglut.sourceforge.net/</p>"},{"location":"available_software/detail/freeglut/#available-modules","title":"Available modules","text":"<p>The overview below shows which freeglut installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using freeglut, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load freeglut/3.4.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 freeglut/3.4.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/freetype/","title":"freetype","text":"<p>FreeType 2 is a software font engine that is designed to be small, efficient, highly customizable, and portable while capable of producing high-quality output (glyph images). It can be used in graphics libraries, display servers, font conversion tools, text image generation tools, and many other products as well.</p> <p>https://www.freetype.org</p>"},{"location":"available_software/detail/freetype/#available-modules","title":"Available modules","text":"<p>The overview below shows which freetype installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using freetype, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load freetype/2.13.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 freetype/2.13.2-GCCcore-13.2.0 - x - - - - freetype/2.13.0-GCCcore-12.3.0 - x x x x x freetype/2.12.1-GCCcore-12.2.0 - x - - - - freetype/2.12.1-GCCcore-11.3.0 - x - - - x freetype/2.10.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/fsom/","title":"fsom","text":"<p>A tiny C library for managing SOM (Self-Organizing Maps) neural networks.</p> <p>https://github.com/ekg/fsom</p>"},{"location":"available_software/detail/fsom/#available-modules","title":"Available modules","text":"<p>The overview below shows which fsom installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using fsom, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load fsom/20151117-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 fsom/20151117-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/g2clib/","title":"g2clib","text":"<p>Library contains GRIB2 encoder/decoder ('C' version).</p> <p>https://www.nco.ncep.noaa.gov/pmb/codes/GRIB2/</p>"},{"location":"available_software/detail/g2clib/#available-modules","title":"Available modules","text":"<p>The overview below shows which g2clib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using g2clib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load g2clib/1.6.3-GCCcore-10.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 g2clib/1.6.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/g2lib/","title":"g2lib","text":"<p>Library contains GRIB2 encoder/decoder and search/indexing routines.</p> <p>https://www.nco.ncep.noaa.gov/pmb/codes/GRIB2/</p>"},{"location":"available_software/detail/g2lib/#available-modules","title":"Available modules","text":"<p>The overview below shows which g2lib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using g2lib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load g2lib/3.2.0-GCCcore-10.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 g2lib/3.2.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/gettext/","title":"gettext","text":"<p>GNU 'gettext' is an important step for the GNU Translation Project, as it is an asset on which we maybuild many other steps. This package offers to programmers, translators, and even users, a well integrated set of toolsand documentation</p> <p>https://www.gnu.org/software/gettext/</p>"},{"location":"available_software/detail/gettext/#available-modules","title":"Available modules","text":"<p>The overview below shows which gettext installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gettext, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gettext/0.22-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gettext/0.22-GCCcore-13.2.0 - x - - - - gettext/0.22 - x - - - - gettext/0.21.1-GCCcore-12.3.0 - x x x x x gettext/0.21.1-GCCcore-12.2.0 - x - - - - gettext/0.21.1 - x x x x x gettext/0.21-GCCcore-11.3.0 - x - - - - gettext/0.21-GCCcore-10.2.0 - x - - - - gettext/0.21 - x - - - -"},{"location":"available_software/detail/gfbf/","title":"gfbf","text":"<p>GNU Compiler Collection (GCC) based compiler toolchain, including FlexiBLAS (BLAS and LAPACK support) and (serial) FFTW.</p> <p>&lt;(none)&gt;</p>"},{"location":"available_software/detail/gfbf/#available-modules","title":"Available modules","text":"<p>The overview below shows which gfbf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gfbf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gfbf/2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gfbf/2023b - x - - - x gfbf/2023a - x x x x x gfbf/2022b - x - - - -"},{"location":"available_software/detail/giflib/","title":"giflib","text":"<p>giflib is a library for reading and writing gif images.It is API and ABI compatible with libungif which was in wide use whilethe LZW compression algorithm was patented.</p> <p>http://giflib.sourceforge.net/</p>"},{"location":"available_software/detail/giflib/#available-modules","title":"Available modules","text":"<p>The overview below shows which giflib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using giflib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load giflib/5.2.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 giflib/5.2.1-GCCcore-13.2.0 - x - - - - giflib/5.2.1-GCCcore-12.3.0 - x - - - - giflib/5.2.1-GCCcore-12.2.0 - x - - - - giflib/5.2.1-GCCcore-11.3.0 - x - - x x"},{"location":"available_software/detail/git-lfs/","title":"git-lfs","text":"<p>Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while  storing the file contents on a remote server like GitHub.com</p> <p>https://git-lfs.github.com</p>"},{"location":"available_software/detail/git-lfs/#available-modules","title":"Available modules","text":"<p>The overview below shows which git-lfs installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using git-lfs, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load git-lfs/3.5.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 git-lfs/3.5.1 - x - - - - git-lfs/3.2.0 - x - - - -"},{"location":"available_software/detail/git/","title":"git","text":"<p>Git is a free and open source distributed version control system designedto handle everything from small to very large projects with speed and efficiency.</p> <p>https://git-scm.com</p>"},{"location":"available_software/detail/git/#available-modules","title":"Available modules","text":"<p>The overview below shows which git installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using git, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load git/2.42.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 git/2.42.0-GCCcore-13.2.0 - x - - - - git/2.41.0-GCCcore-12.3.0-nodocs - x x x x x git/2.38.1-GCCcore-12.2.0-nodocs - x - - - - git/2.36.0-GCCcore-11.3.0-nodocs - x - - - - git/2.28.0-GCCcore-10.2.0-nodocs - x - - - -"},{"location":"available_software/detail/glew/","title":"glew","text":"<p>The OpenGL Extension Wrangler Library (GLEW) is a cross-platform open-sourceC/C++ extension loading library. GLEW provides efficient run-time mechanismsfor determining which OpenGL extensions are supported on the target platform.</p> <p>http://glew.sourceforge.net/</p>"},{"location":"available_software/detail/glew/#available-modules","title":"Available modules","text":"<p>The overview below shows which glew installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using glew, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load glew/2.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 glew/2.1.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/gmpy2/","title":"gmpy2","text":"<p>GMP/MPIR, MPFR, and MPC interface to Python 2.6+ and 3.x</p> <p>https://github.com/aleaxit/gmpy</p>"},{"location":"available_software/detail/gmpy2/#available-modules","title":"Available modules","text":"<p>The overview below shows which gmpy2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gmpy2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gmpy2/2.1.5-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gmpy2/2.1.5-GCC-12.3.0 - - - x - x"},{"location":"available_software/detail/gnuplot/","title":"gnuplot","text":"<p>Portable interactive, function plotting utility</p> <p>http://gnuplot.sourceforge.net</p>"},{"location":"available_software/detail/gnuplot/#available-modules","title":"Available modules","text":"<p>The overview below shows which gnuplot installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gnuplot, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gnuplot/5.4.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gnuplot/5.4.8-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/gompi/","title":"gompi","text":"<p>GNU Compiler Collection (GCC) based compiler toolchain, including OpenMPI for MPI support.</p> <p>&lt;(none)&gt;</p>"},{"location":"available_software/detail/gompi/#available-modules","title":"Available modules","text":"<p>The overview below shows which gompi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gompi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gompi/2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gompi/2023b - x - - - - gompi/2023a - x x x x x gompi/2022b - x - - - - gompi/2022a - x - - - - gompi/2020b - x - - - -"},{"location":"available_software/detail/googletest/","title":"googletest","text":"<p>Google's framework for writing C++ tests on a variety of platforms</p> <p>https://github.com/google/googletest</p>"},{"location":"available_software/detail/googletest/#available-modules","title":"Available modules","text":"<p>The overview below shows which googletest installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using googletest, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load googletest/1.14.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 googletest/1.14.0-GCCcore-13.2.0 - x - - - - googletest/1.13.0-GCCcore-12.3.0 - x - - - - googletest/1.12.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/gperf/","title":"gperf","text":"<p>GNU gperf is a perfect hash function generator. For a given list of strings, it produces a hash function and hash table, in form of C or C++ code, for looking up a value depending on the input string. The hash function is perfect, which means that the hash table has no collisions, and the hash table lookup needs a single string comparison only.</p> <p>https://www.gnu.org/software/gperf/</p>"},{"location":"available_software/detail/gperf/#available-modules","title":"Available modules","text":"<p>The overview below shows which gperf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gperf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gperf/3.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gperf/3.1-GCCcore-13.2.0 - x - - - - gperf/3.1-GCCcore-12.3.0 - x x x x x gperf/3.1-GCCcore-12.2.0 - x - - - - gperf/3.1-GCCcore-11.3.0 - x - - - - gperf/3.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/graphite2/","title":"graphite2","text":"<p>Graphite is a \"smart font\" system developed specifically to handle the complexities of lesser-known languages of the world.</p> <p>https://scripts.sil.org/cms/scripts/page.php?site_id=projects&amp;item_id=graphite_home</p>"},{"location":"available_software/detail/graphite2/#available-modules","title":"Available modules","text":"<p>The overview below shows which graphite2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using graphite2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load graphite2/1.3.14-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 graphite2/1.3.14-GCCcore-13.2.0 - x - - - - graphite2/1.3.14-GCCcore-12.3.0 - x - - - - graphite2/1.3.14-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/groff/","title":"groff","text":"<p>Groff (GNU troff) is a typesetting system that reads plain text mixed with formatting commands and produces formatted output.</p> <p>https://www.gnu.org/software/groff</p>"},{"location":"available_software/detail/groff/#available-modules","title":"Available modules","text":"<p>The overview below shows which groff installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using groff, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load groff/1.23.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 groff/1.23.0-GCCcore-13.2.0 - x - - - - groff/1.22.4-GCCcore-12.3.0 - x x x x x groff/1.22.4-GCCcore-12.2.0 - x - - - - groff/1.22.4-GCCcore-11.3.0 - x - - - - groff/1.22.4-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/gzip/","title":"gzip","text":"<p>gzip (GNU zip) is a popular data compression program as a replacement for compress</p> <p>https://www.gnu.org/software/gzip/</p>"},{"location":"available_software/detail/gzip/#available-modules","title":"Available modules","text":"<p>The overview below shows which gzip installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using gzip, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load gzip/1.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 gzip/1.13-GCCcore-13.2.0 - x - - - - gzip/1.12-GCCcore-12.3.0 - x x x x x gzip/1.12-GCCcore-12.2.0 - x - - - - gzip/1.12-GCCcore-11.3.0 - x - - - - gzip/1.10-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/h5py/","title":"h5py","text":"<p>HDF5 for Python (h5py) is a general-purpose Python interface to the Hierarchical Data Format library, version 5. HDF5 is a versatile, mature scientific software library designed for the fast, flexible storage of enormous amounts of data.</p> <p>https://www.h5py.org/</p>"},{"location":"available_software/detail/h5py/#available-modules","title":"Available modules","text":"<p>The overview below shows which h5py installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using h5py, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load h5py/3.9.0-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 h5py/3.9.0-foss-2023a - x - - - - h5py/3.7.0-foss-2022a - x - - x x"},{"location":"available_software/detail/hatchling/","title":"hatchling","text":"<p>Extensible, standards compliant build backend used by Hatch,a modern, extensible Python project manager.</p> <p>https://hatch.pypa.io</p>"},{"location":"available_software/detail/hatchling/#available-modules","title":"Available modules","text":"<p>The overview below shows which hatchling installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using hatchling, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load hatchling/1.18.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 hatchling/1.18.0-GCCcore-13.2.0 - x - - - - hatchling/1.18.0-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/hatchling/#hatchling1180-gcccore-1320","title":"hatchling/1.18.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>editables-0.5, hatch_fancy_pypi_readme-23.1.0, hatch_vcs-0.3.0, hatchling-1.18.0, packaging-23.2, pathspec-0.11.2, pluggy-1.3.0, setuptools-scm-8.0.4, tomli-2.0.1, trove_classifiers-2023.10.18, typing_extensions-4.8.0</p>"},{"location":"available_software/detail/hatchling/#hatchling1180-gcccore-1230","title":"hatchling/1.18.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>editables-0.3, hatch_fancy_pypi_readme-23.1.0, hatch_vcs-0.3.0, hatchling-1.18.0, pathspec-0.11.1, pluggy-1.2.0, trove_classifiers-2023.5.24</p>"},{"location":"available_software/detail/help2man/","title":"help2man","text":"<p>help2man produces simple manual pages from the '--help' and '--version' output of other commands.</p> <p>https://www.gnu.org/software/help2man/</p>"},{"location":"available_software/detail/help2man/#available-modules","title":"Available modules","text":"<p>The overview below shows which help2man installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using help2man, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load help2man/1.49.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 help2man/1.49.3-GCCcore-13.2.0 - x - - - - help2man/1.49.3-GCCcore-12.3.0 - x x x x x help2man/1.49.2-GCCcore-12.2.0 - x - - - - help2man/1.49.2-GCCcore-11.3.0 - x - - - - help2man/1.48.3-GCCcore-11.2.0 - x - - - - help2man/1.47.16-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/hmmlearn/","title":"hmmlearn","text":"<p>hmmlearn is a set of algorithms for unsupervised learning and inference of Hidden Markov Models</p> <p>https://github.com/hmmlearn/hmmlearn</p>"},{"location":"available_software/detail/hmmlearn/#available-modules","title":"Available modules","text":"<p>The overview below shows which hmmlearn installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using hmmlearn, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load hmmlearn/0.3.0-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 hmmlearn/0.3.0-gfbf-2023a - x - - - -"},{"location":"available_software/detail/hwloc/","title":"hwloc","text":"<p>The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. It also gathers various system attributes such as cache and memory information as well as the locality of I/O devices such as network interfaces, InfiniBand HCAs or GPUs. It primarily aims at helping applications with gathering information about modern computing hardware so as to exploit it accordingly and efficiently.</p> <p>https://www.open-mpi.org/projects/hwloc/</p>"},{"location":"available_software/detail/hwloc/#available-modules","title":"Available modules","text":"<p>The overview below shows which hwloc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using hwloc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load hwloc/2.9.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 hwloc/2.9.2-GCCcore-13.2.0 - x - - - - hwloc/2.9.1-GCCcore-12.3.0 - x x x x x hwloc/2.8.0-GCCcore-12.2.0 - x - - - - hwloc/2.7.1-GCCcore-11.3.0 - x - - - - hwloc/2.2.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/hypothesis/","title":"hypothesis","text":"<p>Hypothesis is an advanced testing library for Python. It lets you write tests which are parametrized by a source of examples, and then generates simple and comprehensible examples that make your tests fail. This lets you find more bugs in your code with less work.</p> <p>https://github.com/HypothesisWorks/hypothesis</p>"},{"location":"available_software/detail/hypothesis/#available-modules","title":"Available modules","text":"<p>The overview below shows which hypothesis installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using hypothesis, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load hypothesis/6.90.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 hypothesis/6.90.0-GCCcore-13.2.0 - x - - - - hypothesis/6.82.0-GCCcore-12.3.0 - x - - - - hypothesis/6.68.2-GCCcore-12.2.0 - x - - - - hypothesis/6.46.7-GCCcore-11.3.0 - x - - - - hypothesis/5.41.2-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/iimpi/","title":"iimpi","text":"<p>Intel C/C++ and Fortran compilers, alongside Intel MPI.</p> <p>https://software.intel.com/parallel-studio-xe</p>"},{"location":"available_software/detail/iimpi/#available-modules","title":"Available modules","text":"<p>The overview below shows which iimpi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using iimpi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load iimpi/2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 iimpi/2023b - x - - - - iimpi/2023a - x - - - - iimpi/2022b - x - - - -"},{"location":"available_software/detail/imkl-FFTW/","title":"imkl-FFTW","text":"<p>FFTW interfaces using Intel oneAPI Math Kernel Library</p> <p>https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html</p>"},{"location":"available_software/detail/imkl-FFTW/#available-modules","title":"Available modules","text":"<p>The overview below shows which imkl-FFTW installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using imkl-FFTW, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load imkl-FFTW/2023.2.0-iimpi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 imkl-FFTW/2023.2.0-iimpi-2023b - x - - - - imkl-FFTW/2023.1.0-iimpi-2023a - x - - - - imkl-FFTW/2022.2.1-iimpi-2022b - x - - - -"},{"location":"available_software/detail/imkl/","title":"imkl","text":"<p>Intel oneAPI Math Kernel Library</p> <p>https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html</p>"},{"location":"available_software/detail/imkl/#available-modules","title":"Available modules","text":"<p>The overview below shows which imkl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using imkl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load imkl/2023.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 imkl/2023.2.0 - x - - - - imkl/2023.1.0 - x - - - - imkl/2022.2.1 - x - - - -"},{"location":"available_software/detail/impi/","title":"impi","text":"<p>Intel MPI Library, compatible with MPICH ABI</p> <p>https://software.intel.com/content/www/us/en/develop/tools/mpi-library.html</p>"},{"location":"available_software/detail/impi/#available-modules","title":"Available modules","text":"<p>The overview below shows which impi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using impi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load impi/2021.10.0-intel-compilers-2023.2.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 impi/2021.10.0-intel-compilers-2023.2.1 - x - - - - impi/2021.9.0-intel-compilers-2023.1.0 - x - - - - impi/2021.7.1-intel-compilers-2022.2.1 - x - - - -"},{"location":"available_software/detail/intel-compilers/","title":"intel-compilers","text":"<p>Intel C, C++ &amp; Fortran compilers (classic and oneAPI)</p> <p>https://software.intel.com/content/www/us/en/develop/tools/oneapi/hpc-toolkit.html</p>"},{"location":"available_software/detail/intel-compilers/#available-modules","title":"Available modules","text":"<p>The overview below shows which intel-compilers installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using intel-compilers, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load intel-compilers/2023.2.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 intel-compilers/2023.2.1 - x - - - - intel-compilers/2023.1.0 - x - - - - intel-compilers/2022.2.1 - x - - - -"},{"location":"available_software/detail/intel/","title":"intel","text":"<p>Compiler toolchain including Intel compilers, Intel MPI and Intel Math Kernel Library (MKL).</p> <p>https://easybuild.readthedocs.io/en/master/Common-toolchains.html#intel-toolchain</p>"},{"location":"available_software/detail/intel/#available-modules","title":"Available modules","text":"<p>The overview below shows which intel installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using intel, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load intel/2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 intel/2023b - x - - - - intel/2023a - x - - - - intel/2022b - x - - - -"},{"location":"available_software/detail/intervaltree/","title":"intervaltree","text":"<p>An interval tree can be used to efficiently find a set of numeric intervals overlapping or containing another interval. This library provides a basic implementation of an interval tree using C++ templates, allowing the insertion of arbitrary types into the tree.</p> <p>https://github.com/ekg/intervaltree</p>"},{"location":"available_software/detail/intervaltree/#available-modules","title":"Available modules","text":"<p>The overview below shows which intervaltree installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using intervaltree, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load intervaltree/0.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 intervaltree/0.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/intltool/","title":"intltool","text":"<p>intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files.</p> <p>https://freedesktop.org/wiki/Software/intltool/</p>"},{"location":"available_software/detail/intltool/#available-modules","title":"Available modules","text":"<p>The overview below shows which intltool installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using intltool, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load intltool/0.51.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 intltool/0.51.0-GCCcore-13.2.0 - x - - - - intltool/0.51.0-GCCcore-12.3.0 - x x x x x intltool/0.51.0-GCCcore-12.2.0 - x - - - - intltool/0.51.0-GCCcore-11.3.0 - x - - - - intltool/0.51.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/ipympl/","title":"ipympl","text":"<p>Leveraging the Jupyter interactive widgets framework, ipympl enables theinteractive features of matplotlib in the Jupyter notebook and in JupyterLab.Besides, the figure canvas element is a proper Jupyter interactive widget whichcan be positioned in interactive widget layouts.</p> <p>https://matplotlib.org/ipympl</p>"},{"location":"available_software/detail/ipympl/#available-modules","title":"Available modules","text":"<p>The overview below shows which ipympl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ipympl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ipympl/0.9.3-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ipympl/0.9.3-gfbf-2023a - x - - - - ipympl/0.9.3-foss-2023a - x - - - -"},{"location":"available_software/detail/ipympl/#ipympl093-gfbf-2023a","title":"ipympl/0.9.3-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>ipympl-0.9.3</p>"},{"location":"available_software/detail/ipympl/#ipympl093-foss-2023a","title":"ipympl/0.9.3-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>ipympl-0.9.3</p>"},{"location":"available_software/detail/jbigkit/","title":"jbigkit","text":"<p>JBIG-KIT is a software implementation of the JBIG1 data compression standard (ITU-T T.82), which was designed for bi-level image data, such as scanned documents.</p> <p>https://www.cl.cam.ac.uk/~mgk25/jbigkit/</p>"},{"location":"available_software/detail/jbigkit/#available-modules","title":"Available modules","text":"<p>The overview below shows which jbigkit installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using jbigkit, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load jbigkit/2.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 jbigkit/2.1-GCCcore-13.2.0 - x - - - - jbigkit/2.1-GCCcore-12.3.0 - x x x x x jbigkit/2.1-GCCcore-12.2.0 - x - - - - jbigkit/2.1-GCCcore-11.3.0 - x - - - - jbigkit/2.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/jemalloc/","title":"jemalloc","text":"<p>jemalloc is a general purpose malloc(3) implementation that emphasizes fragmentation avoidance and scalable concurrency support.</p> <p>http://jemalloc.net</p>"},{"location":"available_software/detail/jemalloc/#available-modules","title":"Available modules","text":"<p>The overview below shows which jemalloc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using jemalloc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load jemalloc/5.3.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 jemalloc/5.3.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/jq/","title":"jq","text":"<p>jq is a lightweight and flexible command-line JSON processor.</p> <p>https://stedolan.github.io/jq/</p>"},{"location":"available_software/detail/jq/#available-modules","title":"Available modules","text":"<p>The overview below shows which jq installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using jq, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load jq/1.6-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 jq/1.6-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/json-c/","title":"json-c","text":"<p>JSON-C implements a reference counting object model that allows you to easily construct JSON objects in C, output them as JSON formatted strings and parse JSON formatted strings back into the C representation of JSONobjects.</p> <p>https://github.com/json-c/json-c</p>"},{"location":"available_software/detail/json-c/#available-modules","title":"Available modules","text":"<p>The overview below shows which json-c installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using json-c, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load json-c/0.16-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 json-c/0.16-GCCcore-12.3.0 - x - - - - json-c/0.16-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/json-fortran/","title":"json-fortran","text":"<p>JSON-Fortran: A Modern Fortran JSON API</p> <p>https://github.com/jacobwilliams/json-fortran</p>"},{"location":"available_software/detail/json-fortran/#available-modules","title":"Available modules","text":"<p>The overview below shows which json-fortran installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using json-fortran, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load json-fortran/8.3.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 json-fortran/8.3.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/jupyter-server/","title":"jupyter-server","text":"<p>The Jupyter Server provides the backend (i.e. the core services, APIs, and RESTendpoints) for Jupyter web applications like Jupyter notebook, JupyterLab, andVoila.</p> <p>https://jupyter.org/</p>"},{"location":"available_software/detail/jupyter-server/#available-modules","title":"Available modules","text":"<p>The overview below shows which jupyter-server installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using jupyter-server, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load jupyter-server/2.7.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 jupyter-server/2.7.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/jupyter-server/#jupyter-server272-gcccore-1230","title":"jupyter-server/2.7.2-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>anyio-3.7.1, argon2-cffi-bindings-21.2.0, argon2_cffi-23.1.0, arrow-1.2.3, bleach-6.0.0, comm-0.1.4, debugpy-1.6.7.post1, defusedxml-0.7.1, deprecation-2.1.0, fastjsonschema-2.18.0, hatch_jupyter_builder-0.8.3, hatch_nodejs_version-0.3.1, ipykernel-6.25.1, ipython_genutils-0.2.0, ipywidgets-8.1.0, jsonschema-4.18.0, jsonschema_specifications-2023.7.1, jupyter_client-8.3.0, jupyter_core-5.3.1, jupyter_events-0.7.0, jupyter_packaging-0.12.3, jupyter_server-2.7.2, jupyter_server_terminals-0.4.4, jupyterlab_pygments-0.2.2, jupyterlab_widgets-3.0.8, mistune-3.0.1, nbclient-0.8.0, nbconvert-7.7.4, nbformat-5.9.2, nest_asyncio-1.5.7, notebook_shim-0.2.3, overrides-7.4.0, pandocfilters-1.5.0, prometheus_client-0.17.1, python-json-logger-2.0.7, referencing-0.30.2, rfc3339_validator-0.1.4, rfc3986_validator-0.1.1, rpds_py-0.9.2, Send2Trash-1.8.2, sniffio-1.3.0, terminado-0.17.1, tinycss2-1.2.1, websocket-client-1.6.1, widgetsnbextension-4.0.8</p>"},{"location":"available_software/detail/jupyterlmod/","title":"jupyterlmod","text":"<p>Jupyter interactive notebook server extension that allows users to interact withenvironment modules before launching kernels. The extension uses Lmod's Pythoninterface to accomplish module-related tasks like loading, unloading, savingcollections, etc.</p> <p>https://github.com/cmd-ntrf/jupyter-lmod</p>"},{"location":"available_software/detail/jupyterlmod/#available-modules","title":"Available modules","text":"<p>The overview below shows which jupyterlmod installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using jupyterlmod, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load jupyterlmod/4.0.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 jupyterlmod/4.0.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/jupyterlmod/#jupyterlmod403-gcccore-1230","title":"jupyterlmod/4.0.3-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>jupyterlmod-4.0.3</p>"},{"location":"available_software/detail/kallisto/","title":"kallisto","text":"<p>kallisto is a program for quantifying abundances of transcripts from RNA-Seq data, or more generally of target sequences using high-throughput sequencing reads.</p> <p>https://pachterlab.github.io/kallisto/</p>"},{"location":"available_software/detail/kallisto/#available-modules","title":"Available modules","text":"<p>The overview below shows which kallisto installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using kallisto, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load kallisto/0.50.0-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 kallisto/0.50.0-gompi-2023a - x - - - -"},{"location":"available_software/detail/kim-api/","title":"kim-api","text":"<p>Open Knowledgebase of Interatomic Models.KIM is an API and OpenKIM is a collection of interatomic models (potentials) foratomistic simulations.  This is a library that can be used by simulation programsto get access to the models in the OpenKIM database.This EasyBuild only installs the API, the models can be installed with thepackage openkim-models, or the user can install them manually by running    kim-api-collections-management install user MODELNAMEor    kim-api-collections-management install user OpenKIMto install them all.</p> <p>https://openkim.org/</p>"},{"location":"available_software/detail/kim-api/#available-modules","title":"Available modules","text":"<p>The overview below shows which kim-api installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using kim-api, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load kim-api/2.3.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 kim-api/2.3.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/libGLU/","title":"libGLU","text":"<p>The OpenGL Utility Library (GLU) is a computer graphics library for OpenGL.</p> <p>https://mesa.freedesktop.org/archive/glu/</p>"},{"location":"available_software/detail/libGLU/#available-modules","title":"Available modules","text":"<p>The overview below shows which libGLU installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libGLU, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libGLU/9.0.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libGLU/9.0.3-GCCcore-13.2.0 - x - - - - libGLU/9.0.3-GCCcore-12.3.0 - x x x x x libGLU/9.0.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libaec/","title":"libaec","text":"<p>Libaec provides fast lossless compression of 1 up to 32 bit wide signed or unsigned integers(samples). The library achieves best results for low entropy data as often encountered in space imaginginstrument data or numerical model output from weather or climate simulations. While floating point representationsare not directly supported, they can also be efficiently coded by grouping exponents and mantissa.</p> <p>https://gitlab.dkrz.de/k202009/libaec</p>"},{"location":"available_software/detail/libaec/#available-modules","title":"Available modules","text":"<p>The overview below shows which libaec installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libaec, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libaec/1.0.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libaec/1.0.6-GCCcore-13.2.0 - x - - - - libaec/1.0.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libaio/","title":"libaio","text":"<p>Asynchronous input/output library that uses the kernels native interface.</p> <p>https://pagure.io/libaio</p>"},{"location":"available_software/detail/libaio/#available-modules","title":"Available modules","text":"<p>The overview below shows which libaio installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libaio, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libaio/0.3.113-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libaio/0.3.113-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libarchive/","title":"libarchive","text":"<p>Multi-format archive and compression library</p> <p>https://www.libarchive.org/</p>"},{"location":"available_software/detail/libarchive/#available-modules","title":"Available modules","text":"<p>The overview below shows which libarchive installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libarchive, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libarchive/3.7.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libarchive/3.7.2-GCCcore-13.2.0 - x - - - - libarchive/3.6.2-GCCcore-12.3.0 - x x x x x libarchive/3.6.1-GCCcore-12.2.0 - x - - - - libarchive/3.6.1-GCCcore-11.3.0 - x - - - - libarchive/3.4.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libcerf/","title":"libcerf","text":"<p>libcerf is a self-contained numeric library that provides an efficient and accurate implementation of complex error functions, along with Dawson, Faddeeva, and Voigt functions.</p> <p>https://jugit.fz-juelich.de/mlz/libcerf</p>"},{"location":"available_software/detail/libcerf/#available-modules","title":"Available modules","text":"<p>The overview below shows which libcerf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libcerf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libcerf/2.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libcerf/2.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libdeflate/","title":"libdeflate","text":"<p>Heavily optimized library for DEFLATE/zlib/gzip compression and decompression.</p> <p>https://github.com/ebiggers/libdeflate</p>"},{"location":"available_software/detail/libdeflate/#available-modules","title":"Available modules","text":"<p>The overview below shows which libdeflate installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libdeflate, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libdeflate/1.19-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libdeflate/1.19-GCCcore-13.2.0 - x - - - - libdeflate/1.18-GCCcore-12.3.0 - x x x x x libdeflate/1.15-GCCcore-12.2.0 - x - - - - libdeflate/1.10-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/libdrm/","title":"libdrm","text":"<p>Direct Rendering Manager runtime library.</p> <p>https://dri.freedesktop.org</p>"},{"location":"available_software/detail/libdrm/#available-modules","title":"Available modules","text":"<p>The overview below shows which libdrm installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libdrm, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libdrm/2.4.117-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libdrm/2.4.117-GCCcore-13.2.0 - x - - - - libdrm/2.4.115-GCCcore-12.3.0 - x x x x x libdrm/2.4.114-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libepoxy/","title":"libepoxy","text":"<p>Epoxy is a library for handling OpenGL function pointer management for you</p> <p>https://github.com/anholt/libepoxy</p>"},{"location":"available_software/detail/libepoxy/#available-modules","title":"Available modules","text":"<p>The overview below shows which libepoxy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libepoxy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libepoxy/1.5.10-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libepoxy/1.5.10-GCCcore-12.3.0 - x - - - - libepoxy/1.5.10-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libevent/","title":"libevent","text":"<p>The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached.  Furthermore, libevent also support callbacks due to signals or regular timeouts.</p> <p>https://libevent.org/</p>"},{"location":"available_software/detail/libevent/#available-modules","title":"Available modules","text":"<p>The overview below shows which libevent installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libevent, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libevent/2.1.12-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libevent/2.1.12-GCCcore-13.2.0 - x - - - - libevent/2.1.12-GCCcore-12.3.0 - x x x x x libevent/2.1.12-GCCcore-12.2.0 - x - - - - libevent/2.1.12-GCCcore-11.3.0 - x - - - - libevent/2.1.12-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libfabric/","title":"libfabric","text":"<p>Libfabric is a core component of OFI. It is the library that defines and exportsthe user-space API of OFI, and is typically the only software that applicationsdeal with directly. It works in conjunction with provider libraries, which areoften integrated directly into libfabric.</p> <p>https://ofiwg.github.io/libfabric/</p>"},{"location":"available_software/detail/libfabric/#available-modules","title":"Available modules","text":"<p>The overview below shows which libfabric installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libfabric, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libfabric/1.19.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libfabric/1.19.0-GCCcore-13.2.0 - x - - - - libfabric/1.18.0-GCCcore-12.3.0 - x x x x x libfabric/1.16.1-GCCcore-12.2.0 - x - - - - libfabric/1.15.1-GCCcore-11.3.0 - x - - - - libfabric/1.11.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libffi/","title":"libffi","text":"<p>The libffi library provides a portable, high level programming interface to various calling conventions. This allows a programmer to call any function specified by a call interface description at run-time.</p> <p>https://sourceware.org/libffi/</p>"},{"location":"available_software/detail/libffi/#available-modules","title":"Available modules","text":"<p>The overview below shows which libffi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libffi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libffi/3.4.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libffi/3.4.4-GCCcore-13.2.0 - x - - - - libffi/3.4.4-GCCcore-12.3.0 - x x x x x libffi/3.4.4-GCCcore-12.2.0 - x - - - - libffi/3.4.2-GCCcore-11.3.0 - x - - - - libffi/3.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libgd/","title":"libgd","text":"<p>GD is an open source code library for the dynamic creation of images by programmers.</p> <p>https://libgd.github.io</p>"},{"location":"available_software/detail/libgd/#available-modules","title":"Available modules","text":"<p>The overview below shows which libgd installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libgd, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libgd/2.3.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libgd/2.3.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libgeotiff/","title":"libgeotiff","text":"<p>Library for reading and writing coordinate system information from/to GeoTIFF files</p> <p>https://directory.fsf.org/wiki/Libgeotiff</p>"},{"location":"available_software/detail/libgeotiff/#available-modules","title":"Available modules","text":"<p>The overview below shows which libgeotiff installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libgeotiff, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libgeotiff/1.7.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libgeotiff/1.7.1-GCCcore-12.3.0 - x - - - - libgeotiff/1.7.1-GCCcore-12.2.0 - x - - - - libgeotiff/1.6.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libgit2/","title":"libgit2","text":"<p>libgit2 is a portable, pure C implementation of the Git core methods provided as a re-entrantlinkable library with a solid API, allowing you to write native speed custom Git applications in any languagewhich supports C bindings.</p> <p>https://libgit2.org/</p>"},{"location":"available_software/detail/libgit2/#available-modules","title":"Available modules","text":"<p>The overview below shows which libgit2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libgit2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libgit2/1.7.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libgit2/1.7.2-GCCcore-13.2.0 - x - - - - libgit2/1.7.1-GCCcore-12.3.0 - x x x x x libgit2/1.5.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libglvnd/","title":"libglvnd","text":"<p>libglvnd is a vendor-neutral dispatch layer for arbitrating OpenGL API calls between multiple vendors.</p> <p>https://gitlab.freedesktop.org/glvnd/libglvnd</p>"},{"location":"available_software/detail/libglvnd/#available-modules","title":"Available modules","text":"<p>The overview below shows which libglvnd installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libglvnd, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libglvnd/1.7.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libglvnd/1.7.0-GCCcore-13.2.0 - x - - - - libglvnd/1.6.0-GCCcore-12.3.0 - x x x x x libglvnd/1.6.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libiconv/","title":"libiconv","text":"<p>Libiconv converts from one character encoding to another through Unicode conversion</p> <p>https://www.gnu.org/software/libiconv</p>"},{"location":"available_software/detail/libiconv/#available-modules","title":"Available modules","text":"<p>The overview below shows which libiconv installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libiconv, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libiconv/1.17-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libiconv/1.17-GCCcore-13.2.0 - x - - - - libiconv/1.17-GCCcore-12.3.0 - x x x x x libiconv/1.17-GCCcore-12.2.0 - x - - - - libiconv/1.17-GCCcore-11.3.0 - x - - x x libiconv/1.16-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libidn2/","title":"libidn2","text":"<p>Libidn2 implements the revised algorithm for internationalized domain names called IDNA2008/TR46.</p> <p>http://www.gnu.org/software/libidn2</p>"},{"location":"available_software/detail/libidn2/#available-modules","title":"Available modules","text":"<p>The overview below shows which libidn2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libidn2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libidn2/2.3.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libidn2/2.3.2-GCCcore-13.2.0 - x - - - - libidn2/2.3.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libjpeg-turbo/","title":"libjpeg-turbo","text":"<p>libjpeg-turbo is a fork of the original IJG libjpeg which uses SIMD to accelerate baseline JPEG compression and decompression. libjpeg is a library that implements JPEG image encoding, decoding and transcoding.</p> <p>https://sourceforge.net/projects/libjpeg-turbo/</p>"},{"location":"available_software/detail/libjpeg-turbo/#available-modules","title":"Available modules","text":"<p>The overview below shows which libjpeg-turbo installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libjpeg-turbo, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libjpeg-turbo/3.0.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libjpeg-turbo/3.0.1-GCCcore-13.2.0 - x - - - - libjpeg-turbo/2.1.5.1-GCCcore-12.3.0 - x x x x x libjpeg-turbo/2.1.4-GCCcore-12.2.0 - x - - - - libjpeg-turbo/2.1.3-GCCcore-11.3.0 - x - - - x libjpeg-turbo/2.0.5-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libmbd/","title":"libmbd","text":"<p>Libmbd implements the many-body dispersion (MBD) method in several programming languages and frameworks: - The Fortran implementation is the reference, most advanced implementation, with support for analytical    gradients and distributed parallelism, and additional functionality beyond the MBD method itself.    It provides a low-level and a high-level Fortran API, as well as a C API. Furthermore, Python bindings    to the C API are provided. - The Python/Numpy implementation is intended for prototyping, and as a high-level language reference. - The Python/Tensorflow implementation is an experiment that should enable rapid prototyping of machine    learning applications with MBD.The Python-based implementations as well as Python bindings to the Libmbd C API are accessible from the Python package called Pymbd.</p> <p>https://libmbd.github.io/index.html</p>"},{"location":"available_software/detail/libmbd/#available-modules","title":"Available modules","text":"<p>The overview below shows which libmbd installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libmbd, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libmbd/0.12.6-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libmbd/0.12.6-foss-2023a - x - - - -"},{"location":"available_software/detail/libogg/","title":"libogg","text":"<p>Ogg is a multimedia container format, and the native file and stream format for the Xiph.orgmultimedia codecs.</p> <p>https://xiph.org/ogg/</p>"},{"location":"available_software/detail/libogg/#available-modules","title":"Available modules","text":"<p>The overview below shows which libogg installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libogg, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libogg/1.3.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libogg/1.3.5-GCCcore-12.3.0 - x - - - - libogg/1.3.5-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libopus/","title":"libopus","text":"<p>Opus is a totally open, royalty-free, highly versatile audio codec. Opus is unmatched for interactive speech and music transmission over the Internet, but is also intended for storage and streaming applications. It is standardized by the Internet Engineering Task Force (IETF) as RFC 6716 which incorporated technology from Skype\u2019s SILK codec and Xiph.Org\u2019s CELT codec.</p> <p>https://www.opus-codec.org/</p>"},{"location":"available_software/detail/libopus/#available-modules","title":"Available modules","text":"<p>The overview below shows which libopus installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libopus, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libopus/1.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libopus/1.4-GCCcore-12.3.0 - x - - - - libopus/1.3.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libpciaccess/","title":"libpciaccess","text":"<p>Generic PCI access library.</p> <p>https://cgit.freedesktop.org/xorg/lib/libpciaccess/</p>"},{"location":"available_software/detail/libpciaccess/#available-modules","title":"Available modules","text":"<p>The overview below shows which libpciaccess installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libpciaccess, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libpciaccess/0.17-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libpciaccess/0.17-GCCcore-13.2.0 - x - - - - libpciaccess/0.17-GCCcore-12.3.0 - x x x x x libpciaccess/0.17-GCCcore-12.2.0 - x - - - - libpciaccess/0.16-GCCcore-11.3.0 - x - - - - libpciaccess/0.16-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libpng/","title":"libpng","text":"<p>libpng is the official PNG reference library</p> <p>http://www.libpng.org/pub/png/libpng.html</p>"},{"location":"available_software/detail/libpng/#available-modules","title":"Available modules","text":"<p>The overview below shows which libpng installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libpng, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libpng/1.6.40-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libpng/1.6.40-GCCcore-13.2.0 - x - - - - libpng/1.6.39-GCCcore-12.3.0 - x x x x x libpng/1.6.38-GCCcore-12.2.0 - x - - - - libpng/1.6.37-GCCcore-11.3.0 - x - - x x libpng/1.6.37-GCCcore-10.2.0 - x - - - - libpng/1.5.30 - x - - - -"},{"location":"available_software/detail/libreadline/","title":"libreadline","text":"<p>The GNU Readline library provides a set of functions for use by applications that allow users to edit command lines as they are typed in. Both Emacs and vi editing modes are available. The Readline library includes additional functions to maintain a list of previously-entered command lines, to recall and perhaps reedit those lines, and perform csh-like history expansion on previous commands.</p> <p>https://tiswww.case.edu/php/chet/readline/rltop.html</p>"},{"location":"available_software/detail/libreadline/#available-modules","title":"Available modules","text":"<p>The overview below shows which libreadline installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libreadline, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libreadline/8.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libreadline/8.2-GCCcore-13.2.0 - x - - - - libreadline/8.2-GCCcore-12.3.0 - x x x x x libreadline/8.2-GCCcore-12.2.0 - x - - - - libreadline/8.1.2-GCCcore-11.3.0 - x - - - - libreadline/8.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libsndfile/","title":"libsndfile","text":"<p>Libsndfile is a C library for reading and writing files containing sampled sound (such as MS Windows WAV and the Apple/SGI AIFF format) through one standard library interface.</p> <p>http://www.mega-nerd.com/libsndfile</p>"},{"location":"available_software/detail/libsndfile/#available-modules","title":"Available modules","text":"<p>The overview below shows which libsndfile installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libsndfile, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libsndfile/1.2.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libsndfile/1.2.2-GCCcore-12.3.0 - x - - - - libsndfile/1.2.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libsodium/","title":"libsodium","text":"<p>Sodium is a modern, easy-to-use software library for encryption, decryption, signatures, password hashing and more.</p> <p>https://doc.libsodium.org/</p>"},{"location":"available_software/detail/libsodium/#available-modules","title":"Available modules","text":"<p>The overview below shows which libsodium installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libsodium, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libsodium/1.0.18-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libsodium/1.0.18-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libspatialindex/","title":"libspatialindex","text":"<p>C++ implementation of R*-tree, an MVR-tree and a TPR-tree with C API</p> <p>https://libspatialindex.org</p>"},{"location":"available_software/detail/libspatialindex/#available-modules","title":"Available modules","text":"<p>The overview below shows which libspatialindex installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libspatialindex, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libspatialindex/1.9.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libspatialindex/1.9.3-GCCcore-13.2.0 - x - - - -"},{"location":"available_software/detail/libtirpc/","title":"libtirpc","text":"<p>Libtirpc is a port of Suns Transport-Independent RPC library to Linux.</p> <p>https://sourceforge.net/projects/libtirpc/</p>"},{"location":"available_software/detail/libtirpc/#available-modules","title":"Available modules","text":"<p>The overview below shows which libtirpc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libtirpc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libtirpc/1.3.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libtirpc/1.3.3-GCCcore-12.3.0 - x - - - - libtirpc/1.3.3-GCCcore-12.2.0 - x - - - - libtirpc/1.3.1-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libtool/","title":"libtool","text":"<p>GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface.</p> <p>https://www.gnu.org/software/libtool</p>"},{"location":"available_software/detail/libtool/#available-modules","title":"Available modules","text":"<p>The overview below shows which libtool installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libtool, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libtool/2.4.7-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libtool/2.4.7-GCCcore-13.2.0 - x - - - - libtool/2.4.7-GCCcore-12.3.0 - x x x x x libtool/2.4.7-GCCcore-12.2.0 - x - - - - libtool/2.4.7-GCCcore-11.3.0 - x - - - - libtool/2.4.7 - x - - - - libtool/2.4.6-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libunwind/","title":"libunwind","text":"<p>The primary goal of libunwind is to define a portable and efficient C programming interface (API) to determine the call-chain of a program. The API additionally provides the means to manipulate the preserved (callee-saved) state of each call-frame and to resume execution at any point in the call-chain (non-local goto). The API supports both local (same-process) and remote (across-process) operation. As such, the API is useful in a number of applications</p> <p>https://www.nongnu.org/libunwind/</p>"},{"location":"available_software/detail/libunwind/#available-modules","title":"Available modules","text":"<p>The overview below shows which libunwind installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libunwind, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libunwind/1.6.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libunwind/1.6.2-GCCcore-13.2.0 - x - - - - libunwind/1.6.2-GCCcore-12.3.0 - x x x x x libunwind/1.6.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libvorbis/","title":"libvorbis","text":"<p>Ogg Vorbis is a fully open, non-proprietary, patent-and-royalty-free, general-purpose compressedaudio format</p> <p>https://xiph.org/vorbis/</p>"},{"location":"available_software/detail/libvorbis/#available-modules","title":"Available modules","text":"<p>The overview below shows which libvorbis installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libvorbis, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libvorbis/1.3.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libvorbis/1.3.7-GCCcore-12.3.0 - x - - - - libvorbis/1.3.7-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/libvori/","title":"libvori","text":"<p>C++ library implementing the Voronoi integration as well as the compressed bqbfile format. The present version of libvori is a very early developmentversion, which is hard-coded to work with the CP2k program package.</p> <p>https://brehm-research.de/libvori.php</p>"},{"location":"available_software/detail/libvori/#available-modules","title":"Available modules","text":"<p>The overview below shows which libvori installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libvori, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libvori/220621-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libvori/220621-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libwebp/","title":"libwebp","text":"<p>WebP is a modern image format that provides superiorlossless and lossy compression for images on the web. Using WebP,webmasters and web developers can create smaller, richer images thatmake the web faster.</p> <p>https://developers.google.com/speed/webp/</p>"},{"location":"available_software/detail/libwebp/#available-modules","title":"Available modules","text":"<p>The overview below shows which libwebp installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libwebp, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libwebp/1.3.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libwebp/1.3.2-GCCcore-13.2.0 - x - - - - libwebp/1.3.1-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/libxc/","title":"libxc","text":"<p>Libxc is a library of exchange-correlation functionals for density-functional theory. The aim is to provide a portable, well tested and reliable set of exchange and correlation functionals.</p> <p>https://libxc.gitlab.io</p>"},{"location":"available_software/detail/libxc/#available-modules","title":"Available modules","text":"<p>The overview below shows which libxc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libxc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libxc/6.2.2-intel-compilers-2023.1.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libxc/6.2.2-intel-compilers-2023.1.0 - x - - - - libxc/6.2.2-NVHPC-23.7-CUDA-12.1.1 - - - x - - libxc/6.2.2-GCC-12.3.0 - x - - - - libxc/6.2.2-GCC-11.3.0 - x - - - - libxc/6.1.0-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/libxml2/","title":"libxml2","text":"<p>Libxml2 is the XML C parser and toolchain developed for the Gnome project (but usable outside of the Gnome platform).</p> <p>http://xmlsoft.org/</p>"},{"location":"available_software/detail/libxml2/#available-modules","title":"Available modules","text":"<p>The overview below shows which libxml2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libxml2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libxml2/2.11.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libxml2/2.11.5-GCCcore-13.2.0 - x - - - - libxml2/2.11.4-GCCcore-12.3.0 - x x x x x libxml2/2.10.3-GCCcore-12.2.0 - x - - - - libxml2/2.9.13-GCCcore-11.3.0 - x - - - - libxml2/2.9.10-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/libxslt/","title":"libxslt","text":"<p>Libxslt is the XSLT C library developed for the GNOME project (but usable outside of the Gnome platform).</p> <p>http://xmlsoft.org/</p>"},{"location":"available_software/detail/libxslt/#available-modules","title":"Available modules","text":"<p>The overview below shows which libxslt installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libxslt, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libxslt/1.1.38-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libxslt/1.1.38-GCCcore-12.3.0 - x - - - - libxslt/1.1.37-GCCcore-12.2.0 - x - - - - libxslt/1.1.34-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/libxsmm/","title":"libxsmm","text":"<p>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplicationstargeting Intel Architecture (x86).</p> <p>https://github.com/hfp/libxsmm</p>"},{"location":"available_software/detail/libxsmm/#available-modules","title":"Available modules","text":"<p>The overview below shows which libxsmm installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libxsmm, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libxsmm/1.17-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libxsmm/1.17-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/libyaml/","title":"libyaml","text":"<p>LibYAML is a YAML parser and emitter written in C.</p> <p>https://pyyaml.org/wiki/LibYAML</p>"},{"location":"available_software/detail/libyaml/#available-modules","title":"Available modules","text":"<p>The overview below shows which libyaml installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using libyaml, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load libyaml/0.2.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 libyaml/0.2.5-GCCcore-12.3.0 - x - - - - libyaml/0.2.5-GCCcore-12.2.0 - x - - - - libyaml/0.2.5-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/lpsolve/","title":"lpsolve","text":"<p>Mixed Integer Linear Programming (MILP) solver</p> <p>https://sourceforge.net/projects/lpsolve/</p>"},{"location":"available_software/detail/lpsolve/#available-modules","title":"Available modules","text":"<p>The overview below shows which lpsolve installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using lpsolve, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load lpsolve/5.5.2.11-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 lpsolve/5.5.2.11-GCC-12.3.0 - x - - - - lpsolve/5.5.2.11-GCC-12.2.0 - x - - - -"},{"location":"available_software/detail/lxml/","title":"lxml","text":"<p>The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt.</p> <p>https://lxml.de/</p>"},{"location":"available_software/detail/lxml/#available-modules","title":"Available modules","text":"<p>The overview below shows which lxml installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using lxml, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load lxml/4.9.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 lxml/4.9.2-GCCcore-12.3.0 - x - - - - lxml/4.9.1-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/lz4/","title":"lz4","text":"<p>LZ4 is lossless compression algorithm, providing compression speed at 400 MB/s per core. It features an extremely fast decoder, with speed in multiple GB/s per core.</p> <p>https://lz4.github.io/lz4/</p>"},{"location":"available_software/detail/lz4/#available-modules","title":"Available modules","text":"<p>The overview below shows which lz4 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using lz4, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load lz4/1.9.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 lz4/1.9.4-GCCcore-13.2.0 - x - - - - lz4/1.9.4-GCCcore-12.3.0 - x x x x x lz4/1.9.4-GCCcore-12.2.0 - x - - - - lz4/1.9.3-GCCcore-11.3.0 - x - - - - lz4/1.9.2-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/magma/","title":"magma","text":"<p>The MAGMA project aims to develop a dense linear algebra library similar to LAPACK but for heterogeneous/hybrid architectures, starting with current Multicore+GPU systems.</p> <p>https://icl.cs.utk.edu/magma/</p>"},{"location":"available_software/detail/magma/#available-modules","title":"Available modules","text":"<p>The overview below shows which magma installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using magma, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load magma/2.7.2-foss-2023a-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 magma/2.7.2-foss-2023a-CUDA-12.1.1 - - - x - x magma/2.6.2-foss-2022a-CUDA-11.7.0 - x - - - -"},{"location":"available_software/detail/make/","title":"make","text":"<p>GNU version of make utility</p> <p>https://www.gnu.org/software/make/make.html</p>"},{"location":"available_software/detail/make/#available-modules","title":"Available modules","text":"<p>The overview below shows which make installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using make, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load make/4.4.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 make/4.4.1-GCCcore-13.2.0 - x - - - - make/4.4.1-GCCcore-12.3.0 - x x x x - make/4.3-GCCcore-12.2.0 - x - - - - make/4.3-GCCcore-11.3.0 - x - - - - make/4.3-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/makedepend/","title":"makedepend","text":"<p>The makedepend package contains a C-preprocessor like utility to determine build-time dependencies.</p> <p>https://linux.die.net/man/1/makedepend</p>"},{"location":"available_software/detail/makedepend/#available-modules","title":"Available modules","text":"<p>The overview below shows which makedepend installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using makedepend, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load makedepend/1.0.7-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 makedepend/1.0.7-GCCcore-12.3.0 - x - - - - makedepend/1.0.6-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/makeinfo/","title":"makeinfo","text":"<p>makeinfo is part of the Texinfo project, the official documentation format of the GNU project.</p> <p>https://www.gnu.org/software/texinfo/</p>"},{"location":"available_software/detail/makeinfo/#available-modules","title":"Available modules","text":"<p>The overview below shows which makeinfo installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using makeinfo, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load makeinfo/7.0.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 makeinfo/7.0.3-GCCcore-12.3.0 - x - - - - makeinfo/6.7-GCCcore-10.2.0-minimal - x - - - -"},{"location":"available_software/detail/matplotlib/","title":"matplotlib","text":"<p>matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. matplotlib can be used in python scripts, the python and ipython shell, web application servers, and six graphical user interface toolkits.</p> <p>https://matplotlib.org</p>"},{"location":"available_software/detail/matplotlib/#available-modules","title":"Available modules","text":"<p>The overview below shows which matplotlib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using matplotlib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load matplotlib/3.8.2-gfbf-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 matplotlib/3.8.2-gfbf-2023b - x - - - - matplotlib/3.7.2-gfbf-2023a - x - - - - matplotlib/3.7.0-gfbf-2022b - x - - - - matplotlib/3.5.2-foss-2022a - x - - - -"},{"location":"available_software/detail/matplotlib/#matplotlib382-gfbf-2023b","title":"matplotlib/3.8.2-gfbf-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>contourpy-1.2.0, Cycler-0.12.1, fonttools-4.47.0, kiwisolver-1.4.5, matplotlib-3.8.2</p>"},{"location":"available_software/detail/matplotlib/#matplotlib372-gfbf-2023a","title":"matplotlib/3.7.2-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>contourpy-1.1.0, Cycler-0.11.0, fonttools-4.42.0, kiwisolver-1.4.4, matplotlib-3.7.2</p>"},{"location":"available_software/detail/matplotlib/#matplotlib370-gfbf-2022b","title":"matplotlib/3.7.0-gfbf-2022b","text":"<p>This is a list of extensions included in the module:</p> <p>contourpy-1.0.7, Cycler-0.11.0, fonttools-4.38.0, kiwisolver-1.4.4, matplotlib-3.7.0</p>"},{"location":"available_software/detail/matplotlib/#matplotlib352-foss-2022a","title":"matplotlib/3.5.2-foss-2022a","text":"<p>This is a list of extensions included in the module:</p> <p>Cycler-0.11.0, fonttools-4.34.0, kiwisolver-1.4.3, matplotlib-3.5.2</p>"},{"location":"available_software/detail/maturin/","title":"maturin","text":"<p>This project is meant as a zero configurationreplacement for setuptools-rust and milksnake. It supports buildingwheels for python 3.5+ on windows, linux, mac and freebsd, can uploadthem to pypi and has basic pypy and graalpy support.</p> <p>https://github.com/pyo3/maturin</p>"},{"location":"available_software/detail/maturin/#available-modules","title":"Available modules","text":"<p>The overview below shows which maturin installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using maturin, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load maturin/1.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 maturin/1.1.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/mctc-lib/","title":"mctc-lib","text":"<p>Common tool chain for working with molecular structure data in variousapplications. This library provides a unified way to perform operations onmolecular structure data, like reading and writing to common geometry fileformats.</p> <p>https://grimme-lab.github.io/mctc-lib</p>"},{"location":"available_software/detail/mctc-lib/#available-modules","title":"Available modules","text":"<p>The overview below shows which mctc-lib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using mctc-lib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load mctc-lib/0.3.1-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 mctc-lib/0.3.1-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/meson-python/","title":"meson-python","text":"<p>Python build backend (PEP 517) for Meson projects</p> <p>https://github.com/mesonbuild/meson-python</p>"},{"location":"available_software/detail/meson-python/#available-modules","title":"Available modules","text":"<p>The overview below shows which meson-python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using meson-python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load meson-python/0.15.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 meson-python/0.15.0-GCCcore-13.2.0 - x - - - - meson-python/0.13.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/meson-python/#meson-python0150-gcccore-1320","title":"meson-python/0.15.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>meson-python-0.15.0, pyproject-metadata-0.7.1</p>"},{"location":"available_software/detail/meson-python/#meson-python0132-gcccore-1230","title":"meson-python/0.13.2-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>meson-python-0.13.2, pyproject-metadata-0.7.1</p>"},{"location":"available_software/detail/minimap2/","title":"minimap2","text":"<p>Minimap2 is a fast sequence mapping and alignmentprogram that can find overlaps between long noisy reads, or map longreads or their assemblies to a reference genome optionally with detailedalignment (i.e. CIGAR). At present, it works efficiently with querysequences from a few kilobases to ~100 megabases in length at an errorrate ~15%. Minimap2 outputs in the PAF or the SAM format. On limitedtest data sets, minimap2 is over 20 times faster than most otherlong-read aligners. It will replace BWA-MEM for long reads and contigalignment.</p> <p>https://github.com/lh3/minimap2</p>"},{"location":"available_software/detail/minimap2/#available-modules","title":"Available modules","text":"<p>The overview below shows which minimap2 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using minimap2, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load minimap2/2.26-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 minimap2/2.26-GCCcore-12.3.0 - x - - - - minimap2/2.26-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/miniprot/","title":"miniprot","text":"<p>Miniprot aligns a protein sequence against a genome with affine gap penalty, splicing and frameshift.It is primarily intended for annotating protein-coding genes in a new species using known genes from other species.</p> <p>https://github.com/lh3/miniprot</p>"},{"location":"available_software/detail/miniprot/#available-modules","title":"Available modules","text":"<p>The overview below shows which miniprot installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using miniprot, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load miniprot/0.13-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 miniprot/0.13-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/motif/","title":"motif","text":"<p>Motif refers to both a graphical user interface (GUI) specification and the widget toolkit for building applications that follow that specification under the X Window System on Unix and other POSIX-compliant systems. It was the standard toolkit for the Common Desktop Environment and thus for Unix.</p> <p>https://motif.ics.com/</p>"},{"location":"available_software/detail/motif/#available-modules","title":"Available modules","text":"<p>The overview below shows which motif installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using motif, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load motif/2.3.8-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 motif/2.3.8-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/mpi4py/","title":"mpi4py","text":"<p>MPI for Python (mpi4py) provides bindings of the Message Passing Interface (MPI) standard for the Python programming language, allowing any Python program to exploit multiple processors.</p> <p>https://github.com/mpi4py/mpi4py</p>"},{"location":"available_software/detail/mpi4py/#available-modules","title":"Available modules","text":"<p>The overview below shows which mpi4py installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using mpi4py, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load mpi4py/3.1.5-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 mpi4py/3.1.5-gompi-2023b - x - - - - mpi4py/3.1.4-gompi-2023a - x - - - -"},{"location":"available_software/detail/mpi4py/#mpi4py315-gompi-2023b","title":"mpi4py/3.1.5-gompi-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>mpi4py-3.1.5</p>"},{"location":"available_software/detail/mpi4py/#mpi4py314-gompi-2023a","title":"mpi4py/3.1.4-gompi-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>mpi4py-3.1.4</p>"},{"location":"available_software/detail/mstore/","title":"mstore","text":"<p>Molecular structure store for testing</p> <p>https://github.com/grimme-lab/mstore</p>"},{"location":"available_software/detail/mstore/#available-modules","title":"Available modules","text":"<p>The overview below shows which mstore installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using mstore, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load mstore/0.2.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 mstore/0.2.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/multichoose/","title":"multichoose","text":"<p>generate multiset combinations (n multichoose k).</p> <p>https://github.com/ekg/multichoose</p>"},{"location":"available_software/detail/multichoose/#available-modules","title":"Available modules","text":"<p>The overview below shows which multichoose installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using multichoose, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load multichoose/1.0.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 multichoose/1.0.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/ncbi-vdb/","title":"ncbi-vdb","text":"<p>The SRA Toolkit and SDK from NCBI is a collection of tools and libraries for using data in the INSDC Sequence Read Archives.</p> <p>https://github.com/ncbi/ncbi-vdb</p>"},{"location":"available_software/detail/ncbi-vdb/#available-modules","title":"Available modules","text":"<p>The overview below shows which ncbi-vdb installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ncbi-vdb, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ncbi-vdb/3.0.10-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ncbi-vdb/3.0.10-gompi-2023a - x - - - -"},{"location":"available_software/detail/ncdu/","title":"ncdu","text":"<p>Ncdu is a disk usage analyzer with an ncurses interface. It is designed to find space hogs on a remote server where you don't have an entire graphical setup available, but it is a useful tool even on regular desktop systems. Ncdu aims to be fast, simple and easy to use, and should be able to run in any minimal POSIX-like environment with ncurses installed.</p> <p>https://dev.yorhel.nl/ncdu</p>"},{"location":"available_software/detail/ncdu/#available-modules","title":"Available modules","text":"<p>The overview below shows which ncdu installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ncdu, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ncdu/1.18-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ncdu/1.18-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/ncurses/","title":"ncurses","text":"<p>The Ncurses (new curses) library is a free software emulation of curses in System V Release 4.0, and more. It uses Terminfo format, supports pads and color and multiple highlights and forms characters and function-key mapping, and has all the other SYSV-curses enhancements over BSD Curses.</p> <p>https://www.gnu.org/software/ncurses/</p>"},{"location":"available_software/detail/ncurses/#available-modules","title":"Available modules","text":"<p>The overview below shows which ncurses installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ncurses, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ncurses/6.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ncurses/6.4-GCCcore-13.2.0 - x - - - - ncurses/6.4-GCCcore-12.3.0 - x x x x x ncurses/6.4 - x - - - - ncurses/6.3-GCCcore-12.2.0 - x - - - - ncurses/6.3-GCCcore-11.3.0 - x - - - - ncurses/6.3 - x x x x x ncurses/6.2-GCCcore-10.2.0 - x - - - - ncurses/6.2 - x - - - - ncurses/5.9 - x - - - -"},{"location":"available_software/detail/netCDF-C%2B%2B4/","title":"netCDF-C++4","text":"<p>NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.</p> <p>https://www.unidata.ucar.edu/software/netcdf/</p>"},{"location":"available_software/detail/netCDF-C%2B%2B4/#available-modules","title":"Available modules","text":"<p>The overview below shows which netCDF-C++4 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using netCDF-C++4, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load netCDF-C++4/4.3.1-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 netCDF-C++4/4.3.1-gompi-2023a - x - - - - netCDF-C++4/4.3.1-gompi-2022b - x - - - - netCDF-C++4/4.3.1-gompi-2022a - x - - - - netCDF-C++4/4.3.1-gompi-2020b - x - - - -"},{"location":"available_software/detail/netCDF-Fortran/","title":"netCDF-Fortran","text":"<p>NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.</p> <p>https://www.unidata.ucar.edu/software/netcdf/</p>"},{"location":"available_software/detail/netCDF-Fortran/#available-modules","title":"Available modules","text":"<p>The overview below shows which netCDF-Fortran installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using netCDF-Fortran, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load netCDF-Fortran/4.6.1-gompi-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 netCDF-Fortran/4.6.1-gompi-2023a - x - - - - netCDF-Fortran/4.6.0-gompi-2022b - x - - - - netCDF-Fortran/4.6.0-gompi-2022a - x - - - - netCDF-Fortran/4.5.3-gompi-2020b - x - - - -"},{"location":"available_software/detail/netCDF/","title":"netCDF","text":"<p>NetCDF (network Common Data Form) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.</p> <p>https://www.unidata.ucar.edu/software/netcdf/</p>"},{"location":"available_software/detail/netCDF/#available-modules","title":"Available modules","text":"<p>The overview below shows which netCDF installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using netCDF, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load netCDF/4.9.2-gompi-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 netCDF/4.9.2-gompi-2023b - x - - - - netCDF/4.9.2-gompi-2023a - x - - - - netCDF/4.9.0-gompi-2022b - x - - - - netCDF/4.9.0-gompi-2022a - x - - - - netCDF/4.7.4-gompi-2020b - x - - - -"},{"location":"available_software/detail/nettle/","title":"nettle","text":"<p>Nettle is a cryptographic library that is designed to fit easily in more or less any context: In crypto toolkits for object-oriented languages (C++, Python, Pike, ...), in applications like LSH or GNUPG, or even in kernel space.</p> <p>https://www.lysator.liu.se/~nisse/nettle/</p>"},{"location":"available_software/detail/nettle/#available-modules","title":"Available modules","text":"<p>The overview below shows which nettle installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nettle, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nettle/3.9.1-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nettle/3.9.1-GCCcore-12.3.0 - x - - - - nettle/3.8.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/networkx/","title":"networkx","text":"<p>NetworkX is a Python package for the creation, manipulation,and study of the structure, dynamics, and functions of complex networks.</p> <p>https://pypi.python.org/pypi/networkx</p>"},{"location":"available_software/detail/networkx/#available-modules","title":"Available modules","text":"<p>The overview below shows which networkx installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using networkx, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load networkx/3.2.1-gfbf-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 networkx/3.2.1-gfbf-2023b - x - - - - networkx/3.1-gfbf-2023a - x - - - - networkx/3.0-gfbf-2022b - x - - - - networkx/2.8.4-foss-2022a - x - - - -"},{"location":"available_software/detail/nlohmann_json/","title":"nlohmann_json","text":"<p>JSON for Modern C++</p> <p>https://github.com/nlohmann/json</p>"},{"location":"available_software/detail/nlohmann_json/#available-modules","title":"Available modules","text":"<p>The overview below shows which nlohmann_json installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nlohmann_json, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nlohmann_json/3.11.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nlohmann_json/3.11.3-GCCcore-13.2.0 - x - - - - nlohmann_json/3.11.2-GCCcore-12.3.0 - x - - - - nlohmann_json/3.11.2-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/nodejs/","title":"nodejs","text":"<p>Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices.</p> <p>https://nodejs.org</p>"},{"location":"available_software/detail/nodejs/#available-modules","title":"Available modules","text":"<p>The overview below shows which nodejs installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nodejs, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nodejs/20.9.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nodejs/20.9.0-GCCcore-13.2.0 - x - - - - nodejs/18.17.1-GCCcore-12.3.0 - x - - - - nodejs/18.12.1-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/nsync/","title":"nsync","text":"<p>nsync is a C library that exports various synchronization primitives, such as mutexes</p> <p>https://github.com/google/nsync</p>"},{"location":"available_software/detail/nsync/#available-modules","title":"Available modules","text":"<p>The overview below shows which nsync installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nsync, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nsync/1.26.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nsync/1.26.0-GCCcore-12.3.0 - x - - - - nsync/1.25.0-GCCcore-11.3.0 - x - - - x"},{"location":"available_software/detail/numactl/","title":"numactl","text":"<p>The numactl program allows you to run your application program on specific cpu's and memory nodes. It does this by supplying a NUMA memory policy to the operating system before running your program. The libnuma library provides convenient ways for you to add NUMA memory policies into your own program.</p> <p>https://github.com/numactl/numactl</p>"},{"location":"available_software/detail/numactl/#available-modules","title":"Available modules","text":"<p>The overview below shows which numactl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using numactl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load numactl/2.0.16-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 numactl/2.0.16-GCCcore-13.2.0 - x - - - - numactl/2.0.16-GCCcore-12.3.0 - x x x x x numactl/2.0.16-GCCcore-12.2.0 - x - - - - numactl/2.0.14-GCCcore-11.3.0 - x - - - - numactl/2.0.13-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/nvofbf/","title":"nvofbf","text":"<p>NVHPC based toolchain, including OpenMPI for MPI support,OpenBLAS (via FlexiBLAS for BLAS and LAPACK support), FFTW and ScaLAPACK.</p> <p>&lt;(none)&gt;</p>"},{"location":"available_software/detail/nvofbf/#available-modules","title":"Available modules","text":"<p>The overview below shows which nvofbf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nvofbf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nvofbf/2022.07\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nvofbf/2022.07 - x - - - -"},{"location":"available_software/detail/nvompi/","title":"nvompi","text":"<p>NVHPC based compiler toolchain, including OpenMPI for MPI support.</p> <p>&lt;(none)&gt;</p>"},{"location":"available_software/detail/nvompi/#available-modules","title":"Available modules","text":"<p>The overview below shows which nvompi installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using nvompi, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load nvompi/2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 nvompi/2023a - - - x - - nvompi/2022.07 - x - - - -"},{"location":"available_software/detail/parallel/","title":"parallel","text":"<p>parallel: Build and execute shell commands in parallel</p> <p>https://savannah.gnu.org/projects/parallel/</p>"},{"location":"available_software/detail/parallel/#available-modules","title":"Available modules","text":"<p>The overview below shows which parallel installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using parallel, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load parallel/20230722-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 parallel/20230722-GCCcore-12.3.0 - x - - - - parallel/20230722-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/patchelf/","title":"patchelf","text":"<p>PatchELF is a small utility to modify the dynamic linker and RPATH of ELF executables.</p> <p>https://github.com/NixOS/patchelf</p>"},{"location":"available_software/detail/patchelf/#available-modules","title":"Available modules","text":"<p>The overview below shows which patchelf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using patchelf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load patchelf/0.18.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 patchelf/0.18.0-GCCcore-13.2.0 - x - - - - patchelf/0.18.0-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/pblat/","title":"pblat","text":"<p>When the query file format is fasta, you can specify many threads to process it. It can reduce run time linearly, and use almost equal memory as the original blat program. This is useful when you blat a big query file to a huge reference like human whole genome sequence.</p> <p>https://github.com/icebert/pblat</p>"},{"location":"available_software/detail/pblat/#available-modules","title":"Available modules","text":"<p>The overview below shows which pblat installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pblat, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pblat/2.5.1-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pblat/2.5.1-foss-2022b - x - - - -"},{"location":"available_software/detail/picard/","title":"picard","text":"<p>A set of tools (in Java) for working with next generation sequencing data in the BAM format.</p> <p>https://broadinstitute.github.io/picard/</p>"},{"location":"available_software/detail/picard/#available-modules","title":"Available modules","text":"<p>The overview below shows which picard installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using picard, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load picard/3.0.0-Java-17\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 picard/3.0.0-Java-17 - x - - - - picard/2.25.1-Java-11 - x - - - -"},{"location":"available_software/detail/pigz/","title":"pigz","text":"<p>pigz, which stands for parallel implementation of gzip, is a fully functional replacement for gzip that exploits multiple processors and multiple cores to the hilt when compressing data. pigz was written by Mark Adler, and uses the zlib and pthread libraries.</p> <p>https://zlib.net/pigz/</p>"},{"location":"available_software/detail/pigz/#available-modules","title":"Available modules","text":"<p>The overview below shows which pigz installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pigz, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pigz/2.7-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pigz/2.7-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/pixman/","title":"pixman","text":"<p>Pixman is a low-level software library for pixel manipulation, providing features such as image compositing and trapezoid rasterization. Important users of pixman are the cairo graphics library and the X server.</p> <p>http://www.pixman.org/</p>"},{"location":"available_software/detail/pixman/#available-modules","title":"Available modules","text":"<p>The overview below shows which pixman installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pixman, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pixman/0.42.2-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pixman/0.42.2-GCCcore-13.2.0 - x - - - - pixman/0.42.2-GCCcore-12.3.0 - x x x x x pixman/0.42.2-GCCcore-12.2.0 - x - - - - pixman/0.40.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/pkg-config/","title":"pkg-config","text":"<p>pkg-config is a helper tool used when compiling applications and libraries. It helps you insert the correct compiler options on the command line so an application can use gcc -o test test.c <code>pkg-config --libs --cflags glib-2.0</code> for instance, rather than hard-coding values on where to find glib (or other libraries).</p> <p>https://www.freedesktop.org/wiki/Software/pkg-config/</p>"},{"location":"available_software/detail/pkg-config/#available-modules","title":"Available modules","text":"<p>The overview below shows which pkg-config installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pkg-config, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pkg-config/0.29.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pkg-config/0.29.2-GCCcore-12.3.0 - x - - - - pkg-config/0.29.2-GCCcore-12.2.0 - x - - - - pkg-config/0.29.2-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/pkgconf/","title":"pkgconf","text":"<p>pkgconf is a program which helps to configure compiler and linker flags for development libraries. It is similar to pkg-config from freedesktop.org.</p> <p>https://github.com/pkgconf/pkgconf</p>"},{"location":"available_software/detail/pkgconf/#available-modules","title":"Available modules","text":"<p>The overview below shows which pkgconf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pkgconf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pkgconf/2.0.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pkgconf/2.0.3-GCCcore-13.2.0 - x - - - - pkgconf/1.9.5-GCCcore-12.3.0 - x x x x x pkgconf/1.9.3-GCCcore-12.2.0 - x - - - - pkgconf/1.8.0-GCCcore-11.3.0 - x - - - - pkgconf/1.8.0 - x x x x x"},{"location":"available_software/detail/pkgconfig/","title":"pkgconfig","text":"<p>pkgconfig is a Python module to interface with the pkg-config command line tool</p> <p>https://github.com/matze/pkgconfig</p>"},{"location":"available_software/detail/pkgconfig/#available-modules","title":"Available modules","text":"<p>The overview below shows which pkgconfig installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pkgconfig, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pkgconfig/1.5.5-GCCcore-12.3.0-python\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pkgconfig/1.5.5-GCCcore-12.3.0-python - x - - - - pkgconfig/1.5.5-GCCcore-11.3.0-python - x - - x x"},{"location":"available_software/detail/plotly.py/","title":"plotly.py","text":"<p>An open-source, interactive graphing library for Python</p> <p>https://plot.ly/python</p>"},{"location":"available_software/detail/plotly.py/#available-modules","title":"Available modules","text":"<p>The overview below shows which plotly.py installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using plotly.py, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load plotly.py/5.16.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 plotly.py/5.16.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/plotly.py/#plotlypy5160-gcccore-1230","title":"plotly.py/5.16.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>packaging-23.1, plotly-5.16.0, tenacity-8.2.3</p>"},{"location":"available_software/detail/pocl/","title":"pocl","text":"<p>PoCL is a portable open source (MIT-licensed) implementation of the OpenCL standard (1.2 with some 2.0 features supported).</p> <p>http://portablecl.org</p>"},{"location":"available_software/detail/pocl/#available-modules","title":"Available modules","text":"<p>The overview below shows which pocl installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pocl, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pocl/4.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pocl/4.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/poetry/","title":"poetry","text":"<p>Python packaging and dependency management made easy. Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere.</p> <p>https://python-poetry.org</p>"},{"location":"available_software/detail/poetry/#available-modules","title":"Available modules","text":"<p>The overview below shows which poetry installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using poetry, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load poetry/1.6.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 poetry/1.6.1-GCCcore-13.2.0 - x - - - - poetry/1.5.1-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/poetry/#poetry161-gcccore-1320","title":"poetry/1.6.1-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>attrs-23.1.0, build-0.10.0, cachecontrol-0.13.1, certifi-2023.7.22, charset-normalizer-3.3.1, cleo-2.0.1, crashtest-0.4.1, dulwich-0.21.6, html5lib-1.1, idna-3.4, importlib_metadata-6.8.0, installer-0.7.0, jaraco.classes-3.3.0, jeepney-0.8.0, jsonschema-4.17.3, keyring-24.2.0, lockfile-0.12.2, more-itertools-10.1.0, msgpack-1.0.7, pexpect-4.8.0, pkginfo-1.9.6, platformdirs-3.11.0, poetry-1.6.1, poetry_core-1.7.0, poetry_plugin_export-1.5.0, ptyprocess-0.7.0, pyproject_hooks-1.0.0, pyrsistent-0.20.0, rapidfuzz-2.15.2, requests-2.31.0, requests-toolbelt-1.0.0, SecretStorage-3.3.3, shellingham-1.5.4, six-1.16.0, tomlkit-0.12.1, urllib3-2.0.7, webencodings-0.5.1, zipp-3.17.0</p>"},{"location":"available_software/detail/poetry/#poetry151-gcccore-1230","title":"poetry/1.5.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>attrs-23.1.0, build-0.10.0, CacheControl-0.12.14, certifi-2023.5.7, charset-normalizer-3.1.0, cleo-2.0.1, crashtest-0.4.1, dulwich-0.21.5, html5lib-1.1, idna-3.4, importlib_metadata-6.7.0, installer-0.7.0, jaraco.classes-3.2.3, jeepney-0.8.0, jsonschema-4.17.3, keyring-23.13.1, lockfile-0.12.2, more-itertools-9.1.0, msgpack-1.0.5, pexpect-4.8.0, pkginfo-1.9.6, platformdirs-3.8.0, poetry-1.5.1, poetry_core-1.6.1, poetry_plugin_export-1.4.0, ptyprocess-0.7.0, pyproject_hooks-1.0.0, pyrsistent-0.19.3, rapidfuzz-2.15.1, requests-2.31.0, requests-toolbelt-1.0.0, SecretStorage-3.3.3, shellingham-1.5.0, six-1.16.0, tomlkit-0.11.8, urllib3-1.26.16, webencodings-0.5.1, zipp-3.15.0</p>"},{"location":"available_software/detail/poppler/","title":"poppler","text":"<p>Poppler is a PDF rendering library</p> <p>https://poppler.freedesktop.org</p>"},{"location":"available_software/detail/poppler/#available-modules","title":"Available modules","text":"<p>The overview below shows which poppler installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using poppler, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load poppler/23.09.0-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 poppler/23.09.0-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/powertools/","title":"powertools","text":"<p>Helpful tools developed by ICER for using the HPCC.</p> <p>https://docs.icer.msu.edu</p>"},{"location":"available_software/detail/powertools/#available-modules","title":"Available modules","text":"<p>The overview below shows which powertools installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using powertools, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load powertools/1.3.4\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 powertools/1.3.4 - x - - - - powertools/1.3.3 - x - - - - powertools/1.3.2 - x - - - - powertools/1.3.1 - x - - - - powertools/1.3.0 - x - - - - powertools/1.2.0 - x - - - -"},{"location":"available_software/detail/prodigal/","title":"prodigal","text":"<p>Prodigal (Prokaryotic Dynamic Programming Genefinding Algorithm)   is a microbial (bacterial and archaeal) gene finding program developed   at Oak Ridge National Laboratory and the University of Tennessee.</p> <p>https://github.com/hyattpd/Prodigal/</p>"},{"location":"available_software/detail/prodigal/#available-modules","title":"Available modules","text":"<p>The overview below shows which prodigal installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using prodigal, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load prodigal/2.6.3-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 prodigal/2.6.3-GCCcore-12.3.0 - x - - - - prodigal/2.6.3-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/prokka/","title":"prokka","text":"<p>Prokka is a software tool for the rapid annotation of prokaryotic genomes.</p> <p>https://www.vicbioinformatics.com/software.prokka.shtml</p>"},{"location":"available_software/detail/prokka/#available-modules","title":"Available modules","text":"<p>The overview below shows which prokka installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using prokka, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load prokka/1.14.5-gompi-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 prokka/1.14.5-gompi-2022b - x - - - -"},{"location":"available_software/detail/protobuf-python/","title":"protobuf-python","text":"<p>Python Protocol Buffers runtime library.</p> <p>https://github.com/google/protobuf/</p>"},{"location":"available_software/detail/protobuf-python/#available-modules","title":"Available modules","text":"<p>The overview below shows which protobuf-python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using protobuf-python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load protobuf-python/4.24.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 protobuf-python/4.24.0-GCCcore-12.3.0 - x - - - - protobuf-python/3.19.4-GCCcore-11.3.0 - x - - - x"},{"location":"available_software/detail/protobuf/","title":"protobuf","text":"<p>Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data.</p> <p>https://github.com/protocolbuffers/protobuf</p>"},{"location":"available_software/detail/protobuf/#available-modules","title":"Available modules","text":"<p>The overview below shows which protobuf installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using protobuf, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load protobuf/24.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 protobuf/24.0-GCCcore-12.3.0 - x - - - - protobuf/23.0-GCCcore-12.2.0 - x - - - - protobuf/3.19.4-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/pyBigWig/","title":"pyBigWig","text":"<p>A python extension, written in C, for quick access to bigBed files and access to and creation of bigWig files.</p> <p>https://github.com/deeptools/pyBigWig</p>"},{"location":"available_software/detail/pyBigWig/#available-modules","title":"Available modules","text":"<p>The overview below shows which pyBigWig installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pyBigWig, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pyBigWig/0.3.18-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pyBigWig/0.3.18-foss-2023a - x - - - -"},{"location":"available_software/detail/pybind11/","title":"pybind11","text":"<p>pybind11 is a lightweight header-only library that exposes C++ types in Python and vice versa, mainly to create Python bindings of existing C++ code.</p> <p>https://pybind11.readthedocs.io</p>"},{"location":"available_software/detail/pybind11/#available-modules","title":"Available modules","text":"<p>The overview below shows which pybind11 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pybind11, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pybind11/2.11.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pybind11/2.11.1-GCCcore-13.2.0 - x - - - - pybind11/2.11.1-GCCcore-12.3.0 - x - - - - pybind11/2.10.3-GCCcore-12.2.0 - x - - - - pybind11/2.9.2-GCCcore-11.3.0 - x - - - - pybind11/2.6.0-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/pytest-flakefinder/","title":"pytest-flakefinder","text":"<p>Runs tests multiple times to expose flakiness.</p> <p>https://github.com/dropbox/pytest-flakefinder</p>"},{"location":"available_software/detail/pytest-flakefinder/#available-modules","title":"Available modules","text":"<p>The overview below shows which pytest-flakefinder installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pytest-flakefinder, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pytest-flakefinder/1.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pytest-flakefinder/1.1.0-GCCcore-12.3.0 - - - x - x"},{"location":"available_software/detail/pytest-rerunfailures/","title":"pytest-rerunfailures","text":"<p>pytest plugin to re-run tests to eliminate flaky failures.</p> <p>https://github.com/pytest-dev/pytest-rerunfailures</p>"},{"location":"available_software/detail/pytest-rerunfailures/#available-modules","title":"Available modules","text":"<p>The overview below shows which pytest-rerunfailures installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pytest-rerunfailures, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pytest-rerunfailures/12.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pytest-rerunfailures/12.0-GCCcore-12.3.0 - - - x - x pytest-rerunfailures/11.1-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/pytest-shard/","title":"pytest-shard","text":"<p>pytest plugin to support parallelism across multiple machines.Shards tests based on a hash of their test name enabling easy parallelism across machines,suitable for a wide variety of continuous integration services.Tests are split at the finest level of granularity, individual test cases,enabling parallelism even if all of your tests are in a single file(or even single parameterized test method).</p> <p>https://github.com/AdamGleave/pytest-shard</p>"},{"location":"available_software/detail/pytest-shard/#available-modules","title":"Available modules","text":"<p>The overview below shows which pytest-shard installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pytest-shard, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pytest-shard/0.1.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pytest-shard/0.1.2-GCCcore-12.3.0 - - - x - x pytest-shard/0.1.2-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/pytest-xdist/","title":"pytest-xdist","text":"<p>xdist: pytest distributed testing pluginThe pytest-xdist plugin extends pytest with some unique test execution modes:    * test run parallelization: if you have multiple CPUs or hosts you      can use those for a combined test run. This allows to speed up      development or to use special resources of remote machines.    * --looponfail: run your tests repeatedly in a subprocess. After        each run pytest waits until a file in your project changes and        then re-runs the previously failing tests. This is repeated        until all tests pass after which again a full run is        performed.    * Multi-Platform coverage: you can specify different Python      interpreters or different platforms and run tests in parallel on      all of them.Before running tests remotely, pytest efficiently \u201crsyncs\u201d yourprogram source code to the remote place. All test results are reportedback and displayed to your local terminal. You may specify differentPython versions and interpreters.</p> <p>https://github.com/pytest-dev/pytest-xdist</p>"},{"location":"available_software/detail/pytest-xdist/#available-modules","title":"Available modules","text":"<p>The overview below shows which pytest-xdist installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using pytest-xdist, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load pytest-xdist/2.5.0-GCCcore-11.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 pytest-xdist/2.5.0-GCCcore-11.3.0 - - - - x x"},{"location":"available_software/detail/pytest-xdist/#pytest-xdist250-gcccore-1130","title":"pytest-xdist/2.5.0-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>apipkg-1.5, execnet-1.9.0, pytest-forked-1.4.0, pytest-xdist-2.5.0</p>"},{"location":"available_software/detail/python-casacore/","title":"python-casacore","text":"<p>Python-casacore is a set of Python bindings for casacore,a c++ library used in radio astronomy. Python-casacore replaces the old pyrap.</p> <p>https://casacore.github.io/python-casacore/#</p>"},{"location":"available_software/detail/python-casacore/#available-modules","title":"Available modules","text":"<p>The overview below shows which python-casacore installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using python-casacore, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load python-casacore/3.5.2-foss-2023b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 python-casacore/3.5.2-foss-2023b - x - - - -"},{"location":"available_software/detail/python-casacore/#python-casacore352-foss-2023b","title":"python-casacore/3.5.2-foss-2023b","text":"<p>This is a list of extensions included in the module:</p> <p>python-casacore-3.5.2, setuptools-69.1.0</p>"},{"location":"available_software/detail/python-isal/","title":"python-isal","text":"<p>Faster zlib and gzip compatible compression and decompression by providing python bindings for the isa-l library.</p> <p>https://github.com/pycompression/python-isal</p>"},{"location":"available_software/detail/python-isal/#available-modules","title":"Available modules","text":"<p>The overview below shows which python-isal installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using python-isal, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load python-isal/1.1.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 python-isal/1.1.0-GCCcore-12.3.0 - x - - - - python-isal/1.1.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/qchem/","title":"qchem","text":"<p>Description not found in module</p>"},{"location":"available_software/detail/qchem/#available-modules","title":"Available modules","text":"<p>The overview below shows which qchem installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using qchem, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load qchem/6.1.1-shm\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 qchem/6.1.1-shm - x - - - - qchem/5.4.2-shm - x - - - -"},{"location":"available_software/detail/rclone/","title":"rclone","text":"<p>Rclone is a command line program to sync files and directories to and from a variety of online storage services</p> <p>https://rclone.org/</p>"},{"location":"available_software/detail/rclone/#available-modules","title":"Available modules","text":"<p>The overview below shows which rclone installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using rclone, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load rclone/1.63.1-amd64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 rclone/1.63.1-amd64 - x - - - -"},{"location":"available_software/detail/re2c/","title":"re2c","text":"<p>re2c is a free and open-source lexer generator for C and C++. Its main goal is generatingfast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of usingtraditional table-driven approach, re2c encodes the generated finite state automata directly in the formof conditional jumps and comparisons.</p> <p>https://re2c.org</p>"},{"location":"available_software/detail/re2c/#available-modules","title":"Available modules","text":"<p>The overview below shows which re2c installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using re2c, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load re2c/3.1-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 re2c/3.1-GCCcore-13.2.0 - x - - - - re2c/3.1-GCCcore-12.3.0 - x - - - - re2c/3.0-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/reportseff/","title":"reportseff","text":"<p>A python script for tabular display of slurm efficiency information</p> <p>https://github.com/troycomi/reportseff</p>"},{"location":"available_software/detail/reportseff/#available-modules","title":"Available modules","text":"<p>The overview below shows which reportseff installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using reportseff, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load reportseff/2.7.6\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 reportseff/2.7.6 - x - - - -"},{"location":"available_software/detail/ruamel.yaml/","title":"ruamel.yaml","text":"<p>ruamel.yaml is a YAML 1.2 loader/dumper package for Python.</p> <p>https://sourceforge.net/projects/ruamel-yaml</p>"},{"location":"available_software/detail/ruamel.yaml/#available-modules","title":"Available modules","text":"<p>The overview below shows which ruamel.yaml installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using ruamel.yaml, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load ruamel.yaml/0.17.21-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 ruamel.yaml/0.17.21-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/ruamel.yaml/#ruamelyaml01721-gcccore-1220","title":"ruamel.yaml/0.17.21-GCCcore-12.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>configobj-5.0.8, lz4-4.3.2, ruamel.yaml-0.17.21, ruamel.yaml.base-0.3.2, ruamel.yaml.clib-0.2.7, ruamel.yaml.cmd-0.6.3, ruamel.yaml.convert-0.3.2</p>"},{"location":"available_software/detail/safestringlib/","title":"safestringlib","text":"<p>The Secure Development Lifecycle (SDL) recommends banning certain C Libraryfunctions because they directly contribute to security vulnerabilities such asbuffer overflows. However routines for the manipulation of strings and memorybuffers are common in software and firmware, and are essential to accomplishcertain programming tasks. Safer replacements for these functions that avoid orprevent serious security vulnerabilities (e.g. buffer overflows, string formatattacks, conversion overflows/underflows, etc.) are available in the SafeStringLibrary.</p> <p>https://github.com/intel/safestringlib</p>"},{"location":"available_software/detail/safestringlib/#available-modules","title":"Available modules","text":"<p>The overview below shows which safestringlib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using safestringlib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load safestringlib/20240228-intel-compilers-2023.1.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 safestringlib/20240228-intel-compilers-2023.1.0 - x - - - -"},{"location":"available_software/detail/scikit-build-core/","title":"scikit-build-core","text":"<p>Scikit-build-core is a complete ground-up rewrite of scikit-build on top ofmodern packaging APIs. It provides a bridge between CMake and the Python buildsystem, allowing you to make Python modules with CMake.</p> <p>https://scikit-build.readthedocs.io/en/latest/</p>"},{"location":"available_software/detail/scikit-build-core/#available-modules","title":"Available modules","text":"<p>The overview below shows which scikit-build-core installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using scikit-build-core, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load scikit-build-core/0.9.3-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 scikit-build-core/0.9.3-GCCcore-13.2.0 - x - - - - scikit-build-core/0.5.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/scikit-build-core/#scikit-build-core093-gcccore-1320","title":"scikit-build-core/0.9.3-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>scikit_build_core-0.9.3</p>"},{"location":"available_software/detail/scikit-build-core/#scikit-build-core050-gcccore-1230","title":"scikit-build-core/0.5.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>scikit_build_core-0.5.0</p>"},{"location":"available_software/detail/scikit-build/","title":"scikit-build","text":"<p>Scikit-Build, or skbuild, is an improved build system generatorfor CPython C/C++/Fortran/Cython extensions.</p> <p>https://scikit-build.readthedocs.io/en/latest</p>"},{"location":"available_software/detail/scikit-build/#available-modules","title":"Available modules","text":"<p>The overview below shows which scikit-build installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using scikit-build, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load scikit-build/0.17.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 scikit-build/0.17.6-GCCcore-13.2.0 - x - - - - scikit-build/0.17.6-GCCcore-12.3.0 - x x x x x scikit-build/0.15.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/scikit-build/#scikit-build0176-gcccore-1320","title":"scikit-build/0.17.6-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>distro-1.8.0, packaging-23.1, scikit_build-0.17.6</p>"},{"location":"available_software/detail/scikit-build/#scikit-build0176-gcccore-1230","title":"scikit-build/0.17.6-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>distro-1.8.0, packaging-23.1, scikit_build-0.17.6</p>"},{"location":"available_software/detail/scikit-build/#scikit-build0150-gcccore-1130","title":"scikit-build/0.15.0-GCCcore-11.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>distro-1.7.0, scikit-build-0.15.0</p>"},{"location":"available_software/detail/scikit-learn/","title":"scikit-learn","text":"<p>Scikit-learn integrates machine learning algorithms in the tightly-knit scientific Python world,building upon numpy, scipy, and matplotlib. As a machine-learning module,it provides versatile tools for data mining and analysis in any field of science and engineering.It strives to be simple and efficient, accessible to everybody, and reusable in various contexts.</p> <p>https://scikit-learn.org/stable/index.html</p>"},{"location":"available_software/detail/scikit-learn/#available-modules","title":"Available modules","text":"<p>The overview below shows which scikit-learn installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using scikit-learn, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load scikit-learn/1.3.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 scikit-learn/1.3.1-gfbf-2023a - x - - - -"},{"location":"available_software/detail/scikit-learn/#scikit-learn131-gfbf-2023a","title":"scikit-learn/1.3.1-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>scikit-learn-1.3.1, sklearn-0.0</p>"},{"location":"available_software/detail/seqtk/","title":"seqtk","text":"<p>Seqtk is a fast and lightweight tool for processing sequences in the FASTA or FASTQ format. It seamlessly parses both FASTA and FASTQ files which can also be optionally compressed by gzip.</p> <p>https://github.com/lh3/seqtk/</p>"},{"location":"available_software/detail/seqtk/#available-modules","title":"Available modules","text":"<p>The overview below shows which seqtk installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using seqtk, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load seqtk/1.4-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 seqtk/1.4-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/setuptools-rust/","title":"setuptools-rust","text":"<p>setuptools-rust is a plugin for setuptools to build Rust Python extensionsimplemented with PyO3 or rust-cpython.</p> <p>https://github.com/PyO3/setuptools-rust</p>"},{"location":"available_software/detail/setuptools-rust/#available-modules","title":"Available modules","text":"<p>The overview below shows which setuptools-rust installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using setuptools-rust, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load setuptools-rust/1.8.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 setuptools-rust/1.8.0-GCCcore-13.2.0 - x - - - - setuptools-rust/1.6.0-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/setuptools-rust/#setuptools-rust180-gcccore-1320","title":"setuptools-rust/1.8.0-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>semantic_version-2.10.0, setuptools-rust-1.8.0, typing_extensions-4.8.0</p>"},{"location":"available_software/detail/setuptools-rust/#setuptools-rust160-gcccore-1230","title":"setuptools-rust/1.6.0-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>semantic_version-2.10.0, setuptools-rust-1.6.0, typing_extensions-4.6.3</p>"},{"location":"available_software/detail/siscone/","title":"siscone","text":"<p>Hadron Seedless Infrared-Safe Cone jet algorithm</p> <p>https://siscone.hepforge.org/</p>"},{"location":"available_software/detail/siscone/#available-modules","title":"Available modules","text":"<p>The overview below shows which siscone installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using siscone, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load siscone/3.0.6-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 siscone/3.0.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/smithwaterman/","title":"smithwaterman","text":"<p>smith-waterman-gotoh alignment algorithm.</p> <p>https://github.com/ekg/smithwaterman</p>"},{"location":"available_software/detail/smithwaterman/#available-modules","title":"Available modules","text":"<p>The overview below shows which smithwaterman installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using smithwaterman, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load smithwaterman/20160702-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 smithwaterman/20160702-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/snakemake/","title":"snakemake","text":"<p>The Snakemake workflow management system is a tool to create reproducible and scalable data analyses.</p> <p>https://snakemake.readthedocs.io</p>"},{"location":"available_software/detail/snakemake/#available-modules","title":"Available modules","text":"<p>The overview below shows which snakemake installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using snakemake, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load snakemake/8.4.2-foss-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 snakemake/8.4.2-foss-2023a - x - - - -"},{"location":"available_software/detail/snakemake/#snakemake842-foss-2023a","title":"snakemake/8.4.2-foss-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>argparse-dataclass-2.0.0, conda-inject-1.3.1, ConfigArgParse-1.7, connection-pool-0.0.3, datrie-0.8.2, dpath-2.1.6, fastjsonschema-2.19.1, humanfriendly-10.0, immutables-0.20, jupyter-core-5.7.1, nbformat-5.9.2, plac-1.4.2, reretry-0.11.8, smart-open-6.4.0, snakemake-8.4.2, snakemake-executor-plugin-cluster-generic-1.0.7, snakemake-executor-plugin-cluster-sync-0.1.3, snakemake-executor-plugin-flux-0.1.0, snakemake-executor-plugin-slurm-0.2.1, snakemake-executor-plugin-slurm-jobstep-0.1.10, snakemake-interface-common-1.15.2, snakemake-interface-executor-plugins-8.2.0, snakemake-interface-storage-plugins-3.0.0, stopit-1.1.2, throttler-1.2.2, toposort-1.10, yte-1.5.4</p>"},{"location":"available_software/detail/snappy/","title":"snappy","text":"<p>Snappy is a compression/decompression library. It does not aimfor maximum compression, or compatibility with any other compression library;instead, it aims for very high speeds and reasonable compression.</p> <p>https://github.com/google/snappy</p>"},{"location":"available_software/detail/snappy/#available-modules","title":"Available modules","text":"<p>The overview below shows which snappy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using snappy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load snappy/1.1.10-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 snappy/1.1.10-GCCcore-13.2.0 - x - - - - snappy/1.1.10-GCCcore-12.3.0 - x - - - - snappy/1.1.9-GCCcore-12.2.0 - x - - - - snappy/1.1.9-GCCcore-11.3.0 - x - - - x"},{"location":"available_software/detail/spaln/","title":"spaln","text":"<p>Spaln (space-efficient spliced alignment) is a stand-alone program that maps and aligns a set of cDNA or protein sequences onto a whole genomic sequence in a single job.</p> <p>https://github.com/ogotoh/spaln</p>"},{"location":"available_software/detail/spaln/#available-modules","title":"Available modules","text":"<p>The overview below shows which spaln installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using spaln, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load spaln/2.4.13f-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 spaln/2.4.13f-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/sparsehash/","title":"sparsehash","text":"<p>An extremely memory-efficient hash_map implementation. 2 bits/entry overhead! The SparseHash library contains several hash-map implementations, including implementations that optimize for space or speed.</p> <p>https://github.com/sparsehash/sparsehash</p>"},{"location":"available_software/detail/sparsehash/#available-modules","title":"Available modules","text":"<p>The overview below shows which sparsehash installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using sparsehash, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load sparsehash/2.0.4-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 sparsehash/2.0.4-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/spglib-python/","title":"spglib-python","text":"<p>Spglib for Python. Spglib is a library for finding and handling crystal symmetries written in C.</p> <p>https://pypi.python.org/pypi/spglib</p>"},{"location":"available_software/detail/spglib-python/#available-modules","title":"Available modules","text":"<p>The overview below shows which spglib-python installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using spglib-python, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load spglib-python/2.1.0-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 spglib-python/2.1.0-gfbf-2023a - x - - - -"},{"location":"available_software/detail/spglib-python/#spglib-python210-gfbf-2023a","title":"spglib-python/2.1.0-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>pyproject-metadata-0.7.1, spglib-2.1.0</p>"},{"location":"available_software/detail/sympy/","title":"sympy","text":"<p>SymPy is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python and does not require any external libraries.</p> <p>https://sympy.org/</p>"},{"location":"available_software/detail/sympy/#available-modules","title":"Available modules","text":"<p>The overview below shows which sympy installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using sympy, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load sympy/1.12-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 sympy/1.12-gfbf-2023a - - - x - x"},{"location":"available_software/detail/tabixpp/","title":"tabixpp","text":"<p>C++ wrapper to tabix indexer</p> <p>https://github.com/ekg/tabixpp</p>"},{"location":"available_software/detail/tabixpp/#available-modules","title":"Available modules","text":"<p>The overview below shows which tabixpp installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tabixpp, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tabixpp/1.1.2-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tabixpp/1.1.2-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/tbb/","title":"tbb","text":"<p>Intel(R) Threading Building Blocks (Intel(R) TBB) lets you easily write parallel C++ programs that take full advantage of multicore performance, that are portable, composable and have future-proof scalability.</p> <p>https://github.com/oneapi-src/oneTBB</p>"},{"location":"available_software/detail/tbb/#available-modules","title":"Available modules","text":"<p>The overview below shows which tbb installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tbb, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tbb/2021.11.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tbb/2021.11.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/tbl2asn/","title":"tbl2asn","text":"<p>Tbl2asn is a command-line program that automates the creation of sequence records for submission to GenBank</p> <p>https://www.ncbi.nlm.nih.gov/genbank/tbl2asn2/</p>"},{"location":"available_software/detail/tbl2asn/#available-modules","title":"Available modules","text":"<p>The overview below shows which tbl2asn installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tbl2asn, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tbl2asn/20230713-linux64\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tbl2asn/20230713-linux64 - x - - - -"},{"location":"available_software/detail/tcsh/","title":"tcsh","text":"<p>Tcsh is an enhanced, but completely compatible version of the Berkeley UNIX C shell (csh). It is a command language interpreter usable both as an interactive login shell and a shell script command processor. It includes a command-line editor, programmable word completion, spelling correction, a history mechanism, job control and a C-like syntax.</p> <p>https://www.tcsh.org</p>"},{"location":"available_software/detail/tcsh/#available-modules","title":"Available modules","text":"<p>The overview below shows which tcsh installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tcsh, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tcsh/6.24.10-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tcsh/6.24.10-GCCcore-12.3.0 - x - - - - tcsh/6.24.07-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/tecplot360ex/","title":"tecplot360ex","text":"<p>Quickly plot and animate your CFD results exactly the way you want. Analyze complex solutions,arrange multiple layouts, and communicate your results with professional images and animations.</p> <p>http://www.tecplot.com/products/tecplot-360/</p>"},{"location":"available_software/detail/tecplot360ex/#available-modules","title":"Available modules","text":"<p>The overview below shows which tecplot360ex installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tecplot360ex, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tecplot360ex/2023r2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tecplot360ex/2023r2 - x - - - -"},{"location":"available_software/detail/texlive/","title":"texlive","text":"<p>TeX is a typesetting language. Instead of visually formatting your text, you enter your manuscript text intertwined with TeX commands in a plain text file. You then run TeX to produce formatted output, such as a PDF file. Thus, in contrast to standard word processors, your document is a separate file that does not pretend to be a representation of the final typeset output, and so can be easily edited and manipulated.</p> <p>https://tug.org</p>"},{"location":"available_software/detail/texlive/#available-modules","title":"Available modules","text":"<p>The overview below shows which texlive installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using texlive, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load texlive/20230313-GCC-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 texlive/20230313-GCC-12.3.0 - x - - - -"},{"location":"available_software/detail/time/","title":"time","text":"<p>The `time' command runs another program, then displays information about the resources used by that program, collected by the system while the program was running.</p> <p>https://www.gnu.org/software/time/</p>"},{"location":"available_software/detail/time/#available-modules","title":"Available modules","text":"<p>The overview below shows which time installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using time, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load time/1.9-GCCcore-12.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 time/1.9-GCCcore-12.2.0 - x - - - -"},{"location":"available_software/detail/torchvision/","title":"torchvision","text":"<p>Datasets, Transforms and Models specific to Computer Vision</p> <p>https://github.com/pytorch/vision</p>"},{"location":"available_software/detail/torchvision/#available-modules","title":"Available modules","text":"<p>The overview below shows which torchvision installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using torchvision, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load torchvision/0.16.0-foss-2023a-CUDA-12.1.1\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 torchvision/0.16.0-foss-2023a-CUDA-12.1.1 - - - x - x"},{"location":"available_software/detail/tornado/","title":"tornado","text":"<p>Tornado is a Python web framework and asynchronous networking library.</p> <p>https://github.com/tornadoweb/tornado</p>"},{"location":"available_software/detail/tornado/#available-modules","title":"Available modules","text":"<p>The overview below shows which tornado installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using tornado, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load tornado/6.3.2-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 tornado/6.3.2-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/typing-extensions/","title":"typing-extensions","text":"<p>Typing Extensions \u2013 Backported and Experimental Type Hints for Python</p> <p>https://github.com/python/typing/blob/master/typing_extensions/README.rst</p>"},{"location":"available_software/detail/typing-extensions/#available-modules","title":"Available modules","text":"<p>The overview below shows which typing-extensions installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using typing-extensions, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load typing-extensions/4.9.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 typing-extensions/4.9.0-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/utf8proc/","title":"utf8proc","text":"<p>utf8proc is a small, clean C library that provides Unicode normalization, case-folding, and other operations for data in the UTF-8 encoding.</p> <p>https://github.com/JuliaStrings/utf8proc</p>"},{"location":"available_software/detail/utf8proc/#available-modules","title":"Available modules","text":"<p>The overview below shows which utf8proc installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using utf8proc, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load utf8proc/2.8.0-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 utf8proc/2.8.0-GCCcore-12.3.0 - x - - - - utf8proc/2.8.0-GCCcore-12.2.0 - x - - - - utf8proc/2.7.0-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/util-linux/","title":"util-linux","text":"<p>Set of Linux utilities</p> <p>https://www.kernel.org/pub/linux/utils/util-linux</p>"},{"location":"available_software/detail/util-linux/#available-modules","title":"Available modules","text":"<p>The overview below shows which util-linux installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using util-linux, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load util-linux/2.39-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 util-linux/2.39-GCCcore-13.2.0 - x - - - - util-linux/2.39-GCCcore-12.3.0 - x x x x x util-linux/2.38.1-GCCcore-12.2.0 - x - - - - util-linux/2.38-GCCcore-11.3.0 - x - - - - util-linux/2.36-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/vcflib/","title":"vcflib","text":"<p>vcflib provides methods to manipulate and interpret sequence variation as it can be described by VCF. The Variant Call Format (VCF) is a flat-file, tab-delimited textual format intended to concisely describe reference-indexed genetic variations between individuals.</p> <p>https://github.com/vcflib/vcflib</p>"},{"location":"available_software/detail/vcflib/#available-modules","title":"Available modules","text":"<p>The overview below shows which vcflib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using vcflib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load vcflib/1.0.9-gfbf-2023a-R-4.3.2\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 vcflib/1.0.9-gfbf-2023a-R-4.3.2 - x - - - -"},{"location":"available_software/detail/virtualenv/","title":"virtualenv","text":"<p>A tool for creating isolated virtual python environments.</p> <p>https://github.com/pypa/virtualenv</p>"},{"location":"available_software/detail/virtualenv/#available-modules","title":"Available modules","text":"<p>The overview below shows which virtualenv installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using virtualenv, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load virtualenv/20.24.6-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 virtualenv/20.24.6-GCCcore-13.2.0 - x - - - - virtualenv/20.23.1-GCCcore-12.3.0 - x x x x x"},{"location":"available_software/detail/virtualenv/#virtualenv20246-gcccore-1320","title":"virtualenv/20.24.6-GCCcore-13.2.0","text":"<p>This is a list of extensions included in the module:</p> <p>distlib-0.3.7, filelock-3.13.0, platformdirs-3.11.0, virtualenv-20.24.6</p>"},{"location":"available_software/detail/virtualenv/#virtualenv20231-gcccore-1230","title":"virtualenv/20.23.1-GCCcore-12.3.0","text":"<p>This is a list of extensions included in the module:</p> <p>distlib-0.3.6, filelock-3.12.2, platformdirs-3.8.0, virtualenv-20.23.1</p>"},{"location":"available_software/detail/waLBerla/","title":"waLBerla","text":"<p>Widely applicable Lattics-Boltzmann from Erlangen is a block-structured high-performance framework for multiphysics simulations</p> <p>https://walberla.net/index.html</p>"},{"location":"available_software/detail/waLBerla/#available-modules","title":"Available modules","text":"<p>The overview below shows which waLBerla installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using waLBerla, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load waLBerla/6.1-foss-2022b\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 waLBerla/6.1-foss-2022b - x - - - -"},{"location":"available_software/detail/wget/","title":"wget","text":"<p>GNU Wget is a free software package for retrieving files using HTTP, HTTPS and FTP, the most widely-used Internet protocols. It is a non-interactive commandline tool, so it may easily be called from scripts, cron jobs, terminals without X-Windows support, etc.</p> <p>https://www.gnu.org/software/wget</p>"},{"location":"available_software/detail/wget/#available-modules","title":"Available modules","text":"<p>The overview below shows which wget installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using wget, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load wget/1.21.4-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 wget/1.21.4-GCCcore-13.2.0 - x - - - - wget/1.21.3-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/wrapt/","title":"wrapt","text":"<p>The aim of the wrapt module is to provide a transparent objectproxy for Python, which can be used as the basis for the construction offunction wrappers and decorator functions.</p> <p>https://pypi.org/project/wrapt/</p>"},{"location":"available_software/detail/wrapt/#available-modules","title":"Available modules","text":"<p>The overview below shows which wrapt installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using wrapt, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load wrapt/1.15.0-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 wrapt/1.15.0-gfbf-2023a - x - - - -"},{"location":"available_software/detail/wrapt/#wrapt1150-gfbf-2023a","title":"wrapt/1.15.0-gfbf-2023a","text":"<p>This is a list of extensions included in the module:</p> <p>wrapt-1.15.0</p>"},{"location":"available_software/detail/x264/","title":"x264","text":"<p>x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL.</p> <p>https://www.videolan.org/developers/x264.html</p>"},{"location":"available_software/detail/x264/#available-modules","title":"Available modules","text":"<p>The overview below shows which x264 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using x264, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load x264/20230226-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 x264/20230226-GCCcore-12.3.0 - x - - - - x264/20220620-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/x265/","title":"x265","text":"<p>x265 is a free software library and application for encoding video streams into the H.265 AVC compression format, and is released under the terms of the GNU GPL.</p> <p>https://x265.org/</p>"},{"location":"available_software/detail/x265/#available-modules","title":"Available modules","text":"<p>The overview below shows which x265 installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using x265, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load x265/3.5-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 x265/3.5-GCCcore-12.3.0 - x - - - - x265/3.5-GCCcore-11.3.0 - x - - - -"},{"location":"available_software/detail/xorg-macros/","title":"xorg-macros","text":"<p>X.org macros utilities.</p> <p>https://gitlab.freedesktop.org/xorg/util/macros</p>"},{"location":"available_software/detail/xorg-macros/#available-modules","title":"Available modules","text":"<p>The overview below shows which xorg-macros installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using xorg-macros, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load xorg-macros/1.20.0-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 xorg-macros/1.20.0-GCCcore-13.2.0 - x - - - - xorg-macros/1.20.0-GCCcore-12.3.0 - x x x x x xorg-macros/1.19.3-GCCcore-12.2.0 - x - - - - xorg-macros/1.19.3-GCCcore-11.3.0 - x - - - - xorg-macros/1.19.2-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/xprop/","title":"xprop","text":"<p>The xprop utility is for displaying window and font properties in an X server. One window or font is selected using the command line arguments or possibly in the case of a window, by clicking on the desired window. A list of properties is then given, possibly with formatting information.</p> <p>https://www.x.org/wiki/</p>"},{"location":"available_software/detail/xprop/#available-modules","title":"Available modules","text":"<p>The overview below shows which xprop installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using xprop, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load xprop/1.2.6-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 xprop/1.2.6-GCCcore-12.3.0 - x - - - -"},{"location":"available_software/detail/xproto/","title":"xproto","text":"<p>X protocol and ancillary headers</p> <p>https://www.freedesktop.org/wiki/Software/xlibs</p>"},{"location":"available_software/detail/xproto/#available-modules","title":"Available modules","text":"<p>The overview below shows which xproto installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using xproto, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load xproto/7.0.31-GCCcore-12.3.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 xproto/7.0.31-GCCcore-12.3.0 - x - - - - xproto/7.0.31-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/xtb/","title":"xtb","text":"<p>xtb - An extended tight-binding semi-empirical program package.</p> <p>https://xtb-docs.readthedocs.io</p>"},{"location":"available_software/detail/xtb/#available-modules","title":"Available modules","text":"<p>The overview below shows which xtb installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using xtb, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load xtb/6.6.1-gfbf-2023a\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 xtb/6.6.1-gfbf-2023a - x - - - -"},{"location":"available_software/detail/xxd/","title":"xxd","text":"<p>xxd is part of the VIM package and this will only install xxd, not vim!xxd converts to/from hexdumps of binary files.</p> <p>https://www.vim.org</p>"},{"location":"available_software/detail/xxd/#available-modules","title":"Available modules","text":"<p>The overview below shows which xxd installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using xxd, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load xxd/9.1.0307-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 xxd/9.1.0307-GCCcore-13.2.0 - x - - - - xxd/9.0.2112-GCCcore-12.3.0 - x - - - - xxd/8.2.4220-GCCcore-10.2.0 - x - - - -"},{"location":"available_software/detail/zlib/","title":"zlib","text":"<p>zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system.</p> <p>https://www.zlib.net/</p>"},{"location":"available_software/detail/zlib/#available-modules","title":"Available modules","text":"<p>The overview below shows which zlib installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using zlib, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load zlib/1.2.13-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 zlib/1.2.13-GCCcore-13.2.0 - x - - - - zlib/1.2.13-GCCcore-12.3.0 - x x x x x zlib/1.2.13 - x x x x x zlib/1.2.12-GCCcore-12.2.0 - x - - - - zlib/1.2.12-GCCcore-11.3.0 - x - - - - zlib/1.2.12 - x - - - - zlib/1.2.11-GCCcore-11.2.0 - x - - - - zlib/1.2.11-GCCcore-10.2.0 - x - - - - zlib/1.2.11 - x - - - -"},{"location":"available_software/detail/zstd/","title":"zstd","text":"<p>Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression/speed trade-off, while being backed by a very fast decoder. It also offers a special mode for small data, called dictionary compression, and can create dictionaries from any sample set.</p> <p>https://facebook.github.io/zstd</p>"},{"location":"available_software/detail/zstd/#available-modules","title":"Available modules","text":"<p>The overview below shows which zstd installations are available per target architecture in the HPCC module system, ordered based on software version (new to old).</p> <p>To start using zstd, load one of these modules using a <code>module load</code> command like:</p> <pre><code>module load zstd/1.5.5-GCCcore-13.2.0\n</code></pre> <p>(This data was automatically generated on Thu, 26 Sep 2024 at 12:00:31 EDT) </p> neoverse_v1 generic zen2 zen3 haswell skylake_avx512 Grace Nodes everywhere (except Grace nodes) amd20 amd22 intel16 intel18,amd20-v100,amd21,intel21 zstd/1.5.5-GCCcore-13.2.0 - x - - - - zstd/1.5.5-GCCcore-12.3.0 - x x x x x zstd/1.5.2-GCCcore-12.2.0 - x - - - - zstd/1.5.2-GCCcore-11.3.0 - x - - - - zstd/1.4.5-GCCcore-10.2.0 - x - - - -"},{"location":"tags/","title":"Tags","text":"<p>Docs pages organized by tag.</p>"},{"location":"tags/#accounts","title":"accounts","text":"<ul> <li>Buy-in and account management</li> <li>Obtaining an HPCC account</li> </ul>"},{"location":"tags/#ai","title":"AI","text":"<ul> <li>(2024-06-19) Lab Notebook: LM Studio Installation</li> </ul>"},{"location":"tags/#alphafold","title":"AlphaFold","text":"<ul> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>AlphaFold on HPCC</li> <li>AlphaFold via Singularity</li> </ul>"},{"location":"tags/#antismash","title":"AntiSMASH","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> </ul>"},{"location":"tags/#appimage","title":"AppImage","text":"<ul> <li>(2024-06-19) Lab Notebook: LM Studio Installation</li> </ul>"},{"location":"tags/#bactopia","title":"Bactopia","text":"<ul> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> </ul>"},{"location":"tags/#bioinformatics","title":"bioinformatics","text":"<ul> <li>Mothur</li> <li>QIIME 2</li> </ul>"},{"location":"tags/#blas","title":"BLAS","text":"<ul> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#buyin","title":"buyin","text":"<ul> <li>Buy-in and account management</li> <li>SLURM queueing and partitions</li> </ul>"},{"location":"tags/#centos-8","title":"CentOS 8","text":"<ul> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> </ul>"},{"location":"tags/#checkpointing","title":"checkpointing","text":"<ul> <li>Checkpoint with DMTCP</li> <li>Overview</li> <li>Powertools <code>longjob</code> by DMTCP</li> </ul>"},{"location":"tags/#cime","title":"CIME","text":"<ul> <li>(2024-06-13) Lab Notebook: CTSM Installation</li> </ul>"},{"location":"tags/#clean-up","title":"clean up","text":"<ul> <li>(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC</li> </ul>"},{"location":"tags/#cloud","title":"cloud","text":"<ul> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#coldfront","title":"ColdFront","text":"<ul> <li>(2024-10-01) Lab Notebook: New ICER User Database Testing Requested (ColdFront)</li> </ul>"},{"location":"tags/#command-line","title":"command line","text":"<ul> <li>Conditional statements</li> <li>Expansion</li> <li>Linux Command Line for Beginners I</li> <li>Linux Command Line for Beginners II</li> <li>Loops</li> <li>Regular expressions</li> <li>Variables I</li> <li>Variables II</li> </ul>"},{"location":"tags/#compilers","title":"compilers","text":"<ul> <li>Compilers and Libraries</li> <li>Compiling for GPUs</li> <li>make</li> </ul>"},{"location":"tags/#conda","title":"Conda","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC</li> <li>Installing TensorFlow using anaconda</li> <li>Installing TF using conda</li> <li>Installing Pytorch with Anaconda</li> <li>Migrating Conda environments</li> <li>Replacing an existing Conda installation</li> <li>Using Conda</li> </ul>"},{"location":"tags/#conda_1","title":"conda","text":"<ul> <li>(2024-08-28) Lab Notebook: cuQuantum Installation and Usage</li> <li>(2024-09-16) Lab Notebook: PyTorch OnDemand Usage</li> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> <li>(2024-09-25) Lab Notebook: Transitioning from Anaconda to Miniforge</li> </ul>"},{"location":"tags/#containers","title":"containers","text":"<ul> <li>Overview</li> <li>Docker</li> <li>Singularity Advanced Topics</li> <li>Singularity Introduction</li> </ul>"},{"location":"tags/#cryosparc","title":"CryoSPARC","text":"<ul> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> </ul>"},{"location":"tags/#crystal","title":"Crystal","text":"<ul> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> </ul>"},{"location":"tags/#csh","title":"csh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#ctsm","title":"CTSM","text":"<ul> <li>(2024-06-13) Lab Notebook: CTSM Installation</li> </ul>"},{"location":"tags/#cuda","title":"CUDA","text":"<ul> <li>(2024-08-28) Lab Notebook: cuQuantum Installation and Usage</li> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> </ul>"},{"location":"tags/#dashboard","title":"dashboard","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"tags/#data-machine","title":"data machine","text":"<ul> <li>Data Machine overview</li> <li>Using the Data Machine</li> </ul>"},{"location":"tags/#ddd","title":"DDD","text":"<ul> <li>Data Display Debugger</li> </ul>"},{"location":"tags/#debugging","title":"Debugging","text":"<ul> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>Data Display Debugger</li> </ul>"},{"location":"tags/#docker","title":"Docker","text":"<ul> <li>Overview</li> <li>Docker</li> <li>Singularity Advanced Topics</li> </ul>"},{"location":"tags/#drive-mapping","title":"drive mapping","text":"<ul> <li>Mapping drives using SSHFS</li> <li>Mapping drives using Samba</li> </ul>"},{"location":"tags/#easybuild","title":"EasyBuild","text":"<ul> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>EasyBuild Reference</li> <li>EasyBuild Tutorial</li> </ul>"},{"location":"tags/#esmf","title":"ESMF","text":"<ul> <li>(2024-06-13) Lab Notebook: CTSM Installation</li> </ul>"},{"location":"tags/#explanation","title":"explanation","text":"<ul> <li>Overview</li> <li>Data Machine overview</li> <li>File permissions</li> <li>HPCC layout</li> <li>Home space</li> <li>Overview</li> <li>Overview</li> <li>Python on HPCC</li> <li>Research space</li> <li>Overview</li> <li>Scratch space</li> <li>Singularity Advanced Topics</li> <li>Migrating from Torque</li> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#faq","title":"FAQ","text":"<ul> <li>FAQ</li> </ul>"},{"location":"tags/#files","title":"files","text":"<ul> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> <li>File count</li> <li>File permissions</li> <li>Guidelines for choosing file systems and I/O</li> <li>Home space</li> <li>Managing file permissions</li> <li>Research space</li> <li>Scratch space</li> <li>Sensitive data storage</li> </ul>"},{"location":"tags/#filesystem","title":"filesystem","text":"<ul> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> </ul>"},{"location":"tags/#flexiblas","title":"FlexiBLAS","text":"<ul> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#ggplot","title":"ggplot","text":"<ul> <li>(2024-08-07) Lab Notebook: Running ggplot in R without X11</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> </ul>"},{"location":"tags/#globus","title":"globus","text":"<ul> <li>Overview</li> </ul>"},{"location":"tags/#gpu","title":"GPU","text":"<ul> <li>Compiling for GPUs</li> <li>GPU resources</li> <li>Requesting GPUs</li> <li>TF GPU usage</li> </ul>"},{"location":"tags/#groups","title":"groups","text":"<ul> <li>Change primary group</li> <li>File permissions</li> <li>Managing file permissions</li> </ul>"},{"location":"tags/#how-to-guide","title":"how-to guide","text":"<ul> <li>SSH tunneling to directly access development nodes</li> <li>Backwards compatibility scripts</li> <li>Buy-in and account management</li> <li>Change primary group</li> <li>SSH connection via VS Code</li> <li>Connect to the HPCC</li> <li>Connections to compute nodes</li> <li>Editing text with nano</li> <li>File transfer</li> <li>Installing TensorFlow using anaconda</li> <li>Installing TF using conda</li> <li>Installing Pytorch with Anaconda</li> <li>Jupyter Notebooks in VS Code</li> <li>Managing file permissions</li> <li>Migrating Conda environments</li> <li>Migrating Python virtual environments</li> <li>Migrating R packages</li> <li>Migrating to the new operating system</li> <li>Migrating to the upgraded module system</li> <li>Cloud storage file transfer</li> <li>Replacing an existing Conda installation</li> <li>SLURM resource request guide</li> <li>SSH keys</li> <li>Scavenger Queue</li> <li>Tensorflow Model Training Code Examples</li> <li>Overview</li> <li>Using Conda</li> </ul>"},{"location":"tags/#hrldas","title":"HRLDAS","text":"<ul> <li>(2023-02-27) Lab Notebook: HRLDAS</li> </ul>"},{"location":"tags/#io","title":"I/O","text":"<ul> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>Guidelines for choosing file systems and I/O</li> </ul>"},{"location":"tags/#incident-report","title":"incident report","text":"<ul> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> </ul>"},{"location":"tags/#interactive-desktop","title":"interactive desktop","text":"<ul> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> </ul>"},{"location":"tags/#jags","title":"JAGS","text":"<ul> <li>Some packages and other information</li> </ul>"},{"location":"tags/#job-script","title":"job script","text":"<ul> <li>Example job scripts</li> <li>Writing and submitting job scripts</li> <li>List of job specifications</li> <li>SLURM environment variables</li> </ul>"},{"location":"tags/#jupyter","title":"Jupyter","text":"<ul> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>Jupyter Notebooks in VS Code</li> <li>Python on HPCC</li> </ul>"},{"location":"tags/#lab-notebook","title":"lab notebook","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC</li> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> <li>(2023-02-27) Lab Notebook: HRLDAS</li> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> <li>(2023-03-14) Lab Notebook: VASP</li> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> <li>(2024-06-13) Lab Notebook: CTSM Installation</li> <li>(2024-06-19) Lab Notebook: LM Studio Installation</li> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> <li>(2024-08-07) Lab Notebook: Running ggplot in R without X11</li> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> <li>(2024-08-28) Lab Notebook: Change to modules in SLURM jobs</li> <li>(2024-08-28) Lab Notebook: cuQuantum Installation and Usage</li> <li>(2024-09-05) Lab Notebook: Setting up a terminal with VS Code to connect to the HPCC</li> <li>(2024-09-16) Lab Notebook: PyTorch OnDemand Usage</li> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> <li>(2024-09-24) Lab Notebook: Ollama Module</li> <li>(2024-09-25) Lab Notebook: Transitioning from Anaconda to Miniforge</li> <li>(2024-10-01) Lab Notebook: New ICER User Database Testing Requested (ColdFront)</li> </ul>"},{"location":"tags/#lapack","title":"LAPACK","text":"<ul> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#linear-algebra","title":"linear algebra","text":"<ul> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#llm","title":"LLM","text":"<ul> <li>(2024-06-19) Lab Notebook: LM Studio Installation</li> </ul>"},{"location":"tags/#make","title":"make","text":"<ul> <li>make</li> </ul>"},{"location":"tags/#many-files","title":"many files","text":"<ul> <li>(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC</li> </ul>"},{"location":"tags/#mathematica","title":"Mathematica","text":"<ul> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> </ul>"},{"location":"tags/#matlab","title":"MATLAB","text":"<ul> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>Matlab</li> </ul>"},{"location":"tags/#mobaxterm","title":"MobaXterm","text":"<ul> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> </ul>"},{"location":"tags/#modules","title":"modules","text":"<ul> <li>(2024-08-28) Lab Notebook: Change to modules in SLURM jobs</li> <li>(2024-09-24) Lab Notebook: Ollama Module</li> <li>User created modules</li> </ul>"},{"location":"tags/#molpro","title":"Molpro","text":"<ul> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> </ul>"},{"location":"tags/#mothur","title":"Mothur","text":"<ul> <li>Mothur</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> </ul>"},{"location":"tags/#namd","title":"NAMD","text":"<ul> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> </ul>"},{"location":"tags/#nano","title":"nano","text":"<ul> <li>Editing text with nano</li> </ul>"},{"location":"tags/#new-os","title":"New OS","text":"<ul> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> </ul>"},{"location":"tags/#ondemand","title":"OnDemand","text":"<ul> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> <li>File transfer</li> <li>Quick start - Open OnDemand</li> </ul>"},{"location":"tags/#openbugs","title":"OpenBUGS","text":"<ul> <li>Some packages and other information</li> </ul>"},{"location":"tags/#openvdb","title":"OpenVDB","text":"<ul> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> </ul>"},{"location":"tags/#os-upgrade","title":"OS upgrade","text":"<ul> <li>Accessing nodes running Ubuntu</li> <li>Backwards compatibility scripts</li> <li>Migrating Conda environments</li> <li>Migrating Python virtual environments</li> <li>Migrating R packages</li> <li>Migrating to the new operating system</li> <li>Migrating to the upgraded module system</li> <li>Overview</li> </ul>"},{"location":"tags/#outage","title":"outage","text":"<ul> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> </ul>"},{"location":"tags/#parallel-comuting-toolbox","title":"Parallel Comuting Toolbox","text":"<ul> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> </ul>"},{"location":"tags/#partitions","title":"partitions","text":"<ul> <li>Buy-in and account management</li> <li>SLURM queueing and partitions</li> <li>Scavenger Queue</li> </ul>"},{"location":"tags/#perl","title":"Perl","text":"<ul> <li>Installing Local Perl Modules with CPAN</li> </ul>"},{"location":"tags/#postgresql","title":"PostgreSQL","text":"<ul> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> </ul>"},{"location":"tags/#powertools","title":"powertools","text":"<ul> <li>Powertools <code>longjob</code> by DMTCP</li> </ul>"},{"location":"tags/#pymesh","title":"PyMesh","text":"<ul> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> </ul>"},{"location":"tags/#python","title":"Python","text":"<ul> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>Migrating Conda environments</li> <li>Migrating Python virtual environments</li> <li>Python on HPCC</li> <li>Replacing an existing Conda installation</li> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> <li>Using virtual env</li> <li>Using Conda</li> </ul>"},{"location":"tags/#python_1","title":"python","text":"<ul> <li>(2024-08-28) Lab Notebook: cuQuantum Installation and Usage</li> <li>(2024-09-16) Lab Notebook: PyTorch OnDemand Usage</li> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> <li>(2024-09-25) Lab Notebook: Transitioning from Anaconda to Miniforge</li> </ul>"},{"location":"tags/#pytorch","title":"PyTorch","text":"<ul> <li>(2024-09-16) Lab Notebook: PyTorch OnDemand Usage</li> <li>Installing Pytorch with Anaconda</li> </ul>"},{"location":"tags/#qiime-2","title":"QIIME 2","text":"<ul> <li>QIIME 2</li> </ul>"},{"location":"tags/#queue","title":"queue","text":"<ul> <li>SLURM resource request guide</li> </ul>"},{"location":"tags/#quota","title":"quota","text":"<ul> <li>File count</li> <li>Home space</li> <li>Research space</li> <li>Scratch space</li> </ul>"},{"location":"tags/#r","title":"R","text":"<ul> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>(2024-08-07) Lab Notebook: Running ggplot in R without X11</li> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> <li>Migrating R packages</li> <li>General information</li> <li>Some packages and other information</li> <li>Using BLAS and FlexiBLAS to speed up linear algebra operations</li> </ul>"},{"location":"tags/#rclone","title":"rclone","text":"<ul> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#reference","title":"reference","text":"<ul> <li>Accessing nodes running Ubuntu</li> <li>Cluster resources</li> <li>Compilers and Libraries</li> <li>Compiling for GPUs</li> <li>Conditional statements</li> <li>SLURM - node status and job partition</li> <li>EasyBuild Reference</li> <li>Example job scripts</li> <li>External Resources</li> <li>File count</li> <li>FAQ</li> <li>GPU resources</li> <li>Guidelines for choosing file systems and I/O</li> <li>How jobs are scheduled</li> <li>Searching software modules</li> <li>Classroom support</li> <li>Interactive jobs</li> <li>Linux shells</li> <li>List of job specifications</li> <li>Local file system</li> <li>Loops</li> <li>Quick start - Open OnDemand</li> <li>Regular expressions</li> <li>Requesting GPUs</li> <li>SLURM queueing and partitions</li> <li>SLURM commands</li> <li>Sensitive data storage</li> <li>SLURM environment variables</li> <li>User created modules</li> <li>Using the Data Machine</li> <li>SLURM - buyin information</li> <li>Development nodes</li> <li>Get software usage examples</li> <li>Install an SSH client</li> <li>Job policies</li> <li>SLURM - display job steps and their resource usages</li> <li>Module changelog</li> <li>SLURM - display job list</li> </ul>"},{"location":"tags/#ros","title":"ROS","text":"<ul> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> </ul>"},{"location":"tags/#rstudio","title":"RStudio","text":"<ul> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> </ul>"},{"location":"tags/#rstudio-server","title":"RStudio Server","text":"<ul> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> </ul>"},{"location":"tags/#rsync","title":"rsync","text":"<ul> <li>File transfer</li> <li>Cloud storage file transfer</li> </ul>"},{"location":"tags/#samba","title":"samba","text":"<ul> <li>Mapping drives using Samba</li> </ul>"},{"location":"tags/#scikit-learn","title":"scikit-learn","text":"<ul> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> </ul>"},{"location":"tags/#scp","title":"scp","text":"<ul> <li>File transfer</li> </ul>"},{"location":"tags/#sensitive-data","title":"sensitive data","text":"<ul> <li>Sensitive data storage</li> </ul>"},{"location":"tags/#sftp","title":"sftp","text":"<ul> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> </ul>"},{"location":"tags/#shell","title":"shell","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#singularity","title":"Singularity","text":"<ul> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>AlphaFold via Singularity</li> <li>Overview</li> <li>Singularity Advanced Topics</li> <li>Singularity Introduction</li> </ul>"},{"location":"tags/#slurm","title":"slurm","text":"<ul> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>(2024-08-28) Lab Notebook: Change to modules in SLURM jobs</li> <li>Example job scripts</li> <li>How jobs are scheduled</li> <li>Interactive jobs</li> <li>Writing and submitting job scripts</li> <li>List of job specifications</li> <li>SLURM commands</li> <li>Overview</li> <li>SLURM resource request guide</li> <li>Scavenger Queue</li> <li>Showing job steps</li> <li>SLURM environment variables</li> <li>Migrating from Torque</li> <li>Job policies</li> </ul>"},{"location":"tags/#software","title":"software","text":"<ul> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>Module changelog</li> </ul>"},{"location":"tags/#srun","title":"srun","text":"<ul> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>Showing job steps</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>SSH tunneling to directly access development nodes</li> <li>SSH connection via VS Code</li> <li>Connect to the HPCC</li> <li>Connections to compute nodes</li> <li>SSH keys</li> <li>Install an SSH client</li> </ul>"},{"location":"tags/#sshfs","title":"sshfs","text":"<ul> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>Mapping drives using SSHFS</li> </ul>"},{"location":"tags/#tcsh","title":"tcsh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"tags/#tensorflow","title":"tensorflow","text":"<ul> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> </ul>"},{"location":"tags/#tensorflow_1","title":"TensorFlow","text":"<ul> <li>Installing TensorFlow using anaconda</li> <li>Installing TF using conda</li> <li>Some packages and other information</li> <li>TF GPU usage</li> <li>Tensorflow Model Training Code Examples</li> </ul>"},{"location":"tags/#text-editors","title":"text editors","text":"<ul> <li>Editing text with nano</li> </ul>"},{"location":"tags/#torque","title":"torque","text":"<ul> <li>Migrating from Torque</li> </ul>"},{"location":"tags/#tractseg","title":"TractSeg","text":"<ul> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> </ul>"},{"location":"tags/#training","title":"training","text":"<ul> <li>External Resources</li> </ul>"},{"location":"tags/#tutorial","title":"tutorial","text":"<ul> <li>AlphaFold on HPCC</li> <li>AlphaFold via Singularity</li> <li>Checkpoint with DMTCP</li> <li>Data Display Debugger</li> <li>Docker</li> <li>EasyBuild Tutorial</li> <li>Expansion</li> <li>Installing Local Perl Modules with CPAN</li> <li>Introduction to the module system</li> <li>Writing and submitting job scripts</li> <li>Linux Command Line for Beginners I</li> <li>Linux Command Line for Beginners II</li> <li>Mapping drives using SSHFS</li> <li>Mapping drives using Samba</li> <li>Matlab</li> <li>Mothur</li> <li>Powertools <code>longjob</code> by DMTCP</li> <li>QIIME 2</li> <li>General information</li> <li>Some packages and other information</li> <li>Replacing an existing Conda installation</li> <li>Showing job steps</li> <li>Singularity Introduction</li> <li>Using virtual env</li> <li>Using Conda</li> <li>Variables I</li> <li>Variables II</li> <li>make</li> </ul>"},{"location":"tags/#vasp","title":"VASP","text":"<ul> <li>(2023-03-14) Lab Notebook: VASP</li> </ul>"},{"location":"tags/#virtual-environment","title":"virtual environment","text":"<ul> <li>Migrating Python virtual environments</li> <li>Using virtual env</li> </ul>"},{"location":"tags/#visit","title":"VisIt","text":"<ul> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> </ul>"},{"location":"tags/#vs-code","title":"VS Code","text":"<ul> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2024-09-05) Lab Notebook: Setting up a terminal with VS Code to connect to the HPCC</li> <li>SSH connection via VS Code</li> <li>Jupyter Notebooks in VS Code</li> </ul>"},{"location":"tags/#vscode","title":"VScode","text":"<ul> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2024-09-05) Lab Notebook: Setting up a terminal with VS Code to connect to the HPCC</li> <li>SSH connection via VS Code</li> <li>Jupyter Notebooks in VS Code</li> </ul>"},{"location":"tags/#windows","title":"Windows","text":"<ul> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> </ul>"},{"location":"tags/#wrf","title":"WRF","text":"<ul> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> </ul>"},{"location":"tags/#x11","title":"X11","text":"<ul> <li>(2024-08-07) Lab Notebook: Running ggplot in R without X11</li> </ul>"},{"location":"tags/#zsh","title":"zsh","text":"<ul> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> </ul>"},{"location":"Lab_Notebooks/","title":"Lab Notebooks","text":"<p>Warning</p> <p>The following Lab Notebooks are intended as a record of how particular problems were solved at a particular time and not updated or maintained in any way to reflect current system setting or versions of installed software. If you are having a problem with software/topic addressed by one of these notebooks, they may provide a solution or at least a starting point, but there is no guarantee that worked here will work again.</p>","tags":["lab notebook"]},{"location":"Lab_Notebooks/#notebooks","title":"Notebooks","text":"","tags":["lab notebook"]},{"location":"Lab_Notebooks/#lab-notebook","title":"lab notebook","text":"<ul> <li>(2022-06-27) Lab Notebook: AntiSMASH installation with Conda</li> <li>(2022-09-20) Lab Notebook: PyMesh installation with Conda</li> <li>(2022-09-21) Lab Notebook: VisIt on HPCC</li> <li>(2022-09-23) Lab Notebook: Singularity --- Using an alternate OS</li> <li>(2022-09-24) Lab Notebook: Singularity Overlays</li> <li>(2022-09-28) Lab Notebook: HPCC via RStudio and GitHub</li> <li>(2022-09-30) Lab Notebook: ROS on HPCC</li> <li>(2022-10-03) Lab Notebook: Bactopia installaion with Conda and Mamba</li> <li>(2022-10-04) Lab Notebook: OnDemand --- Unable to click icons on the Interactive Desktop</li> <li>(2022-10-08) Lab Notebook: Praat installation and execution using Singularity Overlays on CentOS8</li> <li>(2022-10-10) Lab Notebook: OnDemand Interactive Desktop Error</li> <li>(2022-10-19) Lab Notebook: OpenVDB installaion with Conda</li> <li>(2022-10-19) Lab Notebook: TractSeg installation with Conda</li> <li>(2022-10-27) Lab Notebook: SFTP Mapping on HPCC File Systems</li> <li>(2022-12-07) Lab Notebook: OnDemand --- Bad Request from browser</li> <li>(2022-12-20) Lab Notebook: Conda --- Cleanning out your cache on HPCC</li> <li>(2022-12-20) Lab Notebook: SSHFS mapping on Windows</li> <li>(2023-02-07) Lab Notebook:  NAMD --- Running on Multile Nodes</li> <li>(2023-02-07) Lab Notebook: WRF installation with EasyBuild</li> <li>(2023-02-09) Lab Notebooks: Rclone --- Example of copying folder from HPCC to MSU OneDrive</li> <li>(2023-02-16) Lab Notebook: CryoSPARC Installation on HPCC</li> <li>(2023-02-27) Lab Notebook: HRLDAS</li> <li>(2023-03-14) Lab Notebook: CRYSTAL23 --- Parallel submission script on the MSU HPCC</li> <li>(2023-03-14) Lab Notebook: VASP</li> <li>(2023-06-30) Lab Notebooks: Jupyter --- Port-Forwarding Servers</li> <li>(2023-03-27) Lab Notebook: Mathematica --- Fixing Startup Errors</li> <li>(2023-04-04) Lab Notebook: OnDemand RunningMPIJobs</li> <li>(2023-05-04) Lab Notebook: Changes to srun for jobs with multiple CPUs per task</li> <li>(2023-05-18) Lab Notebook: EasyBuild PostgreSQL Installation</li> <li>(2023-07-13) Lab Notebook: Ignoring Local Python Libraries</li> <li>(2023-07-18) Lab Notebook: AlphaFold 2.3.1 DEPRECATED</li> <li>(2023-07-18) Lab Notebook: MATLAB offload from localPC to HPCC</li> <li>(2023-08-02) Lab Notebook: Alternative Linux Shells on HPCC</li> <li>(2023-08-11) Lab Notebooks: Jupyter and MobaXterm--- Port-Forwarding Servers</li> <li>(2023-08-30) Lab Notebooks: Incident Report on System Slowdown due to I/O</li> <li>(2023-09-21) Lab Notebooks: Example home directory software install (with links to videos)</li> <li>(2023-11-03) Lab Notebook: Connecting to a Singularity container with VS Code</li> <li>(2023-11-09) Lab Notebook: Molpro Parallel</li> <li>(2023-11-10) Lab Notebook: AlphaFold 2.3.2 WorkInProgress</li> <li>(2023-11-21) Lab Notebook: Git in Research Spaces</li> <li>(2024-01-13) Lab Notebook: Sharing directories with other HPCC users</li> <li>(2024-06-13) Lab Notebook: CTSM Installation</li> <li>(2024-06-19) Lab Notebook: LM Studio Installation</li> <li>(2024-07-01) Lab Notebook: NewOS and OnDemand Workarounds</li> <li>(2024-08-07) Lab Notebook: Running ggplot in R without X11</li> <li>(2024-08-07) Lab Notebook: RStudio Server Shows Blank or Gray Screen</li> <li>(2024-08-28) Lab Notebook: Change to modules in SLURM jobs</li> <li>(2024-08-28) Lab Notebook: cuQuantum Installation and Usage</li> <li>(2024-09-05) Lab Notebook: Setting up a terminal with VS Code to connect to the HPCC</li> <li>(2024-09-16) Lab Notebook: PyTorch OnDemand Usage</li> <li>(2024-09-17) Lab Notebook: Common Machine Learning Tools on OnDemand</li> <li>(2024-09-24) Lab Notebook: Ollama Module</li> <li>(2024-09-25) Lab Notebook: Transitioning from Anaconda to Miniforge</li> <li>(2024-10-01) Lab Notebook: New ICER User Database Testing Requested (ColdFront)</li> </ul>","tags":["lab notebook"]}]}